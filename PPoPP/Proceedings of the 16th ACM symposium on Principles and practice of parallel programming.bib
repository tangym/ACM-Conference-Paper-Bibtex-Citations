@article{Feng:2011:SSP:2038037.1941564,
 abstract = {In this paper we present an approach to parallel programming called SpiceC. SpiceC simplifies the task of parallel programming through a combination of an intuitive computation model and SpiceC directives. The SpiceC parallel computation model consists of multiple threads where every thread has a private space for data and all threads share data via a shared space. Each thread performs computations using its private space thus offering isolation which allows for speculative computations. SpiceC provides easy to use SpiceC compiler directives using which the programmers can express different forms of parallelism. It allows developers to express high level constraints on data transfers between spaces while the tedious task of generating the code for the data transfers is performed by the compiler. SpiceC also supports data transfers involving dynamic data structures without help from developers. SpiceC allows developers to create clusters of data to enable parallel data transfers. SpiceC programs are portable across modern chip multiprocessor based machines that may or may not support cache coherence. We have developed implementations of SpiceC for shared memory systems with and without cache coherence. We evaluate our implementation using seven benchmarks of which four are parallelized speculatively. Our compiler generated implementations achieve speedups ranging from 2x to 18x on a 24 core system.},
 acmid = {1941564},
 address = {New York, NY, USA},
 author = {Feng, Min and Gupta, Rajiv and Hu, Yi},
 doi = {10.1145/2038037.1941564},
 issn = {0362-1340},
 issue_date = {August 2011},
 journal = {SIGPLAN Not.},
 keyword = {compiler directives, multicores, speculation},
 link = {http://doi.acm.org/10.1145/2038037.1941564},
 month = {feb},
 number = {8},
 numpages = {12},
 pages = {69--80},
 publisher = {ACM},
 title = {SpiceC: Scalable Parallelism via Implicit Copying and Explicit Commit},
 volume = {46},
 year = {2011}
}


@inproceedings{Grosset:2011:EGC:1941553.1941597,
 abstract = {This paper evaluates features of graph coloring algorithms implemented on graphics processing units (GPUs), comparing coloring heuristics and thread decompositions. As compared to prior work on graph coloring for other parallel architectures, we find that the large number of cores and relatively high global memory bandwidth of a GPU lead to different strategies for the parallel implementation. Specifically, we find that a simple uniform block partitioning is very effective on GPUs and our parallel coloring heuristics lead to the same or fewer colors than prior approaches for distributed-memory cluster architecture. Our algorithm resolves many coloring conflicts across partitioned blocks on the GPU by iterating through the coloring process, before returning to the CPU to resolve remaining conflicts. With this approach we get as few color (if not fewer) than the best sequential graph coloring algorithm and performance is close to the fastest sequential graph coloring algorithms which have poor color quality.},
 acmid = {1941597},
 address = {New York, NY, USA},
 author = {Grosset, Andre Vincent Pascal and Zhu, Peihong and Liu, Shusen and Venkatasubramanian, Suresh and Hall, Mary},
 booktitle = {Proceedings of the 16th ACM Symposium on Principles and Practice of Parallel Programming},
 doi = {10.1145/1941553.1941597},
 isbn = {978-1-4503-0119-0},
 keyword = {cuda, gpu, graph coloring, parallel algorithm},
 link = {http://doi.acm.org/10.1145/1941553.1941597},
 location = {San Antonio, TX, USA},
 numpages = {2},
 pages = {297--298},
 publisher = {ACM},
 series = {PPoPP '11},
 title = {Evaluating Graph Coloring on GPUs},
 year = {2011}
}


@article{Chafi:2011:DAH:2038037.1941561,
 abstract = {Exploiting heterogeneous parallel hardware currently requires mapping application code to multiple disparate programming models. Unfortunately, general-purpose programming models available today can yield high performance but are too low-level to be accessible to the average programmer. We propose leveraging domain-specific languages (DSLs) to map high-level application code to heterogeneous devices. To demonstrate the potential of this approach we present OptiML, a DSL for machine learning. OptiML programs are implicitly parallel and can achieve high performance on heterogeneous hardware with no modification required to the source code. For such a DSL-based approach to be tractable at large scales, better tools are required for DSL authors to simplify language creation and parallelization. To address this concern, we introduce Delite, a system designed specifically for DSLs that is both a framework for creating an implicitly parallel DSL as well as a dynamic runtime providing automated targeting to heterogeneous parallel hardware. We show that OptiML running on Delite achieves single-threaded, parallel, and GPU performance superior to explicitly parallelized MATLAB code in nearly all cases.},
 acmid = {1941561},
 address = {New York, NY, USA},
 author = {Chafi, Hassan and Sujeeth, Arvind K. and Brown, Kevin J. and Lee, HyoukJoong and Atreya, Anand R. and Olukotun, Kunle},
 doi = {10.1145/2038037.1941561},
 issn = {0362-1340},
 issue_date = {August 2011},
 journal = {SIGPLAN Not.},
 keyword = {domain-specific languages, dynamic optimizations, parallel programming, runtimes},
 link = {http://doi.acm.org/10.1145/2038037.1941561},
 month = {feb},
 number = {8},
 numpages = {12},
 pages = {35--46},
 publisher = {ACM},
 title = {A Domain-specific Approach to Heterogeneous Parallelism},
 volume = {46},
 year = {2011}
}


@article{Kourtis:2011:CEC:2038037.1941587,
 abstract = {The Sparse Matrix-Vector multiplication (SpMV) kernel scales poorly on shared memory systems with multiple processing units due to the streaming nature of its data access pattern. Previous research has demonstrated that an effective strategy to improve the kernel's performance is to drastically reduce the data volume involved in the computations. Since the storage formats for sparse matrices include metadata describing the structure of non-zero elements within the matrix, we propose a generalized approach to compress metadata by exploiting substructures within the matrix. We call the proposed storage format Compressed Sparse eXtended (CSX). In our implementation we employ runtime code generation to construct specialized SpMV routines for each matrix. Experimental evaluation on two shared memory systems for 15 sparse matrices demonstrates significant performance gains as the number of participating cores increases. Regarding the cost of CSX construction, we propose several strategies which trade performance for preprocessing cost making CSX applicable both to online and offline preprocessing.},
 acmid = {1941587},
 address = {New York, NY, USA},
 author = {Kourtis, Kornilios and Karakasis, Vasileios and Goumas, Georgios and Koziris, Nectarios},
 doi = {10.1145/2038037.1941587},
 issn = {0362-1340},
 issue_date = {August 2011},
 journal = {SIGPLAN Not.},
 keyword = {compression, shared memory, smp, sparse matrix-vector multiplication, spmv},
 link = {http://doi.acm.org/10.1145/2038037.1941587},
 month = {feb},
 number = {8},
 numpages = {10},
 pages = {247--256},
 publisher = {ACM},
 title = {CSX: An Extended Compression Format for Spmv on Shared Memory Systems},
 volume = {46},
 year = {2011}
}


@article{Wu:2011:STC:2038037.1941569,
 abstract = {Performance modeling for scientific applications is important for assessing potential application performance and systems procurement in high-performance computing (HPC). Recent progress on communication tracing opens up novel opportunities for communication modeling due to its lossless yet scalable trace collection. Estimating the impact of scaling on communication efficiency still remains non-trivial due to execution-time variations and exposure to hardware and software artifacts. This work contributes a fundamentally novel modeling scheme. We synthetically generate the application trace for large numbers of nodes by extrapolation from a set of smaller traces. We devise an innovative approach for topology extrapolation of single program, multiple data (SPMD) codes with stencil or mesh communication. The extrapolated trace can subsequently be (a) replayed to assess communication requirements before porting an application, (b) transformed to auto-generate communication benchmarks for various target platforms, and (c) analyzed to detect communication inefficiencies and scalability limitations. To the best of our knowledge, rapidly obtaining the communication behavior of parallel applications at arbitrary scale with the availability of timed replay, yet without actual execution of the application at this scale is without precedence and has the potential to enable otherwise infeasible system simulation at the exascale level.},
 acmid = {1941569},
 address = {New York, NY, USA},
 author = {Wu, Xing and Mueller, Frank},
 doi = {10.1145/2038037.1941569},
 issn = {0362-1340},
 issue_date = {August 2011},
 journal = {SIGPLAN Not.},
 keyword = {high-performance computing, message passing, performance prediction, tracing},
 link = {http://doi.acm.org/10.1145/2038037.1941569},
 month = {feb},
 number = {8},
 numpages = {10},
 pages = {113--122},
 publisher = {ACM},
 title = {ScalaExtrap: Trace-based Communication Extrapolation for Spmd Programs},
 volume = {46},
 year = {2011}
}


@article{Kim:2011:ASC:2038037.1941591,
 abstract = {In this paper, we propose an OpenCL framework that combines multiple GPUs and treats them as a single compute device. Providing a single virtual compute device image to the user makes an OpenCL application written for a single GPU portable to the platform that has multiple GPU devices. It also makes the application exploit full computing power of the multiple GPU devices and the total amount of GPU memories available in the platform. Our OpenCL framework automatically distributes at run-time the OpenCL kernel written for a single GPU into multiple CUDA kernels that execute on the multiple GPU devices. It applies a run-time memory access range analysis to the kernel by performing a sampling run and identifies an optimal workload distribution for the kernel. To achieve a single compute device image, the runtime maintains virtual device memory that is allocated in the main memory. The OpenCL runtime treats the memory as if it were the memory of a single GPU device and keeps it consistent to the memories of the multiple GPU devices. Our OpenCL-C-to-C translator generates the sampling code from the OpenCL kernel code and OpenCL-C-to-CUDA-C translator generates the CUDA kernel code for the distributed OpenCL kernel. We show the effectiveness of our OpenCL framework by implementing the OpenCL runtime and two source-to-source translators. We evaluate its performance with a system that contains 8 GPUs using 11 OpenCL benchmark applications.},
 acmid = {1941591},
 address = {New York, NY, USA},
 author = {Kim, Jungwon and Kim, Honggyu and Lee, Joo Hwan and Lee, Jaejin},
 doi = {10.1145/2038037.1941591},
 issn = {0362-1340},
 issue_date = {August 2011},
 journal = {SIGPLAN Not.},
 keyword = {access range analysis, compilers, opencl, runtime, virtual device memory, workload distribution},
 link = {http://doi.acm.org/10.1145/2038037.1941591},
 month = {feb},
 number = {8},
 numpages = {12},
 pages = {277--288},
 publisher = {ACM},
 title = {Achieving a Single Compute Device Image in OpenCL for Multiple GPUs},
 volume = {46},
 year = {2011}
}


@article{Prabhakar:2011:QAS:2038037.1941593,
 abstract = {In this paper, we propose a novel two-step approach to the management of the storage caches to provide predictable performance in multi-server storage architectures: (1) An adaptive QoS decomposition and optimization step uses max-flow algorithm to determine the best decomposition of application-level QoS to sub-QoSs such that the application performance is optimized, and (2) A storage cache allocation step uses feedback control theory to allocate shared storage cache space such that the specified QoSs are satisfied throughout the execution.},
 acmid = {1941593},
 address = {New York, NY, USA},
 author = {Prabhakar, Ramya and Srikantaiah, Shekhar and Garg, Rajat and Kandemir, Mahmut},
 doi = {10.1145/2038037.1941593},
 issn = {0362-1340},
 issue_date = {August 2011},
 journal = {SIGPLAN Not.},
 keyword = {i/o performance, multi-server, qos, storage cache},
 link = {http://doi.acm.org/10.1145/2038037.1941593},
 month = {feb},
 number = {8},
 numpages = {2},
 pages = {289--290},
 publisher = {ACM},
 title = {QoS Aware Storage Cache Management in Multi-server Environments},
 volume = {46},
 year = {2011}
}


@inproceedings{Jeon:2011:KLG:1941553.1941595,
 abstract = {This paper overviews Kremlin, a software profiling tool designed to assist the parallelization of serial programs. Kremlin accepts a serial source code, profiles it, and provides a list of regions that should be considered in parallelization. Unlike a typical profiler, Kremlin profiles not only work but also parallelism, which is accomplished via a novel technique called hierarchical critical path analysis. Our evaluation demonstrates that Kremlin is highly effective, resulting in a parallelized program whose performance sometimes outperforms, and is mostly comparable to, manual parallelization. At the same time, Kremlin would require that the user parallelize significantly fewer regions of the program. Finally, a user study suggests Kremlin is effective in improving the productivity of programmers.},
 acmid = {1941595},
 address = {New York, NY, USA},
 author = {Jeon, Donghwan and Garcia, Saturnino and Louie, Chris and Kota Venkata, Sravanthi and Taylor, Michael Bedford},
 booktitle = {Proceedings of the 16th ACM Symposium on Principles and Practice of Parallel Programming},
 doi = {10.1145/1941553.1941595},
 isbn = {978-1-4503-0119-0},
 keyword = {parallel programming, parallelism, profiler, software tools},
 link = {http://doi.acm.org/10.1145/1941553.1941595},
 location = {San Antonio, TX, USA},
 numpages = {2},
 pages = {293--294},
 publisher = {ACM},
 series = {PPoPP '11},
 title = {Kremlin: Like Gprof, but for Parallelization},
 year = {2011}
}


@article{Wang:2011:CSP:2038037.1941583,
 abstract = {This paper presents the open-source COREMU, a scalable and portable parallel emulation framework that decouples the complexity of parallelizing full-system emulators from building a mature sequential one. The key observation is that CPU cores and devices in current (and likely future) multiprocessors are loosely-coupled and communicate through well-defined interfaces. Based on this observation, COREMU emulates multiple cores by creating multiple instances of existing sequential emulators, and uses a thin library layer to handle the inter-core and device communication and synchronization, to maintain a consistent view of system resources. COREMU also incorporates lightweight memory transactions, feedback-directed scheduling, lazy code invalidation and adaptive signal control to provide scalable performance. To make COREMU useful in practice, we also provide some preliminary tools and APIs that can help programmers to diagnose performance problems and (concurrency) bugs. A working prototype, which reuses the widely-used QEMU as the sequential emulator, is with only 2500 lines of code (LOCs) changes to QEMU. It currently supports x64 and ARM platforms, and can emulates up to 255 cores running commodity OSes with practical performance, while QEMU cannot scale above 32 cores. A set of performance evaluation against QEMU indicates that, COREMU has negligible uniprocessor emulation overhead, performs and scales significantly better than QEMU. We also show how COREMU could be used to diagnose performance problems and concurrency bugs of both OS kernel and parallel applications.},
 acmid = {1941583},
 address = {New York, NY, USA},
 author = {Wang, Zhaoguo and Liu, Ran and Chen, Yufei and Wu, Xi and Chen, Haibo and Zhang, Weihua and Zang, Binyu},
 doi = {10.1145/2038037.1941583},
 issn = {0362-1340},
 issue_date = {August 2011},
 journal = {SIGPLAN Not.},
 keyword = {full-system emulator, multicore, parallel emulator},
 link = {http://doi.acm.org/10.1145/2038037.1941583},
 month = {feb},
 number = {8},
 numpages = {10},
 pages = {213--222},
 publisher = {ACM},
 title = {COREMU: A Scalable and Portable Parallel Full-system Emulator},
 volume = {46},
 year = {2011}
}


@inproceedings{Hassaan:2011:OVU:1941553.1941557,
 abstract = {Outside of computational science, most problems are formulated in terms of irregular data structures such as graphs, trees and sets. Unfortunately, we understand relatively little about the structure of parallelism and locality in irregular algorithms. In this paper, we study multiple algorithms for four such problems: discrete-event simulation, single-source shortest path, breadth-first search, and minimal spanning trees. We show that the algorithms can be classified into two categories that we call unordered and ordered, and demonstrate experimentally that there is a trade-off between parallelism and work efficiency: unordered algorithms usually have more parallelism than their ordered counterparts for the same problem, but they may also perform more work. Nevertheless, our experimental results show that unordered algorithms typically lead to more scalable implementations, demonstrating that less work-efficient irregular algorithms may be better for parallel execution.},
 acmid = {1941557},
 address = {New York, NY, USA},
 author = {Hassaan, Muhammad Amber and Burtscher, Martin and Pingali, Keshav},
 booktitle = {Proceedings of the 16th ACM Symposium on Principles and Practice of Parallel Programming},
 doi = {10.1145/1941553.1941557},
 isbn = {978-1-4503-0119-0},
 keyword = {amorphous data-parallelism, discrete-event simulation, galois system, minimal spanning tree, multicore processors, parallel breadth first search, single-source shortest path},
 link = {http://doi.acm.org/10.1145/1941553.1941557},
 location = {San Antonio, TX, USA},
 numpages = {10},
 pages = {3--12},
 publisher = {ACM},
 series = {PPoPP '11},
 title = {Ordered vs. Unordered: A Comparison of Parallelism and Work-efficiency in Irregular Algorithms},
 year = {2011}
}


@article{Negara:2011:IOT:2038037.1941566,
 abstract = {One of the more popular paradigms for concurrent programming is the Actor model of message passing; it has been adopted in one form or another by a number of languages and frameworks. By avoiding a shared local state and instead relying on message passing, the Actor model facilitates modular programming. An important challenge for message passing languages is to transmit messages efficiently. This requires retaining the pass-by-value semantics of messages while avoiding making a deep copy on sequential or shared memory multicore processors. A key observation is that many messages have an ownership transfer semantics; such messages can be sent efficiently using pointers without introducing shared state between concurrent objects. We propose a conservative static analysis algorithm which infers if the content of a message is compatible with an ownership transfer semantics. Our tool, called SOTER (for Safe Ownership Transfer enablER) transforms the program to avoid the cost of copying the contents of a message whenever it can infer the content obeys the ownership transfer semantics. Experiments using a range of programs suggest that our conservative static analysis method is usually able to infer ownership transfer. Performance results demonstrate that the transformed programs execute up to an order of magnitude faster than the original programs.},
 acmid = {1941566},
 address = {New York, NY, USA},
 author = {Negara, Stas and Karmani, Rajesh K. and Agha, Gul},
 doi = {10.1145/2038037.1941566},
 issn = {0362-1340},
 issue_date = {August 2011},
 journal = {SIGPLAN Not.},
 keyword = {actors, message passing, ownership transfer, static analysis},
 link = {http://doi.acm.org/10.1145/2038037.1941566},
 month = {feb},
 number = {8},
 numpages = {10},
 pages = {81--90},
 publisher = {ACM},
 title = {Inferring Ownership Transfer for Efficient Message Passing},
 volume = {46},
 year = {2011}
}


@proceedings{Govindarajan:2010:1693453,
 abstract = {Together with the program committee, it is my great pleasure to welcome you to the 15th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming -- PPoPP '10. This year's symposium continues its tradition of being the premier forum for presentation of research in all aspects of parallel software, including theoretical foundations, programming models, algorithms, applications, and systems software. This research area has seen dramatic growth in recent years as homogeneous and heterogeneous multi-core microprocessors have become mainstream. Therefore making parallel programming more accessible has become essential to the very future of the computing industry. This dramatic shift in importance of the area was reflected in a record number of 173 submissions from Asia, Europe, Australia and New Zealand, and North and South America, and more than half of these were from outside North America. The program committee accepted 29 papers, covering a broad range of topics from multi-core computing up to scalable high-end computing, parallel algorithms, synchronization and transactional memory. This year saw an increased emphasis on accelerators such as graphics processors, and on tools to support parallel programming such as performance modeling and debugging. Our program also includes 17 poster presentations in similar areas. In addition to papers and posters, we are pleased to include keynote addresses by Tilak Agerwala from IBM on computing at exascale and Arvind from MIT on innovation in computer architecture, and a panel on extreme-scale computing. To manage the unexpected reviewing load, PPoPP introduced a two-phase review process. Each paper received two program committee reviews in the first phase, and papers with at least one rating above strong reject received an additional one to three reviews, how many depending on existing reviews and the need for more information in the decision process. The program committee meeting was held on September 14, 2009, on the University of Utah campus in Salt Lake City, Utah.},
 address = {New York, NY, USA},
 isbn = {978-1-60558-877-3},
 location = {Bangalore, India},
 note = {551102},
 publisher = {ACM},
 title = {PPoPP '10: Proceedings of the 15th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
 year = {2010}
}


@proceedings{Ramanujam:2012:2145816,
 abstract = {It is our great pleasure to welcome you to the 17th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming (PPoPP'12). PPoPP continues its tradition of serving as a leading forum for research in all aspects of parallel software, including theoretical foundations, programming models, algorithms, applications, and systems software. With the ubiquity of parallelism in commodity processors and the increasing use of GPUs for high-performance computing, the effective use of parallel systems is being recognized as one of the most challenging problems faced today. PPoPP'12 received 173 complete paper submissions. In addition to the 25 program committee members, 75 members of the external review committee provided reviews for the papers. Two rounds of reviewing were conducted, with at least three reviews being obtained in the first round, with additional reviews being obtained in the second round for papers where needed. After extensive discussions at an in-person two-day program committee meeting in November 2011, 26 full papers were selected for presentation at the conference. PPoPP'12 continues the tradition of poster presentations of high quality submissions that could not be accepted as full papers. This year's conference features 32 poster presentations over two sessions. PPoPP'12 is again co-located this year with the International Symposium on High-Performance Computer Architecture (HPCA), allowing attendees of one conference the option of attending talks at the other. We feature two joint HPCA/PPOPP keynote presentations. Sanjeev Kumar from Facebook will present a keynote on "Social Networking at Scale," and Keshav Pingali from the University of Texas at Austin will present a keynote entitled "Parallel Programming Needs Data-Centric Foundations."},
 address = {New York, NY, USA},
 isbn = {978-1-4503-1160-1},
 location = {New Orleans, Louisiana, USA},
 note = {551121},
 publisher = {ACM},
 title = {PPoPP '12: Proceedings of the 17th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
 year = {2012}
}


@article{Xiang:2011:APC:2038037.1941567,
 abstract = {As multi-core processors become commonplace and cloud computing is gaining acceptance, more applications are run in a shared cache environment. Cache sharing depends on a concept called footprint, which depends on all cache accesses not just cache misses. Previous work has recognized the importance of footprint but has not provided a method for accurate measurement, mainly because the complete measurement requires counting data access in all execution windows, which takes time quadratic in the length of a trace. The paper first presents an algorithm efficient enough for off-line use to approximately measure the footprint with a guaranteed precision. The cost of the analysis can be adjusted by changing the precision. Then the paper presents a composable model. For a set of programs, the model uses the all-window footprint of each program to predict its cache interference with other programs without running these programs together. The paper evaluates the efficiency of all-window profiling using the SPEC 2000 benchmarks and compares the footprint interference model with a miss-rate based model and with exhaustive testing.},
 acmid = {1941567},
 address = {New York, NY, USA},
 author = {Xiang, Xiaoya and Bao, Bin and Bai, Tongxin and Ding, Chen and Chilimbi, Trishul},
 doi = {10.1145/2038037.1941567},
 issn = {0362-1340},
 issue_date = {August 2011},
 journal = {SIGPLAN Not.},
 keyword = {cache interference, composable models, data footprint, reuse distance},
 link = {http://doi.acm.org/10.1145/2038037.1941567},
 month = {feb},
 number = {8},
 numpages = {12},
 pages = {91--102},
 publisher = {ACM},
 title = {All-window Profiling and Composable Models of Cache Sharing},
 volume = {46},
 year = {2011}
}


@article{Bauer:2011:PMH:2038037.1941558,
 abstract = {We describe two novel constructs for programming parallel machines with multi-level memory hierarchies: call-up, which allows a child task to invoke computation on its parent, and spawn, which spawns a dynamically determined number of parallel children until some termination condition in the parent is met. Together we show that these constructs allow applications with irregular parallelism to be programmed in a straightforward manner, and furthermore these constructs complement and can be combined with constructs for expressing regular parallelism. We have implemented spawn and call-up in Sequoia and we present an experimental evaluation on a number of irregular applications.},
 acmid = {1941558},
 address = {New York, NY, USA},
 author = {Bauer, Michael and Clark, John and Schkufza, Eric and Aiken, Alex},
 doi = {10.1145/2038037.1941558},
 issn = {0362-1340},
 issue_date = {August 2011},
 journal = {SIGPLAN Not.},
 keyword = {hierarchical memory, locality, parallelism, sequoia},
 link = {http://doi.acm.org/10.1145/2038037.1941558},
 month = {feb},
 number = {8},
 numpages = {12},
 pages = {13--24},
 publisher = {ACM},
 title = {Programming the Memory Hierarchy Revisited: Supporting Irregular Parallelism in Sequoia},
 volume = {46},
 year = {2011}
}


@inproceedings{Lesani:2011:CMT:1941553.1941577,
 abstract = {Many concurrent programming models enable both transactional memory and message passing. For such models, researchers have built increasingly efficient implementations and defined reasonable correctness criteria, while it remains an open problem to obtain the best of both worlds. We present a programming model that is the first to have opaque transactions, safe asynchronous message passing, and an efficient implementation. Our semantics uses tentative message passing and keeps track of dependencies to enable undo of message passing in case a transaction aborts. We can program communication idioms such as barrier and rendezvous that do not deadlock when used in an atomic block. Our experiments show that our model adds little overhead to pure transactions, and that it is significantly more efficient than Transactional Events. We use a novel definition of safe message passing that may be of independent interest.},
 acmid = {1941577},
 address = {New York, NY, USA},
 author = {Lesani, Mohsen and Palsberg, Jens},
 booktitle = {Proceedings of the 16th ACM Symposium on Principles and Practice of Parallel Programming},
 doi = {10.1145/1941553.1941577},
 isbn = {978-1-4503-0119-0},
 keyword = {actor, transactional memory},
 link = {http://doi.acm.org/10.1145/1941553.1941577},
 location = {San Antonio, TX, USA},
 numpages = {12},
 pages = {157--168},
 publisher = {ACM},
 series = {PPoPP '11},
 title = {Communicating Memory Transactions},
 year = {2011}
}


@inproceedings{Ding:2011:UUF:1941553.1941568,
 abstract = {Scientific applications face serious performance challenges on multicore processors, one of which is caused by access contention in last level shared caches from multiple running threads. The contention increases the number of long latency memory accesses, and consequently increases application execution times. Optimizing shared cache performance is critical to reduce significantly execution times of multi-threaded programs on multicores. However, there are two unique problems to be solved before implementing cache optimization techniques on multicores at the user level. First, available cache space for each running thread in a last level cache is difficult to predict due to access contention in the shared space, which makes cache conscious algorithms for single cores ineffective on multicores. Second, at the user level, programmers are not able to allocate cache space at will to running threads in the shared cache, thus data sets with strong locality may not be allocated with sufficient cache space, and cache pollution can easily happen. To address these two critical issues, we have designed ULCC (User Level Cache Control), a software runtime library that enables programmers to explicitly manage and optimize last level cache usage by allocating proper cache space for different data sets of different threads. We have implemented ULCC at the user level based on a page-coloring technique for last level cache usage management. By means of multiple case studies on an Intel multicore processor, we show that with ULCC, scientific applications can achieve significant performance improvements by fully exploiting the benefit of cache optimization algorithms and by partitioning the cache space accordingly to protect frequently reused data sets and to avoid cache pollution. Our experiments with various applications show that ULCC can significantly improve application performance by nearly 40%.},
 acmid = {1941568},
 address = {New York, NY, USA},
 author = {Ding, Xiaoning and Wang, Kaibo and Zhang, Xiaodong},
 booktitle = {Proceedings of the 16th ACM Symposium on Principles and Practice of Parallel Programming},
 doi = {10.1145/1941553.1941568},
 isbn = {978-1-4503-0119-0},
 keyword = {cache, multicore, scientific computing},
 link = {http://doi.acm.org/10.1145/1941553.1941568},
 location = {San Antonio, TX, USA},
 numpages = {10},
 pages = {103--112},
 publisher = {ACM},
 series = {PPoPP '11},
 title = {ULCC: A User-level Facility for Optimizing Shared Cache Performance on Multicores},
 year = {2011}
}


@proceedings{Cascaval:2011:1941553,
 abstract = {It is our great pleasure to welcome you to the 16th ACM Symposium on Principles and Practice of Parallel Programming -- PPoPP'11. PPoPP continues to be the premiere forum where researchers present their work on all aspects of parallelism and concurrency: algorithms and applications, programming models, languages, and environments, system software and runtime systems, and theoretical foundational work. As our industry continues to move toward parallel systems, from large-scale supercomputers to multicore mobile devices, such as smart phones and tablets, research work on concurrency is needed to support developers at all levels of the execution stack. This year we received 165 completed submissions, close to the conference record high. Because of the large number of submissions, in addition to the 25 program committee members, we formed an external review committee and invited 30 experts in various areas to help out in reviewing papers. In addition, committee members also invited external reviewers, to provide our submitting authors with a total of more than 630 reviews. There was a paper bidding process to match up the expertise of the reviewers and the reviewed papers before papers were assigned to reviewers. An on-line discussion period was conducted among all reviewers of each paper to smooth out the differences among the reviewers before the program committee meeting was held. For the final program, the program committee selected 26 full papers and 13 posters for the program. They span a wide spectrum of areas in parallel programming. A few years back, the PPoPP Steering Committee recognized the importance of broadening the conference experience through interaction with hardware architects designing parallel systems. This year we continue the collocation with the International Symposium on High-Performance Computer Architecture. Beside encouraging cross-participation in any of the two conference sessions, PPoPP and HPCA will share two keynotes from leading researchers in the area of parallel programming: Jim Larus (Microsoft Research) and Kathryn McKinley (UT Austin), and all the tutorials and workshops.},
 address = {New York, NY, USA},
 isbn = {978-1-4503-0119-0},
 location = {San Antonio, TX, USA},
 note = {551111},
 publisher = {ACM},
 title = {PPoPP '11: Proceedings of the 16th ACM Symposium on Principles and Practice of Parallel Programming},
 year = {2011}
}


@article{Saraswat:2011:LGL:2038037.1941582,
 abstract = {On shared-memory systems, Cilk-style work-stealing has been used to effectively parallelize irregular task-graph based applications such as Unbalanced Tree Search (UTS). There are two main difficulties in extending this approach to distributed memory. In the shared memory approach, thieves (nodes without work) constantly attempt to asynchronously steal work from randomly chosen victims until they find work. In distributed memory, thieves cannot autonomously steal work from a victim without disrupting its execution. When work is sparse, this results in performance degradation. In essence, a direct extension of traditional work-stealing to distributed memory violates the work-first principle underlying work-stealing. Further, thieves spend useless CPU cycles attacking victims that have no work, resulting in system inefficiencies in multi-programmed contexts. Second, it is non-trivial to detect active distributed termination (detect that programs at all nodes are looking for work, hence there is no work). This problem is well-studied and requires careful design for good performance. Unfortunately, in most existing languages/frameworks, application developers are forced to implement their own distributed termination detection. In this paper, we develop a simple set of ideas that allow work-stealing to be efficiently extended to distributed memory. First, we introduce lifeline graphs: low-degree, low-diameter, fully connected directed graphs. Such graphs can be constructed from k-dimensional hypercubes. When a node is unable to find work after w unsuccessful steals, it quiesces after informing the outgoing edges in its lifeline graph. Quiescent nodes do not disturb other nodes. A quiesced node is reactivated when work arrives from a lifeline and itself shares this work with those of its incoming lifelines that are activated. Termination occurs precisely when computation at all nodes has quiesced. In a language such as X10, such passive distributed termination can be detected automatically using the finish construct -- no application code is necessary. Our design is implemented in a few hundred lines of X10. On the binomial tree described in olivier:08}, the program achieve 87% efficiency on an Infiniband cluster of 1024 Power7 cores, with a peak throughput of 2.37 GNodes/sec. It achieves 87% efficiency on a Blue Gene/P with 2048 processors, and a peak throughput of 0.966 GNodes/s. All numbers are relative to single core sequential performance. This implementation has been refactored into a reusable global load balancing framework. Applications can use this framework to obtain global load balance with minimal code changes. In summary, we claim: (a) the first formulation of UTS that does not involve application level global termination detection, (b) the introduction of lifeline graphs to reduce failed steals (c) the demonstration of simple lifeline graphs based on k-hypercubes, (d) performance with superior efficiency (or the same efficiency but over a wider range) than published results on UTS. In particular, our framework can deliver the same or better performance as an unrestricted random work-stealing implementation, while reducing the number of attempted steals.},
 acmid = {1941582},
 address = {New York, NY, USA},
 author = {Saraswat, Vijay A. and Kambadur, Prabhanjan and Kodali, Sreedhar and Grove, David and Krishnamoorthy, Sriram},
 doi = {10.1145/2038037.1941582},
 issn = {0362-1340},
 issue_date = {August 2011},
 journal = {SIGPLAN Not.},
 keyword = {distributed work-stealing, global load balancing, uts, x10},
 link = {http://doi.acm.org/10.1145/2038037.1941582},
 month = {feb},
 number = {8},
 numpages = {12},
 pages = {201--212},
 publisher = {ACM},
 title = {Lifeline-based Global Load Balancing},
 volume = {46},
 year = {2011}
}


@article{Dotsenko:2011:AFF:2038037.1941589,
 abstract = {We present an auto-tuning framework for FFTs on graphics processors (GPUs). Due to complex design of the memory and compute subsystems on GPUs, the performance of FFT kernels over the range of possible input parameters can vary widely. We generate several variants for each component of the FFT kernel that, for different cases, are likely to perform well. Our auto-tuner composes variants to generate kernels and selects the best ones. We present heuristics to prune the search space and profile only a small fraction of all possible kernels. We compose optimized kernels to improve the performance of larger FFT computations. We implement the system using the NVIDIA CUDA API and compare its performance to the state-of-the-art FFT libraries. On a range of NVIDIA GPUs and input sizes, our auto-tuned FFTs outperform the NVIDIA CUFFT 3.0 library by up to 38x and deliver up to 3x higher performance compared to a manually-tuned FFT.},
 acmid = {1941589},
 address = {New York, NY, USA},
 author = {Dotsenko, Yuri and Baghsorkhi, Sara S. and Lloyd, Brandon and Govindaraju, Naga K.},
 doi = {10.1145/2038037.1941589},
 issn = {0362-1340},
 issue_date = {August 2011},
 journal = {SIGPLAN Not.},
 keyword = {auto-tuning, fast fourier transform, fft, gpu, high performance, performance analysis, performance tuning},
 link = {http://doi.acm.org/10.1145/2038037.1941589},
 month = {feb},
 number = {8},
 numpages = {10},
 pages = {257--266},
 publisher = {ACM},
 title = {Auto-tuning of Fast Fourier Transform on Graphics Processors},
 volume = {46},
 year = {2011}
}


@article{McKinley:2011:HPC:2038037.1941571,
 abstract = {Two trends changed the computing landscape over the past decade: (1) hardware vendors started delivering chip multiprocessors (CMPs) instead of uniprocessors, and (2) software developers increasingly chose managed languages instead of native languages. Unfortunately, the former change is disrupting the virtuous-cycle between performance improvements and software innovation. Establishing a new parallel performance virtuous cycle for managed languages will require scalable applications executing on scalable Virtual Machine (VM) services, since the VM schedules, monitors, compiles, optimizes, garbage collects, and executes together with the application. This talk describes current progress, opportunities, and challenges for scalable VM services. The parallel computing revolution urgently needs more innovations.},
 acmid = {1941571},
 address = {New York, NY, USA},
 author = {McKinley, Kathryn S.},
 doi = {10.1145/2038037.1941571},
 issn = {0362-1340},
 issue_date = {August 2011},
 journal = {SIGPLAN Not.},
 keyword = {managed languages, multicore},
 link = {http://doi.acm.org/10.1145/2038037.1941571},
 month = {feb},
 number = {8},
 numpages = {2},
 pages = {123--124},
 publisher = {ACM},
 title = {How's the Parallel Computing Revolution Going?},
 volume = {46},
 year = {2011}
}


@inproceedings{Stellwag:2011:WNL:1941553.1941599,
 abstract = {We introduce our major ideas of a wait-free, linearizable, and disjoint access parallel NCAS library, called rtNCAS. It focuses the construction of wait-free data structure operations (DSO) in real-time circumstances. rtNCAS is able to conditionally swap multiple independent words (NCAS) in an atomic manner. It allows us, furthermore, to implement arbitrary DSO by means of their sequential specification.},
 acmid = {1941599},
 address = {New York, NY, USA},
 author = {Stellwag, Philippe and Scheler, Fabian and Krainz, Jakob and Schr\"{o}der-Preikschat, Wolfgang},
 booktitle = {Proceedings of the 16th ACM Symposium on Principles and Practice of Parallel Programming},
 doi = {10.1145/1941553.1941599},
 isbn = {978-1-4503-0119-0},
 keyword = {composition, data structures, disjoint-access parallel, linearizable, wait-free},
 link = {http://doi.acm.org/10.1145/1941553.1941599},
 location = {San Antonio, TX, USA},
 numpages = {2},
 pages = {301--302},
 publisher = {ACM},
 series = {PPoPP '11},
 title = {A Wait-free NCAS Library for Parallel Applications with Timing Constraints},
 year = {2011}
}


@inproceedings{Donaldson:2011:STA:1941553.1941604,
 abstract = {We present the SCRATCH tool, which uses bounded model checking and k-induction to automatically analyse software for multicore processors such as the Cell BE, in order to detect DMA races.},
 acmid = {1941604},
 address = {New York, NY, USA},
 author = {Donaldson, Alastair F. and Kroening, Daniel and Ruemmer, Philipp},
 booktitle = {Proceedings of the 16th ACM Symposium on Principles and Practice of Parallel Programming},
 doi = {10.1145/1941553.1941604},
 isbn = {978-1-4503-0119-0},
 keyword = {cell be, dma, k-induction, model checking},
 link = {http://doi.acm.org/10.1145/1941553.1941604},
 location = {San Antonio, TX, USA},
 numpages = {2},
 pages = {311--312},
 publisher = {ACM},
 series = {PPoPP '11},
 title = {SCRATCH: A Tool for Automatic Analysis of Dma Races},
 year = {2011}
}


@article{Luchangco:2011:TCE:2038037.1941578,
 abstract = {In this paper, we propose to extend transactional memory with transaction communicators, special objects through which concurrent transactions can communicate: changes by one transaction to a communicator can be seen by concurrent transactions before the first transaction commits. Although isolation of transactions is compromised by such communication, we constrain the effects of this compromise by tracking dependencies among transactions, and preventing any transaction from committing unless every transaction whose changes it saw also commits. In particular, mutually dependent transactions must commit or abort together, and transactions that do not communicate remain isolated. To help programmers synchronize accesses to communicators, we also provide special communicator-isolating transactions, which ensure isolation even for accesses to communicators. We propose language features to help programmers express the communicator constructs. We implemented a novel communicators-enabled STM runtime in the Maxine VM. Our preliminary evaluation demonstrates that communicators can be used in diverse settings to improve the performance of transactional programs, and to empower programmers with the ability to safely express within transactions important programming idioms that fundamentally require compromise of transaction isolation (e.g., CSP-style synchronous communication).},
 acmid = {1941578},
 address = {New York, NY, USA},
 author = {Luchangco, Victor and Marathe, Virendra J.},
 doi = {10.1145/2038037.1941578},
 issn = {0362-1340},
 issue_date = {August 2011},
 journal = {SIGPLAN Not.},
 keyword = {communication, transactional memory},
 link = {http://doi.acm.org/10.1145/2038037.1941578},
 month = {feb},
 number = {8},
 numpages = {10},
 pages = {169--178},
 publisher = {ACM},
 title = {Transaction Communicators: Enabling Cooperation Among Concurrent Transactions},
 volume = {46},
 year = {2011}
}


@article{Karmani:2011:TCS:2038037.1941573,
 abstract = {We build a framework of thread contracts, called Accord, that allows programmers to annotate their concurrency co-ordination strategies. Accord annotations allow programmers to declaratively specify the parts of memory that a thread may read or write into, and the locks that protect them, reflecting the concurrency co-ordination among threads and the reason why the program is free of data-races. We provide automatic tools to check if the concurrency co-ordination strategy ensures race-freedom, using constraint-solvers (SMT solvers). Hence programmers using Accord can both formally state and prove their co-ordination strategies ensure race freedom. The programmer's implementation of the co-ordination strategy may however be correct or incorrect. We show how the formal Accord contracts allow us to automatically insert runtime assertions that serve to check, during testing, whether the implementation conforms to the contract. Using a large class of data-parallel programs that share memory in intricate ways, we show that natural and simple contracts suffice to document the co-ordination strategy amongst threads, and that the task of showing that the strategy ensures race-freedom can be handled efficiently and automatically by an existing SMT solver (Z3). While co-ordination strategies can be proved race-free in our framework, failure to prove the co-ordination strategy race-free, accompanied by counter-examples produced by the solver, indicates the presence of races. Using such counterexamples, we report hitherto undiscovered data-races that we found in the long-tested applu_l benchmark in the Spec OMP2001 suite.},
 acmid = {1941573},
 address = {New York, NY, USA},
 author = {Karmani, Rajesh K. and Madhusudan, P. and Moore, Brandon M.},
 doi = {10.1145/2038037.1941573},
 issn = {0362-1340},
 issue_date = {August 2011},
 journal = {SIGPLAN Not.},
 keyword = {concurrent contracts, constraint solvers, data-races, testing},
 link = {http://doi.acm.org/10.1145/2038037.1941573},
 month = {feb},
 number = {8},
 numpages = {10},
 pages = {125--134},
 publisher = {ACM},
 title = {Thread Contracts for Safe Parallelism},
 volume = {46},
 year = {2011}
}


@article{Roy:2011:WAU:2038037.1941594,
 abstract = {We consider the problem of building a weakly atomic Software Transactional Memory (STM), that provides Single (Global) Lock Atomicity (SLA) while adhering to the x86 memory consistency model (x86-MM).},
 acmid = {1941594},
 address = {New York, NY, USA},
 author = {Roy, Amitabha and Hand, Steven and Harris, Tim},
 doi = {10.1145/2038037.1941594},
 issn = {0362-1340},
 issue_date = {August 2011},
 journal = {SIGPLAN Not.},
 keyword = {software transactional memory, x86 memory model},
 link = {http://doi.acm.org/10.1145/2038037.1941594},
 month = {feb},
 number = {8},
 numpages = {2},
 pages = {291--292},
 publisher = {ACM},
 title = {Weak Atomicity Under the x86 Memory Consistency Model},
 volume = {46},
 year = {2011}
}


@article{Davies:2011:ARH:2038037.1941600,
 abstract = {When more processors are used for a calculation, the probability that one will fail during the calculation increases. Fault tolerance is a technique for allowing a calculation to survive a failure, and includes recovering lost data. A common method of recovery is diskless checkpointing. However, it has high overhead when a large amount of data is involved, as is the case with matrix operations. A checksum-based method allows fault tolerance of matrix operations with lower overhead. This technique is applicable to the LU decomposition in the benchmark HPL.},
 acmid = {1941600},
 address = {New York, NY, USA},
 author = {Davies, Teresa and Chen, Zizhong and Karlsson, Christer and Liu, Hui},
 doi = {10.1145/2038037.1941600},
 issn = {0362-1340},
 issue_date = {August 2011},
 journal = {SIGPLAN Not.},
 keyword = {algorithm-based recovery, hpl},
 link = {http://doi.acm.org/10.1145/2038037.1941600},
 month = {feb},
 number = {8},
 numpages = {2},
 pages = {303--304},
 publisher = {ACM},
 title = {Algorithm-based Recovery for HPL},
 volume = {46},
 year = {2011}
}


@inproceedings{Kogan:2011:WQM:1941553.1941585,
 abstract = {The queue data structure is fundamental and ubiquitous. Lock-free versions of the queue are well known. However, an important open question is whether practical wait-free queues exist. Until now, only versions with limited concurrency were proposed. In this paper we provide a design for a practical wait-free queue. Our construction is based on the highly efficient lock-free queue of Michael and Scott. To achieve wait-freedom, we employ a priority-based helping scheme in which faster threads help the slower peers to complete their pending operations. We have implemented our scheme on multicore machines and present performance measurements comparing our implementation with that of Michael and Scott in several system configurations.},
 acmid = {1941585},
 address = {New York, NY, USA},
 author = {Kogan, Alex and Petrank, Erez},
 booktitle = {Proceedings of the 16th ACM Symposium on Principles and Practice of Parallel Programming},
 doi = {10.1145/1941553.1941585},
 isbn = {978-1-4503-0119-0},
 keyword = {concurrent queues, wait-free algorithms},
 link = {http://doi.acm.org/10.1145/1941553.1941585},
 location = {San Antonio, TX, USA},
 numpages = {12},
 pages = {223--234},
 publisher = {ACM},
 series = {PPoPP '11},
 title = {Wait-free Queues with Multiple Enqueuers and Dequeuers},
 year = {2011}
}


@inproceedings{Willcock:2011:APP:1941553.1941601,
 abstract = {A variety of programming models exist to support large-scale, distributed memory, parallel computation. These programming models have historically targeted coarse-grained applications with natural locality such as those found in a variety of scientific simulations of the physical world. Fine-grained, irregular, and unstructured applications such as those found in biology, social network analysis, and graph theory are less well supported. We propose Active Pebbles, a programming model which allows these applications to be expressed naturally; an accompanying execution model ensures performance and scalability.},
 acmid = {1941601},
 address = {New York, NY, USA},
 author = {Willcock, Jeremiah James and Hoefler, Torsten and Edmonds, Nicholas Gerard and Lumsdaine, Andrew},
 booktitle = {Proceedings of the 16th ACM Symposium on Principles and Practice of Parallel Programming},
 doi = {10.1145/1941553.1941601},
 isbn = {978-1-4503-0119-0},
 keyword = {active messages, irregular applications, programming models},
 link = {http://doi.acm.org/10.1145/1941553.1941601},
 location = {San Antonio, TX, USA},
 numpages = {2},
 pages = {305--306},
 publisher = {ACM},
 series = {PPoPP '11},
 title = {Active Pebbles: A Programming Model for Highly Parallel Fine-grained Data-driven Computations},
 year = {2011}
}


@inproceedings{Catanzaro:2011:CCE:1941553.1941562,
 abstract = {Modern parallel microprocessors deliver high performance on applications that expose substantial fine-grained data parallelism. Although data parallelism is widely available in many computations, implementing data parallel algorithms in low-level languages is often an unnecessarily difficult task. The characteristics of parallel microprocessors and the limitations of current programming methodologies motivate our design of Copperhead, a high-level data parallel language embedded in Python. The Copperhead programmer describes parallel computations via composition of familiar data parallel primitives supporting both flat and nested data parallel computation on arrays of data. Copperhead programs are expressed in a subset of the widely used Python programming language and interoperate with standard Python modules, including libraries for numeric computation, data visualization, and analysis. In this paper, we discuss the language, compiler, and runtime features that enable Copperhead to efficiently execute data parallel code. We define the restricted subset of Python which Copperhead supports and introduce the program analysis techniques necessary for compiling Copperhead code into efficient low-level implementations. We also outline the runtime support by which Copperhead programs interoperate with standard Python modules. We demonstrate the effectiveness of our techniques with several examples targeting the CUDA platform for parallel programming on GPUs. Copperhead code is concise, on average requiring 3.6 times fewer lines of code than CUDA, and the compiler generates efficient code, yielding 45-100% of the performance of hand-crafted, well optimized CUDA code.},
 acmid = {1941562},
 address = {New York, NY, USA},
 author = {Catanzaro, Bryan and Garland, Michael and Keutzer, Kurt},
 booktitle = {Proceedings of the 16th ACM Symposium on Principles and Practice of Parallel Programming},
 doi = {10.1145/1941553.1941562},
 isbn = {978-1-4503-0119-0},
 keyword = {data parallelism, gpu, python},
 link = {http://doi.acm.org/10.1145/1941553.1941562},
 location = {San Antonio, TX, USA},
 numpages = {10},
 pages = {47--56},
 publisher = {ACM},
 series = {PPoPP '11},
 title = {Copperhead: Compiling an Embedded Data Parallel Language},
 year = {2011}
}


@article{Larus:2011:PC:2038037.1941555,
 abstract = {Client + cloud computing is a disruptive, new computing platform, combining diverse client devices -- PCs, smartphones, sensors, and single-function and embedded devices -- with the unlimited, on-demand computation and data storage offered by cloud computing services such as Amazon's AWS or Microsoft's Windows Azure. As with every advance in computing, programming is a fundamental challenge as client + cloud computing combines many difficult aspects of software development. Systems built for this world are inherently parallel and distributed, run on unreliable hardware, and must be continually available -- a challenging programming model for even the most skilled programmers. How then do ordinary programmers develop software for the Cloud? This talk presents one answer, Orleans, a software framework for building client + cloud applications. Orleans encourages use of simple concurrency patterns that are easy to understand and implement correctly, building on an actor-like model with declarative specification of persistence, replication, and consistency and using lightweight transactions to support the development of reliable and scalable client + cloud software.},
 acmid = {1941555},
 address = {New York, NY, USA},
 author = {Larus, James R.},
 doi = {10.1145/2038037.1941555},
 issn = {0362-1340},
 issue_date = {August 2011},
 journal = {SIGPLAN Not.},
 keyword = {distributed systems},
 link = {http://doi.acm.org/10.1145/2038037.1941555},
 month = {feb},
 number = {8},
 numpages = {2},
 pages = {1--2},
 publisher = {ACM},
 title = {Programming the Cloud},
 volume = {46},
 year = {2011}
}


@inproceedings{Tian:2011:ESP:1941553.1941580,
 abstract = {The widespread availability of multicore systems has led to an increased interest in speculative parallelization of sequential programs using software-based thread level speculation. Many of the proposed techniques are implemented via state separation where non-speculative computation state is maintained separately from the speculative state of threads performing speculative computations. If speculation is successful, the results from speculative state are committed to non-speculative state. However, upon misspeculation, discard-all scheme is employed in which speculatively computed results of a thread are discarded and the computation is performed again. While this scheme is simple to implement, one disadvantage of discard-all is its inability to tolerate high misspeculation rates due to its high runtime overhead. Thus, it is not suitable for use in applications where misspeculation rates are input dependent and therefore may reach high levels. In this paper we develop an approach for incremental recovery in which, instead of discarding all of the results and reexecuting the speculative computation in its entirety, the computation is restarted from the earliest point at which a misspeculation causing value is read. This approach has two advantages. First, the cost of recovery is reduced as only part of the computation is reexecuted. Second, since recovery takes less time, the likelihood of future misspeculations is reduced. We design and implement a strategy for implementing incremental recovery that allows results of partial computations to be efficiently saved and reused. For a set of programs where misspeculation rate is input dependent, our experiments show that with inputs that result in misspeculation rates of around 40% and 80%, applying incremental recovery technique results in 1.2x-3.3x and 2.0x-6.6x speedups respectively over the discard-all recovery scheme. Furthermore, misspeculations observed during discard-all scheme are reduced when incremental recovery is employed -- reductions range from 10% to 85%.},
 acmid = {1941580},
 address = {New York, NY, USA},
 author = {Tian, Chen and Lin, Changhui and Feng, Min and Gupta, Rajiv},
 booktitle = {Proceedings of the 16th ACM Symposium on Principles and Practice of Parallel Programming},
 doi = {10.1145/1941553.1941580},
 isbn = {978-1-4503-0119-0},
 keyword = {incremental recovery, multicore processors, speculative parallelization},
 link = {http://doi.acm.org/10.1145/1941553.1941580},
 location = {San Antonio, TX, USA},
 numpages = {12},
 pages = {189--200},
 publisher = {ACM},
 series = {PPoPP '11},
 title = {Enhanced Speculative Parallelization via Incremental Recovery},
 year = {2011}
}


@article{Yi:2011:CRP:2038037.1941575,
 abstract = {We propose a cooperative methodology for multithreaded software, where threads use traditional synchronization idioms such as locks, but additionally document each point of potential thread interference with a "yield" annotation. Under this methodology, code between two successive yield annotations forms a serializable transaction that is amenable to sequential reasoning. This methodology reduces the burden of reasoning about thread interleavings by indicating only those interference points that matter. We present experimental results showing that very few yield annotations are required, typically one or two per thousand lines of code. We also present dynamic analysis algorithms for detecting cooperability violations, where thread interference is not documented by a yield, and for yield annotation inference for legacy software.},
 acmid = {1941575},
 address = {New York, NY, USA},
 author = {Yi, Jaeheon and Sadowski, Caitlin and Flanagan, Cormac},
 doi = {10.1145/2038037.1941575},
 issn = {0362-1340},
 issue_date = {August 2011},
 journal = {SIGPLAN Not.},
 keyword = {atomicity, concurrency, cooperability, parallelism, yield annotation},
 link = {http://doi.acm.org/10.1145/2038037.1941575},
 month = {feb},
 number = {8},
 numpages = {10},
 pages = {147--156},
 publisher = {ACM},
 title = {Cooperative Reasoning for Preemptive Execution},
 volume = {46},
 year = {2011}
}


@article{Hong:2011:ACG:2038037.1941590,
 abstract = {Graphs are powerful data representations favored in many computational domains. Modern GPUs have recently shown promising results in accelerating computationally challenging graph problems but their performance suffered heavily when the graph structure is highly irregular, as most real-world graphs tend to be. In this study, we first observe that the poor performance is caused by work imbalance and is an artifact of a discrepancy between the GPU programming model and the underlying GPU architecture.We then propose a novel virtual warp-centric programming method that exposes the traits of underlying GPU architectures to users. Our method significantly improves the performance of applications with heavily imbalanced workloads, and enables trade-offs between workload imbalance and ALU underutilization for fine-tuning the performance. Our evaluation reveals that our method exhibits up to 9x speedup over previous GPU algorithms and 12x over single thread CPU execution on irregular graphs. When properly configured, it also yields up to 30% improvement over previous GPU algorithms on regular graphs. In addition to performance gains on graph algorithms, our programming method achieves 1.3x to 15.1x speedup on a set of GPU benchmark applications. Our study also confirms that the performance gap between GPUs and other multi-threaded CPU graph implementations is primarily due to the large difference in memory bandwidth.},
 acmid = {1941590},
 address = {New York, NY, USA},
 author = {Hong, Sungpack and Kim, Sang Kyun and Oguntebi, Tayo and Olukotun, Kunle},
 doi = {10.1145/2038037.1941590},
 issn = {0362-1340},
 issue_date = {August 2011},
 journal = {SIGPLAN Not.},
 keyword = {cuda, gpgpu, parallel graph algorithms},
 link = {http://doi.acm.org/10.1145/2038037.1941590},
 month = {feb},
 number = {8},
 numpages = {10},
 pages = {267--276},
 publisher = {ACM},
 title = {Accelerating CUDA Graph Algorithms at Maximum Warp},
 volume = {46},
 year = {2011}
}


@inproceedings{Tanase:2011:SPC:1941553.1941586,
 abstract = {The Standard Template Adaptive Parallel Library (STAPL) is a parallel programming infrastructure that extends C++ with support for parallelism. It includes a collection of distributed data structures called pContainers that are thread-safe, concurrent objects, i.e., shared objects that provide parallel methods that can be invoked concurrently. In this work, we present the STAPL Parallel Container Framework (PCF), that is designed to facilitate the development of generic parallel containers. We introduce a set of concepts and a methodology for assembling a pContainer from existing sequential or parallel containers, without requiring the programmer to deal with concurrency or data distribution issues. The PCF provides a large number of basic parallel data structures (e.g., pArray, pList, pVector, pMatrix, pGraph, pMap, pSet). The PCF provides a class hierarchy and a composition mechanism that allows users to extend and customize the current container base for improved application expressivity and performance. We evaluate STAPL pContainer performance on a CRAY XT4 massively parallel system and show that pContainer methods, generic pAlgorithms, and different applications provide good scalability on more than 16,000 processors.},
 acmid = {1941586},
 address = {New York, NY, USA},
 author = {Tanase, Gabriel and Buss, Antal and Fidel, Adam and Harshvardhan and Papadopoulos, Ioannis and Pearce, Olga and Smith, Timmie and Thomas, Nathan and Xu, Xiabing and Mourad, Nedal and Vu, Jeremy and Bianco, Mauro and Amato, Nancy M. and Rauchwerger, Lawrence},
 booktitle = {Proceedings of the 16th ACM Symposium on Principles and Practice of Parallel Programming},
 doi = {10.1145/1941553.1941586},
 isbn = {978-1-4503-0119-0},
 keyword = {containers, data, languages, libraries, parallel, structures},
 link = {http://doi.acm.org/10.1145/1941553.1941586},
 location = {San Antonio, TX, USA},
 numpages = {12},
 pages = {235--246},
 publisher = {ACM},
 series = {PPoPP '11},
 title = {The STAPL Parallel Container Framework},
 year = {2011}
}


@article{Zheng:2011:GLM:2038037.1941574,
 abstract = {In recent years, GPUs have emerged as an extremely cost-effective means for achieving high performance. Many application developers, including those with no prior parallel programming experience, are now trying to scale their applications using GPUs. While languages like CUDA and OpenCL have eased GPU programming for non-graphical applications, they are still explicitly parallel languages. All parallel programmers, particularly the novices, need tools that can help ensuring the correctness of their programs. Like any multithreaded environment, data races on GPUs can severely affect the program reliability. Thus, tool support for detecting race conditions can significantly benefit GPU application developers. Existing approaches for detecting data races on CPUs or GPUs have one or more of the following limitations: 1) being illsuited for handling non-lock synchronization primitives on GPUs; 2) lacking of scalability due to the state explosion problem; 3) reporting many false positives because of simplified modeling; and/or 4) incurring prohibitive runtime and space overhead. In this paper, we propose GRace, a new mechanism for detecting races in GPU programs that combines static analysis with a carefully designed dynamic checker for logging and analyzing information at runtime. Our design utilizes GPUs memory hierarchy to log runtime data accesses efficiently. To improve the performance, GRace leverages static analysis to reduce the number of statements that need to be instrumented. Additionally, by exploiting the knowledge of thread scheduling and the execution model in the underlying GPUs, GRace can accurately detect data races with no false positives reported. Based on the above idea, we have built a prototype of GRace with two schemes, i.e., GRace-stmt and GRace-addr, for NVIDIA GPUs. Both schemes are integrated with the same static analysis. We have evaluated GRace-stmt and GRace-addr with three data race bugs in three GPU kernel functions and also have compared them with the existing approach, referred to as B-tool. Our experimental results show that both schemes of GRace are effective in detecting all evaluated cases with no false positives, whereas Btool reports many false positives for one evaluated case. On the one hand, GRace-addr incurs low runtime overhead, i.e., 22-116%, and low space overhead, i.e., 9-18MB, for the evaluated kernels. On the other hand, GRace-stmt offers more help in diagnosing data races with larger overhead.},
 acmid = {1941574},
 address = {New York, NY, USA},
 author = {Zheng, Mai and Ravi, Vignesh T. and Qin, Feng and Agrawal, Gagan},
 doi = {10.1145/2038037.1941574},
 issn = {0362-1340},
 issue_date = {August 2011},
 journal = {SIGPLAN Not.},
 keyword = {concurrency, cuda, data race, gpu, multithreading},
 link = {http://doi.acm.org/10.1145/2038037.1941574},
 month = {feb},
 number = {8},
 numpages = {12},
 pages = {135--146},
 publisher = {ACM},
 title = {GRace: A Low-overhead Mechanism for Detecting Data Races in GPU Programs},
 volume = {46},
 year = {2011}
}


@inproceedings{Ding:2011:TEP:1941553.1941598,
 abstract = {
                  An abstract is not available.
              },
 acmid = {1941598},
 address = {New York, NY, USA},
 author = {Ding, Chen},
 booktitle = {Proceedings of the 16th ACM Symposium on Principles and Practice of Parallel Programming},
 doi = {10.1145/1941553.1941598},
 isbn = {978-1-4503-0119-0},
 keyword = {parallel programming interface},
 link = {http://doi.acm.org/10.1145/1941553.1941598},
 location = {San Antonio, TX, USA},
 numpages = {2},
 pages = {299--300},
 publisher = {ACM},
 series = {PPoPP '11},
 title = {Two Examples of Parallel Programming Without Concurrency Constructs (PP-CC)},
 year = {2011}
}


@inproceedings{Fernandes:2011:LSM:1941553.1941579,
 abstract = {Software Transactional Memory (STM) was initially proposed as a lock-free mechanism for concurrency control. Early implementations had efficiency limitations, and soon obstruction-free proposals appeared, to tackle this problem, often simplifying STM implementation. Today, most of the modern and top-performing STMs use blocking designs, relying on locks to ensure an atomic commit operation. This approach has revealed better in practice, in part due to its simplicity. Yet, it may have scalability problems when we move into many-core computers, requiring fine-tuning and careful programming to avoid contention. In this paper we present and discuss the modifications we made to a lock-based multi-version STM in Java, to turn it into a lock-free implementation that we have tested to scale at least up to 192 cores, and which provides results that compete with, and sometimes exceed, some of today's top-performing lock-based implementations. The new lock-free commit algorithm allows write transactions to proceed in parallel, by allowing them to run their validation phase independently of each other, and by resorting to helping from threads that would otherwise be waiting to commit, during the write-back phase. We also present a new garbage collection algorithm to dispose of old unused object versions that allows for asynchronous identification of unnecessary versions, which minimizes its interference with the rest of the transactional system.},
 acmid = {1941579},
 address = {New York, NY, USA},
 author = {Fernandes, S{\'e}rgio Miguel and Cachopo, Jo\~{a}o},
 booktitle = {Proceedings of the 16th ACM Symposium on Principles and Practice of Parallel Programming},
 doi = {10.1145/1941553.1941579},
 isbn = {978-1-4503-0119-0},
 keyword = {garbage collection, lock-free synchronization, multi-version concurrency control, transactional memory},
 link = {http://doi.acm.org/10.1145/1941553.1941579},
 location = {San Antonio, TX, USA},
 numpages = {10},
 pages = {179--188},
 publisher = {ACM},
 series = {PPoPP '11},
 title = {Lock-free and Scalable Multi-version Software Transactional Memory},
 year = {2011}
}


@inproceedings{Botincan:2011:ASP:1941553.1941605,
 abstract = {We present a work-in-progress proof system and tool, based on separation logic, for analysing memory safety of multicore programs that use asynchronous memory operations.},
 acmid = {1941605},
 address = {New York, NY, USA},
 author = {Botincan, Matko and Dodds, Mike and Donaldson, Alastair F. and Parkinson, Matthew J.},
 booktitle = {Proceedings of the 16th ACM Symposium on Principles and Practice of Parallel Programming},
 doi = {10.1145/1941553.1941605},
 isbn = {978-1-4503-0119-0},
 keyword = {concurrency, memory safety, separation logic},
 link = {http://doi.acm.org/10.1145/1941553.1941605},
 location = {San Antonio, TX, USA},
 numpages = {2},
 pages = {313--314},
 publisher = {ACM},
 series = {PPoPP '11},
 title = {Automatic Safety Proofs for Asynchronous Memory Operations},
 year = {2011}
}


@inproceedings{Fischer:2011:SMC:1941553.1941602,
 abstract = {Improper use of Inter-Process Communication (IPC) within concurrent systems often creates data races which can lead to bugs that are challenging to discover. Techniques that use Satisfiability Modulo Theories (SMT) problems to symbolically model possible executions of concurrent software have recently been proposed for use in the formal verification of software. In this work we describe a new technique for modeling executions of concurrent software that use a message passing API called MCAPI. Our technique uses an execution trace to create an SMT problem that symbolically models all possible concurrent executions and follows the same sequence of conditional branch outcomes as the provided execution trace. We check if there exists a satisfying assignment to the SMT problem with respect to specific safety properties. If such an assignment exists, it provides the conditions that lead to the violation of the property. We show how our method models behaviors of MCAPI applications that are ignored in previously published techniques.},
 acmid = {1941602},
 address = {New York, NY, USA},
 author = {Fischer, Topher and Mercer, Eric and Rungta, Neha},
 booktitle = {Proceedings of the 16th ACM Symposium on Principles and Practice of Parallel Programming},
 doi = {10.1145/1941553.1941602},
 isbn = {978-1-4503-0119-0},
 keyword = {mcapi, multicore, smt, symbolic analysis},
 link = {http://doi.acm.org/10.1145/1941553.1941602},
 location = {San Antonio, TX, USA},
 numpages = {2},
 pages = {307--308},
 publisher = {ACM},
 series = {PPoPP '11},
 title = {Symbolically Modeling Concurrent MCAPI Executions},
 year = {2011}
}


@article{Murarasu:2011:CDS:2038037.1941559,
 abstract = {The sparse grid discretization technique enables a compressed representation of higher-dimensional functions. In its original form, it relies heavily on recursion and complex data structures, thus being far from well-suited for GPUs. In this paper, we describe optimizations that enable us to implement compression and decompression, the crucial sparse grid algorithms for our application, on Nvidia GPUs. The main idea consists of a bijective mapping between the set of points in a multi-dimensional sparse grid and a set of consecutive natural numbers. The resulting data structure consumes a minimum amount of memory. For a 10-dimensional sparse grid with approximately 127 million points, it consumes up to 30 times less memory than trees or hash tables which are typically used. Compared to a sequential CPU implementation, the speedups achieved on GPU are up to 17 for compression and up to 70 for decompression, respectively. We show that the optimizations are also applicable to multicore CPUs.},
 acmid = {1941559},
 address = {New York, NY, USA},
 author = {Murarasu, Alin and Weidendorfer, Josef and Buse, Gerrit and Butnaru, Daniel and Pfl\"{u}ger, Dirk},
 doi = {10.1145/2038037.1941559},
 issn = {0362-1340},
 issue_date = {August 2011},
 journal = {SIGPLAN Not.},
 keyword = {gpu, performance optimization, sparse grids},
 link = {http://doi.acm.org/10.1145/2038037.1941559},
 month = {feb},
 number = {8},
 numpages = {10},
 pages = {25--34},
 publisher = {ACM},
 title = {Compact Data Structure and Scalable Algorithms for the Sparse Grid Technique},
 volume = {46},
 year = {2011}
}


@article{Siegel:2011:AFV:2038037.1941603,
 abstract = {The Toolkit for Accurate Scientific Software (TASS) is a suite of tools for the formal verification of MPI-based parallel programs used in computational science. TASS can verify various safety properties as well as compare two programs for functional equivalence. The TASS front end takes an integer n ≥ 1 and a C/MPI program, and constructs an abstract model of the program with n processes. Procedures, structs, (multi-dimensional) arrays, heap-allocated data, pointers, and pointer arithmetic are all representable in a TASS model. The model is then explored using symbolic execution and explicit state space enumeration. A number of techniques are used to reduce the time and memory consumed. A variety of realistic MPI programs have been verified with TASS, including Jacobi iteration and manager-worker type programs, and some subtle defects have been discovered. TASS is written in Java and is available from http://vsl.cis.udel.edu/tass under the Gnu Public License.},
 acmid = {1941603},
 address = {New York, NY, USA},
 author = {Siegel, Stephen F. and Zirkel, Timothy K.},
 doi = {10.1145/2038037.1941603},
 issn = {0362-1340},
 issue_date = {August 2011},
 journal = {SIGPLAN Not.},
 keyword = {debugging, message-passing, mpi, symbolic execution, verification},
 link = {http://doi.acm.org/10.1145/2038037.1941603},
 month = {feb},
 number = {8},
 numpages = {2},
 pages = {309--310},
 publisher = {ACM},
 title = {Automatic Formal Verification of MPI-based Parallel Programs},
 volume = {46},
 year = {2011}
}


@article{Strzodka:2011:TSM:2038037.1941596,
 abstract = {Time skewing and loop tiling has been known for a long time to be a highly beneficial acceleration technique for nested loops especially on bandwidth hungry multi-core processors, but it is little used in practice because efficient implementations utilize complicated code and simple or abstract ones show much smaller gains over naive nested loops. We break this dilemma with an essential time skewing scheme that is both compact and fast.},
 acmid = {1941596},
 address = {New York, NY, USA},
 author = {Strzodka, Robert and Shaheen, Mohammed and Pajak, Dawid},
 doi = {10.1145/2038037.1941596},
 issn = {0362-1340},
 issue_date = {August 2011},
 journal = {SIGPLAN Not.},
 keyword = {bandwidth, data locality, loop tiling, memory bound, memory wall, stencil, temporal blocking, time skewing},
 link = {http://doi.acm.org/10.1145/2038037.1941596},
 month = {feb},
 number = {8},
 numpages = {2},
 pages = {295--296},
 publisher = {ACM},
 title = {Time Skewing Made Simple},
 volume = {46},
 year = {2011}
}


@inproceedings{Jenista:2011:OSO:1941553.1941563,
 abstract = {Developing parallel software using current tools can be challenging. Even experts find it difficult to reason about the use of locks and often accidentally introduce race conditions and deadlocks into parallel software. OoOJava is a compiler-assisted approach that leverages developer annotations along with static analysis to provide an easy-to-use deterministic parallel programming model. OoOJava extends Java with a task annotation that instructs the compiler to consider a code block for out-of-order execution. OoOJava executes tasks as soon as their data dependences are resolved and guarantees that the execution of an annotated program preserves the exact semantics of the original sequential program. We have implemented OoOJava and achieved an average speedup of 16.6x on our ten benchmarks.},
 acmid = {1941563},
 address = {New York, NY, USA},
 author = {Jenista, James Christopher and Eom, Yong hun and Demsky, Brian Charles},
 booktitle = {Proceedings of the 16th ACM Symposium on Principles and Practice of Parallel Programming},
 doi = {10.1145/1941553.1941563},
 isbn = {978-1-4503-0119-0},
 keyword = {deterministic parallel programming, out-of-order execution},
 link = {http://doi.acm.org/10.1145/1941553.1941563},
 location = {San Antonio, TX, USA},
 numpages = {12},
 pages = {57--68},
 publisher = {ACM},
 series = {PPoPP '11},
 title = {OoOJava: Software Out-of-order Execution},
 year = {2011}
}


