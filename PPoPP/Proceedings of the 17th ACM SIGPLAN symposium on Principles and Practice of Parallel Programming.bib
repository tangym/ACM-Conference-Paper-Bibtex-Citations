@article{Hoefler:2012:COD:2370036.2145866,
 abstract = {The steady increase of parallelism in high-performance computing platforms implies that communication will be most important in large-scale applications. In this work, we tackle the problem of transparent optimization of large-scale communication patterns using online compilation techniques. We utilize the Group Operation Assembly Language (GOAL), an abstract parallel dataflow definition language, to specify our transformations in a device-independent manner. We develop fast schemes that analyze dataflow and synchronization semantics in GOAL and detect if parts of the (or the whole) communication pattern express a known collective communication operation. The detection of collective operations allows us to replace the detected patterns with highly optimized algorithms or low-level hardware calls and thus improve performance significantly. Benchmark results suggest that our technique can lead to a performance improvement of orders of magnitude compared with various optimized algorithms written in Co-Array Fortran. Detecting collective operations also improves the programmability of parallel languages in that the user does not have to understand the detailed semantics of high-level communication operations in order to generate efficient and scalable code.},
 acmid = {2145866},
 address = {New York, NY, USA},
 author = {Hoefler, Torsten and Schneider, Timo},
 doi = {10.1145/2370036.2145866},
 issn = {0362-1340},
 issue_date = {August 2012},
 journal = {SIGPLAN Not.},
 keyword = {collective communication, parallel compiler optimization, parallel dataflow},
 link = {http://doi.acm.org/10.1145/2370036.2145866},
 month = {feb},
 number = {8},
 numpages = {2},
 pages = {305--306},
 publisher = {ACM},
 title = {Communication-centric Optimizations by Dynamically Detecting Collective Operations},
 volume = {47},
 year = {2012}
}


@article{Park:2012:CB:2370036.2145880,
 abstract = {In program debugging, reproducibility of bugs is a key requirement. Unfortunately, bugs in concurrent programs are notoriously difficult to reproduce because bugs due to concurrency happen under very specific thread schedules and the likelihood of taking such corner-case schedules during regular testing is very low. We propose concurrent breakpoints, a light-weight and programmatic way to make a concurrency bug reproducible. We describe a mechanism that helps to hit a concurrent breakpoint in a concurrent execution with high probability. We have implemented concurrent breakpoints as a light-weight library for Java and C/C++ programs. We have used the implementation to deterministically reproduce several known non-deterministic bugs in real-world concurrent Java and C/C++ programs with almost 100% probability.},
 acmid = {2145880},
 address = {New York, NY, USA},
 author = {Park, Chang-Seo and Sen, Koushik},
 doi = {10.1145/2370036.2145880},
 issn = {0362-1340},
 issue_date = {August 2012},
 journal = {SIGPLAN Not.},
 keyword = {concurrency bug reproducibility},
 link = {http://doi.acm.org/10.1145/2370036.2145880},
 month = {feb},
 number = {8},
 numpages = {2},
 pages = {331--332},
 publisher = {ACM},
 title = {Concurrent Breakpoints},
 volume = {47},
 year = {2012}
}


@article{Prokopec:2012:CTE:2370036.2145836,
 abstract = {We describe a non-blocking concurrent hash trie based on shared-memory single-word compare-and-swap instructions. The hash trie supports standard mutable lock-free operations such as insertion, removal, lookup and their conditional variants. To ensure space-efficiency, removal operations compress the trie when necessary. We show how to implement an efficient lock-free snapshot operation for concurrent hash tries. The snapshot operation uses a single-word compare-and-swap and avoids copying the data structure eagerly. Snapshots are used to implement consistent iterators and a linearizable size retrieval. We compare concurrent hash trie performance with other concurrent data structures and evaluate the performance of the snapshot operation.},
 acmid = {2145836},
 address = {New York, NY, USA},
 author = {Prokopec, Aleksandar and Bronson, Nathan Grasso and Bagwell, Phil and Odersky, Martin},
 doi = {10.1145/2370036.2145836},
 issn = {0362-1340},
 issue_date = {August 2012},
 journal = {SIGPLAN Not.},
 keyword = {concurrent data structure, hash trie, nonblocking, snapshot},
 link = {http://doi.acm.org/10.1145/2370036.2145836},
 month = {feb},
 number = {8},
 numpages = {10},
 pages = {151--160},
 publisher = {ACM},
 title = {Concurrent Tries with Efficient Non-blocking Snapshots},
 volume = {47},
 year = {2012}
}


@article{Liu:2012:FPI:2370036.2145853,
 abstract = {In this paper, we present FlexBFS, a parallelism-aware implementation for breadth-first search on GPU. Our implementation can adjust the computation resources according to the feedback of available parallelism dynamically. We also optimized our program in three ways: (1)a simplified two-level queue management,(2)a combined kernel strategy and (3)a high-degree vertices specialization approach. Our experimental results show that it can achieve 3~20 times speedup against the fastest serial version, and can outperform the TBB based multi-threading CPU version and the previous most effective GPU version on all types of input graphs.},
 acmid = {2145853},
 address = {New York, NY, USA},
 author = {Liu, Gu and An, Hong and Han, Wenting and Li, Xiaoqiang and Sun, Tao and Zhou, Wei and Wei, Xuechao and Tang, Xulong},
 doi = {10.1145/2370036.2145853},
 issn = {0362-1340},
 issue_date = {August 2012},
 journal = {SIGPLAN Not.},
 keyword = {CUDA, GPGPU, breadth-first search, graph algorithms},
 link = {http://doi.acm.org/10.1145/2370036.2145853},
 month = {feb},
 number = {8},
 numpages = {2},
 pages = {279--280},
 publisher = {ACM},
 title = {FlexBFS: A Parallelism-aware Implementation of Breadth-first Search on GPU},
 volume = {47},
 year = {2012}
}


@article{DeKoster:2012:SVE:2370036.2145873,
 abstract = {The actor model has already proven itself as an interesting concurrency model that avoids issues such as deadlocks and race conditions by construction, and thus facilitates concurrent programming. The tradeoff is that it sacrifices expressiveness and efficiency especially with respect to data parallelism. However, many standard solutions to computationally expensive problems employ data parallel algorithms for better performance on parallel systems. We identified three problems that inhibit the use of data-parallel algorithms within the actor model. Firstly, one of the main properties of the actor model, the fact that no data is shared, is one of the most severe performance bottlenecks. Especially the fact that shared state can not be read truly in parallel. Secondly, the actor model on its own does not provide a mechanism to specify extra synchronization conditions on batches of messages which leads to event-level data-races. And lastly, programmers are forced to write code in a continuation-passing style (CPS) to handle typical request-response situations. However, CPS breaks the sequential flow of the code and is often hard to understand, which increases complexity and lowers maintainability. We proposes synchronization views to solve these three issues without compromising the semantic properties of the actor model. Thus, the resulting concurrency model maintains deadlock-freedom, avoids low-level race conditions, and keeps the semantics of macro-step execution.},
 acmid = {2145873},
 address = {New York, NY, USA},
 author = {De Koster, Joeri and Marr, Stefan and D'Hondt, Theo},
 doi = {10.1145/2370036.2145873},
 issn = {0362-1340},
 issue_date = {August 2012},
 journal = {SIGPLAN Not.},
 keyword = {actor model, data parallelism, synchronization},
 link = {http://doi.acm.org/10.1145/2370036.2145873},
 month = {feb},
 number = {8},
 numpages = {2},
 pages = {317--318},
 publisher = {ACM},
 title = {Synchronization Views for Event-loop Actors},
 volume = {47},
 year = {2012}
}


@inproceedings{Metreveli:2012:CCH:2145816.2145874,
 abstract = {CPHash is a concurrent hash table for multicore processors. CPHash partitions its table across the caches of cores and uses message passing to transfer lookups/inserts to a partition. CPHash's message passing avoids the need for locks, pipelines batches of asynchronous messages, and packs multiple messages into a single cache line transfer. Experiments on a 80-core machine with 2 hardware threads per core show that CPHash has ~1.6x higher throughput than a hash table implemented using fine-grained locks. An analysis shows that CPHash wins because it experiences fewer cache misses and its cache misses are less expensive, because of less contention for the on-chip interconnect and DRAM. CPServer, a key/value cache server using CPHash, achieves ~5% higher throughput than a key/value cache server that uses a hash table with fine-grained locks, but both achieve better throughput and scalability than memcached. The throughput of CPHash and CPServer also scale near-linearly with the number of cores.},
 acmid = {2145874},
 address = {New York, NY, USA},
 author = {Metreveli, Zviad and Zeldovich, Nickolai and Kaashoek, M. Frans},
 booktitle = {Proceedings of the 17th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
 doi = {10.1145/2145816.2145874},
 isbn = {978-1-4503-1160-1},
 keyword = {message-passing},
 link = {http://doi.acm.org/10.1145/2145816.2145874},
 location = {New Orleans, Louisiana, USA},
 numpages = {2},
 pages = {319--320},
 publisher = {ACM},
 series = {PPoPP '12},
 title = {CPHASH: A Cache-partitioned Hash Table},
 year = {2012}
}


@inproceedings{Crain:2012:SBS:2145816.2145837,
 abstract = {We introduce the first binary search tree algorithm designed for speculative executions. Prior to this work, tree structures were mainly designed for their pessimistic (non-speculative) accesses to have a bounded complexity. Researchers tried to evaluate transactional memory using such tree structures whose prominent example is the red-black tree library developed by Oracle Labs that is part of multiple benchmark distributions. Although well-engineered, such structures remain badly suited for speculative accesses, whose step complexity might raise dramatically with contention. We show that our speculation-friendly tree outperforms the existing transaction-based version of the AVL and the red-black trees. Its key novelty stems from the decoupling of update operations: they are split into one transaction that modifies the abstraction state and multiple ones that restructure its tree implementation in the background. In particular, the speculation-friendly tree is shown correct, reusable and it speeds up a transaction-based travel reservation application by up to 3.5x.},
 acmid = {2145837},
 address = {New York, NY, USA},
 author = {Crain, Tyler and Gramoli, Vincent and Raynal, Michel},
 booktitle = {Proceedings of the 17th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
 doi = {10.1145/2145816.2145837},
 isbn = {978-1-4503-1160-1},
 keyword = {background rebalancing, optimistic concurrency, transactional memory},
 link = {http://doi.acm.org/10.1145/2145816.2145837},
 location = {New Orleans, Louisiana, USA},
 numpages = {10},
 pages = {161--170},
 publisher = {ACM},
 series = {PPoPP '12},
 title = {A Speculation-friendly Binary Search Tree},
 year = {2012}
}


@inproceedings{Nugteren:2012:BHM:2145816.2145859,
 abstract = {Multi-core and many-core were already major trends for the past six years, and are expected to continue for the next decades. With these trends of parallel computing, it becomes increasingly difficult to decide on which architecture to run a given application. In this work, we use an algorithm classification to predict performance prior to algorithm implementation. For this purpose, we modify the roofline model to include class information. In this way, we enable architectural choice through performance prediction prior to the development of architecture specific code. The new model, the boat hull model, is demonstrated using a GPU as a target architecture. We show for 6 example algorithms that performance is predicted accurately without requiring code to be available.},
 acmid = {2145859},
 address = {New York, NY, USA},
 author = {Nugteren, Cedric and Corporaal, Henk},
 booktitle = {Proceedings of the 17th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
 doi = {10.1145/2145816.2145859},
 isbn = {978-1-4503-1160-1},
 keyword = {many-core accelerators, parallel computing, performance prediction, the roofline model},
 link = {http://doi.acm.org/10.1145/2145816.2145859},
 location = {New Orleans, Louisiana, USA},
 numpages = {2},
 pages = {291--292},
 publisher = {ACM},
 series = {PPoPP '12},
 title = {The Boat Hull Model: Adapting the Roofline Model to Enable Performance Prediction for Parallel Computing},
 year = {2012}
}


@inproceedings{Huynh:2012:SFM:2145816.2145818,
 abstract = {Graphics processing units leverage on a large array of parallel processing cores to boost the performance of a specific streaming computation pattern frequently found in graphics applications. Unfortunately, while many other general purpose applications do exhibit the required streaming behavior, they also possess unfavorable data layout and poor computation-to-communication ratios that penalize any straight-forward execution on the GPU. In this paper we describe an efficient and scalable code generation framework that can map general purpose streaming applications onto a multi-GPU system. This framework spans the entire core and memory hierarchy exposed by the multi-GPU system. Several key features in our framework ensure the scalability required by complex streaming applications. First, we propose an efficient stream graph partitioning algorithm that partitions the complex application to achieve the best performance under a given shared memory constraint. Next, the resulting partitions are mapped to multiple GPUs using an efficient architecture-driven strategy. The mapping balances the workload while considering the communication overhead. Finally, a highly effective pipeline execution is employed for the execution of the partitions on the multi-GPU system. The framework has been implemented as a back-end of the StreamIt programming language compiler. Our comprehensive experiments show its scalability and significant performance speedup compared with a previous state-of-the-art solution.},
 acmid = {2145818},
 address = {New York, NY, USA},
 author = {Huynh, Huynh Phung and Hagiescu, Andrei and Wong, Weng-Fai and Goh, Rick Siow Mong},
 booktitle = {Proceedings of the 17th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
 doi = {10.1145/2145816.2145818},
 isbn = {978-1-4503-1160-1},
 keyword = {multi-GPU, scalable, streaming, streamit},
 link = {http://doi.acm.org/10.1145/2145816.2145818},
 location = {New Orleans, Louisiana, USA},
 numpages = {10},
 pages = {1--10},
 publisher = {ACM},
 series = {PPoPP '12},
 title = {Scalable Framework for Mapping Streaming Applications Onto multi-GPU Systems},
 year = {2012}
}


@inproceedings{Chen:2012:PUA:2145816.2145838,
 abstract = {This paper introduces a programming interface called PARRAY (or Parallelizing ARRAYs) that supports system-level succinct programming for heterogeneous parallel systems like GPU clusters. The current practice of software development requires combining several low-level libraries like Pthread, OpenMP, CUDA and MPI. Achieving productivity and portability is hard with different numbers and models of GPUs. PARRAY extends mainstream C programming with novel array types of distinct features: 1) the dimensions of an array type are nested in a tree, conceptually reflecting the memory hierarchy; 2) the definition of an array type may contain references to other array types, allowing sophisticated array types to be created for parallelization; 3) threads also form arrays that allow programming in a Single-Program-Multiple-Codeblock (SPMC) style to unify various sophisticated communication patterns. This leads to shorter, more portable and maintainable parallel codes, while the programmer still has control over performance-related features necessary for deep manual optimization. Although the source-to-source code generator only faithfully generates low-level library calls according to the type information, higher-level programming and automatic performance optimization are still possible through building libraries of sub-programs on top of PARRAY. The case study on cluster FFT illustrates a simple 30-line code that 2x outperforms Intel Cluster MKL on the Tianhe-1A system with 7168 Fermi GPUs and 14336 CPUs.},
 acmid = {2145838},
 address = {New York, NY, USA},
 author = {Chen, Yifeng and Cui, Xiang and Mei, Hong},
 booktitle = {Proceedings of the 17th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
 doi = {10.1145/2145816.2145838},
 isbn = {978-1-4503-1160-1},
 keyword = {array representation, gpu clusters, heterogeneous parallelism, parallel programming},
 link = {http://doi.acm.org/10.1145/2145816.2145838},
 location = {New Orleans, Louisiana, USA},
 numpages = {10},
 pages = {171--180},
 publisher = {ACM},
 series = {PPoPP '12},
 title = {PARRAY: A Unifying Array Representation for Heterogeneous Parallelism},
 year = {2012}
}


@inproceedings{Gong:2012:OCN:2145816.2145862,
 abstract = {Cloud computing enables users to perform distributed computing tasks on many virtual machines, without owning a physical cluster. Recently, various distributed computing tasks such as scientific applications are being moved from supercomputers and private clusters to public clouds. Message passing interface (MPI) is a key and common component in distributed computing tasks. The virtualized computing environment of the public cloud hides the network topology information from the users, and existing topology-aware optimizations for MPI are no longer feasible in the cloud environment. We propose a network performance aware MPI library named CMPI. CMPI embraces a new model for capturing the network performance among different virtual machines in the cloud. Based on the network performance model, we develop novel network performance aware algorithms for communication operations. This poster gives an overview of CMPI design, and presents some preliminary results on collective operations such as broadcast.We demonstrate the effectiveness of our network performance aware optimizations on Amazon EC2.},
 acmid = {2145862},
 address = {New York, NY, USA},
 author = {Gong, Yifan and He, Bingsheng and Zhong, Jianlong},
 booktitle = {Proceedings of the 17th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
 doi = {10.1145/2145816.2145862},
 isbn = {978-1-4503-1160-1},
 keyword = {MPI, cloud computing, collective communication operations, scientific computing},
 link = {http://doi.acm.org/10.1145/2145816.2145862},
 location = {New Orleans, Louisiana, USA},
 numpages = {2},
 pages = {297--298},
 publisher = {ACM},
 series = {PPoPP '12},
 title = {An Overview of CMPI: Network Performance Aware MPI in the Cloud},
 year = {2012}
}


@article{Nobari:2012:SPM:2370036.2145842,
 abstract = {The proliferation of data in graph form calls for the development of scalable graph algorithms that exploit parallel processing environments. One such problem is the computation of a graph's minimum spanning forest (MSF). Past research has proposed several parallel algorithms for this problem, yet none of them scales to large, high-density graphs. In this paper we propose a novel, scalable, parallel MSF algorithm for undirected weighted graphs. Our algorithm leverages Prim's algorithm in a parallel fashion, concurrently expanding several subsets of the computed MSF. Our effort focuses on minimizing the communication among different processors without constraining the local growth of a processor's computed subtree. In effect, we achieve a scalability that previous approaches lacked. We implement our algorithm in CUDA, running on a GPU and study its performance using real and synthetic, sparse as well as dense, structured and unstructured graph data. Our experimental study demonstrates that our algorithm outperforms the previous state-of-the-art GPU-based MSF algorithm, while being several orders of magnitude faster than sequential CPU-based algorithms.},
 acmid = {2145842},
 address = {New York, NY, USA},
 author = {Nobari, Sadegh and Cao, Thanh-Tung and Karras, Panagiotis and Bressan, St{\'e}phane},
 doi = {10.1145/2370036.2145842},
 issn = {0362-1340},
 issue_date = {August 2012},
 journal = {SIGPLAN Not.},
 keyword = {GPU, minimum spanning forest, parallel graph algorithms},
 link = {http://doi.acm.org/10.1145/2370036.2145842},
 month = {feb},
 number = {8},
 numpages = {10},
 pages = {205--214},
 publisher = {ACM},
 title = {Scalable Parallel Minimum Spanning Forest Computation},
 volume = {47},
 year = {2012}
}


@inproceedings{Kjolstad:2012:ADG:2145816.2145878,
 abstract = {Many high performance applications spend considerable time packing noncontiguous data into contiguous communication buffers. MPI Datatypes provide an alternative by describing noncontiguous data layouts. This allows sophisticated hardware to retrieve data directly from application data structures. However, packing codes in real-world applications are often complex and specifying equivalent datatypes is difficult, time-consuming, and error prone. We present an algorithm that automates the transformation. We have implemented the algorithm in a tool that transforms packing code to MPI Datatypes, and evaluated it by transforming 90 packing codes from the NAS Parallel Benchmarks. The transformation allows easy porting of applications to new machines that benefit from datatypes, thus improving programmer productivity.},
 acmid = {2145878},
 address = {New York, NY, USA},
 author = {Kjolstad, Fredrik and Hoefler, Torsten and Snir, Marc},
 booktitle = {Proceedings of the 17th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
 doi = {10.1145/2145816.2145878},
 isbn = {978-1-4503-1160-1},
 keyword = {MPI, compiler technique, datatypes, refactoring},
 link = {http://doi.acm.org/10.1145/2145816.2145878},
 location = {New Orleans, Louisiana, USA},
 numpages = {2},
 pages = {327--328},
 publisher = {ACM},
 series = {PPoPP '12},
 title = {Automatic Datatype Generation and Optimization},
 year = {2012}
}


@article{Dice:2012:LCG:2370036.2145848,
 abstract = {Multicore machines are quickly shifting to NUMA and CC-NUMA architectures, making scalable NUMA-aware locking algorithms, ones that take into account the machines' non-uniform memory and caching hierarchy, ever more important. This paper presents lock cohorting, a general new technique for designing NUMA-aware locks that is as simple as it is powerful. Lock cohorting allows one to transform any spin-lock algorithm, with minimal non-intrusive changes, into scalable NUMA-aware spin-locks. Our new cohorting technique allows us to easily create NUMA-aware versions of the TATAS-Backoff, CLH, MCS, and ticket locks, to name a few. Moreover, it allows us to derive a CLH-based cohort abortable lock, the first NUMA-aware queue lock to support abortability. We empirically compared the performance of cohort locks with prior NUMA-aware and classic NUMA-oblivious locks on a synthetic micro-benchmark, a real world key-value store application memcached, as well as the libc memory allocator. Our results demonstrate that cohort locks perform as well or better than known locks when the load is low and significantly out-perform them as the load increases.},
 acmid = {2145848},
 address = {New York, NY, USA},
 author = {Dice, David and Marathe, Virendra J. and Shavit, Nir},
 doi = {10.1145/2370036.2145848},
 issn = {0362-1340},
 issue_date = {August 2012},
 journal = {SIGPLAN Not.},
 keyword = {NUMA, hierarchical locks, spin locks},
 link = {http://doi.acm.org/10.1145/2370036.2145848},
 month = {feb},
 number = {8},
 numpages = {10},
 pages = {247--256},
 publisher = {ACM},
 title = {Lock Cohorting: A General Technique for Designing NUMA Locks},
 volume = {47},
 year = {2012}
}


@inproceedings{Buhler:2012:EDA:2145816.2145846,
 abstract = {Parallel streaming computations have been studied extensively, and many languages, libraries, and systems have been designed to support this model of computation. In particular, we consider acyclic streaming computations in which individual nodes can choose to filter, or discard, some of their inputs in a data-dependent manner. In these applications, if the channels between nodes have finite buffers, the computation can deadlock. One method of deadlock avoidance is to augment the data streams between nodes with occasional dummy messages; however, for general DAG topologies, no polynomial time algorithm is known to compute the intervals at which dummy messages must be sent to avoid deadlock. In this paper, we show that deadlock avoidance for streaming computations with filtering can be performed efficiently for a large class of DAG topologies. We first present a new method where each dummy message is tagged with a destination, so as to reduce the number of dummy messages sent over the network. We then give efficient algorithms for dummy interval computation in series-parallel DAGs. We finally generalize our results to a larger graph family, which we call the CS4 DAGs, in which every undirected Cycle is Single-Source and Single-Sink (CS4). Our results show that, for a large set of application topologies that are both intuitively useful and formalizable, the streaming model with filtering can be implemented safely with reasonable overhead.},
 acmid = {2145846},
 address = {New York, NY, USA},
 author = {Buhler, Jeremy D. and Agrawal, Kunal and Li, Peng and Chamberlain, Roger D.},
 booktitle = {Proceedings of the 17th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
 doi = {10.1145/2145816.2145846},
 isbn = {978-1-4503-1160-1},
 keyword = {deadlock avoidance, graph theory, streaming computation},
 link = {http://doi.acm.org/10.1145/2145816.2145846},
 location = {New Orleans, Louisiana, USA},
 numpages = {12},
 pages = {235--246},
 publisher = {ACM},
 series = {PPoPP '12},
 title = {Efficient Deadlock Avoidance for Streaming Computation with Filtering},
 year = {2012}
}


@article{Kamil:2012:PPP:2370036.2145865,
 abstract = {Domain-expert productivity programmers desire scalable application performance, but usually must rely on efficiency programmers who are experts in explicit parallel programming to achieve it. Since such programmers are rare, to maximize reuse of their work we propose encapsulating their strategies in mini-compilers for domain-specific embedded languages (DSELs) glued together by a common high-level host language familiar to productivity programmers. The nontrivial applications that use these DSELs perform up to 98% of peak attainable performance, and comparable to or better than existing hand-coded implementations. Our approach is unique in that each mini-compiler not only performs conventional compiler transformations and optimizations, but includes imperative procedural code that captures an efficiency expert's strategy for mapping a narrow domain onto a specific type of hardware. The result is source- and performance-portability for productivity programmers and parallel performance that rivals that of hand-coded efficiency-language implementations of the same applications. We describe a framework that supports our methodology and five implemented DSELs supporting common computation kernels. Our results demonstrate that for several interesting classes of problems, efficiency-level parallel performance can be achieved by packaging efficiency programmers' expertise in a reusable framework that is easy to use for both productivity programmers and efficiency programmers.},
 acmid = {2145865},
 address = {New York, NY, USA},
 author = {Kamil, Shoaib and Coetzee, Derrick and Beamer, Scott and Cook, Henry and Gonina, Ekaterina and Harper, Jonathan and Morlan, Jeffrey and Fox, Armando},
 doi = {10.1145/2370036.2145865},
 issn = {0362-1340},
 issue_date = {August 2012},
 journal = {SIGPLAN Not.},
 keyword = {SEJITS, asp, domain-specific languages},
 link = {http://doi.acm.org/10.1145/2370036.2145865},
 month = {feb},
 number = {8},
 numpages = {2},
 pages = {303--304},
 publisher = {ACM},
 title = {Portable Parallel Performance from Sequential, Productive, Embedded Domain-specific Languages},
 volume = {47},
 year = {2012}
}


@proceedings{Ramanujam:2012:2145816,
 abstract = {It is our great pleasure to welcome you to the 17th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming (PPoPP'12). PPoPP continues its tradition of serving as a leading forum for research in all aspects of parallel software, including theoretical foundations, programming models, algorithms, applications, and systems software. With the ubiquity of parallelism in commodity processors and the increasing use of GPUs for high-performance computing, the effective use of parallel systems is being recognized as one of the most challenging problems faced today. PPoPP'12 received 173 complete paper submissions. In addition to the 25 program committee members, 75 members of the external review committee provided reviews for the papers. Two rounds of reviewing were conducted, with at least three reviews being obtained in the first round, with additional reviews being obtained in the second round for papers where needed. After extensive discussions at an in-person two-day program committee meeting in November 2011, 26 full papers were selected for presentation at the conference. PPoPP'12 continues the tradition of poster presentations of high quality submissions that could not be accepted as full papers. This year's conference features 32 poster presentations over two sessions. PPoPP'12 is again co-located this year with the International Symposium on High-Performance Computer Architecture (HPCA), allowing attendees of one conference the option of attending talks at the other. We feature two joint HPCA/PPOPP keynote presentations. Sanjeev Kumar from Facebook will present a keynote on "Social Networking at Scale," and Keshav Pingali from the University of Texas at Austin will present a keynote entitled "Parallel Programming Needs Data-Centric Foundations."},
 address = {New York, NY, USA},
 isbn = {978-1-4503-1160-1},
 location = {New Orleans, Louisiana, USA},
 note = {551121},
 publisher = {ACM},
 title = {PPoPP '12: Proceedings of the 17th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
 year = {2012}
}


@article{Wernsing:2012:RHA:2370036.2145875,
 abstract = {High-performance computing systems increasingly combine multi-core processors and heterogeneous resources such as graphics-processing units and field-programmable gate arrays. However, significant application design complexity for such systems has often led to untapped performance potential. Application designers targeting such systems currently must determine how to parallelize computation, create device-specialized implementations for each heterogeneous resource, and determine how to partition work for each resource. In this paper, we present the RACECAR heuristic to automate the optimization of applications for multi-core heterogeneous systems by automatically exploring implementation alternatives that include different algorithms, parallelization strategies, and work distributions. Experimental results show RACECAR-specialized implementations achieve speedups up to 117x and average 11x compared to a single CPU thread when parallelizing computation across multiple cores, graphics-processing units, and field-programmable gate arrays.},
 acmid = {2145875},
 address = {New York, NY, USA},
 author = {Wernsing, John R. and Stitt, Greg},
 doi = {10.1145/2370036.2145875},
 issn = {0362-1340},
 issue_date = {August 2012},
 journal = {SIGPLAN Not.},
 keyword = {elastic computing, heterogeneous, optimization, racecar, speedup},
 link = {http://doi.acm.org/10.1145/2370036.2145875},
 month = {feb},
 number = {8},
 numpages = {2},
 pages = {321--322},
 publisher = {ACM},
 title = {RACECAR: A Heuristic for Automatic Function Specialization on Multi-core Heterogeneous Systems},
 volume = {47},
 year = {2012}
}


@article{Noll:2012:IDO:2370036.2145877,
 abstract = {Object-oriented programming languages like Java provide only low-level constructs (e.g., starting a thread) to describe concurrency. High-level abstractions (e.g., thread pools) are merely provided as a library. As a result, a compiler is not aware of the high-level semantics of a parallel library and therefore misses important optimization opportunities. This paper presents a simple source language extension based on which a compiler is provided with the opportunity to perform new optimizations that are particularly effective for parallel code.},
 acmid = {2145877},
 address = {New York, NY, USA},
 author = {Noll, Albert and Gross, Thomas R.},
 doi = {10.1145/2370036.2145877},
 issn = {0362-1340},
 issue_date = {August 2012},
 journal = {SIGPLAN Not.},
 keyword = {jit compilation, parallel programming, performance},
 link = {http://doi.acm.org/10.1145/2370036.2145877},
 month = {feb},
 number = {8},
 numpages = {2},
 pages = {325--326},
 publisher = {ACM},
 title = {An Infrastructure for Dynamic Optimization of Parallel Programs},
 volume = {47},
 year = {2012}
}


@inproceedings{Baghsorkhi:2012:EPE:2145816.2145820,
 abstract = {With the emergence of highly multithreaded architectures, performance monitoring techniques face new challenges in efficiently locating sources of performance discrepancies in the program source code. For example, the state-of-the-art performance counters in highly multithreaded graphics processing units (GPUs) report only the overall occurrences of microarchitecture events at the end of program execution. Furthermore, even if supported, any fine-grained sampling of performance counters will distort the actual program behavior and will make the sampled values inaccurate. On the other hand, it is difficult to achieve high resolution performance information at low sampling rates in the presence of thousands of concurrently running threads. In this paper, we present a novel software-based approach for monitoring the memory hierarchy performance in highly multithreaded general-purpose graphics processors. The proposed analysis is based on memory traces collected for snapshots of an application execution. A trace-based memory hierarchy model with a Monte Carlo experimental methodology generates statistical bounds of performance measures without being concerned about the exact inter-thread ordering of individual events but rather studying the behavior of the overall system. The statistical approach overcomes the classical problem of disturbed execution timing due to fine-grained instrumentation. The approach scales well as we deploy an efficient parallel trace collection technique to reduce the trace generation overhead and a simple memory hierarchy model to reduce the simulation time. The proposed scheme also keeps track of individual memory operations in the source code and can quantify their efficiency with respect to the memory system. A cross-validation of our results shows close agreement with the values read from the hardware performance counters on an NVIDIA Tesla C2050 GPU. Based on the high resolution profile data produced by our model we optimized memory accesses in the sparse matrix vector multiply kernel and achieved speedups ranging from 2.4 to 14.8 depending on the characteristics of the input matrices.},
 acmid = {2145820},
 address = {New York, NY, USA},
 author = {Baghsorkhi, Sara S. and Gelado, Isaac and Delahaye, Matthieu and Hwu, Wen-mei W.},
 booktitle = {Proceedings of the 17th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
 doi = {10.1145/2145816.2145820},
 isbn = {978-1-4503-1160-1},
 keyword = {gpu, memory hierarchy, performance evaluation},
 link = {http://doi.acm.org/10.1145/2145816.2145820},
 location = {New Orleans, Louisiana, USA},
 numpages = {12},
 pages = {23--34},
 publisher = {ACM},
 series = {PPoPP '12},
 title = {Efficient Performance Evaluation of Memory Hierarchy for Highly Multithreaded Graphics Processors},
 year = {2012}
}


@inproceedings{Mittal:2012:CAS:2145816.2145872,
 abstract = {Collective communication over a group of processors is an integral and time consuming component in many HPC applications. Many modern day supercomputers are based on torus interconnects. On such systems, for an irregular communicator comprising of a subset of processors, the algorithms developed so far are not contention free in general and hence non-optimal. In this paper, we present a novel contention-free algorithm to perform collective operations over a subset of processors in a torus network. We also extend previous work on regular communicators to handle special cases of irregular communicators that occur frequently in parallel scientific applications. For the generic case where multiple node disjoint sub-communicators communicate simultaneously in a loosely synchronous fashion, we propose a novel cooperative approach to route the data for individual sub-communicators without contention. Empirical results demonstrate that our algorithms outperform the optimized MPI collective implementation on IBM's Blue Gene/P supercomputer for large data sizes and random node distributions.},
 acmid = {2145872},
 address = {New York, NY, USA},
 author = {Mittal, Anshul and Jain, Nikhil and George, Thomas and Sabharwal, Yogish and Kumar, Sameer},
 booktitle = {Proceedings of the 17th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
 doi = {10.1145/2145816.2145872},
 isbn = {978-1-4503-1160-1},
 keyword = {collectives, sub-communicators, torus},
 link = {http://doi.acm.org/10.1145/2145816.2145872},
 location = {New Orleans, Louisiana, USA},
 numpages = {2},
 pages = {315--316},
 publisher = {ACM},
 series = {PPoPP '12},
 title = {Collective Algorithms for Sub-communicators},
 year = {2012}
}


@article{Burnim:2012:NIN:2370036.2145879,
 abstract = {Nondeterministic Sequential (NDSeq) specifications have been proposed as a means for separating the testing, debugging, and verifying of a program's parallelism correctness and its sequential functional correctness. In this work, we present a technique that, given a few representative executions of a parallel program, combines dynamic data flow analysis and Minimum-Cost Boolean Satisfiability (MinCostSAT) solving for automatically inferring a likely NDSeq specification for the parallel program. For a number of Java benchmarks, our tool NDetermin infers equivalent or stronger NDSeq specifications than those previously written manually.},
 acmid = {2145879},
 address = {New York, NY, USA},
 author = {Burnim, Jacob and Elmas, Tayfun and Necula, George and Sen, Koushik},
 doi = {10.1145/2370036.2145879},
 issn = {0362-1340},
 issue_date = {August 2012},
 journal = {SIGPLAN Not.},
 keyword = {nondeterministic sequential specifications, parallel programs, parallelism correctness},
 link = {http://doi.acm.org/10.1145/2370036.2145879},
 month = {feb},
 number = {8},
 numpages = {2},
 pages = {329--330},
 publisher = {ACM},
 title = {NDetermin: Inferring Nondeterministic Sequential Specifications for Parallelism Correctness},
 volume = {47},
 year = {2012}
}


@article{Bonetta:2012:SSL:2370036.2145829,
 abstract = {There is an urgent need for novel programming abstractions to leverage the parallelism in modern multicore machines. We introduce S, a new domain-specific language targeting the server-side scripting of high-performance RESTful Web services. S promotes an innovative programming model based on explicit (control-flow) and implicit (process-level) parallelism control, allowing the service developer to specify which portions of the control-flow should be executed in parallel. For each service, the choice of the best level of parallelism is left to the runtime system. We assess performance and scalability by implementing two non-trivial composite Web services in S. Experiments show that S-based Web services can handle thousands of concurrent client requests on a modern multicore machine.},
 acmid = {2145829},
 address = {New York, NY, USA},
 author = {Bonetta, Daniele and Peternier, Achille and Pautasso, Cesare and Binder, Walter},
 doi = {10.1145/2370036.2145829},
 issn = {0362-1340},
 issue_date = {August 2012},
 journal = {SIGPLAN Not.},
 keyword = {multicores, restful web services, scalable service execution},
 link = {http://doi.acm.org/10.1145/2370036.2145829},
 month = {feb},
 number = {8},
 numpages = {10},
 pages = {97--106},
 publisher = {ACM},
 title = {S: A Scripting Language for High-performance RESTful Web Services},
 volume = {47},
 year = {2012}
}


@article{Eom:2012:DDP:2370036.2145828,
 abstract = {We present Dynamic Out-of-Order Java (DOJ), a dynamic parallelization approach. In DOJ, a developer annotates code blocks as tasks to decouple these blocks from the parent execution thread. The DOJ compiler then analyzes the code to generate heap examiners that ensure the parallel execution preserves the behavior of the original sequential program. Heap examiners dynamically extract heap dependences between code blocks and determine when it is safe to execute a code block. We have implemented DOJ and evaluated it on twelve benchmarks. We achieved an average compilation speedup of 31.15 times over OoOJava and an average execution speedup of 12.73 times over sequential versions of the benchmarks.},
 acmid = {2145828},
 address = {New York, NY, USA},
 author = {Eom, Yong hun and Yang, Stephen and Jenista, James C. and Demsky, Brian},
 doi = {10.1145/2370036.2145828},
 issn = {0362-1340},
 issue_date = {August 2012},
 journal = {SIGPLAN Not.},
 keyword = {dynamic analysis, heap analysis, object-oriented analysis, parallel programming, parallelization},
 link = {http://doi.acm.org/10.1145/2370036.2145828},
 month = {feb},
 number = {8},
 numpages = {12},
 pages = {85--96},
 publisher = {ACM},
 title = {DOJ: Dynamically Parallelizing Object-oriented Programs},
 volume = {47},
 year = {2012}
}


@article{Zhong:2012:OMS:2370036.2145855,
 abstract = {Graphs are the de facto data structures for many applications, and efficient graph processing is a must for the application performance. GPUs have an order of magnitude higher computational power and memory bandwidth compared to CPUs and have been adopted to accelerate several common graph algorithms. However, it is difficult to write correct and efficient GPU programs and even more difficult for graph processing due to the irregularities of graph structures. To address those difficulties, we propose a programming framework named Medusa to simplify graph processing on GPUs. Medusa offers a small set of APIs, based on which developers can define their application logics by writing sequential code without awareness of GPU architectures. The Medusa runtime system automatically executes the developer defined APIs in parallel on the GPU, with a series of graph-centric optimizations. This poster gives an overview of Medusa, and presents some preliminary results.},
 acmid = {2145855},
 address = {New York, NY, USA},
 author = {Zhong, Jianlong and He, Bingsheng},
 doi = {10.1145/2370036.2145855},
 issn = {0362-1340},
 issue_date = {August 2012},
 journal = {SIGPLAN Not.},
 keyword = {GPGPU, GPU programming, graph processing, runtime framework},
 link = {http://doi.acm.org/10.1145/2370036.2145855},
 month = {feb},
 number = {8},
 numpages = {2},
 pages = {283--284},
 publisher = {ACM},
 title = {An Overview of Medusa: Simplified Graph Processing on GPUs},
 volume = {47},
 year = {2012}
}


@article{Merrill:2012:SGG:2370036.2145832,
 abstract = {Breadth-first search (BFS) is a core primitive for graph traversal and a basis for many higher-level graph analysis algorithms. It is also representative of a class of parallel computations whose memory accesses and work distribution are both irregular and data-dependent. Recent work has demonstrated the plausibility of GPU sparse graph traversal, but has tended to focus on asymptotically inefficient algorithms that perform poorly on graphs with non-trivial diameter. We present a BFS parallelization focused on fine-grained task management constructed from efficient prefix sum that achieves an asymptotically optimal O(|V|+|E|) work complexity. Our implementation delivers excellent performance on diverse graphs, achieving traversal rates in excess of 3.3 billion and 8.3 billion traversed edges per second using single and quad-GPU configurations, respectively. This level of performance is several times faster than state-of-the-art implementations both CPU and GPU platforms.},
 acmid = {2145832},
 address = {New York, NY, USA},
 author = {Merrill, Duane and Garland, Michael and Grimshaw, Andrew},
 doi = {10.1145/2370036.2145832},
 issn = {0362-1340},
 issue_date = {August 2012},
 journal = {SIGPLAN Not.},
 keyword = {GPU, breadth-first search, cooperative allocation, graph algorithms, graph traversal, parallel algorithms, prefix sum, sparse graph},
 link = {http://doi.acm.org/10.1145/2370036.2145832},
 month = {feb},
 number = {8},
 numpages = {12},
 pages = {117--128},
 publisher = {ACM},
 title = {Scalable GPU Graph Traversal},
 volume = {47},
 year = {2012}
}


@article{Liu:2012:LAP:2370036.2145876,
 abstract = {
                  An abstract is not available.
              },
 acmid = {2145876},
 address = {New York, NY, USA},
 author = {Liu, Yujie and Spear, Michael},
 doi = {10.1145/2370036.2145876},
 issn = {0362-1340},
 issue_date = {August 2012},
 journal = {SIGPLAN Not.},
 keyword = {heap, linearizability, lock-free, mound, priority queue, randomization, synchronization},
 link = {http://doi.acm.org/10.1145/2370036.2145876},
 month = {feb},
 number = {8},
 numpages = {2},
 pages = {323--324},
 publisher = {ACM},
 title = {A Lock-free, Array-based Priority Queue},
 volume = {47},
 year = {2012}
}


@article{Andersch:2012:PPE:2370036.2145854,
 abstract = {In this paper, we evaluate the performance and usability of the parallel programming model OpenMP Superscalar (OmpSs), apply it to 10 different benchmarks and compare its performance with corresponding POSIX threads implementations.},
 acmid = {2145854},
 address = {New York, NY, USA},
 author = {Andersch, Michael and Chi, Chi Ching and Juurlink, Ben},
 doi = {10.1145/2370036.2145854},
 issn = {0362-1340},
 issue_date = {August 2012},
 journal = {SIGPLAN Not.},
 keyword = {OmpSs, OpenMP Superscalar, consumer, embedded},
 link = {http://doi.acm.org/10.1145/2370036.2145854},
 month = {feb},
 number = {8},
 numpages = {2},
 pages = {281--282},
 publisher = {ACM},
 title = {Programming Parallel Embedded and Consumer Applications in OpenMP Superscalar},
 volume = {47},
 year = {2012}
}


@proceedings{Cascaval:2011:1941553,
 abstract = {It is our great pleasure to welcome you to the 16th ACM Symposium on Principles and Practice of Parallel Programming -- PPoPP'11. PPoPP continues to be the premiere forum where researchers present their work on all aspects of parallelism and concurrency: algorithms and applications, programming models, languages, and environments, system software and runtime systems, and theoretical foundational work. As our industry continues to move toward parallel systems, from large-scale supercomputers to multicore mobile devices, such as smart phones and tablets, research work on concurrency is needed to support developers at all levels of the execution stack. This year we received 165 completed submissions, close to the conference record high. Because of the large number of submissions, in addition to the 25 program committee members, we formed an external review committee and invited 30 experts in various areas to help out in reviewing papers. In addition, committee members also invited external reviewers, to provide our submitting authors with a total of more than 630 reviews. There was a paper bidding process to match up the expertise of the reviewers and the reviewed papers before papers were assigned to reviewers. An on-line discussion period was conducted among all reviewers of each paper to smooth out the differences among the reviewers before the program committee meeting was held. For the final program, the program committee selected 26 full papers and 13 posters for the program. They span a wide spectrum of areas in parallel programming. A few years back, the PPoPP Steering Committee recognized the importance of broadening the conference experience through interaction with hardware architects designing parallel systems. This year we continue the collocation with the International Symposium on High-Performance Computer Architecture. Beside encouraging cross-participation in any of the two conference sessions, PPoPP and HPCA will share two keynotes from leading researchers in the area of parallel programming: Jim Larus (Microsoft Research) and Kathryn McKinley (UT Austin), and all the tutorials and workshops.},
 address = {New York, NY, USA},
 isbn = {978-1-4503-0119-0},
 location = {San Antonio, TX, USA},
 note = {551111},
 publisher = {ACM},
 title = {PPoPP '11: Proceedings of the 16th ACM Symposium on Principles and Practice of Parallel Programming},
 year = {2011}
}


@inproceedings{Kwon:2012:HAO:2145816.2145827,
 abstract = {We present the first fully automated compiler-runtime system that successfully translates and executes OpenMP shared-address-space programs on laboratory-size clusters, for the complete set of regular, repetitive applications in the NAS Parallel Benchmarks. We introduce a hybrid compiler-runtime translation scheme. Compared to previous work, this scheme features a new runtime data flow analysis and new compiler techniques for improving data affinity and reducing communication costs. We present and discuss the performance of our translated programs, and compare them with the performance of the MPI, HPF and UPC versions of the benchmarks. The results show that our translated programs achieve 75% of the hand-coded MPI programs, on average.},
 acmid = {2145827},
 address = {New York, NY, USA},
 author = {Kwon, Okwan and Jubair, Fahed and Eigenmann, Rudolf and Midkiff, Samuel},
 booktitle = {Proceedings of the 17th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
 doi = {10.1145/2145816.2145827},
 isbn = {978-1-4503-1160-1},
 keyword = {MPI, OpenMP, hybrid, optimization, runtime data flow analysis, runtime environment, translator},
 link = {http://doi.acm.org/10.1145/2145816.2145827},
 location = {New Orleans, Louisiana, USA},
 numpages = {10},
 pages = {75--84},
 publisher = {ACM},
 series = {PPoPP '12},
 title = {A Hybrid Approach of OpenMP for Clusters},
 year = {2012}
}


@inproceedings{Alias:2012:ORA:2145816.2145856,
 abstract = {In the context of the high-level synthesis (HLS) of regular kernels offloaded to FPGA and communicating with an external DDR memory, we show how to automatically generate adequate communicating processes for optimizing the transfer of remote data. This requires a generalized form of communication coalescing where data can be transferred from the external memory even when this memory is not fully up-to-date. Experiments with Altera HLS tools demonstrate that this automatization, based on advanced polyhedral code analysis and code generation techniques, can be used to efficiently map C kernels to FPGA, by generating, entirely at C level, all the necessary glue (the communication processes), which is compiled with the same HLS tool as for the computation kernel.},
 acmid = {2145856},
 address = {New York, NY, USA},
 author = {Alias, Christophe and Darte, Alain and Plesco, Alexandru},
 booktitle = {Proceedings of the 17th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
 doi = {10.1145/2145816.2145856},
 isbn = {978-1-4503-1160-1},
 keyword = {DDR memory, FPGA, HLS, communication coalescing, pipelined processes, polyhedral optimizations},
 link = {http://doi.acm.org/10.1145/2145816.2145856},
 location = {New Orleans, Louisiana, USA},
 numpages = {2},
 pages = {285--286},
 publisher = {ACM},
 series = {PPoPP '12},
 title = {Optimizing Remote Accesses for Offloaded Kernels: Application to High-level Synthesis for FPGA},
 year = {2012}
}


@inproceedings{Feng:2012:SPG:2145816.2145860,
 abstract = {This paper overviews the first speculative parallelization technique for GPUs that can exploit parallelism in loops even in the presence of dynamic irregularities that may give rise to cross-iteration dependences. The execution of a speculatively parallelized loop consists of five phases: scheduling, computation, misspeculation check, result committing, and misspeculation recovery. We perform misspeculation check on the GPU to minimize its cost. We optimize the procedures of result committing and misspeculation recovery to reduce the result copying and recovery overhead. Finally, the scheduling policies are designed according to the types of cross-iteration dependences to reduce the misspeculation rate. Our preliminary evaluation was conducted on an nVidia Tesla C1060 hosted in an Intel(R) Xeon(R) E5540 machine. We use three benchmarks of which two contain irregular memory accesses and one contain irregular control flows that can give rise to cross-iteration dependences. Our implementation achieves 3.6x-13.8x speedups for loops in these benchmarks.},
 acmid = {2145860},
 address = {New York, NY, USA},
 author = {Feng, Min and Gupta, Rajiv and Bhuyan, Laxmi N.},
 booktitle = {Proceedings of the 17th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
 doi = {10.1145/2145816.2145860},
 isbn = {978-1-4503-1160-1},
 keyword = {GPGPUs, speculative parallelization},
 link = {http://doi.acm.org/10.1145/2145816.2145860},
 location = {New Orleans, Louisiana, USA},
 numpages = {2},
 pages = {293--294},
 publisher = {ACM},
 series = {PPoPP '12},
 title = {Speculative Parallelization on GPGPUs},
 year = {2012}
}


@article{Tzenakis:2012:BBD:2370036.2145864,
 abstract = {
                  An abstract is not available.
              },
 acmid = {2145864},
 address = {New York, NY, USA},
 author = {Tzenakis, George and Papatriantafyllou, Angelos and Kesapides, John and Pratikakis, Polyvios and Vandierendonck, Hans and Nikolopoulos, Dimitrios S.},
 doi = {10.1145/2370036.2145864},
 issn = {0362-1340},
 issue_date = {August 2012},
 journal = {SIGPLAN Not.},
 keyword = {compilers and runtime systems, middleware for parallel systems, synchronization and concurrency control, task-parallel libraries},
 link = {http://doi.acm.org/10.1145/2370036.2145864},
 month = {feb},
 number = {8},
 numpages = {2},
 pages = {301--302},
 publisher = {ACM},
 title = {BDDT:: Block-level Dynamic Dependence Analysisfor Deterministic Task-based Parallelism},
 volume = {47},
 year = {2012}
}


@inproceedings{Sack:2012:FTC:2145816.2145823,
 abstract = {Known algorithms for two important collective communication operations, allgather and reduce-scatter, are minimal-communication algorithms; no process sends or receives more than the minimum amount of data. This, combined with the data-ordering semantics of the operations, limits the flexibility and performance of these algorithms. Our novel non-minimal, topology-aware algorithms deliver far better performance with the addition of a very small amount of redundant communication. We develop novel algorithms for Clos networks and single or multi-ported torus networks. Tests on a 32k-node BlueGene/P result in allgather speedups of up to 6x and reduce-scatter speedups of over 11x compared to the native IBM algorithm. Broadcast, reduce, and allreduce can be composed of allgather or reduce-scatter and other collective operations; our techniques also improve the performance of these algorithms.},
 acmid = {2145823},
 address = {New York, NY, USA},
 author = {Sack, Paul and Gropp, William},
 booktitle = {Proceedings of the 17th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
 doi = {10.1145/2145816.2145823},
 isbn = {978-1-4503-1160-1},
 keyword = {collective-communication algorithms},
 link = {http://doi.acm.org/10.1145/2145816.2145823},
 location = {New Orleans, Louisiana, USA},
 numpages = {10},
 pages = {45--54},
 publisher = {ACM},
 series = {PPoPP '12},
 title = {Faster Topology-aware Collective Algorithms Through Non-minimal Communication},
 year = {2012}
}


@article{LeiBa:2012:ECL:2370036.2145825,
 abstract = {SIMD instructions are common in CPUs for years now. Using these instructions effectively requires not only vectorization of code, but also modifications to the data layout. However, automatic vectorization techniques are often not powerful enough and suffer from restricted scope of applicability; hence, programmers often vectorize their programs manually by using intrinsics: compiler-known functions that directly expand to machine instructions. They significantly decrease programmer productivity by enforcing a very error-prone and hard-to-read assembly-like programming style. Furthermore, intrinsics are not portable because they are tied to a specific instruction set. In this paper, we show how a C-like language can be extended to allow for portable and efficient SIMD programming. Our extension puts the programmer in total control over where and how control-flow vectorization is triggered. We present a type system and a formal semantics of our extension and prove the soundness of the type system. Using our prototype implementation IVL that targets Intel's MIC architecture and SSE instruction set, we show that the generated code is roughly on par with handwritten intrinsic code.},
 acmid = {2145825},
 address = {New York, NY, USA},
 author = {Lei\ssa, Roland and Hack, Sebastian and Wald, Ingo},
 doi = {10.1145/2370036.2145825},
 issn = {0362-1340},
 issue_date = {August 2012},
 journal = {SIGPLAN Not.},
 keyword = {SIMD, SIMT, language theory, parallel programming, polymorphism, semantics, type system, vectorization},
 link = {http://doi.acm.org/10.1145/2370036.2145825},
 month = {feb},
 number = {8},
 numpages = {10},
 pages = {65--74},
 publisher = {ACM},
 title = {Extending a C-like Language for Portable SIMD Programming},
 volume = {47},
 year = {2012}
}


@inproceedings{Dinh:2012:SPD:2145816.2145870,
 abstract = {Traditional debuggers are of limited value for modern scientific codes that manipulate large complex data structures. This paper discusses a novel debug-time assertion, called a "Statistical Assertion", that allows a user to reason about large data structures, and the primitives are parallelised to provide an efficient solution. We present the design and implementation of statistical assertions, and illustrate the debugging technique with a molecular dynamics simulation. We evaluate the performance of the tool on a 12,000 cores Cray XE6.},
 acmid = {2145870},
 address = {New York, NY, USA},
 author = {Dinh, Minh Ngoc and Abramson, David and Jin, Chao and Gontarek, Andrew and Moench, Bob and DeRose, Luiz},
 booktitle = {Proceedings of the 17th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
 doi = {10.1145/2145816.2145870},
 isbn = {978-1-4503-1160-1},
 keyword = {assertion, parallel debugging, statistic},
 link = {http://doi.acm.org/10.1145/2145816.2145870},
 location = {New Orleans, Louisiana, USA},
 numpages = {2},
 pages = {311--312},
 publisher = {ACM},
 series = {PPoPP '12},
 title = {Scalable Parallel Debugging with Statistical Assertions},
 year = {2012}
}


@article{Caniou:2012:PAP:2370036.2145883,
 abstract = {We present a parallel implementation of a constraint-based local search algorithm and investigate its performance results for hard combinatorial optimization problems on two different platforms up to several hundreds of cores. On a variety of classical CSPs benchmarks, speedups are very good for a few tens of cores, and good up to a hundred cores. More challenging problems derived from reallife applications (Costas array) shows even better speedups, nearly optimal up to 256 cores.},
 acmid = {2145883},
 address = {New York, NY, USA},
 author = {Caniou, Yves and Diaz, Daniel and Richoux, Florian and Codognet, Philippe and Abreu, Salvador},
 doi = {10.1145/2370036.2145883},
 issn = {0362-1340},
 issue_date = {August 2012},
 journal = {SIGPLAN Not.},
 keyword = {combinatorial optimization, constraints, local search, meta-heuristics},
 link = {http://doi.acm.org/10.1145/2370036.2145883},
 month = {feb},
 number = {8},
 numpages = {2},
 pages = {337--338},
 publisher = {ACM},
 title = {Performance Analysis of Parallel Constraint-based Local Search},
 volume = {47},
 year = {2012}
}


@proceedings{Nicolau:2013:2442516,
 abstract = {It is our great pleasure to welcome you to the 18th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming (PPoPP'13). PPoPP continues its tradition of serving as a leading forum for research in all aspects of parallel software, including theoretical foundations, programming models, programming languages, compilers, algorithms, applications, and systems software. PPoPP'13 received 146 complete paper submissions. In addition to the 29 program committee members, 85 members of the external review committee plus an additional 33 individuals reviewed these papers. We carried out two rounds of reviewing, with at least three reviews being obtained in the first round and additional reviews being obtained in the second round for papers where needed. After extensive discussions at an in-person program committee meeting in Atlanta, Georgia, USA, 26 full papers were selected for presentation at the conference. The program committee also invited 47 high quality submissions that could not be accepted as full papers as poster presentations and 19 accepted this invitation. PPoPP'13 is excited to take place in the dynamic city of Shenzhen, China, our second visit to Asia in recent times. The conference will be co-located with the International Symposium on High-Performance Computer Architecture (HPCA) and International Symposium on Code Generation and Optimization (CGO), allowing attendees of one conference the option of attending talks at the other.},
 address = {New York, NY, USA},
 isbn = {978-1-4503-1922-5},
 location = {Shenzhen, China},
 note = {551131},
 publisher = {ACM},
 title = {PPoPP '13: Proceedings of the 18th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
 year = {2013}
}


@article{Jiang:2012:OPD:2370036.2145882,
 abstract = {R1 is a domain specific language widely used for data analysis by the statistics community as well as by researchers in finance, biology, social sciences, and many other disciplines. As R programs are linked to input data, the exponential growth of available data makes high-performance computing with R imperative. To ease the process of writing parallel programs in R, code transformation from a sequential program to a parallel version would bring much convenience to R users. In this paper, we present our work in semi-automatic parallelization of R codes with user-added OpenMP-style pragmas. While such pragmas are used at the frontend, we take advantage of multiple parallel backends with different R packages. We provide flexibility for importing parallelism with plug-in components, impose built-in MapReduce for data processing, and also maintain code reusability. We illustrate the advantage of the on-the-fly mechanisms which can lead to significant applications in data-centered parallel computing.},
 acmid = {2145882},
 address = {New York, NY, USA},
 author = {Jiang, Lei and Patel, Pragneshkumar B. and Ostrouchov, George and Jamitzky, Ferdinand},
 doi = {10.1145/2370036.2145882},
 issn = {0362-1340},
 issue_date = {August 2012},
 journal = {SIGPLAN Not.},
 keyword = {MapReduce, automatic code generation, data-centered applications, domain specific language, parallelization},
 link = {http://doi.acm.org/10.1145/2370036.2145882},
 month = {feb},
 number = {8},
 numpages = {2},
 pages = {335--336},
 publisher = {ACM},
 title = {OpenMP-style Parallelism in Data-centered Multicore Computing with R},
 volume = {47},
 year = {2012}
}


@article{Tardieu:2012:WSX:2370036.2145850,
 abstract = {The X10 programming language is intended to ease the programming of scalable concurrent and distributed applications. X10 augments a familiar imperative object-oriented programming model with constructs to support light-weight asynchronous tasks as well as execution across multiple address spaces. A crucial aspect of X10's runtime system is the scheduling of concurrent tasks. Work-stealing schedulers have been shown to efficiently load balance fine-grain divide-and-conquer task-parallel program on SMPs and multicores. But X10 is not limited to shared-memory fork-join parallelism. X10 permits tasks to suspend and synchronize by means of conditional atomic blocks and remote task invocations. In this paper, we demonstrate that work-stealing scheduling principles are applicable to a rich programming language such as X10, achieving performance at scale without compromising expressivity, ease of use, or portability. We design and implement a portable work-stealing execution engine for X10. While this engine is biased toward the efficient execution of fork-join parallelism in shared memory, it handles the full X10 language, especially conditional atomic blocks and distribution. We show that this engine improves the run time of a series of benchmark programs by several orders of magnitude when used in combination with the C++ backend compiler and runtime for X10. It achieves scaling comparable to state-of-the art work-stealing scheduler implementations---the Cilk++ compiler and the Java fork/join framework---despite the dramatic increase in generality.},
 acmid = {2145850},
 address = {New York, NY, USA},
 author = {Tardieu, Olivier and Wang, Haichuan and Lin, Haibo},
 doi = {10.1145/2370036.2145850},
 issn = {0362-1340},
 issue_date = {August 2012},
 journal = {SIGPLAN Not.},
 keyword = {X10, scheduling, task parallelism, work-stealing},
 link = {http://doi.acm.org/10.1145/2370036.2145850},
 month = {feb},
 number = {8},
 numpages = {10},
 pages = {267--276},
 publisher = {ACM},
 title = {A Work-stealing Scheduler for X10's Task Parallelism with Suspension},
 volume = {47},
 year = {2012}
}


@article{Stone:2012:EMP:2370036.2145881,
 abstract = {Miniapps serve as test beds for prototyping and evaluating new algorithms, data structures, and programming models before incorporating such changes into larger applications. For the miniapp to accurately predict how a prototyped change would affect a larger application it is necessary that the miniapp be shown to serve as a proxy for that larger application. Although many benchmarks claim to proxy the performance for a set of large applications, little work has explored what criteria must be met for a benchmark to serve as a proxy for examining programmability. In this poster we describe criteria that can be used to establish that a miniapp serves as a performance and programmability proxy.},
 acmid = {2145881},
 address = {New York, NY, USA},
 author = {Stone, Andrew and Dennis, John and Strout, Michelle},
 doi = {10.1145/2370036.2145881},
 issn = {0362-1340},
 issue_date = {August 2012},
 journal = {SIGPLAN Not.},
 keyword = {POP, benchmark, conjugate gradient, miniapp, parallel programming, programmability proxy},
 link = {http://doi.acm.org/10.1145/2370036.2145881},
 month = {feb},
 number = {8},
 numpages = {2},
 pages = {333--334},
 publisher = {ACM},
 title = {Establishing a Miniapp As a Programmability Proxy},
 volume = {47},
 year = {2012}
}


@inproceedings{Zhang:2012:LLL:2145816.2145868,
 abstract = {LHlf is a new hash table designed to allow very high levels of concurrency. The table is lock free and grows and shrinks auto-matically according to the number of items in the table. Insertions, lookups and deletions are never blocked. LHlf is based on linear hashing but adopts recursive split-ordering of the items within a bucket to be able to split and merge lists in a lock free manner. LHlf is as fast as the best previous lock-free design and in addition it offers stable performance, uses less space, and supports both expansions and contractions.},
 acmid = {2145868},
 address = {New York, NY, USA},
 author = {Zhang, Donghui and Larson, Per-\AAke},
 booktitle = {Proceedings of the 17th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
 doi = {10.1145/2145816.2145868},
 isbn = {978-1-4503-1160-1},
 keyword = {hash table, linear hashing, lock-free, split-order},
 link = {http://doi.acm.org/10.1145/2145816.2145868},
 location = {New Orleans, Louisiana, USA},
 numpages = {2},
 pages = {307--308},
 publisher = {ACM},
 series = {PPoPP '12},
 title = {LHlf: Lock-free Linear Hashing (Poster Paper)},
 year = {2012}
}


@inproceedings{Sim:2012:PAF:2145816.2145819,
 abstract = {Tuning code for GPGPU and other emerging many-core platforms is a challenge because few models or tools can precisely pinpoint the root cause of performance bottlenecks. In this paper, we present a performance analysis framework that can help shed light on such bottlenecks for GPGPU applications. Although a handful of GPGPU profiling tools exist, most of the traditional tools, unfortunately, simply provide programmers with a variety of measurements and metrics obtained by running applications, and it is often difficult to map these metrics to understand the root causes of slowdowns, much less decide what next optimization step to take to alleviate the bottleneck. In our approach, we first develop an analytical performance model that can precisely predict performance and aims to provide programmer-interpretable metrics. Then, we apply static and dynamic profiling to instantiate our performance model for a particular input code and show how the model can predict the potential performance benefits. We demonstrate our framework on a suite of micro-benchmarks as well as a variety of computations extracted from real codes.},
 acmid = {2145819},
 address = {New York, NY, USA},
 author = {Sim, Jaewoong and Dasgupta, Aniruddha and Kim, Hyesoon and Vuduc, Richard},
 booktitle = {Proceedings of the 17th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
 doi = {10.1145/2145816.2145819},
 isbn = {978-1-4503-1160-1},
 keyword = {CUDA, GPGPU architecture, analytical model, performance benefit prediction, performance prediction},
 link = {http://doi.acm.org/10.1145/2145816.2145819},
 location = {New Orleans, Louisiana, USA},
 numpages = {12},
 pages = {11--22},
 publisher = {ACM},
 series = {PPoPP '12},
 title = {A Performance Analysis Framework for Identifying Potential Benefits in GPGPU Applications},
 year = {2012}
}


@article{Malkis:2012:VSB:2370036.2145871,
 abstract = {This paper describes frontiers in verification of the software barrier synchronization primitive. So far most software barrier algorithms have not been mechanically verified. We show preliminary results in automatically proving the correctness of the major software barriers.},
 acmid = {2145871},
 address = {New York, NY, USA},
 author = {Malkis, Alexander and Banerjee, Anindya},
 doi = {10.1145/2370036.2145871},
 issn = {0362-1340},
 issue_date = {August 2012},
 journal = {SIGPLAN Not.},
 keyword = {barrier, central, client, combining, counting, dissemination, implementation, invariant, safety, software, static, tournament, verification, verifier},
 link = {http://doi.acm.org/10.1145/2370036.2145871},
 month = {feb},
 number = {8},
 numpages = {2},
 pages = {313--314},
 publisher = {ACM},
 title = {Verification of Software Barriers},
 volume = {47},
 year = {2012}
}


@article{Blelloch:2012:IDP:2370036.2145840,
 abstract = {The virtues of deterministic parallelism have been argued for decades and many forms of deterministic parallelism have been described and analyzed. Here we are concerned with one of the strongest forms, requiring that for any input there is a unique dependence graph representing a trace of the computation annotated with every operation and value. This has been referred to as internal determinism, and implies a sequential semantics---i.e., considering any sequential traversal of the dependence graph is sufficient for analyzing the correctness of the code. In addition to returning deterministic results, internal determinism has many advantages including ease of reasoning about the code, ease of verifying correctness, ease of debugging, ease of defining invariants, ease of defining good coverage for testing, and ease of formally, informally and experimentally reasoning about performance. On the other hand one needs to consider the possible downsides of determinism, which might include making algorithms (i) more complicated, unnatural or special purpose and/or (ii) slower or less scalable. In this paper we study the effectiveness of this strong form of determinism through a broad set of benchmark problems. Our main contribution is to demonstrate that for this wide body of problems, there exist efficient internally deterministic algorithms, and moreover that these algorithms are natural to reason about and not complicated to code. We leverage an approach to determinism suggested by Steele (1990), which is to use nested parallelism with commutative operations. Our algorithms apply several diverse programming paradigms that fit within the model including (i) a strict functional style (no shared state among concurrent operations), (ii) an approach we refer to as deterministic reservations, and (iii) the use of commutative, linearizable operations on data structures. We describe algorithms for the benchmark problems that use these deterministic approaches and present performance results on a 32-core machine. Perhaps surprisingly, for all problems, our internally deterministic algorithms achieve good speedup and good performance even relative to prior nondeterministic solutions.},
 acmid = {2145840},
 address = {New York, NY, USA},
 author = {Blelloch, Guy E. and Fineman, Jeremy T. and Gibbons, Phillip B. and Shun, Julian},
 doi = {10.1145/2370036.2145840},
 issn = {0362-1340},
 issue_date = {August 2012},
 journal = {SIGPLAN Not.},
 keyword = {commutative operations, deterministic parallelism, geometry algorithms, graph algorithms, parallel algorithms, parallel programming, sorting, string processing},
 link = {http://doi.acm.org/10.1145/2370036.2145840},
 month = {feb},
 number = {8},
 numpages = {12},
 pages = {181--192},
 publisher = {ACM},
 title = {Internally Deterministic Parallel Algorithms Can Be Fast},
 volume = {47},
 year = {2012}
}


@inproceedings{Zu:2012:GNI:2145816.2145833,
 abstract = {Regular expression pattern matching is the foundation and core engine of many network functions, such as network intrusion detection, worm detection, traffic analysis, web applications and so on. DFA-based solutions suffer exponentially exploding state space and cannot be remedied without sacrificing matching speed. Given this scalability problem of DFA-based methods, there has been increasing interest in NFA-based methods for memory efficient regular expression matching. To achieve high matching speed using NFA, it requires potentially massive parallel processing, and hence represents an ideal programming task on Graphic Processor Unit (GPU). Based on in-depth understanding of NFA properties as well as GPU architecture, we propose effective methods for fitting NFAs into GPU architecture through proper data structure and parallel programming design, so that GPU's parallel processing power can be better utilized to achieve high speed regular expression matching. Experiment results demonstrate that, compared with the existing GPU-based NFA implementation method [9], our proposed methods can boost matching speed by 29~46 times, consistently yielding above 10Gbps matching speed on NVIDIA GTX-460 GPU. Meanwhile, our design only needs a small amount of memory space, growing exponentially more slowly than DFA size. These results make our design an effective solution for memory efficient high speed regular expression matching, and clearly demonstrate the power and potential of GPU as a platform for memory efficient high speed regular expression matching.},
 acmid = {2145833},
 address = {New York, NY, USA},
 author = {Zu, Yuan and Yang, Ming and Xu, Zhonghu and Wang, Lin and Tian, Xin and Peng, Kunyang and Dong, Qunfeng},
 booktitle = {Proceedings of the 17th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
 doi = {10.1145/2145816.2145833},
 isbn = {978-1-4503-1160-1},
 keyword = {CUDA, GPU, NFA, deep packet inspection, pattern matching, regular expression matching},
 link = {http://doi.acm.org/10.1145/2145816.2145833},
 location = {New Orleans, Louisiana, USA},
 numpages = {12},
 pages = {129--140},
 publisher = {ACM},
 series = {PPoPP '12},
 title = {GPU-based NFA Implementation for Memory Efficient High Speed Regular Expression Matching},
 year = {2012}
}


@inproceedings{Ballard:2012:CAS:2145816.2145822,
 abstract = {The running time of an algorithm depends on both arithmetic and communication (i.e., data movement) costs, and the relative costs of communication are growing over time. In this work, we present both theoretical and practical results for tridiagonalizing a symmetric band matrix: we present an algorithm that asymptotically reduces communication, and we show that it indeed performs well in practice. The tridiagonalization of a symmetric band matrix is a key kernel in solving the symmetric eigenvalue problem for both full and band matrices. In order to preserve sparsity, tridiagonalization routines use annihilate-and-chase procedures that previously have suffered from poor data locality. We improve data locality by reorganizing the computation, asymptotically reducing communication costs compared to existing algorithms. Our sequential implementation demonstrates that avoiding communication improves runtime even at the expense of extra arithmetic: we observe a 2x speedup over Intel MKL while doing 43% more floating point operations. Our parallel implementation targets shared-memory multicore platforms. It uses pipelined parallelism and a static scheduler while retaining the locality properties of the sequential algorithm. Due to lightweight synchronization and effective data reuse, we see 9.5x scaling over our serial code and up to 6x speedup over the PLASMA library, comparing parallel performance on a ten-core processor.},
 acmid = {2145822},
 address = {New York, NY, USA},
 author = {Ballard, Grey and Demmel, James and Knight, Nicholas},
 booktitle = {Proceedings of the 17th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
 doi = {10.1145/2145816.2145822},
 isbn = {978-1-4503-1160-1},
 keyword = {band reduction, communication avoiding algorithms, symmetric eigenvalue problem},
 link = {http://doi.acm.org/10.1145/2145816.2145822},
 location = {New Orleans, Louisiana, USA},
 numpages = {10},
 pages = {35--44},
 publisher = {ACM},
 series = {PPoPP '12},
 title = {Communication Avoiding Successive Band Reduction},
 year = {2012}
}


@article{Leiserson:2012:DPR:2370036.2145841,
 abstract = {Existing concurrency platforms for dynamic multithreading do not provide repeatable parallel random-number generators. This paper proposes that a mechanism called pedigrees be built into the runtime system to enable efficient deterministic parallel random-number generation. Experiments with the open-source MIT Cilk runtime system show that the overhead for maintaining pedigrees is negligible. Specifically, on a suite of 10 benchmarks, the relative overhead of Cilk with pedigrees to the original Cilk has a geometric mean of less than 1%. We persuaded Intel to modify its commercial C/C++ compiler, which provides the Cilk Plus concurrency platform, to include pedigrees, and we built a library implementation of a deterministic parallel random-number generator called DotMix that compresses the pedigree and then "RC6-mixes" the result. The statistical quality of DotMix is comparable to that of the popular Mersenne twister, but somewhat slower than a nondeterministic parallel version of this efficient and high-quality serial random-number generator. The cost of calling DotMix depends on the "spawn depth" of the invocation. For a naive Fibonacci calculation with n=40 that calls DotMix in every node of the computation, this "price of determinism" is a factor of 2.65 in running time, but for more realistic applications with less intense use of random numbers -- such as a maximal-independent-set algorithm, a practical samplesort program, and a Monte Carlo discrete-hedging application from QuantLib -- the observed "price" was less than 5%. Moreover, even if overheads were several times greater, applications using DotMix should be amply fast for debugging purposes, which is a major reason for desiring repeatability.},
 acmid = {2145841},
 address = {New York, NY, USA},
 author = {Leiserson, Charles E. and Schardl, Tao B. and Sukha, Jim},
 doi = {10.1145/2370036.2145841},
 issn = {0362-1340},
 issue_date = {August 2012},
 journal = {SIGPLAN Not.},
 keyword = {cilk, determinism, dynamic multithreading, nondeterminism, parallel computing, pedigree, random-number generator},
 link = {http://doi.acm.org/10.1145/2370036.2145841},
 month = {feb},
 number = {8},
 numpages = {12},
 pages = {193--204},
 publisher = {ACM},
 title = {Deterministic Parallel Random-number Generation for Dynamic-multithreading Platforms},
 volume = {47},
 year = {2012}
}


@article{Baskaran:2012:ACO:2370036.2145852,
 abstract = {Modern parallel architectures are emerging with sophisticated hardware consisting of hierarchically placed parallel processors and memories. The properties of memories in a system vary wildly, not only quantitatively (size, latency, bandwidth, number of banks) but also qualitatively (scratchpad, cache). Along with the emergence of such architectures comes the need for effectively utilizing the parallel processors and properly managing data movement across memories to improve memory bandwidth and hide data transfer latency. In this paper, we describe some of the high-level optimizations that are targeted at the improvement of memory performance in the R-Stream compiler, a high-level source-to-source automatic parallelizing compiler. We direct our focus in this paper on optimizing communications (data transfers) by improving memory reuse at various levels of an explicit memory hierarchy. This general concept is well-suited to the hardware properties of GPGPUs, which is the architecture that we concentrate on for this paper. We apply our techniques and obtain performance improvement on various stencil kernels including an important iterative stencil kernel in seismic processing applications where the performance is comparable to that of the state-of-the-art implementation of the kernel by a CUDA expert.},
 acmid = {2145852},
 address = {New York, NY, USA},
 author = {Baskaran, Muthu Manikandan and Vasilache, Nicolas and Meister, Benoit and Lethin, Richard},
 doi = {10.1145/2370036.2145852},
 issn = {0362-1340},
 issue_date = {August 2012},
 journal = {SIGPLAN Not.},
 keyword = {data reuse, data transfer optimization, memory reuse},
 link = {http://doi.acm.org/10.1145/2370036.2145852},
 month = {feb},
 number = {8},
 numpages = {2},
 pages = {277--278},
 publisher = {ACM},
 title = {Automatic Communication Optimizations Through Memory Reuse Strategies},
 volume = {47},
 year = {2012}
}


@article{Jimborean:2012:APM:2370036.2145861,
 abstract = {In this paper, we present a Thread-Level Speculation (TLS) framework whose main feature is to be able to speculatively parallelize a sequential loop nest in various ways, by re-scheduling its iterations. The transformation to be applied is selected at runtime with the goal of minimizing the number of rollbacks and maximizing performance. We perform code transformations by applying the polyhedral model that we adapted for speculative and runtime code parallelization. For this purpose, we design a parallel code pattern which is patched by our runtime system according to the profiling information collected on some execution samples. Adaptability is ensured by considering chunks of code of various sizes, that are launched successively, each of which being parallelized in a different manner, or run sequentially, depending on the currently observed behavior for accessing memory. We show on several benchmarks that our framework yields good performance on codes which could not be handled efficiently by previously proposed TLS systems.},
 acmid = {2145861},
 address = {New York, NY, USA},
 author = {Jimborean, Alexandra and Clauss, Philippe and Pradelle, Beno\^{\i}t and Mastrangelo, Luis and Loechner, Vincent},
 doi = {10.1145/2370036.2145861},
 issn = {0362-1340},
 issue_date = {August 2012},
 journal = {SIGPLAN Not.},
 keyword = {dynamic code transformations, dynamic system, polyhedral model, speculative parallelization},
 link = {http://doi.acm.org/10.1145/2370036.2145861},
 month = {feb},
 number = {8},
 numpages = {2},
 pages = {295--296},
 publisher = {ACM},
 title = {Adapting the Polyhedral Model As a Framework for Efficient Speculative Parallelization},
 volume = {47},
 year = {2012}
}


@article{Tao:2012:UGA:2370036.2145857,
 abstract = {We present CaCUDA - a GPGPU kernel abstraction and a parallel programming framework for developing highly efficient large scale scientific applications using stencil computations on hybrid CPU/GPU architectures. CaCUDA is built upon the Cactus computational toolkit, an open source problem solving environment designed for scientists and engineers. Due to the flexibility and extensibility of the Cactus toolkit, the addition of a GPGPU programming framework required no changes to the Cactus infrastructure, guaranteeing that existing features and modules will continue to work without modification. CaCUDA was tested and benchmarked using a 3D CFD code based on a finite difference discretization of Navier-Stokes equations.},
 acmid = {2145857},
 address = {New York, NY, USA},
 author = {Tao, Jian and Blazewicz, Marek and Brandt, Steven R.},
 doi = {10.1145/2370036.2145857},
 issn = {0362-1340},
 issue_date = {August 2012},
 journal = {SIGPLAN Not.},
 keyword = {GPGPU programming, HPC, computational framework, stencil computation},
 link = {http://doi.acm.org/10.1145/2370036.2145857},
 month = {feb},
 number = {8},
 numpages = {2},
 pages = {287--288},
 publisher = {ACM},
 title = {Using GPU's to Accelerate Stencil-based Computation Kernels for the Development of Large Scale Scientific Applications on Heterogeneous Systems},
 volume = {47},
 year = {2012}
}


@inproceedings{Marker:2012:MED:2145816.2145858,
 abstract = {The efforts of an expert to parallelize and optimize a dense linear algebra algorithm for distributed-memory targets are largely mechanical and repetitive. We demonstrate that these efforts can be encoded and automatically applied to obviate the manual implementation of many algorithms in high-performance code.},
 acmid = {2145858},
 address = {New York, NY, USA},
 author = {Marker, Bryan and Terrel, Andy and Poulson, Jack and Batory, Don and van de Geijn, Robert},
 booktitle = {Proceedings of the 17th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
 doi = {10.1145/2145816.2145858},
 isbn = {978-1-4503-1160-1},
 keyword = {MDE, dense linear algebra, high-performance numerical algorithms, libraries of the future, program generation, software for distributed-memory computing},
 link = {http://doi.acm.org/10.1145/2145816.2145858},
 location = {New Orleans, Louisiana, USA},
 numpages = {2},
 pages = {289--290},
 publisher = {ACM},
 series = {PPoPP '12},
 title = {Mechanizing the Expert Dense Linear Algebra Developer},
 year = {2012}
}


@article{Li:2012:GCV:2370036.2145844,
 abstract = {Programs written for GPUs often contain correctness errors such as races, deadlocks, or may compute the wrong result. Existing debugging tools often miss these errors because of their limited input-space and execution-space exploration. Existing tools based on conservative static analysis or conservative modeling of SIMD concurrency generate false alarms resulting in wasted bug-hunting. They also often do not target performance bugs (non-coalesced memory accesses, memory bank conflicts, and divergent warps). We provide a new framework called GKLEE that can analyze C++ GPU programs, locating the aforesaid correctness and performance bugs. For these programs, GKLEE can also automatically generate tests that provide high coverage. These tests serve as concrete witnesses for every reported bug. They can also be used for downstream debugging, for example to test the kernel on the actual hardware. We describe the architecture of GKLEE, its symbolic virtual machine model, and describe previously unknown bugs and performance issues that it detected on commercial SDK kernels. We describe GKLEE's test-case reduction heuristics, and the resulting scalability improvement for a given coverage target.},
 acmid = {2145844},
 address = {New York, NY, USA},
 author = {Li, Guodong and Li, Peng and Sawaya, Geof and Gopalakrishnan, Ganesh and Ghosh, Indradeep and Rajan, Sreeranga P.},
 doi = {10.1145/2370036.2145844},
 issn = {0362-1340},
 issue_date = {August 2012},
 journal = {SIGPLAN Not.},
 keyword = {CUDA, GPU, automatic test generation, formal verification, parallelism, symbolic execution, virtual machine},
 link = {http://doi.acm.org/10.1145/2370036.2145844},
 month = {feb},
 number = {8},
 numpages = {10},
 pages = {215--224},
 publisher = {ACM},
 title = {GKLEE: Concolic Verification and Test Generation for GPUs},
 volume = {47},
 year = {2012}
}


@inproceedings{Mendez-Lojo:2012:GII:2145816.2145831,
 abstract = {Graphics Processing Units (GPUs) have emerged as powerful accelerators for many regular algorithms that operate on dense arrays and matrices. In contrast, we know relatively little about using GPUs to accelerate highly irregular algorithms that operate on pointer-based data structures such as graphs. For the most part, research has focused on GPU implementations of graph analysis algorithms that do not modify the structure of the graph, such as algorithms for breadth-first search and strongly-connected components. In this paper, we describe a high-performance GPU implementation of an important graph algorithm used in compilers such as gcc and LLVM: Andersen-style inclusion-based points-to analysis. This algorithm is challenging to parallelize effectively on GPUs because it makes extensive modifications to the structure of the underlying graph and performs relatively little computation. In spite of this, our program, when executed on a 14 Streaming Multiprocessor GPU, achieves an average speedup of 7x compared to a sequential CPU implementation and outperforms a parallel implementation of the same algorithm running on 16 CPU cores. Our implementation provides general insights into how to produce high-performance GPU implementations of graph algorithms, and it highlights key differences between optimizing parallel programs for multicore CPUs and for GPUs.},
 acmid = {2145831},
 address = {New York, NY, USA},
 author = {Mendez-Lojo, Mario and Burtscher, Martin and Pingali, Keshav},
 booktitle = {Proceedings of the 17th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
 doi = {10.1145/2145816.2145831},
 isbn = {978-1-4503-1160-1},
 keyword = {CUDA, GPU, graph algorithms, inclusion-based points-to analysis, irregular programs},
 link = {http://doi.acm.org/10.1145/2145816.2145831},
 location = {New Orleans, Louisiana, USA},
 numpages = {10},
 pages = {107--116},
 publisher = {ACM},
 series = {PPoPP '12},
 title = {A GPU Implementation of Inclusion-based Points-to Analysis},
 year = {2012}
}


@inproceedings{Kim:2012:OUP:2145816.2145863,
 abstract = {In this paper, we propose an OpenCL framework for heterogeneous CPU/GPU clusters, and show that the framework achieves both high performance and ease of programming. The framework provides an illusion of a single system for the user. It allows the application to utilize multiple heterogeneous compute devices, such as multicore CPUs and GPUs, in a remote node as if they were in a local node. No communication API, such as the MPI library, is required in the application source. We implement the OpenCL framework and evaluate its performance on a heterogeneous CPU/GPU cluster that consists of one host node and nine compute nodes using eleven OpenCL benchmark applications.},
 acmid = {2145863},
 address = {New York, NY, USA},
 author = {Kim, Jungwon and Seo, Sangmin and Lee, Jun and Nah, Jeongho and Jo, Gangwon and Lee, Jaejin},
 booktitle = {Proceedings of the 17th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
 doi = {10.1145/2145816.2145863},
 isbn = {978-1-4503-1160-1},
 keyword = {OpenCL, clusters, heterogeneous computing, programming models},
 link = {http://doi.acm.org/10.1145/2145816.2145863},
 location = {New Orleans, Louisiana, USA},
 numpages = {2},
 pages = {299--300},
 publisher = {ACM},
 series = {PPoPP '12},
 title = {OpenCL As a Unified Programming Model for Heterogeneous CPU/GPU Clusters},
 year = {2012}
}


@article{Kogan:2012:MCF:2370036.2145835,
 abstract = {Lock-freedom is a progress guarantee that ensures overall program progress. Wait-freedom is a stronger progress guarantee that ensures the progress of each thread in the program. While many practical lock-free algorithms exist, wait-free algorithms are typically inefficient and hardly used in practice. In this paper, we propose a methodology called fast-path-slow-path for creating efficient wait-free algorithms. The idea is to execute the efficient lock-free version most of the time and revert to the wait-free version only when things go wrong. The generality and effectiveness of this methodology is demonstrated by two examples. In this paper, we apply this idea to a recent construction of a wait-free queue, bringing the wait-free implementation to perform in practice as efficient as the lock-free implementation. In another work, the fast-path-slow-path methodology has been used for (dramatically) improving the performance of a wait-free linked-list.},
 acmid = {2145835},
 address = {New York, NY, USA},
 author = {Kogan, Alex and Petrank, Erez},
 doi = {10.1145/2370036.2145835},
 issn = {0362-1340},
 issue_date = {August 2012},
 journal = {SIGPLAN Not.},
 keyword = {concurrent data structures, lock-free algorithms, non-blocking synchronization, wait-free queues},
 link = {http://doi.acm.org/10.1145/2370036.2145835},
 month = {feb},
 number = {8},
 numpages = {10},
 pages = {141--150},
 publisher = {ACM},
 title = {A Methodology for Creating Fast Wait-free Data Structures},
 volume = {47},
 year = {2012}
}


@article{Kim:2012:ESC:2370036.2145824,
 abstract = {Array indirection causes several challenges for compilers to utilize single instruction, multiple data (SIMD) instructions. Disjoint memory references, arbitrarily misaligned memory references, and dependence cycles in loops are main challenges to handle for SIMD compilers. Due to those challenges, existing SIMD compilers have excluded loops with array indirection from their candidate loops for SIMD vectorization. However, addressing those challenges is inevitable, since many important compute-intensive applications extensively use array indirection to reduce memory and computation requirements. In this work, we propose a method to generate efficient SIMD code for loops containing indirected memory references. We extract both inter- and intra-iteration parallelism, taking data reorganization overhead into consideration. We also optimally place data reorganization code in order to amortize the reorganization overhead through the performance gain of SIMD vectorization. Experiments on four array indirection kernels, which are extracted from real-world scientific applications, show that our proposed method effectively generates SIMD code for irregular kernels with array indirection. Compared to the existing SIMD vectorization methods, our proposed method significantly improves the performance of irregular kernels by 91%, on average.},
 acmid = {2145824},
 address = {New York, NY, USA},
 author = {Kim, Seonggun and Han, Hwansoo},
 doi = {10.1145/2370036.2145824},
 issn = {0362-1340},
 issue_date = {August 2012},
 journal = {SIGPLAN Not.},
 keyword = {DFG-based vectorization, SIMD processors, irregular kernels},
 link = {http://doi.acm.org/10.1145/2370036.2145824},
 month = {feb},
 number = {8},
 numpages = {10},
 pages = {55--64},
 publisher = {ACM},
 title = {Efficient SIMD Code Generation for Irregular Kernels},
 volume = {47},
 year = {2012}
}


@article{Fatourou:2012:RCS:2370036.2145849,
 abstract = {Fine-grain thread synchronization has been proved, in several cases, to be outperformed by efficient implementations of the combining technique where a single thread, called the combiner, holding a coarse-grain lock, serves, in addition to its own synchronization request, active requests announced by other threads while they are waiting by performing some form of spinning. Efficient implementations of this technique significantly reduce the cost of synchronization, so in many cases they exhibit much better performance than the most efficient finely synchronized algorithms. In this paper, we revisit the combining technique with the goal to discover where its real performance power resides and whether or how ensuring some desired properties (e.g., fairness in serving requests) would impact performance. We do so by presenting two new implementations of this technique; the first (CC-Synch) addresses systems that support coherent caches, whereas the second (DSM-Synch) works better in cacheless NUMA machines. In comparison to previous such implementations, the new implementations (1) provide bounds on the number of remote memory references (RMRs) that they perform, (2) support a stronger notion of fairness, and (3) use simpler and less basic primitives than previous approaches. In all our experiments, the new implementations outperform by far all previous state-of-the-art combining-based and fine-grain synchronization algorithms. Our experimental analysis sheds light to the questions we aimed to answer. Several modern multi-core systems organize the cores into clusters and provide fast communication within the same cluster and much slower communication across clusters. We present an hierarchical version of CC-Synch, called H-Synch which exploits the hierarchical communication nature of such systems to achieve better performance. Experiments show that H-Synch significantly outper forms previous state-of-the-art hierarchical approaches. We provide new implementations of common shared data structures (like stacks and queues) based on CC-Synch, DSM-Synch and H-Synch. Our experiments show that these implementations outperform by far all previous (fine-grain or combined-based) implementations of shared stacks and queues.},
 acmid = {2145849},
 address = {New York, NY, USA},
 author = {Fatourou, Panagiota and Kallimanis, Nikolaos D.},
 doi = {10.1145/2370036.2145849},
 issn = {0362-1340},
 issue_date = {August 2012},
 journal = {SIGPLAN Not.},
 keyword = {blocking algorithms, combining, concurrent data structures, hierarchical algorithms, synchronization techniques},
 link = {http://doi.acm.org/10.1145/2370036.2145849},
 month = {feb},
 number = {8},
 numpages = {10},
 pages = {257--266},
 publisher = {ACM},
 title = {Revisiting the Combining Synchronization Technique},
 volume = {47},
 year = {2012}
}


@inproceedings{Du:2012:AFT:2145816.2145845,
 abstract = {Dense matrix factorizations, such as LU, Cholesky and QR, are widely used for scientific applications that require solving systems of linear equations, eigenvalues and linear least squares problems. Such computations are normally carried out on supercomputers, whose ever-growing scale induces a fast decline of the Mean Time To Failure (MTTF). This paper proposes a new hybrid approach, based on Algorithm-Based Fault Tolerance (ABFT), to help matrix factorizations algorithms survive fail-stop failures. We consider extreme conditions, such as the absence of any reliable component and the possibility of loosing both data and checksum from a single failure. We will present a generic solution for protecting the right factor, where the updates are applied, of all above mentioned factorizations. For the left factor, where the panel has been applied, we propose a scalable checkpointing algorithm. This algorithm features high degree of checkpointing parallelism and cooperatively utilizes the checksum storage leftover from the right factor protection. The fault-tolerant algorithms derived from this hybrid solution is applicable to a wide range of dense matrix factorizations, with minor modifications. Theoretical analysis shows that the fault tolerance overhead sharply decreases with the scaling in the number of computing units and the problem size. Experimental results of LU and QR factorization on the Kraken (Cray XT5) supercomputer validate the theoretical evaluation and confirm negligible overhead, with- and without-errors.},
 acmid = {2145845},
 address = {New York, NY, USA},
 author = {Du, Peng and Bouteiller, Aurelien and Bosilca, George and Herault, Thomas and Dongarra, Jack},
 booktitle = {Proceedings of the 17th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
 doi = {10.1145/2145816.2145845},
 isbn = {978-1-4503-1160-1},
 keyword = {ABFT, LU, QR, fail-stop failure, fault-tolerance},
 link = {http://doi.acm.org/10.1145/2145816.2145845},
 location = {New Orleans, Louisiana, USA},
 numpages = {10},
 pages = {225--234},
 publisher = {ACM},
 series = {PPoPP '12},
 title = {Algorithm-based Fault Tolerance for Dense Matrix Factorizations},
 year = {2012}
}


@inproceedings{Timnat:2012:WL:2145816.2145869,
 abstract = {The linked-list data structure is fundamental and ubiquitous. Lock-free versions of the linked-list are well known. However, the existence of a practical wait-free linked-list has been open. In this work we designed such a linked-list. To achieve better performance, we have also extended this design using the fast-path-slow-path methodology. The resulting implementation achieves performance which is competitive with that of Harris's lock-free list, while still guaranteeing non-starvation via wait-freedom. We have also developed a proof for the correctness and the wait-freedom of our design.},
 acmid = {2145869},
 address = {New York, NY, USA},
 author = {Timnat, Shahar and Braginsky, Anastasia and Kogan, Alex and Petrank, Erez},
 booktitle = {Proceedings of the 17th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
 doi = {10.1145/2145816.2145869},
 isbn = {978-1-4503-1160-1},
 keyword = {linked-list, lock-freedom, wait-freedom},
 link = {http://doi.acm.org/10.1145/2145816.2145869},
 location = {New Orleans, Louisiana, USA},
 numpages = {2},
 pages = {309--310},
 publisher = {ACM},
 series = {PPoPP '12},
 title = {Wait-free Linked-lists},
 year = {2012}
}


