@article{Thebault:2015:SEI:2858788.2688517,
 abstract = { Exposing massive parallelism on 3D unstructured meshes computation with efficient load balancing and minimal synchronizations is challenging. Current approaches relying on domain decomposition and mesh coloring struggle to scale with the increasing number of cores per nodes, especially with new many-core processors. In this paper, we propose an hybrid approach using domain decomposition to exploit distributed memory parallelism, Divide-and-Conquer, D&C;, to exploit shared memory parallelism and improve locality, and mesh coloring at core level to exploit vectors. It illustrates a new trade-off for many-cores between structuredness, memory locality, and vectorization. We evaluate our approach on the finite element matrix assembly of an industrial fluid dynamic code developed by Dassault Aviation. We compare our D&C; approach to domain decomposition and to mesh coloring. D&C; achieves a high parallel efficiency, a good data locality as well as an improved bandwidth usage. It competes on current nodes with the optimized pure MPI version with a minimum 10% speed-up. D&C; shows an impressive 319x strong scaling on 512 cores (32 nodes) with only 2000 vertices per core. Finally, the Intel Xeon Phi version has a performance similar to 10 Intel E5-2665 Xeon Sandy Bridge cores and 95% parallel efficiency on the 60 physical cores. Running on 4 Xeon Phi (240 cores), D&C; has 92% efficiency on the physical cores and performance similar to 33 Intel E5-2665 Xeon Sandy Bridge cores. },
 acmid = {2688517},
 address = {New York, NY, USA},
 author = {Th{\'e}bault, Lo\"{\i}c and Petit, Eric and Dinh, Quang},
 doi = {10.1145/2858788.2688517},
 issn = {0362-1340},
 issue_date = {August 2015},
 journal = {SIGPLAN Not.},
 keyword = {Divide-and-conquer, computational fluid dynamic, domain decomposition, matrix assembly, task, unstructured mesh, vectorization},
 link = {http://doi.acm.org/10.1145/2858788.2688517},
 month = {jan},
 number = {8},
 numpages = {10},
 pages = {120--129},
 publisher = {ACM},
 title = {Scalable and Efficient Implementation of 3D Unstructured Meshes Computation: A Case Study on Matrix Assembly},
 volume = {50},
 year = {2015}
}


@inproceedings{Pearce:2015:DLB:2688500.2688539,
 abstract = { Modern scientific simulations divide work between parallel processors by decomposing a spatial domain of mesh cells, particles, or other elements. A balanced assignment of the computational load is critical for parallel performance. If the computation per element changes over the simulation time, simulations can use dynamic load balance algorithms to evenly redistribute work to processes. Graph partitioners are widely used and balance very effectively, but they do not strong scale well. Typical SPMD simulations wait while a load balance algorithm runs on all processors, so a poorly scaling algorithm can itself become a bottleneck. We observe that the load balance algorithm is separate from the main application computation and has its own scaling properties. We propose to decouple the load balance algorithm from the application, and to offload the load balance computation so that it runs concurrently with the application on a smaller number of processors. We demonstrate the costs of decoupling and offloading the load balancing algorithm from a Barnes-Hut application. },
 acmid = {2688539},
 address = {New York, NY, USA},
 author = {Pearce, Olga and Gamblin, Todd and de Supinski, Bronis R. and Schulz, Martin and Amato, Nancy M.},
 booktitle = {Proceedings of the 20th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
 doi = {10.1145/2688500.2688539},
 isbn = {978-1-4503-3205-7},
 keyword = {load balance, parallel algorithm, performance},
 link = {http://doi.acm.org/10.1145/2688500.2688539},
 location = {San Francisco, CA, USA},
 numpages = {2},
 pages = {267--268},
 publisher = {ACM},
 series = {PPoPP 2015},
 title = {Decoupled Load Balancing},
 year = {2015}
}


@inproceedings{Jin:2015:CPI:2688500.2688541,
 abstract = { Parallel application benchmarks are indispensable for evaluating/optimizing HPC software and hardware. However, it is very challenging and costly to obtain high-fidelity benchmarks reflecting the scale and complexity of state-of-the-art parallel applications. Hand-extracted synthetic benchmarks are time- and labor-intensive to create. Real applications themselves, while offering most accurate performance evaluation, are expensive to compile, port, recon- figure, and often plainly inaccessible due to security or ownership concerns. This work contributes APPRIME, a novel tool for trace-based automatic parallel benchmark generation. Taking as input standard communication-I/O traces of an application’s execution, it couples accurate automatic phase identification with statistical regeneration of event parameters to create compact, portable, and to some degree reconfigurable parallel application benchmarks. Experiments with four NAS Parallel Benchmarks (NPB) and three real scientific simulation codes confirm the fidelity of APPRIME benchmarks. They retain the original applications’ performance characteristics, in particular the relative performance across platforms. },
 acmid = {2688541},
 address = {New York, NY, USA},
 author = {Jin, Ye and Liu, Mingliang and Ma, Xiaosong and Liu, Qing and Logan, Jeremy and Podhorszki, Norbert and Choi, Jong Youl and Klasky, Scott},
 booktitle = {Proceedings of the 20th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
 doi = {10.1145/2688500.2688541},
 isbn = {978-1-4503-3205-7},
 keyword = {HPC applications, automatic benchmark generation, phase identification, statistical profiling, trace},
 link = {http://doi.acm.org/10.1145/2688500.2688541},
 location = {San Francisco, CA, USA},
 numpages = {2},
 pages = {269--270},
 publisher = {ACM},
 series = {PPoPP 2015},
 title = {Combining Phase Identification and Statistic Modeling for Automated Parallel Benchmark Generation},
 year = {2015}
}


@inproceedings{Majo:2015:LPC:2688500.2688509,
 abstract = { Many recent multiprocessor systems are realized with a non-uniform memory architecture (NUMA) and accesses to remote memory locations take more time than local memory accesses. Optimizing NUMA memory system performance is difficult and costly for three principal reasons: (1) today's programming languages/libraries have no explicit support for NUMA systems, (2) NUMA optimizations are not~portable, and (3) optimizations are not~composable (i.e., they can become ineffective or worsen performance in environments that support composable parallel software). This paper presents TBB-NUMA, a parallel programming library based on Intel Threading Building Blocks (TBB) that supports portable and composable NUMA-aware programming. TBB-NUMA provides a model of task affinity that captures a programmer's insights on mapping tasks to resources. NUMA-awareness affects all layers of the library (i.e., resource management, task scheduling, and high-level parallel algorithm templates) and requires close coupling between all these layers. Optimizations implemented with TBB-NUMA (for a set of standard benchmark programs) result in up to 44% performance improvement over standard TBB, but more important, optimized programs are portable across different NUMA architectures and preserve data locality also when composed with other parallel computations. },
 acmid = {2688509},
 address = {New York, NY, USA},
 author = {Majo, Zoltan and Gross, Thomas R.},
 booktitle = {Proceedings of the 20th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
 doi = {10.1145/2688500.2688509},
 isbn = {978-1-4503-3205-7},
 keyword = {NUMA, data placement, scheduling},
 link = {http://doi.acm.org/10.1145/2688500.2688509},
 location = {San Francisco, CA, USA},
 numpages = {12},
 pages = {227--238},
 publisher = {ACM},
 series = {PPoPP 2015},
 title = {A Library for Portable and Composable Data Locality Optimizations for NUMA Systems},
 year = {2015}
}


@inproceedings{Shi:2015:OAG:2688500.2688542,
 abstract = { Modern GPUs have been widely used to accelerate the graph processing for complicated computational problems regarding graph theory. Many parallel graph algorithms adopt the asynchronous computing model to accelerate the iterative convergence. Unfortunately, the consistent asynchronous computing requires locking or the atomic operations, leading to significant penalties/overheads when implemented on GPUs. To this end, coloring algorithm is adopted to separate the vertices with potential updating conflicts, guaranteeing the consistency/correctness of the parallel processing. We propose a light-weight asynchronous processing framework called Frog with a hybrid coloring model. We find that majority of vertices (about 80%) are colored with only a few colors, such that they can be read and updated in a very high degree of parallelism without violating the sequential consistency. Accordingly, our solution will separate the processing of the vertices based on the distribution of colors. },
 acmid = {2688542},
 address = {New York, NY, USA},
 author = {Shi, Xuanhua and Liang, Junling and Di, Sheng and He, Bingsheng and Jin, Hai and Lu, Lu and Wang, Zhixiang and Luo, Xuan and Zhong, Jianlong},
 booktitle = {Proceedings of the 20th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
 doi = {10.1145/2688500.2688542},
 isbn = {978-1-4503-3205-7},
 keyword = {Asynchronous Computing, GPGPU, Graph Processing},
 link = {http://doi.acm.org/10.1145/2688500.2688542},
 location = {San Francisco, CA, USA},
 numpages = {2},
 pages = {271--272},
 publisher = {ACM},
 series = {PPoPP 2015},
 title = {Optimization of Asynchronous Graph Processing on GPU with Hybrid Coloring Model},
 year = {2015}
}


@article{Arbel:2015:PRR:2858788.2688518,
 abstract = { Read-copy update (RCU) is a shared memory synchronization mechanism with scalable synchronization-free reads that nevertheless execute correctly with concurrent updates. To guarantee the consistency of such reads, an RCU update transitioning the data structure between certain states must wait for the completion of all existing reads. Unfortunately, these waiting periods quickly become a bottleneck, and thus RCU remains unused in data structures that require scalable, fine-grained, update operations. To solve this problem, we present Predicate RCU (PRCU), an RCU variant in which an update waits only for the reads whose consistency it affects, which are specified by a user-supplied predicate. We explore the trade-offs in implementing PRCU, describing implementations that reduce wait times by 10--100x with varying overhead on reads on modern x86 multiprocessor machines. We demonstrate the applicability of PRCU by applying it to two RCU-based concurrent algorithms---the Citrus binary search tree and a resizable hash table---and show experimentally that PRCU significantly improves the performance of both algorithms. },
 acmid = {2688518},
 address = {New York, NY, USA},
 author = {Arbel, Maya and Morrison, Adam},
 doi = {10.1145/2858788.2688518},
 issn = {0362-1340},
 issue_date = {August 2015},
 journal = {SIGPLAN Not.},
 keyword = {Concurrent data structures, RCU, Synchronization},
 link = {http://doi.acm.org/10.1145/2858788.2688518},
 month = {jan},
 number = {8},
 numpages = {10},
 pages = {21--30},
 publisher = {ACM},
 title = {Predicate RCU: An RCU for Scalable Concurrent Updates},
 volume = {50},
 year = {2015}
}


@article{Golan-Gueta:2015:ASA:2858788.2688511,
 abstract = { In this paper, we consider concurrent programs in which the shared state consists of instances of linearizable ADTs (abstract data types). We present an automated approach to concurrency control that addresses a common need: the need to atomically execute a code fragment, which may contain multiple ADT operations on multiple ADT instances. We present a synthesis algorithm that automatically enforces atomicity of given code fragments (in a client program) by inserting pessimistic synchronization that guarantees atomicity and deadlock-freedom (without using any rollback mechanism). Our algorithm takes a commutativity specification as an extra input. This specification indicates for every pair of ADT operations the conditions under which the operations commute. Our algorithm enables greater parallelism by permitting commuting operations to execute concurrently. We have implemented the synthesis algorithm in a Java compiler, and applied it to several Java programs. Our results show that our approach produces efficient and scalable synchronization. },
 acmid = {2688511},
 address = {New York, NY, USA},
 author = {Golan-Gueta, Guy and Ramalingam, G. and Sagiv, Mooly and Yahav, Eran},
 doi = {10.1145/2858788.2688511},
 issn = {0362-1340},
 issue_date = {August 2015},
 journal = {SIGPLAN Not.},
 keyword = {Automatic synchronization, Semantics, Transactions},
 link = {http://doi.acm.org/10.1145/2858788.2688511},
 month = {jan},
 number = {8},
 numpages = {11},
 pages = {31--41},
 publisher = {ACM},
 title = {Automatic Scalable Atomicity via Semantic Locking},
 volume = {50},
 year = {2015}
}


@inproceedings{Radoi:2015:WAR:2688500.2700995,
 abstract = { In recent years, web applications have become pervasive. Their backbone is JavaScript, the only programming language supported by all major web browsers. Most browsers run on desktop or mobile devices with parallel hardware. However, JavaScript is by design sequential, and current web applications make little use of hardware parallelism. Are web applications ready to exploit parallel hardware? We answer the question in two steps: First, we survey 174 web developers about the potential and challenges of using parallelism. Then, we study the performance and computation shape of a set of web applications that are representative for the emerging web. Our findings indicate that emerging web applications do have latent data parallelism, and JavaScript developers' programming style is not a significant impediment to exploiting this parallelism. },
 acmid = {2700995},
 address = {New York, NY, USA},
 author = {Radoi, Cosmin and Herhut, Stephan and Sreeram, Jaswanth and Dig, Danny},
 booktitle = {Proceedings of the 20th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
 doi = {10.1145/2688500.2700995},
 isbn = {978-1-4503-3205-7},
 keyword = {javascript, performance, survey, web},
 link = {http://doi.acm.org/10.1145/2688500.2700995},
 location = {San Francisco, CA, USA},
 numpages = {2},
 pages = {289--290},
 publisher = {ACM},
 series = {PPoPP 2015},
 title = {Are Web Applications Ready for Parallelism?},
 year = {2015}
}


@inproceedings{Chabbi:2015:BEP:2688500.2688502,
 abstract = { Large scientific code bases are often composed of several layers of runtime libraries, implemented in multiple programming languages. In such situation, programmers often choose conservative synchronization patterns leading to suboptimal performance. In this paper, we present context-sensitive dynamic optimizations that elide barriers redundant during the program execution. In our technique, we perform data race detection alongside the program to identify redundant barriers in their calling contexts; after an initial learning, we start eliding all future instances of barriers occurring in the same calling context. We present an automatic on-the-fly optimization and a multi-pass guided optimization. We apply our techniques to NWChem--a 6 million line computational chemistry code written in C/C++/Fortran that uses several runtime libraries such as Global Arrays, ComEx, DMAPP, and MPI. Our technique elides a surprisingly high fraction of barriers (as many as 63%) in production runs. This redundancy elimination translates to application speedups as high as 14% on 2048 cores. Our techniques also provided valuable insight about the application behavior, later used by NWChem developers. Overall, we demonstrate the value of holistic context-sensitive analyses that consider the domain science in conjunction with the associated runtime software stack. },
 acmid = {2688502},
 address = {New York, NY, USA},
 author = {Chabbi, Milind and Lavrijsen, Wim and de Jong, Wibe and Sen, Koushik and Mellor-Crummey, John and Iancu, Costin},
 booktitle = {Proceedings of the 20th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
 doi = {10.1145/2688500.2688502},
 isbn = {978-1-4503-3205-7},
 keyword = {Barrier Elision, Dynamic Analysis, Dynamic Optimization, HPC, NWChem, PGAS, Synchronization},
 link = {http://doi.acm.org/10.1145/2688500.2688502},
 location = {San Francisco, CA, USA},
 numpages = {11},
 pages = {109--119},
 publisher = {ACM},
 series = {PPoPP 2015},
 title = {Barrier Elision for Production Parallel Programs},
 year = {2015}
}


@inproceedings{Gopalakrishnan:2015:AFP:2753524.2753531,
 abstract = {Many disruptive changes are happening in the arena of parallel computing, including the use of multiple compute element types (CPUs and GPUs), memory and interconnect types, as well as multiple concurrency models. In the face of these changes, cybersoftware development and debugging will become increasingly hard, requiring principled (formal) debugging approaches that are focused on anticipated classes of bugs. Academic research groups must play a central role in the creation of formal debugging solutions that are to become part of the nation's cyberinfrastructure. These solutions must be scalable and prove effective for debugging the parallel and distributed systems being planned for the coming decade. However, at present, there isn't a critical mass of academic groups engaged in designing such debugging solutions for HPC. In this paper, we summarize some of our own experiences in creating verification tools based on formal principles, pointing out the few instances in which we managed to impact practice. Our key observation is that our successes went hand-in-hand with collaborative tool development with practitioners. Seeking creative ways to facilitate such collaborations should therefore be high priority in our cyberinfrastructure roadmap.},
 acmid = {2753531},
 address = {New York, NY, USA},
 author = {Gopalakrishnan, Ganesh and Sawaya, Jeffrey},
 booktitle = {Proceedings of the 1st Workshop on The Science of Cyberinfrastructure: Research, Experience, Applications and Models},
 doi = {10.1145/2753524.2753531},
 isbn = {978-1-4503-3566-9},
 keyword = {concurrent program debugging, heterogeneous concurrency, test coverage},
 link = {http://doi.acm.org/10.1145/2753524.2753531},
 location = {Portland, Oregon, USA},
 numpages = {8},
 pages = {11--18},
 publisher = {ACM},
 series = {SCREAM '15},
 title = {Achieving Formal Parallel Program Debugging by Incentivizing CS/HPC Collaborative Tool Development},
 year = {2015}
}


@inproceedings{Acharya:2015:PNM:2688500.2688512,
 abstract = { Affine transformations have proven to be very powerful for loop restructuring due to their ability to model a very wide range of transformations. A single multi-dimensional affine function can represent a long and complex sequence of simpler transformations. Existing affine transformation frameworks like the Pluto algorithm, that include a cost function for modern multicore architectures where coarse-grained parallelism and locality are crucial, consider only a sub-space of transformations to avoid a combinatorial explosion in finding the transformations. The ensuing practical trade-offs lead to the exclusion of certain useful transformations, in particular, transformation compositions involving loop reversals and loop skewing by negative factors. In this paper, we propose an approach to address this limitation by modeling a much larger space of affine transformations in conjunction with the Pluto algorithm's cost function. We perform an experimental evaluation of both, the effect on compilation time, and performance of generated codes. The evaluation shows that our new framework, Pluto+, provides no degradation in performance in any of the Polybench benchmarks. For Lattice Boltzmann Method (LBM) codes with periodic boundary conditions, it provides a mean speedup of 1.33x over Pluto. We also show that Pluto+ does not increase compile times significantly. Experimental results on Polybench show that Pluto+ increases overall polyhedral source-to-source optimization time only by 15%. In cases where it improves execution time significantly, it increased polyhedral optimization time only by 2.04x. },
 acmid = {2688512},
 address = {New York, NY, USA},
 author = {Acharya, Aravind and Bondhugula, Uday},
 booktitle = {Proceedings of the 20th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
 doi = {10.1145/2688500.2688512},
 isbn = {978-1-4503-3205-7},
 keyword = {Affine transformations, affine scheduling, automatic parallelization, polyhedral model, stencil computations, tiling},
 link = {http://doi.acm.org/10.1145/2688500.2688512},
 location = {San Francisco, CA, USA},
 numpages = {11},
 pages = {54--64},
 publisher = {ACM},
 series = {PPoPP 2015},
 title = {PLUTO+: Near-complete Modeling of Affine Transformations for Parallelism and Locality},
 year = {2015}
}


@inproceedings{Wimmer:2015:LKR:2688500.2688547,
 abstract = { We present a new, concurrent, lock-free priority queue that relaxes the delete-min operation to allow deletion of any of the ρ smallest keys instead of only a minimal one, where ρ is a parameter that can be configured at runtime. It is built from a logarithmic number of sorted arrays, similar to log-structured merge-trees (LSM). For keys added and removed by the same thread the behavior is identical to a non-relaxed priority queue. We compare to state-of-the-art lock-free priority queues with both relaxed and non-relaxed semantics, showing high performance and good scalability of our approach. },
 acmid = {2688547},
 address = {New York, NY, USA},
 author = {Wimmer, Martin and Gruber, Jakob and Tr\"{a}ff, Jesper Larsson and Tsigas, Philippas},
 booktitle = {Proceedings of the 20th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
 doi = {10.1145/2688500.2688547},
 isbn = {978-1-4503-3205-7},
 keyword = {Task-parallel programming, concurrent data structure relaxation, priority-queue, shared memory},
 link = {http://doi.acm.org/10.1145/2688500.2688547},
 location = {San Francisco, CA, USA},
 numpages = {2},
 pages = {277--278},
 publisher = {ACM},
 series = {PPoPP 2015},
 title = {The Lock-free k-LSM Relaxed Priority Queue},
 year = {2015}
}


@inproceedings{Chen:2015:TNL:2688500.2688555,
 abstract = { This paper studies the essence of heterogeneity from the perspective of language mechanism design. The proposed mechanism, called tiles, is a program construct that bridges two relative levels of computation: an outer level of source data in larger, slower or more distributed memory and an inner level of data blocks in smaller, faster or more localized memory. },
 acmid = {2688555},
 address = {New York, NY, USA},
 author = {Chen, Yifeng and Cui, Xiang and Mei, Hong},
 booktitle = {Proceedings of the 20th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
 doi = {10.1145/2688500.2688555},
 isbn = {978-1-4503-3205-7},
 keyword = {Parallel Programming},
 link = {http://doi.acm.org/10.1145/2688500.2688555},
 location = {San Francisco, CA, USA},
 numpages = {2},
 pages = {287--288},
 publisher = {ACM},
 series = {PPoPP 2015},
 title = {Tiles: A New Language Mechanism for Heterogeneous Parallelism},
 year = {2015}
}


@article{Tallent:2015:DCS:2858788.2688516,
 abstract = { Two trends suggest network contention for one-sided messages is poised to become a performance problem that concerns application developers: an increased interest in one-sided programming models and a rising ratio of hardware threads to network injection bandwidth. Often it is difficult to reason about when one-sided tasks decrease or increase network contention. We present effective and portable techniques for diagnosing the causes and severity of one-sided message contention. To detect that a message is affected by contention, we maintain statistics representing instantaneous network resource demand. Using lightweight measurement and modeling, we identify the portion of a message's latency that is due to contention and whether contention occurs at the initiator or target. We attribute these metrics to program statements in their full static and dynamic context. We characterize contention for an important computational chemistry benchmark on InfiniBand, Cray Aries, and IBM Blue Gene/Q interconnects. We pinpoint the sources of contention, estimate their severity, and show that when message delivery time deviates from an ideal model, there are other messages contending for the same network links. With a small change to the benchmark, we reduce contention by 50% and improve total runtime by 20%. },
 acmid = {2688516},
 address = {New York, NY, USA},
 author = {Tallent, Nathan R. and Vishnu, Abhinav and Van Dam, Hubertus and Daily, Jeff and Kerbyson, Darren J. and Hoisie, Adolfy},
 doi = {10.1145/2858788.2688516},
 issn = {0362-1340},
 issue_date = {August 2015},
 journal = {SIGPLAN Not.},
 keyword = {Network contention/congestion, one-sided messages, performance analysis, performance modeling},
 link = {http://doi.acm.org/10.1145/2858788.2688516},
 month = {jan},
 number = {8},
 numpages = {10},
 pages = {130--139},
 publisher = {ACM},
 title = {Diagnosing the Causes and Severity of One-sided Message Contention},
 volume = {50},
 year = {2015}
}


@article{Alistarh:2015:SSR:2858788.2688523,
 abstract = { High-performance concurrent priority queues are essential for applications such as task scheduling and discrete event simulation. Unfortunately, even the best performing implementations do not scale past a number of threads in the single digits. This is because of the sequential bottleneck in accessing the elements at the head of the queue in order to perform a DeleteMin operation. In this paper, we present the SprayList, a scalable priority queue with relaxed ordering semantics. Starting from a non-blocking SkipList, the main innovation behind our design is that the DeleteMin operations avoid a sequential bottleneck by ``spraying'' themselves onto the head of the SkipList list in a coordinated fashion. The spraying is implemented using a carefully designed random walk, so that DeleteMin returns an element among the first O(p log^3 p) in the list, with high probability, where p is the number of threads. We prove that the running time of a DeleteMin operation is O(log^3 p), with high probability, independent of the size of the list. Our experiments show that the relaxed semantics allow the data structure to scale for high thread counts, comparable to a classic unordered SkipList. Furthermore, we observe that, for reasonably parallel workloads, the scalability benefits of relaxation considerably outweigh the additional work due to out-of-order execution. },
 acmid = {2688523},
 address = {New York, NY, USA},
 author = {Alistarh, Dan and Kopinsky, Justin and Li, Jerry and Shavit, Nir},
 doi = {10.1145/2858788.2688523},
 issn = {0362-1340},
 issue_date = {August 2015},
 journal = {SIGPLAN Not.},
 keyword = {Concurrent data structures, parallel algorithms},
 link = {http://doi.acm.org/10.1145/2858788.2688523},
 month = {jan},
 number = {8},
 numpages = {10},
 pages = {11--20},
 publisher = {ACM},
 title = {The SprayList: A Scalable Relaxed Priority Queue},
 volume = {50},
 year = {2015}
}


@inproceedings{Chabbi:2015:HPL:2688500.2688503,
 abstract = { Efficient locking mechanisms are critically important for high performance computers. On highly-threaded systems with a deep memory hierarchy, the throughput of traditional queueing locks, e.g., MCS locks, falls off due to NUMA effects. Two-level cohort locks perform better on NUMA systems, but fail to deliver top performance for deep NUMA hierarchies. In this paper, we describe a hierarchical variant of the MCS lock that adapts the principles of cohort locking for architectures with deep NUMA hierarchies. We describe analytical models for throughput and fairness of Cohort-MCS (C-MCS) and Hierarchical MCS (HMCS) locks that enable us to tailor these locks for high performance on any target platform without empirical tuning. Using these models, one can select parameters such that an HMCS lock will deliver better fairness than a C-MCS lock for a given throughput, or deliver better throughput for a given fairness. Our experiments show that, under high contention, a three-level HMCS lock delivers up to 7.6x higher lock throughput than a C-MCS lock on a 128-thread IBM Power 755 and a five-level HMCS lock delivers up to 72x higher lock throughput on a 4096-thread SGI UV 1000. On the K-means clustering code from the MineBench suit, a three-level HMCS lock reduces the running time by up to 55% compared to the C-MCS lock on a IBM Power 755. },
 acmid = {2688503},
 address = {New York, NY, USA},
 author = {Chabbi, Milind and Fagan, Michael and Mellor-Crummey, John},
 booktitle = {Proceedings of the 20th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
 doi = {10.1145/2688500.2688503},
 isbn = {978-1-4503-3205-7},
 keyword = {Analytical modeling, Hierarchical locks, Lock fairness, Lock throughput, MCS, NUMA, Spin locks},
 link = {http://doi.acm.org/10.1145/2688500.2688503},
 location = {San Francisco, CA, USA},
 numpages = {12},
 pages = {215--226},
 publisher = {ACM},
 series = {PPoPP 2015},
 title = {High Performance Locks for Multi-level NUMA Systems},
 year = {2015}
}


@article{Zhang:2015:LST:2858788.2688510,
 abstract = { Software transactional memory offers an appealing alternative to locks by improving programmability, reliability, and scalability. However, existing STMs are impractical because they add high instrumentation costs and often provide weak progress guarantees and/or semantics. This paper introduces a novel STM called LarkTM that provides three significant features. (1) Its instrumentation adds low overhead except when accesses actually conflict, enabling low single-thread overhead and scaling well on low-contention workloads. (2) It uses eager concurrency control mechanisms, yet naturally supports flexible conflict resolution, enabling strong progress guarantees. (3) It naturally provides strong atomicity semantics at low cost. LarkTM's design works well for low-contention workloads, but adds significant overhead under higher contention, so we design an adaptive version of LarkTM that uses alternative concurrency control for high-contention objects. An implementation and evaluation in a Java virtual machine show that the basic and adaptive versions of LarkTM not only provide low single-thread overhead, but their multithreaded performance compares favorably with existing high-performance STMs. },
 acmid = {2688510},
 address = {New York, NY, USA},
 author = {Zhang, Minjia and Huang, Jipeng and Cao, Man and Bond, Michael D.},
 doi = {10.1145/2858788.2688510},
 issn = {0362-1340},
 issue_date = {August 2015},
 journal = {SIGPLAN Not.},
 keyword = {Software transactional memory, biased reader-writer locks, concurrency control, managed languages, strong atomicity},
 link = {http://doi.acm.org/10.1145/2858788.2688510},
 month = {jan},
 number = {8},
 numpages = {12},
 pages = {97--108},
 publisher = {ACM},
 title = {Low-overhead Software Transactional Memory with Progress Guarantees and Strong Semantics},
 volume = {50},
 year = {2015}
}


@article{Gramoli:2015:MYE:2858788.2688501,
 abstract = { In this paper, we present the most extensive comparison of synchronization techniques. We evaluate 5 different synchronization techniques through a series of 31 data structure algorithms from the recent literature on 3 multicore platforms from Intel, Sun Microsystems and AMD. To this end, we developed in C/C++ and Java a new micro-benchmark suite, called Synchrobench, hence helping the community evaluate new data structures and synchronization techniques. The main conclusion of this evaluation is threefold: (i) although compare-and-swap helps achieving the best performance on multicores, doing so correctly is hard; (ii) optimistic locking offers varying performance results while transactional memory offers more consistent results; and (iii) copy-on-write and read-copy-update suffer more from contention than any other technique but could be combined with others to derive efficient algorithms. },
 acmid = {2688501},
 address = {New York, NY, USA},
 author = {Gramoli, Vincent},
 doi = {10.1145/2858788.2688501},
 issn = {0362-1340},
 issue_date = {August 2015},
 journal = {SIGPLAN Not.},
 keyword = {Benchmark, data structure, lock-freedom, reusability},
 link = {http://doi.acm.org/10.1145/2858788.2688501},
 month = {jan},
 number = {8},
 numpages = {10},
 pages = {1--10},
 publisher = {ACM},
 title = {More Than You Ever Wanted to Know About Synchronization: Synchrobench, Measuring the Impact of the Synchronization on Concurrent Algorithms},
 volume = {50},
 year = {2015}
}


@inproceedings{Saillard:2015:SVM:2688500.2688548,
 abstract = { Scientific applications mainly rely on the MPI parallel programming model to reach high performance on supercomputers. The advent of manycore architectures (larger number of cores and lower amount of memory per core) leads to mix MPI with a thread-based model like OpenMP. But integrating two different programming models inside the same application can be tricky and generate complex bugs. Thus, the correctness of hybrid programs requires a special care regarding MPI calls location. For example, identical MPI collective operations cannot be performed by multiple non-synchronized threads. To tackle this issue, this paper proposes a static analysis and a reduced dynamic instrumentation to detect bugs related to misuse of MPI collective operations inside or outside threaded regions. This work extends PARCOACH designed for MPI-only applications and keeps the compatibility with these algorithms. We validated our method on multiple hybrid benchmarks and applications with a low overhead. },
 acmid = {2688548},
 address = {New York, NY, USA},
 author = {Saillard, Emmanuelle and Carribault, Patrick and Barthou, Denis},
 booktitle = {Proceedings of the 20th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
 doi = {10.1145/2688500.2688548},
 isbn = {978-1-4503-3205-7},
 keyword = {Control Flow, MPI+OpenMP, Static, Verification},
 link = {http://doi.acm.org/10.1145/2688500.2688548},
 location = {San Francisco, CA, USA},
 numpages = {2},
 pages = {279--280},
 publisher = {ACM},
 series = {PPoPP 2015},
 title = {Static/Dynamic Validation of MPI Collective Communications in Multi-threaded Context},
 year = {2015}
}


@inproceedings{Chang:2015:PAG:2688500.2688520,
 abstract = { Verifying the correctness of the executions of a concurrent program is difficult because of its nondeterministic behavior. One of the verification methods is predicate detection, which predicts whether the user specified condition (predicate) could become true in any global states of the program. The method is predictive because it generates inferred execution paths from the observed execution path and then checks the predicate on the global states of inferred paths. One important part of predicate detection is global states enumeration, which generates the global states on inferred paths. Cooper and Marzullo gave the first enumeration algorithm based on a breadth first strategy (BFS). Later, many algorithms have been proposed to improve space and time complexity. None of them, however, takes parallelism into consideration. In this paper, we present the first parallel and online algorithm, named ParaMount, for global state enumeration. Our experimental results show that ParaMount speeds up the existing sequential algorithms by a factor of 6 with 8 threads. We have implemented an online predicate detector using ParaMount. For predicate detection, our detector based on ParaMount is 10 to 50 times faster than RV runtime (a verification tool that uses Cooper and Marzullo’s BFS enumeration algorithm). },
 acmid = {2688520},
 address = {New York, NY, USA},
 author = {Chang, Yen-Jung and Garg, Vijay K.},
 booktitle = {Proceedings of the 20th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
 doi = {10.1145/2688500.2688520},
 isbn = {978-1-4503-3205-7},
 keyword = {Global states, parallel algorithm, predicate detection},
 link = {http://doi.acm.org/10.1145/2688500.2688520},
 location = {San Francisco, CA, USA},
 numpages = {10},
 pages = {140--149},
 publisher = {ACM},
 series = {PPoPP 2015},
 title = {A Parallel Algorithm for Global States Enumeration in Concurrent Systems},
 year = {2015}
}


@proceedings{Moreira:2014:2555243,
 abstract = {It is our great pleasure to welcome you to the 19th ACM Symposium on Principles and Practice of Parallel Programming -- PPoPP'14. PPoPP is a forum for leading work on all aspects of parallel programming, including foundational and theoretical aspects, techniques, tools, and practical experiences. Given the rise of parallel architectures into the consumer market (desktops, laptops, and mobile devices), we made an effort to attract work that addresses new parallel workloads, techniques and tools that attempt to improve the productivity of parallel programming, and work towards improved synergy with such emerging architectures. In 2014, PPoPP will again be co-located with the 20th IEEE International Symposium on High Performance Computer Architecture (HPCA-2014) and 2014 International Symposium on Code Generation and Optimization (CGO-2014). Our joint events will take place in sunny Orlando, Florida, providing both a mid-winter escape from colder climates and also an opportunity for children and family-friendly activities. Our call for papers was enthusiastically answered with 184 submissions from academia, industry and national research facilities from all geographies. The PPoPP'14 Program Committee and external reviewers diligently evaluated each of the papers carefully and, after a rebuttal period and an in-person meeting, selected 28 papers for formal presentation at the symposium. An additional 17 submissions were selected for presentation at the Poster Session. Two joint (with HPCA and CGO) keynotes, by Mark Hill (University of Wisconsin-Madison) and Norm Rubin (NVIDIA), and a PPoPP keynote by Kunle Olukotun (Stanford University) complete our technical program.},
 address = {New York, NY, USA},
 isbn = {978-1-4503-2656-8},
 location = {Orlando, Florida, USA},
 publisher = {ACM},
 title = {PPoPP '14: Proceedings of the 19th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
 year = {2014}
}


@article{Ramachandran:2015:CFC:2858788.2688551,
 abstract = { We present a new lock-based algorithm for concurrent manipulation of a binary search tree in an asynchronous shared memory system that supports search, insert and delete operations. Some of the desirable characteristics of our algorithm are: (i) a search operation uses only read and write instructions, (ii) an insert operation does not acquire any locks, and (iii) a delete operation only needs to lock up to four edges in the absence of contention. Our algorithm is based on an internal representation of a search tree and it operates at edge-level (locks edges) rather than at node-level (locks nodes); this minimizes the contention window of a write operation and improves the system throughput. Our experiments indicate that our lock-based algorithm outperforms existing algorithms for a concurrent binary search tree for medium-sized and larger trees, achieving up to 59% higher throughput than the next best algorithm. },
 acmid = {2688551},
 address = {New York, NY, USA},
 author = {Ramachandran, Arunmoezhi and Mittal, Neeraj},
 doi = {10.1145/2858788.2688551},
 issn = {0362-1340},
 issue_date = {August 2015},
 journal = {SIGPLAN Not.},
 keyword = {Binary Search Tree, Concurrent Data Structure, Edge-Based Locking, Internal Representation, Lock-Based Algorithm},
 link = {http://doi.acm.org/10.1145/2858788.2688551},
 month = {jan},
 number = {8},
 numpages = {2},
 pages = {281--282},
 publisher = {ACM},
 title = {CASTLE: Fast Concurrent Internal Binary Search Tree Using Edge-based Locking},
 volume = {50},
 year = {2015}
}


@article{Haidar:2015:TBL:2858788.2688534,
 abstract = { As hardware evolves, an increasingly effective approach to develop energy efficient, high-performance solvers, is to design them to work on many small and independent problems. Indeed, many applications already need this functionality, especially for GPUs, which are known to be currently about four to five times more energy efficient than multicore CPUs for every floating-point operation. In this paper, we describe the development of the main one-sided factorizations: LU, QR, and Cholesky; that are needed for a set of small dense matrices to work in parallel. We refer to such algorithms as batched factorizations. Our approach is based on representing the algorithms as a sequence of batched BLAS routines for GPU-contained execution. Note that this is similar in functionality to the LAPACK and the hybrid MAGMA algorithms for large-matrix factorizations. But it is different from a straightforward approach, whereby each of GPU's symmetric multiprocessors factorizes a single problem at a time. We illustrate how our performance analysis together with the profiling and tracing tools guided the development of batched factorizations to achieve up to 2-fold speedup and 3-fold better energy efficiency compared to our highly optimized batched CPU implementations based on the MKL library on a two-sockets, Intel Sandy Bridge server. Compared to a batched LU factorization featured in the NVIDIA's CUBLAS library for GPUs, we achieves up to 2.5-fold speedup on the K40 GPU. },
 acmid = {2688534},
 address = {New York, NY, USA},
 author = {Haidar, Azzam and Dong, Tingxing and Luszczek, Piotr and Tomov, Stanimire and Dongarra, Jack},
 doi = {10.1145/2858788.2688534},
 issn = {0362-1340},
 issue_date = {August 2015},
 journal = {SIGPLAN Not.},
 keyword = {batched factorization, hardware accelerators, numerical linear algebra, numerical software libraries, one-sided factorization algorithms},
 link = {http://doi.acm.org/10.1145/2858788.2688534},
 month = {jan},
 number = {8},
 numpages = {2},
 pages = {261--262},
 publisher = {ACM},
 title = {Towards Batched Linear Solvers on Accelerated Hardware Platforms},
 volume = {50},
 year = {2015}
}


@article{Vassiliadis:2015:PMR:2858788.2688546,
 abstract = { We introduce a task-based programming model and runtime system that exploit the observation that not all parts of a program are equally significant for the accuracy of the end-result, in order to trade off the quality of program outputs for increased energy-efficiency. This is done in a structured and flexible way, allowing for easy exploitation of different points in the quality/energy space, without adversely affecting application performance. The runtime system can apply a number of different policies to decide whether it will execute less-significant tasks accurately or approximately. The experimental evaluation indicates that our system can achieve an energy reduction of up to 83% compared with a fully accurate execution and up to 35% compared with an approximate version employing loop perforation. At the same time, our approach always results in graceful quality degradation. },
 acmid = {2688546},
 address = {New York, NY, USA},
 author = {Vassiliadis, Vassilis and Parasyris, Konstantinos and Chalios, Charalambos and Antonopoulos, Christos D. and Lalis, Spyros and Bellas, Nikolaos and Vandierendonck, Hans and Nikolopoulos, Dimitrios S.},
 doi = {10.1145/2858788.2688546},
 issn = {0362-1340},
 issue_date = {August 2015},
 journal = {SIGPLAN Not.},
 keyword = {Energy saving, approximate computing, programming model, runtime system},
 link = {http://doi.acm.org/10.1145/2858788.2688546},
 month = {jan},
 number = {8},
 numpages = {2},
 pages = {275--276},
 publisher = {ACM},
 title = {A Programming Model and Runtime System for Significance-aware Energy-efficient Computing},
 volume = {50},
 year = {2015}
}


@inproceedings{Ravishankar:2015:DMC:2688500.2688515,
 abstract = { Many applications feature a mix of irregular and regular computational structures. For example, codes using adaptive mesh refinement (AMR) typically use a collection of regular blocks, where the number of blocks and the relationship between blocks is irregular. The computational structure in such applications generally involves regular (affine) loop computations within some number of innermost loops, while outer loops exhibit irregularity due to data-dependent control flow and indirect array access patterns. Prior approaches to distributed memory parallelization do not handle such computations effectively. They either target loop nests that are completely affine using polyhedral frameworks, or treat all loops as irregular. Consequently, the generated distributed memory code contains artifacts that disrupt the regular nature of previously affine innermost loops of the computation. This hampers subsequent optimizations to improve on-node performance. We propose a code generation framework that can effectively transform such applications for execution on distributed memory systems. Our approach generates distributed memory code which preserves program properties that enable subsequent polyhederal optimizations. Simultaneously, it addresses a major memory bottleneck of prior techniques that limits the scalability of the generated code. The effectiveness of the proposed framework is demonstrated on computations that are mixed regular/irregular, completely regular, and completely irregular. },
 acmid = {2688515},
 address = {New York, NY, USA},
 author = {Ravishankar, Mahesh and Dathathri, Roshan and Elango, Venmugil and Pouchet, Louis-No\"{e}l and Ramanujam, J. and Rountev, Atanas and Sadayappan, P.},
 booktitle = {Proceedings of the 20th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
 doi = {10.1145/2688500.2688515},
 isbn = {978-1-4503-3205-7},
 keyword = {Distributed Memory, Inspector/Executor, Irregular Computation, Polyhedral Compilation},
 link = {http://doi.acm.org/10.1145/2688500.2688515},
 location = {San Francisco, CA, USA},
 numpages = {11},
 pages = {65--75},
 publisher = {ACM},
 series = {PPoPP 2015},
 title = {Distributed Memory Code Generation for Mixed Irregular/Regular Computations},
 year = {2015}
}


@inproceedings{Tang:2015:CWI:2688500.2688514,
 abstract = { State-of-the-art cache-oblivious parallel algorithms for dynamic programming (DP) problems usually guarantee asymptotically optimal cache performance without any tuning of cache parameters, but they often fail to exploit the theoretically best parallelism at the same time. While these algorithms achieve cache-optimality through the use of a recursive divide-and-conquer (DAC) strategy, scheduling tasks at the granularity of task dependency introduces artificial dependencies in addition to those arising from the defining recurrence equations. We removed the artificial dependency by scheduling tasks ready for execution as soon as all its real dependency constraints are satisfied, while preserving the cache-optimality by inheriting the DAC strategy. We applied our approach to a set of widely known dynamic programming problems, such as Floyd-Warshall's All-Pairs Shortest Paths, Stencil, and LCS. Theoretical analyses show that our techniques improve the span of 2-way DAC-based Floyd Warshall's algorithm on an $n$ node graph from $Thn^2n$ to $Thn$, stencil computations on a $d$-dimensional hypercubic grid of width $w$ for $h$ time steps from $Th(d^2 h) w^ (d+2) - 1$ to $Thh$, and LCS on two sequences of length $n$ each from $Thn^_2 3$ to $Thn$. In each case, the total work and cache complexity remain asymptotically optimal. Experimental measurements exhibit a $3$ - $5$ times improvement in absolute running time, $10$ - $20$ times improvement in burdened span by Cilkview, and approximately the same L1/L2 cache misses by PAPI. },
 acmid = {2688514},
 address = {New York, NY, USA},
 author = {Tang, Yuan and You, Ronghui and Kan, Haibin and Tithi, Jesmin Jahan and Ganapathi, Pramod and Chowdhury, Rezaul A.},
 booktitle = {Proceedings of the 20th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
 doi = {10.1145/2688500.2688514},
 isbn = {978-1-4503-3205-7},
 keyword = {Cilk, cache-oblivious parallel algorithms, cache-oblivious wavefront, dynamic programming, multi-core, nested parallel computation},
 link = {http://doi.acm.org/10.1145/2688500.2688514},
 location = {San Francisco, CA, USA},
 numpages = {10},
 pages = {205--214},
 publisher = {ACM},
 series = {PPoPP 2015},
 title = {Cache-oblivious Wavefront: Improving Parallelism of Recursive Dynamic Programming Algorithms Without Losing Cache-efficiency},
 year = {2015}
}


@proceedings{Asenjo:2016:2851141,
 abstract = {It is our great pleasure to welcome you to the 21st ACM Symposium on Principles and Practice of Parallel Programming --- PPoPP'16, in Barcelona, Spain. For the first time, PPoPP is to be held in Europe and we hope you enjoy the history, culture, cuisine and cosmopolitan atmosphere of the thriving and spectacular city of Barcelona. PPoPP is the leading forum for work on all aspects of parallel programming, including foundational and theoretical aspects, techniques, tools, and practical experiences. Given the rise of parallel architectures into the consumer market (desktops, laptops, and mobile devices), we made an effort to attract work that addresses new parallel workloads, techniques and tools that attempt to improve the productivity of parallel programming, and work towards improved synergy with such emerging architectures.},
 address = {New York, NY, USA},
 isbn = {978-1-4503-4092-2},
 location = {Barcelona, Spain},
 publisher = {ACM},
 title = {PPoPP '16: Proceedings of the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
 year = {2016}
}


@inproceedings{Kim:2015:OUP:2688500.2688531,
 abstract = { This paper proposes a novel SPMD programming model of OpenACC. Our model integrates the different granularities of parallelism from vector-level parallelism to node-level parallelism into a single, unified model based on OpenACC. It allows programmers to write programs for multiple accelerators using a uniform programming model whether they are in shared or distributed memory systems. We implement a prototype of our model and evaluate its performance with a GPU-based supercomputer using three benchmark applications. },
 acmid = {2688531},
 address = {New York, NY, USA},
 author = {Kim, Jungwon and Lee, Seyong and Vetter, Jeffrey S.},
 booktitle = {Proceedings of the 20th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
 doi = {10.1145/2688500.2688531},
 isbn = {978-1-4503-3205-7},
 keyword = {Accelerators, Heterogeneous computing, OpenACC, Programming models},
 link = {http://doi.acm.org/10.1145/2688500.2688531},
 location = {San Francisco, CA, USA},
 numpages = {2},
 pages = {257--258},
 publisher = {ACM},
 series = {PPoPP 2015},
 title = {An OpenACC-based Unified Programming Model for Multi-accelerator Systems},
 year = {2015}
}


@article{West:2015:ERO:2858788.2688545,
 abstract = { Making threaded programs safe and easy to reason about is one of the chief difficulties in modern programming. This work provides an efficient execution model and implementation for SCOOP, a concurrency approach that provides not only data-race freedom but also pre/postcondition reasoning guarantees between threads. The extensions we propose influence the underlying semantics to increase the amount of concurrent execution that is possible, exclude certain classes of deadlocks, and enable greater performance. },
 acmid = {2688545},
 address = {New York, NY, USA},
 author = {West, Scott and Nanz, Sebastian and Meyer, Bertrand},
 doi = {10.1145/2858788.2688545},
 issn = {0362-1340},
 issue_date = {August 2015},
 journal = {SIGPLAN Not.},
 keyword = {Concurrency, object-oriented, optimization, performance},
 link = {http://doi.acm.org/10.1145/2858788.2688545},
 month = {jan},
 number = {8},
 numpages = {2},
 pages = {273--274},
 publisher = {ACM},
 title = {Efficient and Reasonable Object-oriented Concurrency},
 volume = {50},
 year = {2015}
}


@inproceedings{Al-Saber:2015:SSC:2688500.2688527,
 abstract = { Offloading computations to multiple GPUs is not an easy task. It requires decomposing data, distributing computations and handling communication manually. Drop-in GPU libraries have made it easy to offload computations to multiple GPUs by hiding this complexity inside library calls. Such encapsulation prevents the reuse of the data between successive kernel invocations resulting in redundant communication. This limitation exists in multi-GPU libraries like CUBLASXT. In this paper, we introduce SemCache++, a semantics-aware GPU cache that automatically manages communication between the CPU and multiple GPUs in addition to optimizing communication by eliminating redundant transfers using caching. SemCache++ is used to build the first multi-GPU drop-in replacement library that (a) uses the virtual memory to automatically manage and optimize multi-GPU communication and (b) requires no program rewriting or annotations. Our caching technique is efficient; it uses a two level caching directory to track matrices and sub-matrices. Experimental results show that our system can eliminate redundant communication and deliver significant performance improvements over multi-GPU libraries like CUBLASXT. },
 acmid = {2688527},
 address = {New York, NY, USA},
 author = {Al-Saber, Nabeel and Kulkarni, Milind},
 booktitle = {Proceedings of the 20th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
 doi = {10.1145/2688500.2688527},
 isbn = {978-1-4503-3205-7},
 keyword = {Communication optimization, GPGPU, Multi-GPU offloading},
 link = {http://doi.acm.org/10.1145/2688500.2688527},
 location = {San Francisco, CA, USA},
 numpages = {2},
 pages = {255--256},
 publisher = {ACM},
 series = {PPoPP 2015},
 title = {SemCache++: Semantics-aware Caching for Efficient multi-GPU Offloading},
 year = {2015}
}


@inproceedings{You:2015:VFO:2688500.2688505,
 abstract = { The interest in using multiple graphics processing units (GPUs) to accelerate applications has increased in recent years. However, the existing heterogeneous programming models (e.g., OpenCL) abstract details of GPU devices at the per-device level and require programmers to explicitly schedule their kernel tasks on a system equipped with multiple GPU devices. Unfortunately, multiple applications running on a multi-GPU system may compete for some of the GPU devices while leaving other GPU devices unused. Moreover, the distributed memory model defined in OpenCL, where each device has its own memory space, increases the complexity of managing the memory among multiple GPU devices. In this article we propose a framework (called VirtCL) that reduces the programming burden by acting as a layer between the programmer and the native OpenCL run-time system for abstracting multiple devices into a single virtual device and for scheduling computations and communications among the multiple devices. VirtCL comprises two main components: (1) a front-end library, which exposes primary OpenCL APIs and the virtual device, and (2) a back-end run-time system (called CLDaemon) for scheduling and dispatching kernel tasks based on a history-based scheduler. The front-end library forwards computation requests to the back-end CLDaemon, which then schedules and dispatches the requests. We also propose a history-based scheduler that is able to schedule kernel tasks in a contention- and communication-aware manner. Experiments demonstrated that the VirtCL framework introduced a small overhead (mean of 6%) but outperformed the native OpenCL run-time system for most benchmarks in the Rodinia benchmark suite, which was due to the abstraction layer eliminating the time-consuming initialization of OpenCL contexts. We also evaluated different scheduling policies in VirtCL with a real-world application (clsurf) and various synthetic workload traces. The results indicated that the VirtCL framework provides scalability for multiple kernel tasks running on multi-GPU systems. },
 acmid = {2688505},
 address = {New York, NY, USA},
 author = {You, Yi-Ping and Wu, Hen-Jung and Tsai, Yeh-Ning and Chao, Yen-Ting},
 booktitle = {Proceedings of the 20th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
 doi = {10.1145/2688500.2688505},
 isbn = {978-1-4503-3205-7},
 keyword = {OpenCL, device abstraction, multitasking},
 link = {http://doi.acm.org/10.1145/2688500.2688505},
 location = {San Francisco, CA, USA},
 numpages = {12},
 pages = {161--172},
 publisher = {ACM},
 series = {PPoPP 2015},
 title = {VirtCL: A Framework for OpenCL Device Abstraction and Management},
 year = {2015}
}


@inproceedings{Xie:2015:SAT:2688500.2688508,
 abstract = { Large-scale graph-structured computation usually exhibits iterative and convergence-oriented computing nature, where input data is computed iteratively until a convergence condition is reached. Such features have led to the development of two different computation modes for graph-structured programs, namely synchronous (Sync) and asynchronous (Async) modes. Unfortunately, there is currently no in-depth study on their execution properties and thus programmers have to manually choose a mode, either requiring a deep understanding of underlying graph engines, or suffering from suboptimal performance. This paper makes the first comprehensive characterization on the performance of the two modes on a set of typical graph-parallel applications. Our study shows that the performance of the two modes varies significantly with different graph algorithms, partitioning methods, execution stages, input graphs and cluster scales, and no single mode consistently outperforms the other. To this end, this paper proposes Hsync, a hybrid graph computation mode that adaptively switches a graph-parallel program between the two modes for optimal performance. Hsync constantly collects execution statistics on-the-fly and leverages a set of heuristics to predict future performance and determine when a mode switch could be profitable. We have built online sampling and offline profiling approaches combined with a set of heuristics to accurately predicting future performance in the two modes. A prototype called PowerSwitch has been built based on PowerGraph, a state-of-the-art distributed graph-parallel system, to support adaptive execution of graph algorithms. On a 48-node EC2-like cluster, PowerSwitch consistently outperforms the best of both modes, with a speedup ranging from 9% to 73% due to timely switch between two modes. },
 acmid = {2688508},
 address = {New York, NY, USA},
 author = {Xie, Chenning and Chen, Rong and Guan, Haibing and Zang, Binyu and Chen, Haibo},
 booktitle = {Proceedings of the 20th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
 doi = {10.1145/2688500.2688508},
 isbn = {978-1-4503-3205-7},
 keyword = {Computation Modes, Distributed Graph-parallel Computation},
 link = {http://doi.acm.org/10.1145/2688500.2688508},
 location = {San Francisco, CA, USA},
 numpages = {11},
 pages = {194--204},
 publisher = {ACM},
 series = {PPoPP 2015},
 title = {SYNC or ASYNC: Time to Fuse for Distributed Graph-parallel Computation},
 year = {2015}
}


@inproceedings{Xiang:2015:SPH:2688500.2688506,
 abstract = { Best-effort hardware transactional memory (HTM) allows complex operations to execute atomically and in parallel, so long as hardware buffers do not overflow, and conflicts are not encountered with concurrent operations. We describe a programming technique and compiler support to reduce both overflow and conflict rates by partitioning common operations into read-mostly (planning) and write-mostly (completion) operations, which then execute separately. The completion operation remains transactional; planning can often occur in ordinary code. High-level (semantic) atomicity for the overall operation is ensured by passing an application-specific validator object between planning and completion. Transparent composition of partitioned operations is made possible through fully-automated compiler support, which migrates all planning operations out of the parent transaction while respecting all program data flow and dependences. For both micro- and macro-benchmarks, experiments on IBM z-Series and Intel Haswell machines demonstrate that partitioning can lead to dramatically lower abort rates and higher scalability. },
 acmid = {2688506},
 address = {New York, NY, USA},
 author = {Xiang, Lingxiang and Scott, Michael L.},
 booktitle = {Proceedings of the 20th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
 doi = {10.1145/2688500.2688506},
 isbn = {978-1-4503-3205-7},
 keyword = {Compiler Automation, Hardware Transactional Memory, Partitioned Transactions},
 link = {http://doi.acm.org/10.1145/2688500.2688506},
 location = {San Francisco, CA, USA},
 numpages = {11},
 pages = {76--86},
 publisher = {ACM},
 series = {PPoPP 2015},
 title = {Software Partitioning of Hardware Transactions},
 year = {2015}
}


@article{Baldassin:2015:PID:2858788.2688504,
 abstract = { Although dynamic memory management accounts for a significant part of the execution time on many modern software systems, its impact on the performance of transactional memory systems has been mostly overlooked. In order to shed some light into this subject, this paper conducts a thorough investigation of the interplay between memory allocators and software transactional memory (STM) systems. We show that allocators can interfere with the way memory addresses are mapped to versioned locks on state-of-the-art software transactional memory implementations. Moreover, we observed that key aspects of allocators such as false sharing avoidance, scalability, and locality have a drastic impact on the final performance. For instance, we have detected performance differences of up to 171% in the STAMP applications when using distinct allocators. Moreover, we show that optimizations at the STM-level (such as caching transactional objects) are not effective when a modern allocator is already in use. All in all, our study highlights the importance of reporting the allocator utilized in the performance evaluation of transactional memory systems. },
 acmid = {2688504},
 address = {New York, NY, USA},
 author = {Baldassin, Alexandro and Borin, Edson and Araujo, Guido},
 doi = {10.1145/2858788.2688504},
 issn = {0362-1340},
 issue_date = {August 2015},
 journal = {SIGPLAN Not.},
 keyword = {Transactional memory, dynamic memory allocation, performance evaluation},
 link = {http://doi.acm.org/10.1145/2858788.2688504},
 month = {jan},
 number = {8},
 numpages = {10},
 pages = {87--96},
 publisher = {ACM},
 title = {Performance Implications of Dynamic Memory Allocators on Transactional Memory Systems},
 volume = {50},
 year = {2015}
}


@article{Benson:2015:FPP:2858788.2688513,
 abstract = { Matrix multiplication is a fundamental computation in many scientific disciplines. In this paper, we show that novel fast matrix multiplication algorithms can significantly outperform vendor implementations of the classical algorithm and Strassen's fast algorithm on modest problem sizes and shapes. Furthermore, we show that the best choice of fast algorithm depends not only on the size of the matrices but also the shape. We develop a code generation tool to automatically implement multiple sequential and shared-memory parallel variants of each fast algorithm, including our novel parallelization scheme. This allows us to rapidly benchmark over 20 fast algorithms on several problem sizes. Furthermore, we discuss a number of practical implementation issues for these algorithms on shared-memory machines that can direct further research on making fast algorithms practical. },
 acmid = {2688513},
 address = {New York, NY, USA},
 author = {Benson, Austin R. and Ballard, Grey},
 doi = {10.1145/2858788.2688513},
 issn = {0362-1340},
 issue_date = {August 2015},
 journal = {SIGPLAN Not.},
 keyword = {dense linear algebra, fast matrix multiplication, parallel linear algebra, shared memory},
 link = {http://doi.acm.org/10.1145/2858788.2688513},
 month = {jan},
 number = {8},
 numpages = {12},
 pages = {42--53},
 publisher = {ACM},
 title = {A Framework for Practical Parallel Fast Matrix Multiplication},
 volume = {50},
 year = {2015}
}


@article{Ashari:2015:OML:2858788.2688521,
 abstract = { Exploitation of parallel architectures has become critical to scalable machine learning (ML). Since a wide range of ML algorithms employ linear algebraic operators, GPUs with BLAS libraries are a natural choice for such an exploitation. Two approaches are commonly pursued: (i) developing specific GPU accelerated implementations of complete ML algorithms; and (ii) developing GPU kernels for primitive linear algebraic operators like matrix-vector multiplication, which are then used in developing ML algorithms. This paper extends the latter approach by developing fused kernels for a combination of primitive operators that are commonly found in popular ML algorithms. We identify the generic pattern of computation (alpha * X^T (v * (X * y)) + beta * z) and its various instantiations. We develop a fused kernel to optimize this computation on GPUs -- with specialized techniques to handle both sparse and dense matrices. This approach not only reduces the cost of data loads due to improved temporal locality but also enables other optimizations like coarsening and hierarchical aggregation of partial results. We also present an analytical model that considers input data characteristics and available GPU resources to estimate near-optimal settings for kernel launch parameters. The proposed approach provides speedups ranging from 2 to 67 for different instances of the generic pattern compared to launching multiple operator-level kernels using GPU accelerated libraries. We conclude by demonstrating the effectiveness of the approach in improving end-to-end performance on an entire ML algorithm. },
 acmid = {2688521},
 address = {New York, NY, USA},
 author = {Ashari, Arash and Tatikonda, Shirish and Boehm, Matthias and Reinwald, Berthold and Campbell, Keith and Keenleyside, John and Sadayappan, P.},
 doi = {10.1145/2858788.2688521},
 issn = {0362-1340},
 issue_date = {August 2015},
 journal = {SIGPLAN Not.},
 keyword = {Dense, Fused Kernel, GPU, Machine Learning, Sparse},
 link = {http://doi.acm.org/10.1145/2858788.2688521},
 month = {jan},
 number = {8},
 numpages = {10},
 pages = {173--182},
 publisher = {ACM},
 title = {On Optimizing Machine Learning Workloads via Kernel Fusion},
 volume = {50},
 year = {2015}
}


@article{Wang:2015:GHG:2858788.2688538,
 abstract = { For large-scale graph analytics on the GPU, the irregularity of data access and control flow and the complexity of programming GPUs have been two significant challenges for developing a programmable high-performance graph library. "Gunrock", our graph-processing system, uses a high-level bulk-synchronous abstraction with traversal and computation steps, designed specifically for the GPU. Gunrock couples high performance with a high-level programming model that allows programmers to quickly develop new graph primitives with less than 300 lines of code. We evaluate Gunrock on five graph primitives and show that Gunrock has at least an order of magnitude speedup over Boost and PowerGraph, comparable performance to the fastest GPU hardwired primitives, and better performance than any other GPU high-level graph library. },
 acmid = {2688538},
 address = {New York, NY, USA},
 author = {Wang, Yangzihao and Davidson, Andrew and Pan, Yuechao and Wu, Yuduo and Riffel, Andy and Owens, John D.},
 doi = {10.1145/2858788.2688538},
 issn = {0362-1340},
 issue_date = {August 2015},
 journal = {SIGPLAN Not.},
 keyword = {GPU, Graph Processing, Runtime Framework},
 link = {http://doi.acm.org/10.1145/2858788.2688538},
 month = {jan},
 number = {8},
 numpages = {2},
 pages = {265--266},
 publisher = {ACM},
 title = {Gunrock: A High-performance Graph Processing Library on the GPU},
 volume = {50},
 year = {2015}
}


@inproceedings{Cogumbreiro:2015:DDV:2688500.2688519,
 abstract = { We present Armus, a dynamic verification tool for deadlock detection and avoidance specialised in barrier synchronisation. Barriers are used to coordinate the execution of groups of tasks, and serve as a building block of parallel computing. Our tool verifies more barrier synchronisation patterns than current state-of-the-art. To improve the scalability of verification, we introduce a novel event-based representation of concurrency constraints, and a graph-based technique for deadlock analysis. The implementation is distributed and fault-tolerant, and can verify X10 and Java programs. To formalise the notion of barrier deadlock, we introduce a core language expressive enough to represent the three most widespread barrier synchronisation patterns: group, split-phase, and dynamic membership. We propose a graph analysis technique that selects from two alternative graph representations: the Wait-For Graph, that favours programs with more tasks than barriers; and the State Graph, optimised for programs with more barriers than tasks. We prove that finding a deadlock in either representation is equivalent, and that the verification algorithm is sound and complete with respect to the notion of deadlock in our core language. Armus is evaluated with three benchmark suites in local and distributed scenarios. The benchmarks show that graph analysis with automatic graph-representation selection can record a 7-fold execution increase versus the traditional fixed graph representation. The performance measurements for distributed deadlock detection between 64 processes show negligible overheads. },
 acmid = {2688519},
 address = {New York, NY, USA},
 author = {Cogumbreiro, Tiago and Hu, Raymond and Martins, Francisco and Yoshida, Nobuko},
 booktitle = {Proceedings of the 20th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
 doi = {10.1145/2688500.2688519},
 isbn = {978-1-4503-3205-7},
 keyword = {Java, X10, barrier synchronisation, deadlock avoidance, deadlock detection, phasers},
 link = {http://doi.acm.org/10.1145/2688500.2688519},
 location = {San Francisco, CA, USA},
 numpages = {11},
 pages = {150--160},
 publisher = {ACM},
 series = {PPoPP 2015},
 title = {Dynamic Deadlock Verification for General Barrier Synchronisation},
 year = {2015}
}


@article{Seo:2015:GGS:2858788.2688526,
 abstract = { Fast processing graph algorithms for large-scale graphs becomes increasingly important. Besides, there have been many attempts to process graph applications by exploiting the massive amount of parallelism of GPUs. However, most of the existing methods fail to process large-scale graphs that do not fit in GPU device memory. We propose a fast and scalable parallel processing method GStream that fully exploits the computational power of GPUs for processing large-scale graphs (e.g., billions vertices) very efficiently. It exploits the concept of nested-loop theta-join and multiple asynchronous GPU streams. Extensive experimental results show that GStream consistently and significantly outperforms the state-of-the art method. },
 acmid = {2688526},
 address = {New York, NY, USA},
 author = {Seo, Hyunseok and Kim, Jinwook and Kim, Min-Soo},
 doi = {10.1145/2858788.2688526},
 issn = {0362-1340},
 issue_date = {August 2015},
 journal = {SIGPLAN Not.},
 keyword = {GPU, Graph processing, Large-scale, Stream},
 link = {http://doi.acm.org/10.1145/2858788.2688526},
 month = {jan},
 number = {8},
 numpages = {2},
 pages = {253--254},
 publisher = {ACM},
 title = {GStream: A Graph Streaming Processing Method for Large-scale Graphs on GPUs},
 volume = {50},
 year = {2015}
}


@inproceedings{Piao:2015:JJF:2688500.2688525,
 abstract = { This paper introduces jAWS, a JavaScript framework for adaptive work sharing between CPU and GPU for data-parallel workloads. Unlike conventional heterogeneous parallel programming environments for JavaScript, which use only one compute device when executing a single kernel, jAWS accelerates kernel execution by exploiting both devices to realize full performance potential of heterogeneous multicores. jAWS employs an efficient work partitioning algorithm that finds an optimal work distribution between the two devices without requiring offline profiling. The jAWS runtime provides shared arrays for multiple parallel contexts, hence eliminating extra copy overhead for input and output data. Our preliminary evaluation with both CPU-friendly and GPU-friendly benchmarks demonstrates that jAWS provides good load balancing and efficient data communication between parallel contexts, to significantly outperform best single-device execution. },
 acmid = {2688525},
 address = {New York, NY, USA},
 author = {Piao, Xianglan and Kim, Channoh and Oh, Younghwan and Li, Huiying and Kim, Jincheon and Kim, Hanjun and Lee, Jae W.},
 booktitle = {Proceedings of the 20th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
 doi = {10.1145/2688500.2688525},
 isbn = {978-1-4503-3205-7},
 keyword = {GPU, JavaScript, Web browser, data parallelism, heterogeneity, multi-core, scheduler, work sharing},
 link = {http://doi.acm.org/10.1145/2688500.2688525},
 location = {San Francisco, CA, USA},
 numpages = {2},
 pages = {251--252},
 publisher = {ACM},
 series = {PPoPP 2015},
 title = {JAWS: A JavaScript Framework for Adaptive CPU-GPU Work Sharing},
 year = {2015}
}


@inproceedings{McPherson:2015:FPL:2688500.2688524,
 abstract = { Fence placement is required to ensure legacy parallel programs operate correctly on relaxed architectures. The challenge is to place as few fences as possible without compromising correctness. By identifying necessary conditions for a read to be an acquire we improve upon the state of the art for legacy DRF programs by up to 2.64x. },
 acmid = {2688524},
 address = {New York, NY, USA},
 author = {McPherson, Andrew J. and Nagarajan, Vijay and Sarkar, Susmit and Cintra, Marcelo},
 booktitle = {Proceedings of the 20th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
 doi = {10.1145/2688500.2688524},
 isbn = {978-1-4503-3205-7},
 keyword = {Fence Placement, Relaxed Memory Models},
 link = {http://doi.acm.org/10.1145/2688500.2688524},
 location = {San Francisco, CA, USA},
 numpages = {2},
 pages = {249--250},
 publisher = {ACM},
 series = {PPoPP 2015},
 title = {Fence Placement for Legacy Data-race-free Programs via Synchronization Read Detection},
 year = {2015}
}


@inproceedings{Harshvardhan:2015:HAR:2688500.2700994,
 abstract = { Large-scale graph computing has become critical due to the ever-increasing size of data. However, distributed graph computations are limited in their scalability and performance due to the heavy communication inherent in such computations. This is exacerbated in scale-free networks, such as social and web graphs, which contain hub vertices that have large degrees and therefore send a large number of messages over the network. Furthermore, many graph algorithms and computations send the same data to each of the neighbors of a vertex. Our proposed approach recognizes this, and reduces communication performed by the algorithm without change to user-code, through a hierarchical machine model imposed upon the input graph. The hierarchical model takes advantage of locale information of the neighboring vertices to reduce communication, both in message volume and total number of bytes sent. It is also able to better exploit the machine hierarchy to further reduce the communication costs, by aggregating traffic between different levels of the machine hierarchy. Results of an implementation in the STAPL GL shows improved scalability and performance over the traditional level-synchronous approach, with 2.5×-8× improvement for a variety of graph algorithms at 12,000+ cores. },
 acmid = {2700994},
 address = {New York, NY, USA},
 author = {Harshvardhan and Amato, Nancy M. and Rauchwerger, Lawrence},
 booktitle = {Proceedings of the 20th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
 doi = {10.1145/2688500.2700994},
 isbn = {978-1-4503-3205-7},
 keyword = {Big Data, Distributed Computing, Graph Analytics, Parallel Graph Processing},
 link = {http://doi.acm.org/10.1145/2688500.2700994},
 location = {San Francisco, CA, USA},
 numpages = {2},
 pages = {285--286},
 publisher = {ACM},
 series = {PPoPP 2015},
 title = {A Hierarchical Approach to Reducing Communication in Parallel Graph Algorithms},
 year = {2015}
}


@article{Muralidharan:2015:CPM:2858788.2688537,
 abstract = { This paper describes Surge, a collection-oriented programming model that enables programmers to compose parallel computations using nested high-level data collections and operators. Surge exposes a code generation interface, decoupled from the core computation, that enables programmers and autotuners to easily generate multiple implementations of the same computation on various parallel architectures such as multi-core CPUs and GPUs. By decoupling computations from architecture-specific implementation, programmers can target multiple architectures more easily, and generate a search space that facilitates optimization and customization for specific architectures. We express in Surge four real-world benchmarks from domains such as sparse linear-algebra and machine learning and from the same performance-portable specification, generate OpenMP and CUDA C++ implementations. Surge generates efficient, scalable code which achieves up to 1.32x speedup over handcrafted, well-optimized CUDA code. },
 acmid = {2688537},
 address = {New York, NY, USA},
 author = {Muralidharan, Saurav and Garland, Michael and Catanzaro, Bryan and Sidelnik, Albert and Hall, Mary},
 doi = {10.1145/2858788.2688537},
 issn = {0362-1340},
 issue_date = {August 2015},
 journal = {SIGPLAN Not.},
 keyword = {nested-data-parallelism, performance-portability},
 link = {http://doi.acm.org/10.1145/2858788.2688537},
 month = {jan},
 number = {8},
 numpages = {2},
 pages = {263--264},
 publisher = {ACM},
 title = {A Collection-oriented Programming Model for Performance Portability},
 volume = {50},
 year = {2015}
}


@article{Amer:2015:MRC:2858788.2688522,
 abstract = { Hybrid MPI+Threads programming has emerged as an alternative model to the “MPI everywhere” model to better handle the increasing core density in cluster nodes. While the MPI standard allows multithreaded concurrent communication, such flexibility comes with the cost of maintaining thread safety within the MPI implementation, typically implemented using critical sections. In contrast to previous works that studied the importance of critical-section granularity in MPI implementations, in this paper we investigate the implication of critical-section arbitration on communication performance. We first analyze the MPI runtime when multithreaded concurrent communication takes place on hierarchical memory systems. Our results indicate that the mutex-based approach that most MPI implementations use today can incur performance penalties due to unfair arbitration. We then present methods to mitigate these penalties with a first-come, first-served arbitration and a priority locking scheme that favors threads doing useful work. Through evaluations using several benchmarks and applications, we demonstrate up to 5-fold improvement in performance. },
 acmid = {2688522},
 address = {New York, NY, USA},
 author = {Amer, Abdelhalim and Lu, Huiwei and Wei, Yanjie and Balaji, Pavan and Matsuoka, Satoshi},
 doi = {10.1145/2858788.2688522},
 issn = {0362-1340},
 issue_date = {August 2015},
 journal = {SIGPLAN Not.},
 keyword = {MPI, critical section, runtime contention, threads},
 link = {http://doi.acm.org/10.1145/2858788.2688522},
 month = {jan},
 number = {8},
 numpages = {10},
 pages = {239--248},
 publisher = {ACM},
 title = {MPI+Threads: Runtime Contention and Remedies},
 volume = {50},
 year = {2015}
}


@inproceedings{Thomson:2015:LHR:2688500.2688533,
 abstract = { We present the lazy happens-before relation (lazy HBR), which ignores mutex-induced edges to provide a more precise notion of state equivalence compared with the traditional happens-before relation. We demonstrate experimentally that the lazy HBR has the potential to provide greater schedule reduction during systematic concurrency testing with respect to a set of 79 Java benchmarks. },
 acmid = {2688533},
 address = {New York, NY, USA},
 author = {Thomson, Paul and Donaldson, Alastair F.},
 booktitle = {Proceedings of the 20th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
 doi = {10.1145/2688500.2688533},
 isbn = {978-1-4503-3205-7},
 keyword = {Partial-order reduction, systematic concurrency testing},
 link = {http://doi.acm.org/10.1145/2688500.2688533},
 location = {San Francisco, CA, USA},
 numpages = {2},
 pages = {259--260},
 publisher = {ACM},
 series = {PPoPP 2015},
 title = {The Lazy Happens-before Relation: Better Partial-order Reduction for Systematic Concurrency Testing},
 year = {2015}
}


@article{Zhang:2015:NGA:2858788.2688507,
 abstract = { Graph-structured analytics has been widely adopted in a number of big data applications such as social computation, web-search and recommendation systems. Though much prior research focuses on scaling graph-analytics on distributed environments, the strong desire on performance per core, dollar and joule has generated considerable interests of processing large-scale graphs on a single server-class machine, which may have several terabytes of RAM and 80 or more cores. However, prior graph-analytics systems are largely neutral to NUMA characteristics and thus have suboptimal performance. This paper presents a detailed study of NUMA characteristics and their impact on the efficiency of graph-analytics. Our study uncovers two insights: 1) either random or interleaved allocation of graph data will significantly hamper data locality and parallelism; 2) sequential inter-node (i.e., remote) memory accesses have much higher bandwidth than both intra- and inter-node random ones. Based on them, this paper describes Polymer, a NUMA-aware graph-analytics system on multicore with two key design decisions. First, Polymer differentially allocates and places topology data, application-defined data and mutable runtime states of a graph system according to their access patterns to minimize remote accesses. Second, for some remaining random accesses, Polymer carefully converts random remote accesses into sequential remote accesses, by using lightweight replication of vertices across NUMA nodes. To improve load balance and vertex convergence, Polymer is further built with a hierarchical barrier to boost parallelism and locality, an edge-oriented balanced partitioning for skewed graphs, and adaptive data structures according to the proportion of active vertices. A detailed evaluation on an 80-core machine shows that Polymer often outperforms the state-of-the-art single-machine graph-analytics systems, including Ligra, X-Stream and Galois, for a set of popular real-world and synthetic graphs. },
 acmid = {2688507},
 address = {New York, NY, USA},
 author = {Zhang, Kaiyuan and Chen, Rong and Chen, Haibo},
 doi = {10.1145/2858788.2688507},
 issn = {0362-1340},
 issue_date = {August 2015},
 journal = {SIGPLAN Not.},
 keyword = {Graph-structured Analytics, Non-uniform Memory Access (NUMA)},
 link = {http://doi.acm.org/10.1145/2858788.2688507},
 month = {jan},
 number = {8},
 numpages = {11},
 pages = {183--193},
 publisher = {ACM},
 title = {NUMA-aware Graph-structured Analytics},
 volume = {50},
 year = {2015}
}


@proceedings{Cohen:2015:2688500,
 abstract = {
                  An abstract is not available.
              },
 address = {New York, NY, USA},
 isbn = {978-1-4503-3205-7},
 location = {San Francisco, CA, USA},
 publisher = {ACM},
 title = {PPoPP 2015: Proceedings of the 20th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
 year = {2015}
}


@article{Das:2015:SBP:2858788.2688552,
 abstract = { We propose Section Based Program Analysis (SBPA), a novel way to decompose programs into disjoint sections to identify non-communicating loads and stores during program compilation. We implemented SBPA for a deterministic execution runtime environment and reduced 63% of dynamic memory access instrumentations. We also integrated SBPA with ThreadSanitizer, and achieved a speed-up of 2.74 on a geometric mean basis. },
 acmid = {2688552},
 address = {New York, NY, USA},
 author = {Das, Madan and Southern, Gabriel and Renau, Jose},
 doi = {10.1145/2858788.2688552},
 issn = {0362-1340},
 issue_date = {August 2015},
 journal = {SIGPLAN Not.},
 keyword = {Deterministic Execution, Program Analysis, Race Detection, Software Transactional Memory},
 link = {http://doi.acm.org/10.1145/2858788.2688552},
 month = {jan},
 number = {8},
 numpages = {2},
 pages = {283--284},
 publisher = {ACM},
 title = {Section Based Program Analysis to Reduce Overhead of Detecting Unsynchronized Thread Communication},
 volume = {50},
 year = {2015}
}


