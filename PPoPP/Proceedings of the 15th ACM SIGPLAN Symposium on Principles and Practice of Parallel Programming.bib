@article{Zhang:2010:FTS:1837853.1693472,
 abstract = {We study the performance of three parallel algorithms and their hybrid variants for solving tridiagonal linear systems on a GPU: cyclic reduction (CR), parallel cyclic reduction (PCR) and recursive doubling (RD). We develop an approach to measure, analyze, and optimize the performance of GPU programs in terms of memory access, computation, and control overhead. We find that CR enjoys linear algorithm complexity but suffers from more algorithmic steps and bank conflicts, while PCR and RD have fewer algorithmic steps but do more work each step. To combine the benefits of the basic algorithms, we propose hybrid CR+PCR and CR+RD algorithms, which improve the performance of PCR, RD and CR by 21%, 31% and 61% respectively. Our GPU solvers achieve up to a 28x speedup over a sequential LAPACK solver, and a 12x speedup over a multi-threaded CPU solver.},
 acmid = {1693472},
 address = {New York, NY, USA},
 author = {Zhang, Yao and Cohen, Jonathan and Owens, John D.},
 doi = {10.1145/1837853.1693472},
 issn = {0362-1340},
 issue_date = {May 2010},
 journal = {SIGPLAN Not.},
 keyword = {gpgpu, performance optimization, tridiagonal linear system},
 link = {http://doi.acm.org/10.1145/1837853.1693472},
 month = {jan},
 number = {5},
 numpages = {10},
 pages = {127--136},
 publisher = {ACM},
 title = {Fast Tridiagonal Solvers on the GPU},
 volume = {45},
 year = {2010}
}


@inproceedings{Zhai:2010:PPP:1693453.1693493,
 abstract = {For designers of large-scale parallel computers, it is greatly desired that performance of parallel applications can be predicted at the design phase. However, this is difficult because the execution time of parallel applications is determined by several factors, including sequential computation time in each process, communication time and their convolution. Despite previous efforts, it remains an open problem to estimate sequential computation time in each process accurately and efficiently for large-scale parallel applications on non-existing target machines. This paper proposes a novel approach to predict the sequential computation time accurately and efficiently. We assume that there is at least one node of the target platform but the whole target system need not be available. We make two main technical contributions. First, we employ deterministic replay techniques to execute any process of a parallel application on a single node at real speed. As a result, we can simply measure the real sequential computation time on a target node for each process one by one. Second, we observe that computation behavior of processes in parallel applications can be clustered into a few groups while processes in each group have similar computation behavior. This observation helps us reduce measurement time significantly because we only need to execute representative parallel processes instead of all of them. We have implemented a performance prediction framework, called PHANTOM, which integrates the above computation-time acquisition approach with a trace-driven network simulator. We validate our approach on several platforms. For ASCI Sweep3D, the error of our approach is less than 5% on 1024 processor cores. Compared to a recent regression-based prediction approach, PHANTOM presents better prediction accuracy across different platforms.},
 acmid = {1693493},
 address = {New York, NY, USA},
 author = {Zhai, Jidong and Chen, Wenguang and Zheng, Weimin},
 booktitle = {Proceedings of the 15th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
 doi = {10.1145/1693453.1693493},
 isbn = {978-1-60558-877-3},
 keyword = {deterministic replay, parallel application, performance prediction, trace-driven simulation},
 link = {http://doi.acm.org/10.1145/1693453.1693493},
 location = {Bangalore, India},
 numpages = {10},
 pages = {305--314},
 publisher = {ACM},
 series = {PPoPP '10},
 title = {PHANTOM: Predicting Performance of Parallel Applications on Large-scale Parallel Machines Using a Single Node},
 year = {2010}
}


@article{Zyulkyarov:2010:DPU:1837853.1693463,
 abstract = {With the emergence of research prototypes, programming using atomic blocks and transactional memory (TM) is becoming more attractive. This paper describes our experience building and using a debugger for programs written with these abstractions. We introduce three approaches: (i) debugging at the level of atomic blocks, where the programmer is shielded from implementation details (such as exactly what kind of TM is used, or indeed whether lock inference is used instead), (ii) debugging at the level of transactions, where conflict rates, read sets, write sets, and other TM internals are visible, and (iii) debug-time transactions, which let the programmer manipulate synchronization from within the debugger - e.g., enlarging the scope of an atomic block to try to identify a bug. In this paper we explain the rationale behind the new debugging approaches that we propose. We describe the design and implementation of an extension to the WinDbg debugger, enabling support for C# programs using atomic blocks and TM. We also demonstrate the design of a "conflict point discovery" technique for identifying program statements that introduce contention between transactions. We illustrate how these techniques can be used by optimizing a C# version of the Genome application from STAMP TM benchmark suite.},
 acmid = {1693463},
 address = {New York, NY, USA},
 author = {Zyulkyarov, Ferad and Harris, Tim and Unsal, Osman S. and Cristal, Adr\'{\i}an and Valero, Mateo},
 doi = {10.1145/1837853.1693463},
 issn = {0362-1340},
 issue_date = {May 2010},
 journal = {SIGPLAN Not.},
 keyword = {debugging, transactional memory},
 link = {http://doi.acm.org/10.1145/1837853.1693463},
 month = {jan},
 number = {5},
 numpages = {10},
 pages = {57--66},
 publisher = {ACM},
 title = {Debugging Programs That Use Atomic Blocks and Transactional Memory},
 volume = {45},
 year = {2010}
}


@inproceedings{Liu:2010:IPL:1693453.1693483,
 abstract = {As multicore chips become the main building blocks for high performance computers, many numerical applications face a performance impediment due to the limited hardware capacity to move data between the CPU and the off-chip memory. This is especially true for large computing problems solved by iterative algorithms because of the large data set typically used. Loop tiling, also known as loop blocking, was shown previously to be an effective way to enhance data locality, and hence to reduce the memory bandwidth pressure, for a class of iterative algorithms executed on a single processor. Unfortunately, the tiled programs suffer from reduced parallelism because only the loop iterations within a single tile can be easily parallelized. In this work, we propose to use the asynchronous model to enable effective loop tiling such that both parallelism and locality can be attained simultaneously. Asynchronous algorithms were previously proposed to reduce the communication cost and synchronization overhead between processors. Our new discovery is that carefully controlled asynchrony and loop tiling can significantly improve the performance of parallel iterative algorithms on multicore processors due to simultaneously attained data locality and loop-level parallelism. We present supporting evidence from experiments with three well-known numerical kernels.},
 acmid = {1693483},
 address = {New York, NY, USA},
 author = {Liu, Lixia and Li, Zhiyuan},
 booktitle = {Proceedings of the 15th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
 doi = {10.1145/1693453.1693483},
 isbn = {978-1-60558-877-3},
 keyword = {asynchronous algorithms, data locality, loop tiling, memory performance, parallel numerical programs},
 link = {http://doi.acm.org/10.1145/1693453.1693483},
 location = {Bangalore, India},
 numpages = {10},
 pages = {213--222},
 publisher = {ACM},
 series = {PPoPP '10},
 title = {Improving Parallelism and Locality with Asynchronous Algorithms},
 year = {2010}
}


@article{Sandes:2010:CUG:1837853.1693473,
 abstract = {Biological sequence comparison is a very important operation in Bioinformatics. Even though there do exist exact methods to compare biological sequences, these methods are often neglected due to their quadratic time and space complexity. In order to accelerate these methods, many GPU algorithms were proposed in the literature. Nevertheless, all of them restrict the size of the smallest sequence in such a way that Megabase genome comparison is prevented. In this paper, we propose and evaluate CUDAlign, a GPU algorithm that is able to compare Megabase biological sequences with an exact Smith-Waterman affine gap variant. CUDAlign was implemented in CUDA and tested in two GPU boards, separately. For real sequences whose size range from 1MBP (Megabase Pairs) to 47MBP, a close to uniform GCUPS (Giga Cells Updates per Second) was obtained, showing the potential scalability of our approach. Also, CUDAlign was able to compare the human chromosome 21 and the chimpanzee chromosome 22. This operation took 21 hours on GeForce GTX 280, resulting in a peak performance of 20.375 GCUPS. As far as we know, this is the first time such huge chromosomes are compared with an exact method.},
 acmid = {1693473},
 address = {New York, NY, USA},
 author = {Sandes, Edans Flavius O. and de Melo, Alba Cristina M.A.},
 doi = {10.1145/1837853.1693473},
 issn = {0362-1340},
 issue_date = {May 2010},
 journal = {SIGPLAN Not.},
 keyword = {biological sequence comparison, gpu, smith-waterman},
 link = {http://doi.acm.org/10.1145/1837853.1693473},
 month = {jan},
 number = {5},
 numpages = {10},
 pages = {137--146},
 publisher = {ACM},
 title = {CUDAlign: Using GPU to Accelerate the Comparison of Megabase Genomic Sequences},
 volume = {45},
 year = {2010}
}


@inproceedings{Agerwala:2010:ECC:1693453.1693454,
 abstract = {Supercomputing systems have made great strides in recent years as the extensive computing needs of cutting-edge engineering work and scientific discovery have driven the development of more powerful systems. In 2008, the first petaflop machine was released, and historic trends indicate that in ten years, we should be at the exascale level. Indeed, various agencies are targeting a computer system capable of 1 Exaop (10**18 ops) of computation within the next decade. We believe that applications in many industries will be materially transformed by exascale computers. Meeting the exascale challenge will require significant innovation in technology, architecture and programmability. Power is a fundamental problem at all levels; traditional memory cost and performance are not keeping pace with compute potential; the storage hierarchy will have to be re-architected; networks will be a much bigger part of the system cost; reliability at exascale levels will require a holistic approach to architecture design, and programmability and ease-of-use will be an essential component to extract the promised performance at the exascale level. In this talk, I will discuss the importance of exascale computing and address the major challenges, touching on the areas of technology, architecture, reliability and usability.},
 acmid = {1693454},
 address = {New York, NY, USA},
 author = {Agerwala, Tilak},
 booktitle = {Proceedings of the 15th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
 doi = {10.1145/1693453.1693454},
 isbn = {978-1-60558-877-3},
 keyword = {architecture, challenges, exascale},
 link = {http://doi.acm.org/10.1145/1693453.1693454},
 location = {Bangalore, India},
 numpages = {2},
 pages = {1--2},
 publisher = {ACM},
 series = {PPoPP '10},
 title = {Exascale Computing: The Challenges and Opportunities in the Next Decade},
 year = {2010}
}


@article{Perarnau:2010:KRC:1837853.1693497,
 abstract = {In this article we present KRASH, a tool for reproducible generation of system-level CPU load. This tool is intended for use in shared memory machines equipped with multiple CPU cores which are usually exploited concurrently by several users. The objective of KRASH is to enable parallel application developers to validate their resources use strategies on a partially loaded machine by replaying an observed load in concurrence with their application. To reach this objective, we present a method for CPU load generation which behaves as realistically as possible: the resulting load is similar to the load that would be produced by concurrent processes run by other users. Nevertheless, contrary to a simple run of a CPU-intensive application, KRASH is not sensitive to system scheduling decisions. The main benefit brought by KRASH is this reproducibility: no matter how many processes are present in the system the load generated by our tool strictly respects a given load profile. To our knowledge, KRASH is the only tool that implements the generation of a dynamic load profile (a load varying with time). When used to generate a constant load, KRASH result is among the most realistic ones. Furthermore, KRASH provides more flexibility than other tools.},
 acmid = {1693497},
 address = {New York, NY, USA},
 author = {Perarnau, Swann and Huard, Guillaume},
 doi = {10.1145/1837853.1693497},
 issn = {0362-1340},
 issue_date = {May 2010},
 journal = {SIGPLAN Not.},
 keyword = {cpu load generation, experimentation testbed, many cores},
 link = {http://doi.acm.org/10.1145/1837853.1693497},
 month = {jan},
 number = {5},
 numpages = {2},
 pages = {327--328},
 publisher = {ACM},
 title = {KRASH: Reproducible CPU Load Generation on Many Cores Machines},
 volume = {45},
 year = {2010}
}


@article{Dash:2010:SPT:1837853.1693499,
 abstract = {We present a static analysis for the automatic generation of symbolic prefetches in a transactional distributed shared memory. A symbolic prefetch specifies the first object to be prefetched followed by a list of field offsets or array indices that define a path through the heap. We also provide an object caching framework and language extensions to support our approach. To our knowledge, this is the first prefetching approach that can prefetch objects whose addresses have not been computed or predicted. Our approach makes aggressive use of both prefetching and caching of remote objects to hide network latency. It relies on the transaction commit mechanism to preserve the simple transactional consistency model that we present to the developer. We have evaluated this approach on several shared memory parallel benchmarks and a distributed gaming benchmark to observe speedups due to prefetching and caching. Categories and Subject Descriptors},
 acmid = {1693499},
 address = {New York, NY, USA},
 author = {Dash, Alokika and Demsky, Brian},
 doi = {10.1145/1837853.1693499},
 issn = {0362-1340},
 issue_date = {May 2010},
 journal = {SIGPLAN Not.},
 keyword = {distributed shared memory, symbolic prefetching, transactional memory},
 link = {http://doi.acm.org/10.1145/1837853.1693499},
 month = {jan},
 number = {5},
 numpages = {2},
 pages = {331--332},
 publisher = {ACM},
 title = {Symbolic Prefetching in Transactional Distributed Shared Memory},
 volume = {45},
 year = {2010}
}


@article{Hoffmann:2010:AHS:1837853.1693507,
 abstract = {Adaptive, or self-aware, computing has been proposed to help application programmers confront the growing complexity of multicore software development. However, existing approaches to adaptive systems are largely ad hoc and often do not manage to incorporate the true performance goals of the applications they are designed to support. This paper presents an enabling technology for adaptive computing systems: Application Heartbeats. The Application Heartbeats framework provides a simple, standard programming interface that applications can use to indicate their performance and system software (and hardware) can use to query an application's performance. The PARSEC benchmark suite is instrumented with Application Heartbeats to show the broad applicability of the interface and an external resource scheduler demonstrates the use of the interface by assigning cores to an application to maintain a designated performance goal.},
 acmid = {1693507},
 address = {New York, NY, USA},
 author = {Hoffmann, Henry and Eastep, Jonathan and Santambrogio, Marco D. and Miller, Jason E. and Agarwal, Anant},
 doi = {10.1145/1837853.1693507},
 issn = {0362-1340},
 issue_date = {May 2010},
 journal = {SIGPLAN Not.},
 keyword = {adaptive algorithms},
 link = {http://doi.acm.org/10.1145/1837853.1693507},
 month = {jan},
 number = {5},
 numpages = {2},
 pages = {347--348},
 publisher = {ACM},
 title = {Application Heartbeats for Software Performance and Health},
 volume = {45},
 year = {2010}
}


@article{Agrawal:2010:HLF:1837853.1693487,
 abstract = {Helper locks allow programs with large parallel critical sections, called parallel regions, to execute more efficiently by enlisting processors that might otherwise be waiting on the helper lock to aid in the execution of the parallel region. Suppose that a processor p is executing a parallel region A after having acquired the lock L protecting A. If another processor p′ tries to acquire L, then instead of blocking and waiting for p to complete A, processor p′ joins p to help it complete A. Additional processors not blocked on L may also help to execute A. The HELPER runtime system can execute fork-join computations augmented with helper locks and parallel regions. HELPER supports the unbounded nesting of parallel regions. We provide theoretical completion-time and space-usage bounds for a design of HELPER based on work stealing. Specifically, let V be the number of parallel regions in a computation, let T1 be its work, and let T∞ be its "aggregate span" --- the sum of the spans (critical-path lengths) of all its parallel regions. We prove that HELPER completes the computation in expected time O(T1/PP + T∞+ PV on P processors. This bound indicates that programs with a small number of highly parallel critical sections can attain linear speedup. For the space bound, we prove that HELPER completes a program using only O(PS1 stack space, where S1 is the sum, over all regions, of the stack space used by each region in a serial execution. Finally, we describe a prototype of HELPER implemented by modifying the Cilk multithreaded runtime system. We used this prototype to implement a concurrent hash table with a resize operation protected by a helper lock.},
 acmid = {1693487},
 address = {New York, NY, USA},
 author = {Agrawal, Kunal and Leiserson, Charles E. and Sukha, Jim},
 doi = {10.1145/1837853.1693487},
 issn = {0362-1340},
 issue_date = {May 2010},
 journal = {SIGPLAN Not.},
 keyword = {cilk, dynamic multithreading, helper lock, nested parallelism, parallel region, scheduling, work stealing},
 link = {http://doi.acm.org/10.1145/1837853.1693487},
 month = {jan},
 number = {5},
 numpages = {12},
 pages = {245--256},
 publisher = {ACM},
 title = {Helper Locks for Fork-join Parallel Programming},
 volume = {45},
 year = {2010}
}


@article{Zhang:2010:CSM:1837853.1693482,
 abstract = {Most modern Chip Multiprocessors (CMP) feature shared cache on chip. For multithreaded applications, the sharing reduces communication latency among co-running threads, but also results in cache contention. A number of studies have examined the influence of cache sharing on multithreaded applications, but most of them have concentrated on the design or management of shared cache, rather than a systematic measurement of the influence. Consequently, prior measurements have been constrained by the reliance on simulators, the use of out-of-date benchmarks, and the limited coverage of deciding factors. The influence of CMP cache sharing on contemporary multithreaded applications remains preliminarily understood. In this work, we conduct a systematic measurement of the influence on two kinds of commodity CMP machines, using a recently released CMP benchmark suite, PARSEC, with a number of potentially important factors on program, OS, and architecture levels considered. The measurement shows some surprising results. Contrary to commonly perceived importance of cache sharing, neither positive nor negative effects from the cache sharing are significant for most of the program executions, regardless of the types of parallelism, input datasets, architectures, numbers of threads, and assignments of threads to cores. After a detailed analysis, we find that the main reason is the mismatch of current development and compilation of multithreaded applications and CMP architectures. By transforming the programs in a cache-sharing-aware manner, we observe up to 36% performance increase when the threads are placed on cores appropriately.},
 acmid = {1693482},
 address = {New York, NY, USA},
 author = {Zhang, Eddy Z. and Jiang, Yunlian and Shen, Xipeng},
 doi = {10.1145/1837853.1693482},
 issn = {0362-1340},
 issue_date = {May 2010},
 journal = {SIGPLAN Not.},
 keyword = {chip multiprocessors, parallel program optimizations, shared cache, thread scheduling},
 link = {http://doi.acm.org/10.1145/1837853.1693482},
 month = {jan},
 number = {5},
 numpages = {10},
 pages = {203--212},
 publisher = {ACM},
 title = {Does Cache Sharing on Modern CMP Matter to the Performance of Contemporary Multithreaded Programs?},
 volume = {45},
 year = {2010}
}


@inproceedings{Porter:2010:MTM:1693453.1693508,
 abstract = {Transactional memory promises to make parallel programming easier than with fine-grained locking, while performing just as well. This performance claim is not always borne out because an application may violate a common-case assumption of the TM designer or because of external system effects. In order to help programmers assess the suitability of their code for transactional memory, this work introduces a formal model of transactional memory as well as a tool, called Syncchar. Syncchar can predict the speedup of a conversion from locks to transactions within 25% for the STAMP benchmarks. Because getting good performance from transactions is more difficult than commonly appreciated, developers need tools to tune transactional performance.},
 acmid = {1693508},
 address = {New York, NY, USA},
 author = {Porter, Donald E. and Witchel, Emmett},
 booktitle = {Proceedings of the 15th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
 doi = {10.1145/1693453.1693508},
 isbn = {978-1-60558-877-3},
 keyword = {performance, syncchar, transactional memory},
 link = {http://doi.acm.org/10.1145/1693453.1693508},
 location = {Bangalore, India},
 numpages = {2},
 pages = {349--350},
 publisher = {ACM},
 series = {PPoPP '10},
 title = {Modeling Transactional Memory Workload Performance},
 year = {2010}
}


@article{Romein:2010:LCI:1837853.1693477,
 abstract = {LOFAR is the first of a new generation of radio telescopes.Rather than using expensive dishes, it forms a distributed sensor network that combines the signals from many thousands of simple antennas. Its revolutionary design allows observations in a frequency range that has hardly been studied before. Another novel feature of LOFAR is the elaborate use of software to process data, where traditional telescopes use customized hardware. This dramatically increases flexibility and substantially reduces costs, but the high processing and bandwidth requirements compel the use of a supercomputer. The antenna signals are centrally combined, filtered, optionally beam-formed, and correlated by an IBM Blue Gene/P. This paper describes the implementation of the so-called correlator. To meet the real-time requirements, the application is highly optimized, and reaches exceptionally high computational and I/O efficiencies. Additionally, we study the scalability of the system, and show that it scales well beyond the requirements. The optimizations allows us to use only half the planned amount of resources, and process 50% more telescope data, significantly improving the effectiveness of the entire telescope.},
 acmid = {1693477},
 address = {New York, NY, USA},
 author = {Romein, John W. and Broekema, P. Chris and Mol, Jan David and van Nieuwpoort, Rob V.},
 doi = {10.1145/1837853.1693477},
 issn = {0362-1340},
 issue_date = {May 2010},
 journal = {SIGPLAN Not.},
 keyword = {correlator, ibm blue gene/p, lofar},
 link = {http://doi.acm.org/10.1145/1837853.1693477},
 month = {jan},
 number = {5},
 numpages = {10},
 pages = {169--178},
 publisher = {ACM},
 title = {The LOFAR Correlator: Implementation and Performance Analysis},
 volume = {45},
 year = {2010}
}


@inproceedings{Hoefler:2010:SCP:1693453.1693476,
 abstract = {Many large-scale parallel programs follow a bulk synchronous parallel (BSP) structure with distinct computation and communication phases. Although the communication phase in such programs may involve all (or large numbers) of the participating processes, the actual communication operations are usually sparse in nature. As a result, communication phases are typically expressed explicitly using point-to-point communication operations or collective operations. We define the dynamic sparse data-exchange (DSDE) problem and derive bounds in the well known LogGP model. While current approaches work well with static applications, they run into limitations as modern applications grow in scale, and as the problems that are being solved become increasingly irregular and dynamic. To enable the compact and efficient expression of the communication phase, we develop suitable sparse communication protocols for irregular applications at large scale. We discuss different irregular applications and show the sparsity in the communication for real-world input data. We discuss the time and memory complexity of commonly used protocols for the DSDE problem and develop NBX--a novel fast algorithm with constant memory overhead for solving it. Algorithm NBX improves the runtime of a sparse data-exchange among 8,192 processors on BlueGene/P by a factor of 5.6. In an application study, we show improvements of up to a factor of 28.9 for a parallel breadth first search on 8,192 BlueGene/P processors.},
 acmid = {1693476},
 address = {New York, NY, USA},
 author = {Hoefler, Torsten and Siebert, Christian and Lumsdaine, Andrew},
 booktitle = {Proceedings of the 15th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
 doi = {10.1145/1693453.1693476},
 isbn = {978-1-60558-877-3},
 keyword = {alltoall, distributed termination, irregular algorithms, nonblocking collective operations, sparse data exchange},
 link = {http://doi.acm.org/10.1145/1693453.1693476},
 location = {Bangalore, India},
 numpages = {10},
 pages = {159--168},
 publisher = {ACM},
 series = {PPoPP '10},
 title = {Scalable Communication Protocols for Dynamic Sparse Data Exchange},
 year = {2010}
}


@article{Lee:2010:FXC:1837853.1693459,
 abstract = {We present a core calculus with two of X10's key constructs for parallelism, namely async and finish. Our calculus forms a convenient basis for type systems and static analyses for languages with async-finish parallelism, and for tractable proofs of correctness. For example, we give a short proof of the deadlock-freedom theorem of Saraswat and Jagadeesan. Our main contribution is a type system that solves the open problem of context-sensitive may-happen-in-parallel analysis for languages with async-finish parallelism. We prove the correctness of our type system and we report experimental results of performing type inference on 13,000 lines of X10 code. Our analysis runs in polynomial time, takes a total of 28 seconds on our benchmarks, and produces a low number of false positives, which suggests that our analysis is a good basis for other analyses such as race detectors.},
 acmid = {1693459},
 address = {New York, NY, USA},
 author = {Lee, Jonathan K. and Palsberg, Jens},
 doi = {10.1145/1837853.1693459},
 issn = {0362-1340},
 issue_date = {May 2010},
 journal = {SIGPLAN Not.},
 keyword = {operational semantics, parallelism, static analysis},
 link = {http://doi.acm.org/10.1145/1837853.1693459},
 month = {jan},
 number = {5},
 numpages = {12},
 pages = {25--36},
 publisher = {ACM},
 title = {Featherweight X10: A Core Calculus for Async-finish Parallelism},
 volume = {45},
 year = {2010}
}


@proceedings{Govindarajan:2010:1693453,
 abstract = {Together with the program committee, it is my great pleasure to welcome you to the 15th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming -- PPoPP '10. This year's symposium continues its tradition of being the premier forum for presentation of research in all aspects of parallel software, including theoretical foundations, programming models, algorithms, applications, and systems software. This research area has seen dramatic growth in recent years as homogeneous and heterogeneous multi-core microprocessors have become mainstream. Therefore making parallel programming more accessible has become essential to the very future of the computing industry. This dramatic shift in importance of the area was reflected in a record number of 173 submissions from Asia, Europe, Australia and New Zealand, and North and South America, and more than half of these were from outside North America. The program committee accepted 29 papers, covering a broad range of topics from multi-core computing up to scalable high-end computing, parallel algorithms, synchronization and transactional memory. This year saw an increased emphasis on accelerators such as graphics processors, and on tools to support parallel programming such as performance modeling and debugging. Our program also includes 17 poster presentations in similar areas. In addition to papers and posters, we are pleased to include keynote addresses by Tilak Agerwala from IBM on computing at exascale and Arvind from MIT on innovation in computer architecture, and a panel on extreme-scale computing. To manage the unexpected reviewing load, PPoPP introduced a two-phase review process. Each paper received two program committee reviews in the first phase, and papers with at least one rating above strong reject received an additional one to three reviews, how many depending on existing reviews and the need for more information in the decision process. The program committee meeting was held on September 14, 2009, on the University of Utah campus in Salt Lake City, Utah.},
 address = {New York, NY, USA},
 isbn = {978-1-60558-877-3},
 location = {Bangalore, India},
 note = {551102},
 publisher = {ACM},
 title = {PPoPP '10: Proceedings of the 15th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
 year = {2010}
}


@inproceedings{Aleen:2010:IDE:1693453.1693494,
 abstract = {Streaming applications are promising targets for effectively utilizing multicores because of their inherent amenability to pipelined parallelism. While existing methods of orchestrating streaming programs on multicores have mostly been static, real-world applications show ample variations in execution time that may cause the achieved speedup and throughput to be sub-optimal. One of the principle challenges for moving towards dynamic orchestration has been the lack of approaches that can predict or accurately estimate upcoming dynamic variations in execution efficiently, well before they occur. In this paper, we propose an automated dynamic execution behavior prediction approach that can be used to efficiently estimate the time that will be spent in different pipeline stages for upcoming inputs without requiring program execution. This enables dynamic balancing or scheduling of execution to achieve better speedup. Our approach first uses dynamic taint analysis to automatically generates an input-based execution characterization of the streaming program, which identifies the key control points where variation in execution might occur with the associated input elements that cause these variations.We then automatically generate a light-weight emulator from the program using this characterization that can simulate the execution paths taken for new streaming inputs and provide an estimate of execution time that will be spent in processing these inputs, enabling prediction of possible dynamic variations. We present experimental evidence that our technique can accurately and efficiently estimate execution behaviors for several benchmarks. Our experiments show that dynamic orchestration using our predicted execution behavior can achieve considerably higher speedup than static orchestration.},
 acmid = {1693494},
 address = {New York, NY, USA},
 author = {Aleen, Farhana and Sharif, Monirul and Pande, Santosh},
 booktitle = {Proceedings of the 15th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
 doi = {10.1145/1693453.1693494},
 isbn = {978-1-60558-877-3},
 keyword = {dynamic execution, parallelization, software pipeline},
 link = {http://doi.acm.org/10.1145/1693453.1693494},
 location = {Bangalore, India},
 numpages = {10},
 pages = {315--324},
 publisher = {ACM},
 series = {PPoPP '10},
 title = {Input-driven Dynamic Execution Prediction of Streaming Applications},
 year = {2010}
}


@inproceedings{Carter:2010:PLN:1693453.1693509,
 abstract = {The Pilot library is a new method for programming MPI-enabled clusters in C, targeted at novice parallel programmers. Formal elements from Communicating Sequential Processes (CSP) are used to realize a process/channel model of parallel computation that reduces opportunities for deadlock and other communication errors. This simple model, plus an application programming inter-face (API) styled after C's formatted I/O, are designed to make the library easy to learn. The Pilot library exists as a thin layer on top of any standard Message Passing Interface (MPI) implementation, preserving MPI's portability and efficiency, with little per-formance overhead arising as result of Pilot's additional features.},
 acmid = {1693509},
 address = {New York, NY, USA},
 author = {Carter, John D. and Gardner, William B. and Grewal, Gary},
 booktitle = {Proceedings of the 15th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
 doi = {10.1145/1693453.1693509},
 isbn = {978-1-60558-877-3},
 keyword = {c, cluster programming, collective operations, deadlock detection, high-performance computing, mpi},
 link = {http://doi.acm.org/10.1145/1693453.1693509},
 location = {Bangalore, India},
 numpages = {2},
 pages = {351--352},
 publisher = {ACM},
 series = {PPoPP '10},
 title = {The Pilot Library for Novice MPI Programmers},
 year = {2010}
}


@article{Yang:2010:OCG:1837853.1693505,
 abstract = {Developing high performance GPGPU programs is challenging for application developers since the performance is dependent upon how well the code leverages the hardware features of specific graphics processors. To solve this problem and relieve application developers of low-level hardware-specific optimizations, we introduce a novel compiler to optimize GPGPU programs. Our compiler takes a naive GPU kernel function, which is functionally correct but without any consideration for performance optimization. The compiler then analyzes the code, identifies memory access patterns, and generates optimized code. The proposed compiler optimizations target at one category of scientific and media processing algorithms, which has the characteristics of input-data sharing when computing neighboring output pixels/elements. Many commonly used algorithms, such as matrix multiplication, convolution, etc., share such characteristics. For these algorithms, novel approaches are proposed to enforce memory coalescing and achieve effective data reuse. Data prefetching and hardware-specific tuning are also performed automatically with our compiler framework. The experimental results based on a set of applications show that our compiler achieves very high performance, either superior or very close to the highly fine-tuned library, NVIDIA CUBLAS 2.1.},
 acmid = {1693505},
 address = {New York, NY, USA},
 author = {Yang, Yi and Xiang, Ping and Kong, Jingfei and Zhou, Huiyang},
 doi = {10.1145/1837853.1693505},
 issn = {0362-1340},
 issue_date = {May 2010},
 journal = {SIGPLAN Not.},
 keyword = {compiler, gpgpu},
 link = {http://doi.acm.org/10.1145/1837853.1693505},
 month = {jan},
 number = {5},
 numpages = {2},
 pages = {343--344},
 publisher = {ACM},
 title = {An Optimizing Compiler for GPGPU Programs with Input-data Sharing},
 volume = {45},
 year = {2010}
}


@inproceedings{Cederman:2010:SLC:1693453.1693503,
 abstract = {Lock-free data objects offer several advantages over their blocking counterparts, such as being immune to deadlocks and convoying and, more importantly, being highly concurrent. But they share a common disadvantage in that the operations they provide are difficult to compose into larger atomic operations while still guaranteeing lock-freedom. We present a lock-free methodology for composing highly concurrent linearizable objects together by unifying their linearization points. This makes it possible to relatively easily introduce atomic lock-free move operations to a wide range of concurrent objects. Experimental evaluation has shown that the operations originally supported by the data objects keep their performance behavior under our methodology.},
 acmid = {1693503},
 address = {New York, NY, USA},
 author = {Cederman, Daniel and Tsigas, Philippas},
 booktitle = {Proceedings of the 15th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
 doi = {10.1145/1693453.1693503},
 isbn = {978-1-60558-877-3},
 keyword = {composition, data structures, lock-free},
 link = {http://doi.acm.org/10.1145/1693453.1693503},
 location = {Bangalore, India},
 numpages = {2},
 pages = {339--340},
 publisher = {ACM},
 series = {PPoPP '10},
 title = {Supporting Lock-free Composition of Concurrent Data Objects},
 year = {2010}
}


@proceedings{Reed:2009:1504176,
 abstract = {It is our great pleasure to welcome you to the 14th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming (PPoPP'09). Since its inception (as PPEALS) in 1988, PPoPP has served as a leading forum for research in all aspects of parallel software, including theoretical foundations, programming models, algorithms, applications, and systems software. These topics have now moved to center stage in their relevance to mainstream computing, as parallelism becomes ubiquitous with the widespread adoption of homogeneous and heterogeneous multicore processors. This year's call attracted 109 paper submissions, which is a record for PPoPP. Each paper was read by at least three members of the program committee, and additional external reviews were solicited when needed. At an in-person meeting of the program committee in October 2008, 26 papers were selected for presentation at the conference that spanned a broad range of software topics (languages, compilers, tools, runtimes) for a diversity of hardware platforms (multicore, accelerators, and high end computing). Authors of high quality submissions that could not be accepted as full papers were invited to present their work as posters. 14 of 20 accepted this invitation; 2 other posters were chosen from among 6 poster submissions. Following last year's successful co-location with the International Symposium on High-Performance Computer Architecture (HPCA) as back-to-back events, the PPoPP and HPCA conference committees decided to go one step further in 2009 and co-locate the conferences in both space and time as a single federated event. As a result, attendees of one conference have the option of attending talks at another conference, and a number of events (two keynotes, two panels, a poster session, and the conference excursion) have been scheduled as shared events. Given the significance of software-hardware co-design in addressing future research challenges in parallel systems, we hope that this coordination of PPoPP and HPCA will foster greater interaction between the two communities. In addition, our hope is that this overlap will strengthen participation in the workshops and tutorials that preceded both conferences, including the 4th ACM SIGPLAN Workshop on Transactional Computing (TRANSACT 2009).},
 address = {New York, NY, USA},
 isbn = {978-1-60558-397-6},
 location = {Raleigh, NC, USA},
 note = {551091},
 publisher = {ACM},
 title = {PPoPP '09: Proceedings of the 14th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
 year = {2009}
}


@article{Buehrer:2010:DPS:1837853.1693511,
 abstract = {Effective data placement strategies can enhance the performance of data-intensive applications implemented on high end computing clusters. Such strategies can have a significant impact in localizing the computation, in minimizing synchronization (communication) costs, in enhancing reliability (via strategic replication policies), and in ensuring a balanced workload or enhancing the available bandwidth from massive storage devices (e.g. disk arrays). Existing work has largely targeted the placement of relatively simple data types or entities (e.g. elements, vectors, sets, and arrays). Here we investigate several hash-based distributed data placement methods targeting tree- and graph- structured data, and develop a locality enhancing placement service for large cluster systems. Target applications include the placement of a single large graph (e.g. Web graph), a single large tree (e.g. large XML file), a forest of graphs or trees (e.g. XML database) and other specialized graph data types - bi-partite (query-click graphs), directed acyclic graphs etc. We empirically evaluate our service by demonstrating its use in improving mining executions for pattern discovery, nearest neighbor searching, graph computations, and applications that combine link and content analysis.},
 acmid = {1693511},
 address = {New York, NY, USA},
 author = {Buehrer, Gregory and Parthasarathy, Srinivasan and Tatikonda, Shirish},
 doi = {10.1145/1837853.1693511},
 issn = {0362-1340},
 issue_date = {May 2010},
 journal = {SIGPLAN Not.},
 keyword = {data placement, distributed computing, structured data},
 link = {http://doi.acm.org/10.1145/1837853.1693511},
 month = {jan},
 number = {5},
 numpages = {2},
 pages = {355--356},
 publisher = {ACM},
 title = {A Distributed Placement Service for Graph-structured and Tree-structured Data},
 volume = {45},
 year = {2010}
}


@proceedings{Cascaval:2011:1941553,
 abstract = {It is our great pleasure to welcome you to the 16th ACM Symposium on Principles and Practice of Parallel Programming -- PPoPP'11. PPoPP continues to be the premiere forum where researchers present their work on all aspects of parallelism and concurrency: algorithms and applications, programming models, languages, and environments, system software and runtime systems, and theoretical foundational work. As our industry continues to move toward parallel systems, from large-scale supercomputers to multicore mobile devices, such as smart phones and tablets, research work on concurrency is needed to support developers at all levels of the execution stack. This year we received 165 completed submissions, close to the conference record high. Because of the large number of submissions, in addition to the 25 program committee members, we formed an external review committee and invited 30 experts in various areas to help out in reviewing papers. In addition, committee members also invited external reviewers, to provide our submitting authors with a total of more than 630 reviews. There was a paper bidding process to match up the expertise of the reviewers and the reviewed papers before papers were assigned to reviewers. An on-line discussion period was conducted among all reviewers of each paper to smooth out the differences among the reviewers before the program committee meeting was held. For the final program, the program committee selected 26 full papers and 13 posters for the program. They span a wide spectrum of areas in parallel programming. A few years back, the PPoPP Steering Committee recognized the importance of broadening the conference experience through interaction with hardware architects designing parallel systems. This year we continue the collocation with the International Symposium on High-Performance Computer Architecture. Beside encouraging cross-participation in any of the two conference sessions, PPoPP and HPCA will share two keynotes from leading researchers in the area of parallel programming: Jim Larus (Microsoft Research) and Kathryn McKinley (UT Austin), and all the tutorials and workshops.},
 address = {New York, NY, USA},
 isbn = {978-1-4503-0119-0},
 location = {San Antonio, TX, USA},
 note = {551111},
 publisher = {ACM},
 title = {PPoPP '11: Proceedings of the 16th ACM Symposium on Principles and Practice of Parallel Programming},
 year = {2011}
}


@inproceedings{Guo:2010:SSL:1693453.1693504,
 abstract = {This poster introduces SLAW, a Scalable Locality-aware Adaptive Work-stealing scheduler. The SLAW features an adaptive task scheduling algorithm combined with a locality-aware scheduling framework. Past work has demonstrated the pros and cons of using fixed scheduling policies, such as work-first and help-first, in different cases without a clear winner. Prior work also assumes the availability and successful execution of a serial version of the parallel program. This assumption can limit the expressiveness of dynamic task parallel languages. The SLAW scheduler supports both work-first and help-first policies simultaneously. It does so by using an adaptive approach that selects a scheduling policy on a per-task basis at runtime. The SLAW scheduler also establishes bounds on the stack usage and the heap space needed to store tasks. The experimental results for the benchmarks studied show that SLAW's adaptive scheduler achieves 0.98x - 9.2$x speedup over the help-first scheduler and 0.97x - 4.5x speedup over the work-first scheduler for 64-thread executions, thereby establishing the robustness of using an adaptive approach instead of a fixed policy. In contrast, the help-first policy is 9.2x slower than work-first in the worst case for a fixed help-first policy, and the work-first policy is 3.7x slower than help-first in the worst case for a fixed work-first policy. Further, for large irregular recursive parallel computations, the adaptive scheduler runs with bounded stack usage and achieves performance (and supports data sizes) that cannot be delivered by the use of any single fixed policy. The SLAW scheduler is designed for programming models where locality hints are provided to the runtime by the programmer or compiler, and achieves locality-awareness by grouping workers into places. Locality awareness can lead to improved performance by increasing temporal data reuse within a worker and among workers in the same place. Our experimental results show that locality-aware scheduling can achieve up to 2.6x speedup over locality-oblivious scheduling, for the benchmarks studied.},
 acmid = {1693504},
 address = {New York, NY, USA},
 author = {Guo, Yi and Zhao, Jisheng and Cave, Vincent and Sarkar, Vivek},
 booktitle = {Proceedings of the 15th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
 doi = {10.1145/1693453.1693504},
 isbn = {978-1-60558-877-3},
 keyword = {help-first, work-first, work-stealing},
 link = {http://doi.acm.org/10.1145/1693453.1693504},
 location = {Bangalore, India},
 numpages = {2},
 pages = {341--342},
 publisher = {ACM},
 series = {PPoPP '10},
 title = {SLAW: A Scalable Locality-aware Adaptive Work-stealing Scheduler for Multi-core Systems},
 year = {2010}
}


@article{Hofmeyr:2010:LBS:1837853.1693475,
 abstract = {To fully exploit multicore processors, applications are expected to provide a large degree of thread-level parallelism. While adequate for low core counts and their typical workloads, the current load balancing support in operating systems may not be able to achieve efficient hardware utilization for parallel workloads. Balancing run queue length globally ignores the needs of parallel applications where threads are required to make equal progress. In this paper we present a load balancing technique designed specifically for parallel applications running on multicore systems. Instead of balancing run queue length, our algorithm balances the time a thread has executed on ``faster'' and ``slower'' cores. We provide a user level implementation of speed balancing on UMA and NUMA multi-socket architectures running Linux and discuss behavior across a variety of workloads, usage scenarios and programming models. Our results indicate that speed balancing when compared to the native Linux load balancing improves performance and provides good performance isolation in all cases considered. Speed balancing is also able to provide comparable or better performance than DWRR, a fair multi-processor scheduling implementation inside the Linux kernel. Furthermore, parallel application performance is often determined by the implementation of synchronization operations and speed balancing alleviates the need for tuning the implementations of such primitives.},
 acmid = {1693475},
 address = {New York, NY, USA},
 author = {Hofmeyr, Steven and Iancu, Costin and Blagojevi\'{c}, Filip},
 doi = {10.1145/1837853.1693475},
 issn = {0362-1340},
 issue_date = {May 2010},
 journal = {SIGPLAN Not.},
 keyword = {load balancing, operating systems, parallel applications},
 link = {http://doi.acm.org/10.1145/1837853.1693475},
 month = {jan},
 number = {5},
 numpages = {12},
 pages = {147--158},
 publisher = {ACM},
 title = {Load Balancing on Speed},
 volume = {45},
 year = {2010}
}


@inproceedings{Upadhyaya:2010:UDS:1693453.1693490,
 abstract = {To achieve high-performance on multicore systems, sharedmemory parallel languages must efficiently implement atomic operations. The commonly used and studied paradigms for atomicity are fine-grained locking, which is both difficult to program and error-prone; optimistic software transactions, which require substantial overhead to detect and recover from atomicity violations; and compiler-generation of locks from programmer-specified atomic sections, which leads to serialization whenever imprecise pointer analysis suggests the mere possibility of a conflicting operation. This paper presents a new strategy for compiler-generated locking that uses data structure knowledge to facilitate more precise alias and lock generation analyses and reduce unnecessary serialization. Implementing and evaluating these ideas in the Java language shows that the new strategy achieves eight-thread speedups of 0.83 to 5.9 for the five STAMP benchmarks studied, outperforming software transactions on all but one benchmark, and nearly matching programmer-specified fine-grained locks on all but one benchmark. The results also indicate that compiler knowledge of data structures improves the effectiveness of compiler analysis, boosting eight-thread performance by up to 300%. Further, the new analysis allows for software support of strong atomicity with less than 1% overhead for two benchmarks and less than 20% for three others.The strategy also nearly matches the performance of programmer-specified fine-grained locks for the SPECjbb2000 benchmark, which has traditionally not been amenable to static analyses.},
 acmid = {1693490},
 address = {New York, NY, USA},
 author = {Upadhyaya, Gautam and Midkiff, Samuel P. and Pai, Vijay S.},
 booktitle = {Proceedings of the 15th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
 doi = {10.1145/1693453.1693490},
 isbn = {978-1-60558-877-3},
 keyword = {automatic lock generation, parallel programming, transactional memory},
 link = {http://doi.acm.org/10.1145/1693453.1693490},
 location = {Bangalore, India},
 numpages = {12},
 pages = {281--292},
 publisher = {ACM},
 series = {PPoPP '10},
 title = {Using Data Structure Knowledge for Efficient Lock Generation and Strong Atomicity},
 year = {2010}
}


@article{Barreto:2010:LPN:1837853.1693466,
 abstract = {Exploiting the emerging reality of affordable multi-core architectures goes through providing programmers with simple abstractions that would enable them to easily turn their sequential programs into concurrent ones that expose as much parallelism as possible. While transactional memory promises to make concurrent programming easy to a wide programmer community, current implementations either disallow nested transactions to run in parallel or do not scale to arbitrary parallel nesting depths. This is an important obstacle to the central goal of transactional memory, as programmers can only start parallel threads in restricted parts of their code. This paper addresses the intrinsic difficulty behind the support for parallel nesting in transactional memory, and proposes a novel solution that, to the best of our knowledge, is the first practical solution to meet the lowest theoretical upper bound known for the problem. Using a synthetic workload configured to test parallel transactions on a multi-core machine, a practical implementation of our algorithm yields substantial speed-ups (up to 22x with 33 threads) relatively to serial nesting, and shows that the time to start and commit transactions, as well as to detect conflicts, is independent of nesting depth.},
 acmid = {1693466},
 address = {New York, NY, USA},
 author = {Barreto, Jo\~{a}o and Dragojevi\'{c}, Aleksandar and Ferreira, Paulo and Guerraoui, Rachid and Kapalka, Michal},
 doi = {10.1145/1837853.1693466},
 issn = {0362-1340},
 issue_date = {May 2010},
 journal = {SIGPLAN Not.},
 keyword = {fork-join, nested parallel programs, transactional memory, work-stealing},
 link = {http://doi.acm.org/10.1145/1837853.1693466},
 month = {jan},
 number = {5},
 numpages = {10},
 pages = {91--100},
 publisher = {ACM},
 title = {Leveraging Parallel Nesting in Transactional Memory},
 volume = {45},
 year = {2010}
}


@article{Choi:2010:MAS:1837853.1693471,
 abstract = {We present a performance model-driven framework for automated performance tuning (autotuning) of sparse matrix-vector multiply (SpMV) on systems accelerated by graphics processing units (GPU). Our study consists of two parts. First, we describe several carefully hand-tuned SpMV implementations for GPUs, identifying key GPU-specific performance limitations, enhancements, and tuning opportunities. These implementations, which include variants on classical blocked compressed sparse row (BCSR) and blocked ELLPACK (BELLPACK) storage formats, match or exceed state-of-the-art implementations. For instance, our best BELLPACK implementation achieves up to 29.0 Gflop/s in single-precision and 15.7 Gflop/s in double-precision on the NVIDIA T10P multiprocessor (C1060), enhancing prior state-of-the-art unblocked implementations (Bell and Garland, 2009) by up to 1.8× and 1.5× for single-and double-precision respectively. However, achieving this level of performance requires input matrix-dependent parameter tuning. Thus, in the second part of this study, we develop a performance model that can guide tuning. Like prior autotuning models for CPUs (e.g., Im, Yelick, and Vuduc, 2004), this model requires offline measurements and run-time estimation, but more directly models the structure of multithreaded vector processors like GPUs. We show that our model can identify the implementations that achieve within 15% of those found through exhaustive search.},
 acmid = {1693471},
 address = {New York, NY, USA},
 author = {Choi, Jee W. and Singh, Amik and Vuduc, Richard W.},
 doi = {10.1145/1837853.1693471},
 issn = {0362-1340},
 issue_date = {May 2010},
 journal = {SIGPLAN Not.},
 keyword = {gpu, performance modeling, sparse matrix-vector multiplication},
 link = {http://doi.acm.org/10.1145/1837853.1693471},
 month = {jan},
 number = {5},
 numpages = {12},
 pages = {115--126},
 publisher = {ACM},
 title = {Model-driven Autotuning of Sparse Matrix-vector Multiply on GPUs},
 volume = {45},
 year = {2010}
}


@article{Arvind:2010:HIO:1837853.1693455,
 abstract = {My colleagues, promotion committees, research funding agencies and business people often wonder if there is need for any architecture research. There seems to be no room to dislodge Intel IA-32. Even the number of new Application-Specific Integrated Circuits (ASICs) seems to be declining each year, because of the ever-increasing development cost. This viewpoint ignores another reality which is that the future will be dominated by mobile devices such as smart phones and the infrastructure needed to support consumer services on these devices. This is already restructuring the IT industry. To the first-order, in the mobile world functionality is determined by what can be supported within a 3W power budget. The only way to reduce power by one to two orders of magnitude is via functionally specialized hardware blocks. A fundamental shift is needed in the current design flow of systems-on-a-chip (SoCs) to produce them in a less-risky and cost-effective manner. In this talk we will present, via examples, a method of designing systems that facilitates the synthesis of complex SoCs from reusable "IP" modules. The technical challenge is to provide a method for connecting modules in a parallel setting so that the functionality and the performance of the composite are predictable.},
 acmid = {1693455},
 address = {New York, NY, USA},
 author = {Arvind, Professor},
 doi = {10.1145/1837853.1693455},
 issn = {0362-1340},
 issue_date = {May 2010},
 journal = {SIGPLAN Not.},
 keyword = {hardware innovation, system-on-chip},
 link = {http://doi.acm.org/10.1145/1837853.1693455},
 month = {jan},
 number = {5},
 numpages = {2},
 pages = {103--104},
 publisher = {ACM},
 title = {Is Hardware Innovation over?},
 volume = {45},
 year = {2010}
}


@article{Tzannes:2010:LBR:1837853.1693479,
 abstract = {We present Lazy Binary Splitting (LBS), a user-level scheduler of nested parallelism for shared-memory multiprocessors that builds on existing Eager Binary Splitting work-stealing (EBS) implemented in Intel's Threading Building Blocks (TBB), but improves performance and ease-of-programming. In its simplest form (SP), EBS requires manual tuning by repeatedly running the application under carefully controlled conditions to determine a stop-splitting-threshold (sst)for every do-all loop in the code. This threshold limits the parallelism and prevents excessive overheads for fine-grain parallelism. Besides being tedious, this tuning also over-fits the code to some particular dataset, platform and calling context of the do-all loop, resulting in poor performance portability for the code. LBS overcomes both the performance portability and ease-of-programming pitfalls of a manually fixed threshold by adapting dynamically to run-time conditions without requiring tuning. We compare LBS to Auto-Partitioner (AP), the latest default scheduler of TBB, which does not require manual tuning either but lacks context portability, and outperform it by 38.9% using TBB's default AP configuration, and by 16.2% after we tuned AP to our experimental platform. We also compare LBS to SP by manually finding SP's sst using a training dataset and then running both on a different execution dataset. LBS outperforms SP by 19.5% on average. while allowing for improved performance portability without requiring tedious manual tuning. LBS also outperforms SP with sst=1, its default value when undefined, by 56.7%, and serializing work-stealing (SWS), another work-stealer by 54.7%. Finally, compared to serializing inner parallelism (SI) which has been used by OpenMP, LBS is 54.2% faster.},
 acmid = {1693479},
 address = {New York, NY, USA},
 author = {Tzannes, Alexandros and Caragea, George C. and Barua, Rajeev and Vishkin, Uzi},
 doi = {10.1145/1837853.1693479},
 issn = {0362-1340},
 issue_date = {May 2010},
 journal = {SIGPLAN Not.},
 keyword = {dynamic scheduling, load balancing, nested parallelism, thread scheduling, work stealing},
 link = {http://doi.acm.org/10.1145/1837853.1693479},
 month = {jan},
 number = {5},
 numpages = {12},
 pages = {179--190},
 publisher = {ACM},
 title = {Lazy Binary-splitting: A Run-time Adaptive Work-stealing Scheduler},
 volume = {45},
 year = {2010}
}


@inproceedings{Radojkovic:2010:TSB:1693453.1693480,
 abstract = {In processors with several levels of hardware resource sharing,like CMPs in which each core is an SMT, the scheduling process becomes more complex than in processors with a single level of resource sharing, such as pure-SMT or pure-CMP processors. Once the operating system selects the set of applications to simultaneously schedule on the processor (workload), each application/thread must be assigned to one of the hardware contexts(strands). We call this last scheduling step the Thread to Strand Binding or TSB. In this paper, we show that the TSB impact on the performance of processors with several levels of shared resources is high. We measure a variation of up to 59% between different TSBs of real multithreaded network applications running on the UltraSPARC T2 processor which has three levels of resource sharing. In our view, this problem is going to be more acute in future multithreaded architectures comprising more cores, more contexts per core, and more levels of resource sharing. We propose a resource-sharing aware TSB algorithm (TSBSched) that significantly facilitates the problem of thread to strand binding for software-pipelined applications, representative of multithreaded network applications. Our systematic approach encapsulates both, the characteristics of multithreaded processors under the study and the structure of the software pipelined applications. Once calibrated for a given processor architecture, our proposal does not require hardware knowledge on the side of the programmer, nor extensive profiling of the application. We validate our algorithm on the UltraSPARC T2 processor running a set of real multithreaded network applications on which we report improvements of up to 46% compared to the current state-of-the-art dynamic schedulers.},
 acmid = {1693480},
 address = {New York, NY, USA},
 author = {Radojkovi\'{c}, Petar and \v{C}akarevi\'{c}, Vladimir and Verd\'{u}, Javier and Pajuelo, Alex and Cazorla, Francisco J. and Nemirovsky, Mario and Valero, Mateo},
 booktitle = {Proceedings of the 15th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
 doi = {10.1145/1693453.1693480},
 isbn = {978-1-60558-877-3},
 keyword = {cmt, process scheduling, simultaneous multithreading, ultrasparc t2},
 link = {http://doi.acm.org/10.1145/1693453.1693480},
 location = {Bangalore, India},
 numpages = {12},
 pages = {191--202},
 publisher = {ACM},
 series = {PPoPP '10},
 title = {Thread to Strand Binding of Parallel Network Applications in Massive Multi-threaded Systems},
 year = {2010}
}


@inproceedings{Sutherland:2010:CTC:1693453.1693485,
 abstract = {This paper introduces the language-independent concept of ``thread usage policy.'' Many multi-threaded software systems contain policies that regulate associations among threads, executable code, and potentially shared state. A system, for example, may constrain which threads are permitted to execute particular code segments, usually as a means to constrain those threads from accessing or writing particular elements of state. These policies ensure properties such as state confinement or reader/writer constraints, often without recourse to locking or transaction discipline. Our approach allows developers to concisely document their thread usage policies in a manner that enables the use of sound scalable analysis to assess consistency of policy and as-written code. This paper identifies the key semantic concepts of our thread coloring language and illustrates how to use its succinct source-level annotations to express models of thread usage policies, following established annotation conventions for Java. We have built a prototype static analysis tool, implemented as an integrated development environment plug-in (for the Eclipse IDE), that notifies developers of discrepancies between policy annotations and as-written code. Our analysis technique uses several underlying algorithms based on abstract interpretation, call-graphs, and type inference. The resulting overall analysis is both sound and composable. We have used this prototype analysis tool in case studies to model and analyze more than a million lines of code. Our validation process included field trials on a wide variety of complex large-scale production code selected by the host organizations. Our in-field experience led us to focus on potential adoptability by real-world developers. We have developed techniques that can reduce annotation density to less than one line per thousand lines of code (KLOC). In addition, the prototype analysis tool supports an incremental and iterative approach to modeling and analysis. This approach enabled field trial partners to directly target areas of greatest concern and to achieve useful results within a few hours.},
 acmid = {1693485},
 address = {New York, NY, USA},
 author = {Sutherland, Dean F. and Scherlis, William L.},
 booktitle = {Proceedings of the 15th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
 doi = {10.1145/1693453.1693485},
 isbn = {978-1-60558-877-3},
 keyword = {annotation, java, keywords: state consistency, multicore, race conditions, state confinement, thread policy},
 link = {http://doi.acm.org/10.1145/1693453.1693485},
 location = {Bangalore, India},
 numpages = {12},
 pages = {233--244},
 publisher = {ACM},
 series = {PPoPP '10},
 title = {Composable Thread Coloring},
 year = {2010}
}


@article{Castaldo:2010:SLP:1837853.1693484,
 abstract = {In LAPACK many matrix operations are cast as block algorithms which iteratively process a panel using an unblocked algorithm and then update a remainder matrix using the high performance Level 3 BLAS. The Level~3 BLAS have excellent weak scaling, but panel processing tends to be bus bound, and thus scales with bus speed rather than the number of processors (p). Amdahl's law therefore ensures that as p grows, the panel computation will become the dominant cost of these LAPACK routines. Our contribution is a novel parallel cache assignment approach which we show scales well with p. We apply this general approach to the QR and LU panel factorizations on two commodity 8-core platforms with very different cache structures, and demonstrate superlinear panel factorization speedups on both machines. Other approaches to this problem demand complicated reformulations of the computational approach, new kernels to be tuned, new mathematics, an inflation of the high-order flop count, and do not perform as well. By demonstrating a straight-forward alternative that avoids all of these contortions and scales with p, we address a critical stumbling block for dense linear algebra in the age of massive parallelism.},
 acmid = {1693484},
 address = {New York, NY, USA},
 author = {Castaldo, Anthony M. and Whaley, R. Clint},
 doi = {10.1145/1837853.1693484},
 issn = {0362-1340},
 issue_date = {May 2010},
 journal = {SIGPLAN Not.},
 keyword = {atlas, factorization, gpu, lapack, lu, multi-core, multicore, parallel, qr},
 link = {http://doi.acm.org/10.1145/1837853.1693484},
 month = {jan},
 number = {5},
 numpages = {10},
 pages = {223--232},
 publisher = {ACM},
 title = {Scaling LAPACK Panel Operations Using Parallel Cache Assignment},
 volume = {45},
 year = {2010}
}


@article{Dalessandro:2010:NSS:1837853.1693464,
 abstract = {Drawing inspiration from several previous projects, we present an ownership-record-free software transactional memory (STM) system that combines extremely low overhead with unusually clean semantics. While unlikely to scale to hundreds of active threads, this "NOrec" system offers many appealing features: very low fast-path latency--as low as any system we know of that admits concurrent updates; publication and privatization safety; livelock freedom; a small, constant amount of global metadata, and full compatibility with existing data structure layouts; no false conflicts due to hash collisions; compatibility with both managed and unmanaged languages, and both static and dynamic compilation; and easy acccommodation of closed nesting, inevitable (irrevocable) transactions, and starvation avoidance mechanisms. To the best of our knowledge, no extant STM system combines this set of features. While transactional memory for processors with hundreds of cores is likely to require hardware support, software implementations will be required for backward compatibility with current and near-future processors with 2--64 cores, as well as for fall-back in future machines when hardware resources are exhausted. Our experience suggests that NOrec may be an ideal candidate for such a software system. We also observe that it has considerable appeal for use within the operating system, and in systems that require both closed nesting and publication safety.},
 acmid = {1693464},
 address = {New York, NY, USA},
 author = {Dalessandro, Luke and Spear, Michael F. and Scott, Michael L.},
 doi = {10.1145/1837853.1693464},
 issn = {0362-1340},
 issue_date = {May 2010},
 journal = {SIGPLAN Not.},
 keyword = {ownership records, software transactional memory, transactional memory, transactional memory models},
 link = {http://doi.acm.org/10.1145/1837853.1693464},
 month = {jan},
 number = {5},
 numpages = {12},
 pages = {67--78},
 publisher = {ACM},
 title = {NOrec: Streamlining STM by Abolishing Ownership Records},
 volume = {45},
 year = {2010}
}


@article{Baghsorkhi:2010:APM:1837853.1693470,
 abstract = {This paper presents an analytical model to predict the performance of general-purpose applications on a GPU architecture. The model is designed to provide performance information to an auto-tuning compiler and assist it in narrowing down the search to the more promising implementations. It can also be incorporated into a tool to help programmers better assess the performance bottlenecks in their code. We analyze each GPU kernel and identify how the kernel exercises major GPU microarchitecture features. To identify the performance bottlenecks accurately, we introduce an abstract interpretation of a GPU kernel, work flow graph, based on which we estimate the execution time of a GPU kernel. We validated our performance model on the NVIDIA GPUs using CUDA (Compute Unified Device Architecture). For this purpose, we used data parallel benchmarks that stress different GPU microarchitecture events such as uncoalesced memory accesses, scratch-pad memory bank conflicts, and control flow divergence, which must be accurately modeled but represent challenges to the analytical performance models. The proposed model captures full system complexity and shows high accuracy in predicting the performance trends of different optimized kernel implementations. We also describe our approach to extracting the performance model automatically from a kernel code.},
 acmid = {1693470},
 address = {New York, NY, USA},
 author = {Baghsorkhi, Sara S. and Delahaye, Matthieu and Patel, Sanjay J. and Gropp, William D. and Hwu, Wen-mei W.},
 doi = {10.1145/1837853.1693470},
 issn = {0362-1340},
 issue_date = {May 2010},
 journal = {SIGPLAN Not.},
 keyword = {analytical model, gpu, parallel programming, performance estimation},
 link = {http://doi.acm.org/10.1145/1837853.1693470},
 month = {jan},
 number = {5},
 numpages = {10},
 pages = {105--114},
 publisher = {ACM},
 title = {An Adaptive Performance Modeling Tool for GPU Architectures},
 volume = {45},
 year = {2010}
}


@article{Marjanovic:2010:ECC:1837853.1693502,
 abstract = {Communication overhead is one of the dominant factors affecting performance in high-performance computing systems. To reduce the negative impact of communication, programmers overlap communication and computation by using asynchronous communication primitives. This increases code complexity, requiring more development effort and making less readable programs. This paper presents the hybrid use of MPI and SMPSs (SMP superscalar, a task-based shared-memory programming model) that allows the programmer to easily introduce the asynchrony necessary to overlap communication and computation. We demonstrate the hybrid use of MPI/SMPSs with the high-performance LINPACK benchmark (HPL), and compare it to the pure MPI implementation, which uses the look-ahead technique to overlap communication and computation. The hybrid MPI/SMPSs version significantly improves the performance of the pure MPI version, getting close to the asymptotic performance at medium problem sizes and still getting significant benefits at small/large problem sizes.},
 acmid = {1693502},
 address = {New York, NY, USA},
 author = {Marjanovic, Vladimir and Labarta, Jes\'{u}s and Ayguad{\'e}, Eduard and Valero, Mateo},
 doi = {10.1145/1837853.1693502},
 issn = {0362-1340},
 issue_date = {May 2010},
 journal = {SIGPLAN Not.},
 keyword = {hybrid mpi/smpss, linpack, mpi, parallel programming model},
 link = {http://doi.acm.org/10.1145/1837853.1693502},
 month = {jan},
 number = {5},
 numpages = {2},
 pages = {337--338},
 publisher = {ACM},
 title = {Effective Communication and Computation Overlap with Hybrid MPI/SMPSs},
 volume = {45},
 year = {2010}
}


@article{Chandramowlishwaran:2010:ACC:1837853.1693506,
 abstract = {This poster is a case study on the application of a novel programming model, called Concurrent Collections (CnC), to the implementation of an asynchronous-parallel algorithm for computing the Cholesky factorization of dense matrices. In CnC, the programmer expresses her computation in terms of application-specific operations, partially-ordered by semantic scheduling constraints. We demonstrate the performance potential of CnC in this poster, by showing that our Cholesky implementation nearly matches or exceeds competing vendor-tuned codes and alternative programming models. We conclude that the CnC model is well-suited for expressing asynchronous-parallel algorithms on emerging multicore systems.},
 acmid = {1693506},
 address = {New York, NY, USA},
 author = {Chandramowlishwaran, Aparna and Knobe, Kathleen and Vuduc, Richard},
 doi = {10.1145/1837853.1693506},
 issn = {0362-1340},
 issue_date = {May 2010},
 journal = {SIGPLAN Not.},
 keyword = {asynchronous algorithms, concurrent collections, dense linear algebra},
 link = {http://doi.acm.org/10.1145/1837853.1693506},
 month = {jan},
 number = {5},
 numpages = {2},
 pages = {345--346},
 publisher = {ACM},
 title = {Applying the Concurrent Collections Programming Model to Asynchronous Parallel Dense Linear Algebra},
 volume = {45},
 year = {2010}
}


@article{Maldonado:2010:SST:1837853.1693465,
 abstract = {Transactional Memory (TM) is considered as one of the most promising paradigms for developing concurrent applications. TM has been shown to scale well on >multiple cores when the data access pattern behaves "well," i.e., when few conflicts are induced. In contrast, data patterns with frequent write sharing, with long transactions, or when many threads contend for a smaller number of cores, result in numerous conflicts. Until recently, TM implementations had little control of transactional threads, which remained under the supervision of the kernel's transaction-ignorant scheduler. Conflicts are thus traditionally resolved by consulting an STM-level contention manager. Consequently, the contention managers of these "conventional" TM implementations suffer from a lack of precision and often fail to ensure reasonable performance in high-contention workloads. Recently, scheduling-based TM contention-management has been proposed for increasing TM efficiency under high-contention [2, 5, 19]. However, only user-level schedulers have been considered. In this work, we propose, implement and evaluate several novel kernel-level scheduling support mechanisms for TM contention management. We also investigate different strategies for efficient communication between the kernel and the user-level TM library. To the best of our knowledge, our work is the first to investigate kernel-level support for TM contention management. We have introduced kernel-level TM scheduling support into both the Linux and Solaris kernels. Our experimental evaluation demonstrates that lightweight kernel-level scheduling support significantly reduces the number of aborts while improving transaction throughput on various workloads.},
 acmid = {1693465},
 address = {New York, NY, USA},
 author = {Maldonado, Walther and Marlier, Patrick and Felber, Pascal and Suissa, Adi and Hendler, Danny and Fedorova, Alexandra and Lawall, Julia L. and Muller, Gilles},
 doi = {10.1145/1837853.1693465},
 issn = {0362-1340},
 issue_date = {May 2010},
 journal = {SIGPLAN Not.},
 keyword = {contention management, scheduling, transactional memory},
 link = {http://doi.acm.org/10.1145/1837853.1693465},
 month = {jan},
 number = {5},
 numpages = {12},
 pages = {79--90},
 publisher = {ACM},
 title = {Scheduling Support for Transactional Memory Contention Management},
 volume = {45},
 year = {2010}
}


@inproceedings{Tallent:2010:ALC:1693453.1693489,
 abstract = {Many programs exploit shared-memory parallelism using multithreading. Threaded codes typically use locks to coordinate access to shared data. In many cases, contention for locks reduces parallel efficiency and hurts scalability. Being able to quantify and attribute lock contention is important for understanding where a multithreaded program needs improvement. This paper proposes and evaluates three strategies for gaining insight into performance losses due to lock contention. First, we consider using a straightforward strategy based on call stack profiling to attribute idle time and show that it fails to yield insight into lock contention. Second, we consider an approach that builds on a strategy previously used for analyzing idleness in work-stealing computations; we show that this strategy does not yield insight into lock contention. Finally, we propose a new technique for measurement and analysis of lock contention that uses data associated with locks to blame lock holders for the idleness of spinning threads. Our approach incurs ≤ 5% overhead on a quantum chemistry application that makes extensive use of locking (65M distinct locks, a maximum of 340K live locks, and an average of 30K lock acquisitions per second per thread) and attributes lock contention to its full static and dynamic calling contexts. Our strategy, implemented in HPCToolkit, is fully distributed and should scale well to systems with large core counts.},
 acmid = {1693489},
 address = {New York, NY, USA},
 author = {Tallent, Nathan R. and Mellor-Crummey, John M. and Porterfield, Allan},
 booktitle = {Proceedings of the 15th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
 doi = {10.1145/1693453.1693489},
 isbn = {978-1-60558-877-3},
 keyword = {hpctoolkit, lock contention, multithreading, performance analysis},
 link = {http://doi.acm.org/10.1145/1693453.1693489},
 location = {Bangalore, India},
 numpages = {12},
 pages = {269--280},
 publisher = {ACM},
 series = {PPoPP '10},
 title = {Analyzing Lock Contention in Multithreaded Applications},
 year = {2010}
}


@article{Li:2010:SVC:1837853.1693512,
 abstract = {We present a preliminary automated verifier based on mechanical decision procedures which is able to prove functional correctness of CUDA programs and guarantee to detect bugs such as race conditions. We also employ a symbolic partial order reduction (POR) technique to mitigate the interleaving explosion problem.},
 acmid = {1693512},
 address = {New York, NY, USA},
 author = {Li, Guodong and Gopalakrishnan, Ganesh and Kirby, Robert M. and Quinlan, Dan},
 doi = {10.1145/1837853.1693512},
 issn = {0362-1340},
 issue_date = {May 2010},
 journal = {SIGPLAN Not.},
 keyword = {cuda, formal verification, spmd, symbolic analysis},
 link = {http://doi.acm.org/10.1145/1837853.1693512},
 month = {jan},
 number = {5},
 numpages = {2},
 pages = {357--358},
 publisher = {ACM},
 title = {A Symbolic Verifier for CUDA Programs},
 volume = {45},
 year = {2010}
}


@article{Bronson:2010:PCB:1837853.1693488,
 abstract = {We propose a concurrent relaxed balance AVL tree algorithm that is fast, scales well, and tolerates contention. It is based on optimistic techniques adapted from software transactional memory, but takes advantage of specific knowledge of the the algorithm to reduce overheads and avoid unnecessary retries. We extend our algorithm with a fast linearizable clone operation, which can be used for consistent iteration of the tree. Experimental evidence shows that our algorithm outperforms a highly tuned concurrent skip list for many access patterns, with an average of 39% higher single-threaded throughput and 32% higher multi-threaded throughput over a range of contention levels and operation mixes.},
 acmid = {1693488},
 address = {New York, NY, USA},
 author = {Bronson, Nathan G. and Casper, Jared and Chafi, Hassan and Olukotun, Kunle},
 doi = {10.1145/1837853.1693488},
 issn = {0362-1340},
 issue_date = {May 2010},
 journal = {SIGPLAN Not.},
 keyword = {optimistic concurrency, snapshot isolation},
 link = {http://doi.acm.org/10.1145/1837853.1693488},
 month = {jan},
 number = {5},
 numpages = {12},
 pages = {257--268},
 publisher = {ACM},
 title = {A Practical Concurrent Binary Search Tree},
 volume = {45},
 year = {2010}
}


@article{Torrellas:2010:ESC:1837853.1693468,
 abstract = {
                  An abstract is not available.
              },
 acmid = {1693468},
 address = {New York, NY, USA},
 author = {Torrellas, Josep and Gropp, Bill and Moreno, Jaime and Olukotun, Kunle and Sarkar, Vivek},
 doi = {10.1145/1837853.1693468},
 issn = {0362-1340},
 issue_date = {May 2010},
 journal = {SIGPLAN Not.},
 keyword = {architecture, challenges, exascale},
 link = {http://doi.acm.org/10.1145/1837853.1693468},
 month = {jan},
 number = {5},
 numpages = {2},
 pages = {101--102},
 publisher = {ACM},
 title = {Extreme Scale Computing: Challenges and Opportunities},
 volume = {45},
 year = {2010}
}


@article{Lupei:2010:TST:1837853.1693496,
 abstract = {This work addresses the problem of parallelizing multiplayer games using software Transactional Memory (STM) support. Using a realistic high impact application, we show that STM provides not only ease of programming, but also better performance than that achievable with state-of-the-art lock-based programming. Towards this goal, we use SynQuake, a game benchmark which extracts the main data structures and the essential features of the popular multiplayer game Quake, but can be driven with a synthetic workload generator that flexibly emulates client game actions and various hot-spot scenarios in the game world. We implement, evaluate and compare the STM version of SynQuake with a state-of-the-art lock-based parallelization of Quake, which we ported to SynQuake. While in STM-SynQuake support for maintaining the consistency of each potentially complex game action is automatic, conservative locking of surrounding objects within a bounding box for the duration of the game action is inherently needed in lock-based SynQuake. This leads to a higher scalability factor of STM-SynQuake versus lock-based SynQuake, due to a higher degree of false sharing in the latter.},
 acmid = {1693496},
 address = {New York, NY, USA},
 author = {Lupei, Daniel and Simion, Bogdan and Pinto, Don and Misler, Matthew and Burcea, Mihai and Krick, William and Amza, Cristiana},
 doi = {10.1145/1837853.1693496},
 issn = {0362-1340},
 issue_date = {May 2010},
 journal = {SIGPLAN Not.},
 keyword = {massively multiplayer games, scalability, software transactional memory, synchronization},
 link = {http://doi.acm.org/10.1145/1837853.1693496},
 month = {jan},
 number = {5},
 numpages = {2},
 pages = {325--326},
 publisher = {ACM},
 title = {Towards Scalable and Transparent Parallelization of Multiplayer Games Using Transactional Memory Support},
 volume = {45},
 year = {2010}
}


@inproceedings{Zhang:2010:CSP:1693453.1693501,
 abstract = {This paper addresses the problem of extracting coarse-grained parallelism from large sequential code. It builds on BOP, a system for software speculative parallelization. BOP lets a user to mark possibly parallel regions (PPR) in a program and at run-time speculatively executes PPR instances using Unix processes. This short paper presents a new run-time support called continuous speculation, which fully utilizes available parallelism to tolerate differences in PPR task size and processor speed.},
 acmid = {1693501},
 address = {New York, NY, USA},
 author = {Zhang, Chao and Ding, Chen and Gu, Xiaoming and Kelsey, Kirk and Bai, Tongxin and Feng, Xiaobing},
 booktitle = {Proceedings of the 15th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
 doi = {10.1145/1693453.1693501},
 isbn = {978-1-60558-877-3},
 keyword = {software speculative parallelization},
 link = {http://doi.acm.org/10.1145/1693453.1693501},
 location = {Bangalore, India},
 numpages = {2},
 pages = {335--336},
 publisher = {ACM},
 series = {PPoPP '10},
 title = {Continuous Speculative Program Parallelization in Software},
 year = {2010}
}


@inproceedings{Jang:2010:DTE:1693453.1693510,
 abstract = {Loop vectorization, a key feature exploited to obtain high performance on Single Instruction Multiple Data (SIMD) vector architectures, is significantly hindered by irregular memory access patterns in the data stream. This paper describes data transformations that allow us to vectorize loops targeting massively multithreaded data parallel architectures. We present a mathematical model that captures loop-based memory access patterns and computes the most appropriate data transformations in order to enable vectorization. Our experimental results show that the proposed data transformations can significantly increase the number of loops that can be vectorized and enhance the data-level parallelism of applications. Our results also show that the overhead associated with our data transformations can be easily amortized as the size of the input data set increases. For the set of high performance benchmark kernels studied, we achieve consistent and significant performance improvements (up to 11.4X) by applying vectorization using our data transformation approach.},
 acmid = {1693510},
 address = {New York, NY, USA},
 author = {Jang, Byunghyun and Mistry, Perhaad and Schaa, Dana and Dominguez, Rodrigo and Kaeli, David},
 booktitle = {Proceedings of the 15th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
 doi = {10.1145/1693453.1693510},
 isbn = {978-1-60558-877-3},
 keyword = {data transformation, gpgpu, loop vectorization},
 link = {http://doi.acm.org/10.1145/1693453.1693510},
 location = {Bangalore, India},
 numpages = {2},
 pages = {353--354},
 publisher = {ACM},
 series = {PPoPP '10},
 title = {Data Transformations Enabling Loop Vectorization on Multithreaded Data Parallel Architectures},
 year = {2010}
}


@article{Muralidhara:2010:ISC:1837853.1693498,
 abstract = {In this paper, we address the problem of partitioning a shared cache when the executing threads belong to the same application.},
 acmid = {1693498},
 address = {New York, NY, USA},
 author = {Muralidhara, Sai Prashanth and Kandemir, Mahmut and Raghavan, Padma},
 doi = {10.1145/1837853.1693498},
 issn = {0362-1340},
 issue_date = {May 2010},
 journal = {SIGPLAN Not.},
 keyword = {cache, multicore, parallel applications},
 link = {http://doi.acm.org/10.1145/1837853.1693498},
 month = {jan},
 number = {5},
 numpages = {2},
 pages = {329--330},
 publisher = {ACM},
 title = {Intra-application Shared Cache Partitioning for Multithreaded Applications},
 volume = {45},
 year = {2010}
}


@inproceedings{Chakrabarti:2010:NAE:1693453.1693500,
 abstract = {We present the design and implementation of a dynamic conflict graph annotated with fine grain transaction characteristics and show that this is important information for effective performance analysis of a software transactional memory (STM) program. We show how to implement the necessary support in a compiler and an STM with minimal perturbation of the original behavior of the application.},
 acmid = {1693500},
 address = {New York, NY, USA},
 author = {Chakrabarti, Dhruva R.},
 booktitle = {Proceedings of the 15th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
 doi = {10.1145/1693453.1693500},
 isbn = {978-1-60558-877-3},
 keyword = {concurrency, software transactional memory},
 link = {http://doi.acm.org/10.1145/1693453.1693500},
 location = {Bangalore, India},
 numpages = {2},
 pages = {333--334},
 publisher = {ACM},
 series = {PPoPP '10},
 title = {New Abstractions for Effective Performance Analysis of STM Programs},
 year = {2010}
}


@article{Mannarswamy:2010:CAS:1837853.1693460,
 abstract = {Atomic sections have been recently introduced as a language construct to improve the programmability of concurrent software. They simplify programming by not requiring the explicit specification of locks for shared data. Typically atomic sections are supported in software either through the use of optimistic concurrency by using transactional memory or through the use of pessimistic concurrency using compiler-assigned locks. As a software transactional memory (STM) system does not take advantage of the specific memory access patterns of an application it often suffers from false conflicts and high validation overheads. On the other hand, the compiler usually ends up assigning coarse grain locks as it relies on whole program points-to analysis which is conservative by nature. This adversely affects performance by limiting concurrency. In order to mitigate the disadvantages associated with STM's lock assignment scheme, we propose a hybrid approach which combines STM's lock assignment with a compiler aided selective lock assignment scheme (referred to as SCLA-STM). SCLA-STM overcomes the inefficiencies associated with a purely compile-time lock assignment approach by (i) using the underlying STM for shared variables where only a conservative analysis is possible by the compiler (e.g., in the presence of may-alias points to information) and (ii) being selective about the shared data chosen for the compiler-aided lock assignment. We describe our prototype SCLA-STM scheme implemented in the hp-ux IA-64 C/C++ compiler, using TL2 as our STM implementation. We show that SCLA-STM improves application performance for certain STAMP benchmarks from 1.68% to 37.13%.},
 acmid = {1693460},
 address = {New York, NY, USA},
 author = {Mannarswamy, Sandya and Chakrabarti, Dhruva R. and Rajan, Kaushik and Saraswati, Sujoy},
 doi = {10.1145/1837853.1693460},
 issn = {0362-1340},
 issue_date = {May 2010},
 journal = {SIGPLAN Not.},
 keyword = {compilers, multithreading, parallelization, performance},
 link = {http://doi.acm.org/10.1145/1837853.1693460},
 month = {jan},
 number = {5},
 numpages = {10},
 pages = {37--46},
 publisher = {ACM},
 title = {Compiler Aided Selective Lock Assignment for Improving the Performance of Software Transactional Memory},
 volume = {45},
 year = {2010}
}


@inproceedings{Mendez-Lojo:2010:SOA:1693453.1693457,
 abstract = {Irregular algorithms are organized around pointer-based data structures such as graphs and trees, and they are ubiquitous in applications. Recent work by the Galois project has provided a systematic approach for parallelizing irregular applications based on the idea of optimistic or speculative execution of programs. However, the overhead of optimistic parallel execution can be substantial. In this paper, we show that many irregular algorithms have structure that can be exploited and present three key optimizations that take advantage of algorithmic structure to reduce speculative overheads. We describe the implementation of these optimizations in the Galois system and present experimental results to demonstrate their benefits. To the best of our knowledge, this is the first system to exploit algorithmic structure to optimize the execution of irregular programs.},
 acmid = {1693457},
 address = {New York, NY, USA},
 author = {M{\'e}ndez-Lojo, Mario and Nguyen, Donald and Prountzos, Dimitrios and Sui, Xin and Hassaan, M. Amber and Kulkarni, Milind and Burtscher, Martin and Pingali, Keshav},
 booktitle = {Proceedings of the 15th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
 doi = {10.1145/1693453.1693457},
 isbn = {978-1-60558-877-3},
 keyword = {amorphous data-parallelism, cautious operator implementations, irregular programs, iteration coalescing, one-shot optimization, optimistic parallelization, synchronization overheads},
 link = {http://doi.acm.org/10.1145/1693453.1693457},
 location = {Bangalore, India},
 numpages = {12},
 pages = {3--14},
 publisher = {ACM},
 series = {PPoPP '10},
 title = {Structure-driven Optimizations for Amorphous Data-parallel Programs},
 year = {2010}
}


@article{Rossbach:2010:TPA:1837853.1693462,
 abstract = {Chip multi-processors (CMPs) have become ubiquitous, while tools that ease concurrent programming have not. The promise of increased performance for all applications through ever more parallel hardware requires good tools for concurrent programming, especially for average programmers. Transactional memory (TM) has enjoyed recent interest as a tool that can help programmers program concurrently. The transactional memory (TM) research community is heavily invested in the claim that programming with transactional memory is easier than alternatives (like locks), but evidence for or against the veracity of this claim is scant. In this paper, we describe a user-study in which 237 undergraduate students in an operating systems course implement the same programs using coarse and fine-grain locks, monitors, and transactions. We surveyed the students after the assignment, and examined their code to determine the types and frequency of programming errors for each synchronization technique. Inexperienced programmers found baroque syntax a barrier to entry for transactional programming. On average, subjective evaluation showed that students found transactions harder to use than coarse-grain locks, but slightly easier to use than fine-grained locks. Detailed examination of synchronization errors in the students' code tells a rather different story. Overwhelmingly, the number and types of programming errors the students made was much lower for transactions than for locks. On a similar programming problem, over 70% of students made errors with fine-grained locking, while less than 10% made errors with transactions.},
 acmid = {1693462},
 address = {New York, NY, USA},
 author = {Rossbach, Christopher J. and Hofmann, Owen S. and Witchel, Emmett},
 doi = {10.1145/1837853.1693462},
 issn = {0362-1340},
 issue_date = {May 2010},
 journal = {SIGPLAN Not.},
 keyword = {optimistic concurrency, synchronization, transactional memory},
 link = {http://doi.acm.org/10.1145/1837853.1693462},
 month = {jan},
 number = {5},
 numpages = {10},
 pages = {47--56},
 publisher = {ACM},
 title = {Is Transactional Programming Actually Easier?},
 volume = {45},
 year = {2010}
}


@inproceedings{Ali:2010:MAC:1693453.1693492,
 abstract = {This paper presents and validates performance models for a varietyvof high-performance collective communication algorithms for systems with Cell processors. The systems modeled include a single Cell processor, two Cell chips on a Cell Blade, and a cluster of Cell Blades. The models extend PLogP, the well-known point-topoint performance model, by accounting for the unique hardware characteristics of the Cell (e.g., heterogeneous interconnects and DMA engines) and by applying the model to collective communication. This paper also presents a micro-benchmark suite to accurately measure the extended PLogP parameters on the Cell Blade and then uses these parameters to model different algorithms for the barrier, broadcast, reduce, all-reduce, and all-gather collective operations. Out of 425 total performance predictions, 398 of them see less than 10% error compared to the actual execution time and all of them see less than 15%.},
 acmid = {1693492},
 address = {New York, NY, USA},
 author = {Ali, Qasim and Midkiff, Samuel Pratt and Pai, Vijay S.},
 booktitle = {Proceedings of the 15th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
 doi = {10.1145/1693453.1693492},
 isbn = {978-1-60558-877-3},
 keyword = {algorithms, collective communication, modeling},
 link = {http://doi.acm.org/10.1145/1693453.1693492},
 location = {Bangalore, India},
 numpages = {12},
 pages = {293--304},
 publisher = {ACM},
 series = {PPoPP '10},
 title = {Modeling Advanced Collective Communication Algorithms on Cell-based Systems},
 year = {2010}
}


@inproceedings{Coons:2010:GEU:1693453.1693458,
 abstract = {As concurrent programming becomes prevalent, software providers are investing in concurrency libraries to improve programmer productivity. Concurrency libraries improve productivity by hiding error-prone, low-level synchronization from programmers and providing higher-level concurrent abstractions. Testing such libraries is difficult, however, because concurrency failures often manifest only under particular scheduling circumstances. Current best testing practices are often inadequate: heuristic-guided fuzzing is not systematic, systematic schedule enumeration does not find bugs quickly, and stress testing is neither systematic nor fast. To address these shortcomings, we propose a prioritized search technique called GAMBIT that combines the speed benefits of heuristic-guided fuzzing with the soundness, progress, and reproducibility guarantees of stateless model checking. GAMBIT combines known techniques such as partial-order reduction and preemption-bounding with a generalized best-first search frame- work that prioritizes schedules likely to expose bugs. We evaluate GAMBIT's effectiveness on newly released concurrency libraries for Microsoft's .NET framework. Our experiments show that GAMBIT finds bugs more quickly than prior stateless model checking techniques without compromising coverage guarantees or reproducibility.},
 acmid = {1693458},
 address = {New York, NY, USA},
 author = {Coons, Katherine E. and Burckhardt, Sebastian and Musuvathi, Madanlal},
 booktitle = {Proceedings of the 15th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
 doi = {10.1145/1693453.1693458},
 isbn = {978-1-60558-877-3},
 keyword = {concurrency, model checking, multithreading, partial-order reduction, preemption bound, software testing},
 link = {http://doi.acm.org/10.1145/1693453.1693458},
 location = {Bangalore, India},
 numpages = {10},
 pages = {15--24},
 publisher = {ACM},
 series = {PPoPP '10},
 title = {GAMBIT: Effective Unit Testing for Concurrency Libraries},
 year = {2010}
}


