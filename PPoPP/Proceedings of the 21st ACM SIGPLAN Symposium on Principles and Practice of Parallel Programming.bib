@inproceedings{Saad:2016:OTC:2851141.2851191,
 abstract = {In this poster paper, we briefly introduce an effective solution to address the problem of committing transactions enforcing a predefined order. To do that, we overview the design of two algorithms that deploy a cooperative transaction execution that circumvents the transaction isolation constraint in favor of propagating written values among conflicting transactions. A preliminary implementation shows that even in the presence of data conflicts, the proposed algorithms outperform other competitors, significantly.},
 acmid = {2851191},
 address = {New York, NY, USA},
 articleno = {46},
 author = {Saad, Mohamed M. and Palmieri, Roberto and Ravindran, Binoy},
 booktitle = {Proceedings of the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
 doi = {10.1145/2851141.2851191},
 isbn = {978-1-4503-4092-2},
 keyword = {commitment ordering, transactional memory},
 link = {http://doi.acm.org/10.1145/2851141.2851191},
 location = {Barcelona, Spain},
 numpages = {2},
 pages = {46:1--46:2},
 publisher = {ACM},
 series = {PPoPP '16},
 title = {On Ordering Transaction Commit},
 year = {2016}
}


@inproceedings{Yang:2016:WQF:2851141.2851168,
 abstract = {Concurrent data structures that have fast and predictable performance are of critical importance for harnessing the power of multicore processors, which are now ubiquitous. Although wait-free objects, whose operations complete in a bounded number of steps, were devised more than two decades ago, wait-free objects that can deliver scalable high performance are still rare. In this paper, we present the first wait-free FIFO queue based on fetch-and-add (FAA). While compare-and-swap (CAS) based non-blocking algorithms may perform poorly due to work wasted by CAS failures, algorithms that coordinate using FAA, which is guaranteed to succeed, can in principle perform better under high contention. Along with FAA, our queue uses a custom epoch-based scheme to reclaim memory; on x86 architectures, it requires no extra memory fences on our algorithm's typical execution path. An empirical study of our new FAA-based wait-free FIFO queue under high contention on four different architectures with many hardware threads shows that it outperforms prior queue designs that lack a wait-free progress guarantee. Surprisingly, at the highest level of contention, the throughput of our queue is often as high as that of a microbenchmark that only performs FAA. As a result, our fast wait-free queue implementation is useful in practice on most multi-core systems today. We believe that our design can serve as an example of how to construct other fast wait-free objects.},
 acmid = {2851168},
 address = {New York, NY, USA},
 articleno = {16},
 author = {Yang, Chaoran and Mellor-Crummey, John},
 booktitle = {Proceedings of the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
 doi = {10.1145/2851141.2851168},
 isbn = {978-1-4503-4092-2},
 keyword = {fast-path-slow-path, non-blocking queue, wait-free},
 link = {http://doi.acm.org/10.1145/2851141.2851168},
 location = {Barcelona, Spain},
 numpages = {13},
 pages = {16:1--16:13},
 publisher = {ACM},
 series = {PPoPP '16},
 title = {A Wait-free Queue As Fast As Fetch-and-add},
 year = {2016}
}


@inproceedings{Rehman:2016:VMJ:2851141.2851192,
 abstract = {Development of concurrent software requires the programmer to be aware of non-determinism, data races, and deadlocks. MPI (message passing interface) is a popular standard for writing message oriented distributed applications. Some messages in MPI systems can be processed by one of the many machines and in many possible orders. This non-determinism can affect the result of an MPI application. The alternate results may or may not be correct. To verify MPI applications, we need to check all these possible orderings and use an application specific oracle to decide if these orderings give correct output. MPJ Express is an open source Java implementation of the MPI standard. We developed a Java based model of MPJ Express, where processes are modeled as threads, and which can run unmodified MPI Java programs on a single system. This enabled us to adapt the Java PathFinder explicit state software model checker (JPF) using a custom listener to verify our model running real MPI Java programs. We evaluated our approach using small examples where model checking revealed message orders that would result in incorrect system behavior.},
 acmid = {2851192},
 address = {New York, NY, USA},
 articleno = {55},
 author = {Rehman, Waqas Ur and Ayub, Muhammad Sohaib and Siddiqui, Junaid Haroon},
 booktitle = {Proceedings of the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
 doi = {10.1145/2851141.2851192},
 isbn = {978-1-4503-4092-2},
 keyword = {Java PathFinder(JPF), message passing interface in Java (MPJ), model checking},
 link = {http://doi.acm.org/10.1145/2851141.2851192},
 location = {Barcelona, Spain},
 numpages = {2},
 pages = {55:1--55:2},
 publisher = {ACM},
 series = {PPoPP '16},
 title = {Verification of MPI Java Programs Using Software Model Checking},
 year = {2016}
}


@inproceedings{Liu:2016:HCS:2851141.2851174,
 abstract = {GPUs offer the promise of massive, power-efficient parallelism. However, exploiting this parallelism requires code to be carefully structured to deal with the limitations of the SIMT execution model. In recent years, there has been much interest in mapping irregular applications to GPUs: applications with unpredictable, data-dependent behaviors. While most of the work in this space has focused on ad hoc implementations of specific algorithms, recent work has looked at generic techniques for mapping a large class of tree traversal algorithms to GPUs, through careful restructuring of the tree traversal algorithms to make them behave more regularly. Unfortunately, even this general approach for GPU execution of tree traversal algorithms is reliant on ad hoc, handwritten, algorithm-specific scheduling (i.e., assignment of threads to warps) to achieve high performance. The key challenge of scheduling is that it is a highly irregular process, that requires the inspection of thread behavior and then careful sorting of the threads into warps. In this paper, we present a novel scheduling and execution technique for tree traversal algorithms that is both general and automatic. The key novelty is a hybrid approach: the GPU partially executes tasks to inspect thread behavior and transmits information back to the CPU, which uses that information to perform the scheduling itself, before executing the remaining, carefully scheduled, portion of the traversals on the GPU. We applied this framework to five tree traversal algorithms, achieving significant speedups over optimized GPU code that does not perform application-specific scheduling. Further, we show that in many cases, our hybrid approach is able to deliver better performance even than GPU code that uses hand-tuned, application-specific scheduling.},
 acmid = {2851174},
 address = {New York, NY, USA},
 articleno = {41},
 author = {Liu, Jianqiao and Hegde, Nikhil and Kulkarni, Milind},
 booktitle = {Proceedings of the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
 doi = {10.1145/2851141.2851174},
 isbn = {978-1-4503-4092-2},
 keyword = {heterogeneous architectures, irregular applications, scheduling, tree traversal},
 link = {http://doi.acm.org/10.1145/2851141.2851174},
 location = {Barcelona, Spain},
 numpages = {2},
 pages = {41:1--41:2},
 publisher = {ACM},
 series = {PPoPP '16},
 title = {Hybrid CPU-GPU Scheduling and Execution of Tree Traversals},
 year = {2016}
}


@article{Chabbi:2016:CLL:3016078.2851166,
 abstract = {Over the last decade, the growing use of cache-coherent NUMA architectures has spurred the development of numerous locality-preserving mutual exclusion algorithms. NUMA-aware locks such as HCLH, HMCS, and cohort locks exploit locality of reference among nearby threads to deliver high lock throughput under high contention. However, the hierarchical nature of these locality-aware locks increases latency, which reduces the throughput of uncontended or lightly-contended critical sections. To date, no lock design for NUMA systems has delivered both low latency under low contention and high throughput under high contention. In this paper, we describe the design and evaluation of an adaptive mutual exclusion scheme (AHMCS lock), which employs several orthogonal strategies---a hierarchical MCS (HMCS) lock for high throughput under high contention, Lamport's fast path approach for low latency under low contention, an adaptation mechanism that employs hysteresis to balance latency and throughput under moderate contention, and hardware transactional memory for lowest latency in the absence of contention. The result is a top performing lock that has most properties of an ideal mutual exclusion algorithm. AHMCS exploits the strengths of multiple contention management techniques to deliver high performance over a broad range of contention levels. Our empirical evaluations demonstrate the effectiveness of AHMCS over prior art.},
 acmid = {2851166},
 address = {New York, NY, USA},
 articleno = {22},
 author = {Chabbi, Milind and Mellor-Crummey, John},
 doi = {10.1145/3016078.2851166},
 issn = {0362-1340},
 issue_date = {August 2016},
 journal = {SIGPLAN Not.},
 keyword = {NUMA, dynamic locks, hierarchical locks, spin locks},
 link = {http://doi.acm.org/10.1145/3016078.2851166},
 month = {feb},
 number = {8},
 numpages = {14},
 pages = {22:1--22:14},
 publisher = {ACM},
 title = {Contention-conscious, Locality-preserving Locks},
 volume = {51},
 year = {2016}
}


@article{Perrin:2016:CCB:3016078.2851170,
 abstract = {In distributed systems where strong consistency is costly when not impossible, causal consistency provides a valuable abstraction to represent program executions as partial orders. In addition to the sequential program order of each computing entity, causal order also contains the semantic links between the events that affect the shared objects -- messages emission and reception in a communication channel, reads and writes on a shared register. Usual approaches based on semantic links are very difficult to adapt to other data types such as queues or counters because they require a specific analysis of causal dependencies for each data type. This paper presents a new approach to define causal consistency for any abstract data type based on sequential specifications. It explores, formalizes and studies the differences between three variations of causal consistency and highlights them in the light of PRAM, eventual consistency and sequential consistency: weak causal consistency, that captures the notion of causality preservation when focusing on convergence; causal convergence that mixes weak causal consistency and convergence; and causal consistency, that coincides with causal memory when applied to shared memory.},
 acmid = {2851170},
 address = {New York, NY, USA},
 articleno = {26},
 author = {Perrin, Matthieu and Mostefaoui, Achour and Jard, Claude},
 doi = {10.1145/3016078.2851170},
 issn = {0362-1340},
 issue_date = {August 2016},
 journal = {SIGPLAN Not.},
 keyword = {causal consistency, consistency criteria, pipelined consistency, sequential consistency, shared objects, weak causal consistency},
 link = {http://doi.acm.org/10.1145/3016078.2851170},
 month = {feb},
 number = {8},
 numpages = {12},
 pages = {26:1--26:12},
 publisher = {ACM},
 title = {Causal Consistency: Beyond Memory},
 volume = {51},
 year = {2016}
}


@inproceedings{Muddukrishna:2016:GGO:2851141.2851156,
 abstract = {Average programmers struggle to solve performance problems in OpenMP programs with tasks and parallel for-loops. Existing performance analysis tools visualize OpenMP task performance from the runtime system's perspective where task execution is interleaved with other tasks in an unpredictable order. Problems with OpenMP parallel for-loops are similarly difficult to resolve since tools only visualize aggregate thread-level statistics such as load imbalance without zooming into a per-chunk granularity. The runtime system/threads oriented visualization provides poor support for understanding problems with task and chunk execution time, parallelism, and memory hierarchy utilization, forcing average programmers to rely on experts or use tedious trial-and-error tuning methods for performance. We present grain graphs, a new OpenMP performance analysis method that visualizes grains -- computation performed by a task or a parallel for-loop chunk instance -- and highlights problems such as low parallelism, work inflation and poor parallelization benefit at the grain level. We demonstrate that grain graphs can quickly reveal performance problems that are difficult to detect and characterize in fine detail using existing visualizations in standard OpenMP programs, simplifying OpenMP performance analysis. This enables average programmers to make portable optimizations for poor performing OpenMP programs, reducing pressure on experts and removing the need for tedious trial-and-error tuning.},
 acmid = {2851156},
 address = {New York, NY, USA},
 articleno = {28},
 author = {Muddukrishna, Ananya and Jonsson, Peter A. and Podobas, Artur and Brorsson, Mats},
 booktitle = {Proceedings of the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
 doi = {10.1145/2851141.2851156},
 isbn = {978-1-4503-4092-2},
 keyword = {OpenMP, performance analysis, performance visualization, task-based programs},
 link = {http://doi.acm.org/10.1145/2851141.2851156},
 location = {Barcelona, Spain},
 numpages = {13},
 pages = {28:1--28:13},
 publisher = {ACM},
 series = {PPoPP '16},
 title = {Grain Graphs: OpenMP Performance Analysis Made Easy},
 year = {2016}
}


@article{Li:2016:WSI:3016078.2851151,
 abstract = {Interactive web services increasingly drive critical business workloads such as search, advertising, games, shopping, and finance. Whereas optimizing parallel programs and distributed server systems have historically focused on average latency and throughput, the primary metric for interactive applications is instead consistent responsiveness, i.e., minimizing the number of requests that miss a target latency. This paper is the first to show how to generalize work-stealing, which is traditionally used to minimize the makespan of a single parallel job, to optimize for a target latency in interactive services with multiple parallel requests. We design a new adaptive work stealing policy, called tail-control, that reduces the number of requests that miss a target latency. It uses instantaneous request progress, system load, and a target latency to choose when to parallelize requests with stealing, when to admit new requests, and when to limit parallelism of large requests. We implement this approach in the Intel Thread Building Block (TBB) library and evaluate it on real-world workloads and synthetic workloads. The tail-control policy substantially reduces the number of requests exceeding the desired target latency and delivers up to 58% relative improvement over various baseline policies. This generalization of work stealing for multiple requests effectively optimizes the number of requests that complete within a target latency, a key metric for interactive services.},
 acmid = {2851151},
 address = {New York, NY, USA},
 articleno = {14},
 author = {Li, Jing and Agrawal, Kunal and Elnikety, Sameh and He, Yuxiong and Lee, I-Ting Angelina and Lu, Chenyang and McKinley, Kathryn S.},
 doi = {10.1145/3016078.2851151},
 issn = {0362-1340},
 issue_date = {August 2016},
 journal = {SIGPLAN Not.},
 link = {http://doi.acm.org/10.1145/3016078.2851151},
 month = {feb},
 number = {8},
 numpages = {13},
 pages = {14:1--14:13},
 publisher = {ACM},
 title = {Work Stealing for Interactive Services to Meet Target Latency},
 volume = {51},
 year = {2016}
}


@inproceedings{Rabozzi:2016:PPB:2851141.2851187,
 abstract = {Recent developments in Big Data frameworks are moving towards reservation based approaches as a mean to manage the increasingly complex mix of computations, whereas preemption techniques are employed to meet strict jobs deadlines. Within this work we propose and evaluate a new planning algorithm in the context of reservation based scheduling. Our approach is able to achieve high cluster utilization while minimizing the need for preemption that causes system overheads and planning mispredictions.},
 acmid = {2851187},
 address = {New York, NY, USA},
 articleno = {48},
 author = {Rabozzi, Marco and Mazzucchelli, Matteo and Cordone, Roberto and Fumarola, Giovanni Matteo and Santambrogio, Marco D.},
 booktitle = {Proceedings of the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
 doi = {10.1145/2851141.2851187},
 isbn = {978-1-4503-4092-2},
 link = {http://doi.acm.org/10.1145/2851141.2851187},
 location = {Barcelona, Spain},
 numpages = {2},
 pages = {48:1--48:2},
 publisher = {ACM},
 series = {PPoPP '16},
 title = {Preemption-aware Planning on Big-data Systems},
 year = {2016}
}


@article{Haider:2016:LAS:3016078.2851155,
 abstract = {High memory contention is generally agreed to be a worst-case scenario for concurrent data structures. There has been a significant amount of research effort spent investigating designs which minimize contention, and several programming techniques have been proposed to mitigate its effects. However, there are currently few architectural mechanisms to allow scaling contended data structures at high thread counts. In this paper, we investigate hardware support for scalable contended data structures. We propose Lease/Release, a simple addition to standard directory-based MSI cache coherence protocols, allowing participants to lease memory, at the granularity of cache lines, by delaying coherence messages for a short, bounded period of time. Our analysis shows that Lease/Release can significantly reduce the overheads of contention for both non-blocking (lock-free) and lock-based data structure implementations, while ensuring that no deadlocks are introduced. We validate Lease/Release empirically on the Graphite multiprocessor simulator, on a range of data structures, including queue, stack, and priority queue implementations, as well as on transactional applications. Results show that Lease/Release consistently improves both throughput and energy usage, by up to 5x, both for lock-free and lock-based data structure designs.},
 acmid = {2851155},
 address = {New York, NY, USA},
 articleno = {17},
 author = {Haider, Syed Kamran and Hasenplaugh, William and Alistarh, Dan},
 doi = {10.1145/3016078.2851155},
 issn = {0362-1340},
 issue_date = {August 2016},
 journal = {SIGPLAN Not.},
 link = {http://doi.acm.org/10.1145/3016078.2851155},
 month = {feb},
 number = {8},
 numpages = {12},
 pages = {17:1--17:12},
 publisher = {ACM},
 title = {Lease/Release: Architectural Support for Scaling Contended Data Structures},
 volume = {51},
 year = {2016}
}


@article{Tallada:2016:CGP:3016078.2851158,
 abstract = {Deep neural networks (DNN) have recently achieved extraordinary results in domains like computer vision and speech recognition. An essential element for this success has been the introduction of high performance computing (HPC) techniques in the critical step of training the neural network. This paper describes the implementation and analysis of a network-agnostic and convergence-invariant coarse-grain parallelization of the DNN training algorithm. The coarse-grain parallelization is achieved through the exploitation of the batch-level parallelism. This strategy is independent from the support of specialized and optimized libraries. Therefore, the optimization is immediately available for accelerating the DNN training. The proposal is compatible with multi-GPU execution without altering the algorithm convergence rate. The parallelization has been implemented in Caffe, a state-of-the-art DNN framework. The paper describes the code transformations for the parallelization and we also identify the limiting performance factors of the approach. We show competitive performance results for two state-of-the-art computer vision datasets, MNIST and CIFAR-10. In particular, on a 16-core Xeon E5-2667v2 at 3.30GHz we observe speedups of 8× over the sequential execution, at similar performance levels of those obtained by the GPU optimized Caffe version in a NVIDIA K40 GPU.},
 acmid = {2851158},
 address = {New York, NY, USA},
 articleno = {1},
 author = {Tallada, Marc Gonzalez},
 doi = {10.1145/3016078.2851158},
 issn = {0362-1340},
 issue_date = {August 2016},
 journal = {SIGPLAN Not.},
 keyword = {OpenMP, coarse-grain parallelism, deep learning, neural networks, shared memory algorithms, stochastic gradient descent},
 link = {http://doi.acm.org/10.1145/3016078.2851158},
 month = {feb},
 number = {8},
 numpages = {12},
 pages = {1:1--1:12},
 publisher = {ACM},
 title = {Coarse Grain Parallelization of Deep Neural Networks},
 volume = {51},
 year = {2016}
}


@article{Salucci:2016:GMC:3016078.2851184,
 abstract = {Systems based on event-loops have been popularized by Node.JS, and are becoming a key technology in the domain of cloud computing. Despite their popularity, such systems support only share-nothing parallelism via message passing between parallel entities usually called workers. In this paper, we introduce a novel parallel programming abstraction called Generic Messages (GEMs), which enables shared-memory parallelism for share-nothing event-based systems. A key characteristic of GEMs is that they enable workers to share state by specifying how the state can be accessed once it is shared. We call this aspect of the GEMs model capability-based parallelism.},
 acmid = {2851184},
 address = {New York, NY, USA},
 articleno = {40},
 author = {Salucci, Luca and Bonetta, Daniele and Marr, Stefan and Binder, Walter},
 doi = {10.1145/3016078.2851184},
 issn = {0362-1340},
 issue_date = {August 2016},
 journal = {SIGPLAN Not.},
 keyword = {Node.JS, event-loop systems, generic messages, shared memory},
 link = {http://doi.acm.org/10.1145/3016078.2851184},
 month = {feb},
 number = {8},
 numpages = {2},
 pages = {40:1--40:2},
 publisher = {ACM},
 title = {Generic Messages: Capability-based Shared Memory Parallelism for Event-loop Systems},
 volume = {51},
 year = {2016}
}


@article{Drebes:2016:NSM:3016078.2851193,
 abstract = {Dynamic task parallelism is a popular programming model on shared-memory systems. Compared to data parallel loop-based concurrency, it promises enhanced scalability, load balancing and locality. These promises, however, are undermined by non-uniform memory access (NUMA) systems. We show that it is possible to preserve the uniform hardware abstraction of contemporary task-parallel programming models, for both computing and memory resources, while achieving near-optimal data locality. Our run-time algorithms for NUMA-aware task and data placement are fully automatic, application-independent, performance-portable across NUMA machines, and adapt to dynamic changes. Placement decisions use information about inter-task data dependences and reuse. This information is readily available in the run-time systems of modern task-parallel programming frameworks, and from the operating system regarding the placement of previously allocated memory. Our algorithms take advantage of data-flow style task parallelism, where the privatization of task data enhances scalability through the elimination of false dependences and enables fine-grained dynamic control over the placement of application data. We demonstrate that the benefits of dynamically managing data placement outweigh the privatization cost, even when comparing with target-specific optimizations through static, NUMA-aware data interleaving. Our implementation and the experimental evaluation on a set of high-performance benchmarks executing on a 192-core system with 24 NUMA nodes show that the fraction of local memory accesses can be increased to more than 99%, resulting in a speedup of up to 5× compared to a NUMA-aware hierarchical work-stealing baseline.},
 acmid = {2851193},
 address = {New York, NY, USA},
 articleno = {44},
 author = {Drebes, Andi and Pop, Antoniu and Heydemann, Karine and Drach, Nathalie and Cohen, Albert},
 doi = {10.1145/3016078.2851193},
 issn = {0362-1340},
 issue_date = {August 2016},
 journal = {SIGPLAN Not.},
 link = {http://doi.acm.org/10.1145/3016078.2851193},
 month = {feb},
 number = {8},
 numpages = {2},
 pages = {44:1--44:2},
 publisher = {ACM},
 title = {NUMA-aware Scheduling and Memory Allocation for Data-flow Task-parallel Applications},
 volume = {51},
 year = {2016}
}


@inproceedings{Guerraoui:2016:OCO:2851141.2851146,
 abstract = {We introduce OPTIK, a new practical design pattern for designing and implementing fast and scalable concurrent data structures. OPTIK relies on the commonly-used technique of version numbers for detecting conflicting concurrent operations. We show how to implement the OPTIK pattern using the novel concept of OPTIK locks. These locks enable the use of version numbers for implementing very efficient optimistic concurrent data structures. Existing state-of-the-art lock-based data structures acquire the lock and then check for conflicts. In contrast, with OPTIK locks, we merge the lock acquisition with the detection of conflicting concurrency in a single atomic step, similarly to lock-free algorithms. We illustrate the power of our OPTIK pattern and its implementation by introducing four new algorithms and by optimizing four state-of-the-art algorithms for linked lists, skip lists, hash tables, and queues. Our results show that concurrent data structures built using OPTIK are more scalable than the state of the art.},
 acmid = {2851146},
 address = {New York, NY, USA},
 articleno = {18},
 author = {Guerraoui, Rachid and Trigonakis, Vasileios},
 booktitle = {Proceedings of the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
 doi = {10.1145/2851141.2851146},
 isbn = {978-1-4503-4092-2},
 link = {http://doi.acm.org/10.1145/2851141.2851146},
 location = {Barcelona, Spain},
 numpages = {12},
 pages = {18:1--18:12},
 publisher = {ACM},
 series = {PPoPP '16},
 title = {Optimistic Concurrency with OPTIK},
 year = {2016}
}


@article{Hegde:2016:SRS:3016078.2851177,
 abstract = {Repeated, depth-first traversal of trees is a common algorithmic pattern in an important set of applications from diverse domains such as cosmological simulations, data mining, and computer graphics. As these applications operate over massive data sets, it is often necessary to distribute the trees to process all of the data. In this work, we introduce SPIRIT, a runtime system to ease the writing of distributed tree applications. SPIRIT automates the challenging tasks of tree distribution, optimizing communication and parallelizing independent computations. The common algorithmic pattern in tree traversals is exploited to effectively schedule parallel computations and improve locality. As a result, pipeline parallelism in distributed traversals is identified, which is complemented by load-balancing, and locality-enhancing, message aggregation optimizations. Evaluation of SPIRIT on tree traversal in Point Correlation (PC) shows a scalable system, achieving speedups upto 38x on a 16-node, 64 process system compared to a 1-node, baseline configuration. We also find that SPIRIT results in substantially less communication and achieves significant performance improvements over implementations in other distributed graph systems.},
 acmid = {2851177},
 address = {New York, NY, USA},
 articleno = {51},
 author = {Hegde, Nikhil and Liu, Jianqiao and Kulkarni, Milind},
 doi = {10.1145/3016078.2851177},
 issn = {0362-1340},
 issue_date = {August 2016},
 journal = {SIGPLAN Not.},
 keyword = {distributed computing, irregular programs},
 link = {http://doi.acm.org/10.1145/3016078.2851177},
 month = {feb},
 number = {8},
 numpages = {2},
 pages = {51:1--51:2},
 publisher = {ACM},
 title = {SPIRIT: A Runtime System for Distributed Irregular Tree Applications},
 volume = {51},
 year = {2016}
}


@article{Mastoras:2016:UFC:3016078.2851172,
 abstract = {Some loops with cross-iteration dependences can execute in parallel by pipelining. The loop body is partitioned into stages such that the data dependences are not violated and then the stages are mapped onto threads. Two well-known mapping techniques are fixed code and fixed data; they achieve high performance for load-balanced loops, but they fail to perform well for load-imbalanced loops. In this article, we present a novel hybrid mapping that eliminates drawbacks of both prior mapping techniques and enables dynamic scheduling of stages.},
 acmid = {2851172},
 address = {New York, NY, USA},
 articleno = {53},
 author = {Mastoras, Aristeidis and Gross, Thomas R.},
 doi = {10.1145/3016078.2851172},
 issn = {0362-1340},
 issue_date = {August 2016},
 journal = {SIGPLAN Not.},
 keyword = {dynamic scheduling, mapping, multi-threading, pipeline model},
 link = {http://doi.acm.org/10.1145/3016078.2851172},
 month = {feb},
 number = {8},
 numpages = {2},
 pages = {53:1--53:2},
 publisher = {ACM},
 title = {Unifying Fixed Code and Fixed Data Mapping of Load-imbalanced Pipelined Loops},
 volume = {51},
 year = {2016}
}


@inproceedings{Chang:2016:PSF:2851141.2851178,
 abstract = {We present Tangram, a programming system for writing performance-portable programs. The language enables programmers to write computation and composition codelets, supported by tuning knobs and primitives for expressing data parallelism and work decomposition. The compiler and runtime use a set of techniques such as hierarchical composition, coarsening, data placement, tuning, and runtime selection based on input characteristics and micro-profiling. The resulting performance is competitive with optimized vendor libraries.},
 acmid = {2851178},
 address = {New York, NY, USA},
 articleno = {32},
 author = {Chang, Li-Wen and El Hajj, Izzat and Kim, Hee-Seok and G\'{o}mez-Luna, Juan and Dakkak, Abdul and Hwu, Wen-mei},
 booktitle = {Proceedings of the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
 doi = {10.1145/2851141.2851178},
 isbn = {978-1-4503-4092-2},
 link = {http://doi.acm.org/10.1145/2851141.2851178},
 location = {Barcelona, Spain},
 numpages = {2},
 pages = {32:1--32:2},
 publisher = {ACM},
 series = {PPoPP '16},
 title = {A Programming System for Future Proofing Performance Critical Libraries},
 year = {2016}
}


@article{Steele:2016:AAC:3016078.2851147,
 abstract = {We describe a general framework for adding the values of two approximate counters to produce a new approximate counter value whose expected estimated value is equal to the sum of the expected estimated values of the given approximate counters. (To the best of our knowledge, this is the first published description of any algorithm for adding two approximate counters.) We then work out implementation details for five different kinds of approximate counter and provide optimized pseudocode. For three of them, we present proofs that the variance of a counter value produced by adding two counter values in this way is bounded, and in fact is no worse, or not much worse, than the variance of the value of a single counter to which the same total number of increment operations have been applied. Addition of approximate counters is useful in massively parallel divide-and-conquer algorithms that use a distributed representation for large arrays of counters. We describe two machine-learning algorithms for topic modeling that use millions of integer counters, and confirm that replacing the integer counters with approximate counters is effective, speeding up a GPU-based implementation by over 65% and a CPU-based by nearly 50%, as well as reducing memory requirements, without degrading their statistical effectiveness.},
 acmid = {2851147},
 address = {New York, NY, USA},
 articleno = {15},
 author = {Steele,Jr., Guy L. and Tristan, Jean-Baptiste},
 doi = {10.1145/3016078.2851147},
 issn = {0362-1340},
 issue_date = {August 2016},
 journal = {SIGPLAN Not.},
 keyword = {approximate counters, distributed computing, divide and conquer, multithreading, parallel computing, statistical counters},
 link = {http://doi.acm.org/10.1145/3016078.2851147},
 month = {feb},
 number = {8},
 numpages = {12},
 pages = {15:1--15:12},
 publisher = {ACM},
 title = {Adding Approximate Counters},
 volume = {51},
 year = {2016}
}


@article{Maleki:2016:DSD:3016078.2851183,
 abstract = {The Single-Source Shortest Path (SSSP) problem is to find the shortest paths from a source vertex to all other vertices in a graph. In this paper, we introduce the Dijkstra Strip-Mined Relaxation (DSMR) algorithm, an efficient parallel SSSP algorithm for shared and distributed memory systems. Our results show that, DSMR is faster than parallel Δ-Stepping by a factor of up-to 1.66.},
 acmid = {2851183},
 address = {New York, NY, USA},
 articleno = {39},
 author = {Maleki, Saeed and Nguyen, Donald and Lenharth, Andrew and Garzar\'{a}n, Mar\'{\i}a and Padua, David and Pingali, Keshav},
 doi = {10.1145/3016078.2851183},
 issn = {0362-1340},
 issue_date = {August 2016},
 journal = {SIGPLAN Not.},
 link = {http://doi.acm.org/10.1145/3016078.2851183},
 month = {feb},
 number = {8},
 numpages = {2},
 pages = {39:1--39:2},
 publisher = {ACM},
 title = {DSMR: A Shared and Distributed Memory Algorithm for Single-source Shortest Path Problem},
 volume = {51},
 year = {2016}
}


@article{Wang:2016:GHG:3016078.2851145,
 abstract = {For large-scale graph analytics on the GPU, the irregularity of data access/control flow and the complexity of programming GPUs have been two significant challenges for developing a programmable high-performance graph library. "Gunrock," our high-level bulk-synchronous graph-processing system targeting the GPU, takes a new approach to abstracting GPU graph analytics: rather than designing an abstraction around computation, Gunrock instead implements a novel data-centric abstraction centered on operations on a vertex or edge frontier. Gunrock achieves a balance between performance and expressiveness by coupling high-performance GPU computing primitives and optimization strategies with a high-level programming model that allows programmers to quickly develop new graph primitives with small code size and minimal GPU programming knowledge. We evaluate Gunrock on five graph primitives (BFS, BC, SSSP, CC, and PageRank) and show that Gunrock has on average at least an order of magnitude speedup over Boost and PowerGraph, comparable performance to the fastest GPU hardwired primitives, and better performance than any other GPU high-level graph library.},
 acmid = {2851145},
 address = {New York, NY, USA},
 articleno = {11},
 author = {Wang, Yangzihao and Davidson, Andrew and Pan, Yuechao and Wu, Yuduo and Riffel, Andy and Owens, John D.},
 doi = {10.1145/3016078.2851145},
 issn = {0362-1340},
 issue_date = {August 2016},
 journal = {SIGPLAN Not.},
 link = {http://doi.acm.org/10.1145/3016078.2851145},
 month = {feb},
 number = {8},
 numpages = {12},
 pages = {11:1--11:12},
 publisher = {ACM},
 title = {Gunrock: A High-performance Graph Processing Library on the GPU},
 volume = {51},
 year = {2016}
}


@inproceedings{Qian:2016:ODG:2851141.2851179,
 abstract = {The ability to reproduce a parallel execution is desirable for debugging and program reliability purposes. In debugging (13), the programmer needs to manually step back in time, while for resilience (6) this is automatically performed by the the application upon failure. To be useful, replay has to faithfully reproduce the original execution. For parallel programs the main challenge is inferring and maintaining the order of conflicting operations (data races). Deterministic record and replay (R&R) techniques have been developed for multithreaded shared memory programs (5), as well as distributed memory programs (14). Our main interest is techniques for large scale scientific (3; 4) programming models.},
 acmid = {2851179},
 address = {New York, NY, USA},
 articleno = {47},
 author = {Qian, Xuehai and Sen, Koushik and Hargrove, Paul and Iancu, Costin},
 booktitle = {Proceedings of the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
 doi = {10.1145/2851141.2851179},
 isbn = {978-1-4503-4092-2},
 link = {http://doi.acm.org/10.1145/2851141.2851179},
 location = {Barcelona, Spain},
 numpages = {2},
 pages = {47:1--47:2},
 publisher = {ACM},
 series = {PPoPP '16},
 title = {OPR: Deterministic Group Replay for One-sided Communication},
 year = {2016}
}


@article{Wang:2016:HPM:3016078.2851163,
 abstract = {Computed Tomography (CT) Image Reconstruction is an important technique used in a wide range of applications, ranging from explosive detection, medical imaging to scientific imaging. Among available reconstruction methods, Model Based Iterative Reconstruction (MBIR) produces higher quality images and allows for the use of more general CT scanner geometries than is possible with more commonly used methods. The high computational cost of MBIR, however, often makes it impractical in applications for which it would otherwise be ideal. This paper describes a new MBIR implementation that significantly reduces the computational cost of MBIR while retaining its benefits. It describes a novel organization of the scanner data into super-voxels (SV) that, combined with a super-voxel buffer (SVB), dramatically increase locality and prefetching, enable parallelism across SVs and lead to an average speedup of 187 on 20 cores.},
 acmid = {2851163},
 address = {New York, NY, USA},
 articleno = {2},
 author = {Wang, Xiao and Sabne, Amit and Kisner, Sherman and Raghunathan, Anand and Bouman, Charles and Midkiff, Samuel},
 doi = {10.1145/3016078.2851163},
 issn = {0362-1340},
 issue_date = {August 2016},
 journal = {SIGPLAN Not.},
 keyword = {CT image reconstruction, MBIR, applications, multicore, parallel algorithm},
 link = {http://doi.acm.org/10.1145/3016078.2851163},
 month = {feb},
 number = {8},
 numpages = {12},
 pages = {2:1--2:12},
 publisher = {ACM},
 title = {High Performance Model Based Image Reconstruction},
 volume = {51},
 year = {2016}
}


@inproceedings{Chen:2016:SPN:2851141.2851185,
 abstract = {Many time-dependent problems like molecular dynamics of protein folding require a large number of time steps. The latencies and overheads of common-purpose clusters with accelerators are too big for high-frequency iteration. We introduce an algorithmic model called Samsara Parallel (or SP) which, unlike BSP, relies on asynchronous communications and can repeatedly return to earlier time steps to refine the precision of computation. This also extends a line of research called Parallel-in-Time in computational chemistry and physics.},
 acmid = {2851185},
 address = {New York, NY, USA},
 articleno = {49},
 author = {Chen, Yifeng and Huang, Kun and Wang, Bei and Li, Guohui and Cui, Xiang},
 booktitle = {Proceedings of the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
 doi = {10.1145/2851141.2851185},
 isbn = {978-1-4503-4092-2},
 keyword = {BSP, molecular dynamics, parallel-in-time},
 link = {http://doi.acm.org/10.1145/2851141.2851185},
 location = {Barcelona, Spain},
 numpages = {2},
 pages = {49:1--49:2},
 publisher = {ACM},
 series = {PPoPP '16},
 title = {Samsara Parallel: A non-BSP Parallel-in-time Model},
 year = {2016}
}


@proceedings{Sarkar:2017:3018743,
 abstract = {It is our great pleasure to welcome you to PPoPP 2017, the 22nd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming held in Austin, Texas during February 4-8, 2017, and co-located with the CGO 2017 and HPCA 2017 conferences. This year's symposium continues and reinforces the PPoPP tradition of publishing leading work on all aspects of parallel programming, including foundational and theoretical aspects, techniques, languages, compilers, runtime systems, tools, and practical experiences. Given the pervasiveness of parallel architectures in the general consumer market, PPoPP, with its interest in new parallel workloads, techniques and productivity tools for parallel programming, is becoming more relevant than ever to the computer science community. PPoPP 2017 received 132 submissions from countries all over the world. The submitted papers underwent a rigorous two-phase review process. To maintain fairness and uniform standards, the review process was double-blind, throughout. Almost all of the 132 submissions were reviewed in the first phase by four members of the combined PC and ERC. (A very small fraction received three reviews in the first phase.) All papers were assigned a discussion lead from the PC. After the first rebuttal phase and ongoing online discussions, reviewers reached a consensus to relegate half of the submissions. Their authors were subsequently notified and given the choice of withdrawing their papers. The submissions for which there was no clear consensus or that had only three reviews (very few) were retained for the second evaluation stage. In the second phase, the remaining papers received at least two additional reviews exclusively from PC members and some external specialists. After a second rebuttal period, PC and ERC members continued their online discussions and grouped papers in top, bottom and discuss categories. Finally, 35 PC members met in person over two half days from noon, November 5th through the afternoon of November 6th at the Department of Computer Science in Rice University, Houston, TX, and concluded the meeting by accepting (or conditionally accepting) a total of 29 papers. The 14 conditionally accepted papers were shepherded by volunteer PC members, and the final version was made available to all original reviewers for their approval. All in all, this process resulted in a manageable average load of 12 papers for PC members, 6 papers for ERC members, offered all authors the possibility to respond to all reviews, and helped ensure that reviewing efforts were focused on where they were needed the most. Because many quality papers could not be accommodated as regular contributions, all papers retained for the second review phase were invited to be presented as posters at the conference. As a result, 17 posters were included in the proceedings as 2 page abstracts. The posters were presented during a special two-hour late afternoon session. All authors of accepted papers were given the option of participating in a joint CGO-PPoPP Artifact Evaluation (AE) process. The AE process is intended to encourage researchers to conduct experiments in a reproducible way, to package experimental work-flows and all related materials for broad availability, and ultimately, to enable fair comparison of published techniques. This year saw a considerable increase in the amount of submitted artifacts: 27 versus 18 two years ago, almost equally split between CGO and PPoPP. The Artifact Evaluation Committee of 41 researchers and engineers spent two weeks validating and evaluating the artifacts. Each submission received at least three reviews and only eight of them fell below acceptance criteria. To help educate authors about result reproducibility, these papers were shepherded during a week's time by the AE Committee. Concurrently, the AE committee successfully tried an "open reviewing model", i.e., asked the community to publicly evaluate several artifacts already available at Github, Gila and other project hosting services. This enabled us to find additional external reviewers with access to HPC servers or proprietary benchmarks and tools. At the end of this process all submissions qualified to receive the AE seal and their authors were encouraged to submit a two page Artifact Appendix to document the process. The success of a major conference like PPoPP very much depends on the hard work of all members of the organizing committee who volunteer their time in this service. We thank all PC and ERC members for their thoughtful reviews and extensive online discussions, with special thanks to the PC members who came from all over the world to the meeting in Houston and deliberated for two half days and read additional papers overnight to produce the PPoPP 2017 program. Several of the PC members volunteered to shepherd papers, and deserve special thanks for their efforts. The AE process was lengthy and demanding, and the AE Chairs, Wonsun Ahn (for PPoPP) and Joe Devietti (for CGO) and their team did an amazing job on that Herculean task.},
 address = {New York, NY, USA},
 isbn = {978-1-4503-4493-7},
 location = {Austin, Texas, USA},
 publisher = {ACM},
 title = {PPoPP '17: Proceedings of the 22Nd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
 year = {2017}
}


@inproceedings{Ramachandran:2016:IEI:2851141.2851173,
 abstract = {Binary Search Tree (BST) is an important data structure for managing ordered data. Many algorithms---blocking as well as non-blocking---have been proposed for concurrent manipulation of a binary search tree in an asynchronous shared memory system that supports search, insert and delete operations based on both external and internal representations of a search tree. An important step in executing an operation on a tree is to traverse the tree from top-to-down in order to locate the operation's window. A process may need to perform this traversal several times to handle any failures occurring due to other processes performing conflicting actions on the tree. Most concurrent algorithms that have proposed so far use a naïve approach and simply restart the traversal from the root of the tree. In this work, we present a new approach to recover from such failures more efficiently in a concurrent binary search tree based on internal representation using local recovery by restarting the traversal from the "middle" of the tree in order to locate an operation's window. Our approach is sufficiently general in the sense that it can be applied to a variety of concurrent binary search trees based on both blocking and non-blocking approaches. Using experimental evaluation, we demonstrate that our local recovery approach can yield significant speed-ups of up to 69% for many concurrent algorithms.},
 acmid = {2851173},
 address = {New York, NY, USA},
 articleno = {42},
 author = {Ramachandran, Arunmoezhi and Mittal, Neeraj},
 booktitle = {Proceedings of the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
 doi = {10.1145/2851141.2851173},
 isbn = {978-1-4503-4092-2},
 keyword = {binary search tree, concurrent data structure, internal representation, local recovery},
 link = {http://doi.acm.org/10.1145/2851141.2851173},
 location = {Barcelona, Spain},
 numpages = {2},
 pages = {42:1--42:2},
 publisher = {ACM},
 series = {PPoPP '16},
 title = {Improving Efficacy of Internal Binary Search Trees Using Local Recovery},
 year = {2016}
}


@article{Cao:2016:DBG:3016078.2851143,
 abstract = {It is notoriously challenging to develop parallel software systems that are both scalable and correct. Runtime support for parallelism---such as multithreaded record & replay, data race detectors, transactional memory, and enforcement of stronger memory models---helps achieve these goals, but existing commodity solutions slow programs substantially in order to track (i.e., detect or control) an execution's cross-thread dependences accurately. Prior work tracks cross-thread dependences either "pessimistically," slowing every program access, or "optimistically," allowing for lightweight instrumentation of most accesses but dramatically slowing accesses involved in cross-thread dependences. This paper seeks to hybridize pessimistic and optimistic tracking, which is challenging because there exists a fundamental mismatch between pessimistic and optimistic tracking. We address this challenge based on insights about how dependence tracking and program synchronization interact, and introduce a novel approach called hybrid tracking. Hybrid tracking is suitable for building efficient runtime support, which we demonstrate by building hybrid-tracking-based versions of a dependence recorder and a region serializability enforcer. An adaptive, profile-based policy makes runtime decisions about switching between pessimistic and optimistic tracking. Our evaluation shows that hybrid tracking enables runtime support to overcome the performance limitations of both pessimistic and optimistic tracking alone.},
 acmid = {2851143},
 address = {New York, NY, USA},
 articleno = {20},
 author = {Cao, Man and Zhang, Minjia and Sengupta, Aritra and Bond, Michael D.},
 doi = {10.1145/3016078.2851143},
 issn = {0362-1340},
 issue_date = {August 2016},
 journal = {SIGPLAN Not.},
 link = {http://doi.acm.org/10.1145/3016078.2851143},
 month = {feb},
 number = {8},
 numpages = {13},
 pages = {20:1--20:13},
 publisher = {ACM},
 title = {Drinking from Both Glasses: Combining Pessimistic and Optimistic Tracking of Cross-thread Dependences},
 volume = {51},
 year = {2016}
}


@inproceedings{Nielsen:2016:SLH:2851141.2851196,
 abstract = {Concurrent data structures synchronized with locks do not scale well with the number of threads. As more scalable alternatives, concurrent data structures and algorithms based on widely available, however advanced, atomic operations have been proposed. These data structures allow for correct and concurrent operations without any locks. In this paper, we present a new fully lock-free open addressed hash table with a simpler design than prior published work. We split hash table insertions into two atomic phases: first inserting a value ignoring other concurrent operations, then in the second phase resolve any duplicate or conflicting values. Our hash table has a constant and low memory usage that is less than existing lock-free hash tables at a fill level of 33% and above. The hash table exhibits good cache locality. Compared to prior art, our hash table results in 16% and 15% fewer L1 and L2 cache misses respectively, leading to 21% fewer memory stall cycles. Our experiments show that our hash table scales close to linearly with the number of threads and outperforms, in throughput, other lock-free hash tables by 19%.},
 acmid = {2851196},
 address = {New York, NY, USA},
 articleno = {33},
 author = {Nielsen, Jesper Puge and Karlsson, Sven},
 booktitle = {Proceedings of the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
 doi = {10.1145/2851141.2851196},
 isbn = {978-1-4503-4092-2},
 link = {http://doi.acm.org/10.1145/2851141.2851196},
 location = {Barcelona, Spain},
 numpages = {2},
 pages = {33:1--33:2},
 publisher = {ACM},
 series = {PPoPP '16},
 title = {A Scalable Lock-free Hash Table with Open Addressing},
 year = {2016}
}


@article{Parikh:2016:EDW:3016078.2851175,
 abstract = {Many classes of high-performance applications and combinatorial problems exhibit large degree of runtime load variability. One approach to achieving balanced resource use is to over decompose the problem on fine-grained tasks that are then dynamically balanced using approaches such as workstealing. Existing work stealing techniques for such irregular applications, running on large clusters, exhibit high overheads due to potential untimely interruption of busy nodes, excessive communication messages and delays experienced by idle nodes in finding work due to repeated failed steals. We contend that the fundamental problem of distributed work-stealing is of rapidly bringing together work producers and consumers. In response, we develop an algorithm that performs timely, lightweight and highly efficient matchmaking between work producers and consumers which results in accurate load balance. Experimental evaluations show that our scheduler is able to outperform other distributed work stealing schedulers, and to achieve scale beyond what is possible with current approaches.},
 acmid = {2851175},
 address = {New York, NY, USA},
 articleno = {37},
 author = {Parikh, Hrushit and Deodhar, Vinit and Gavrilovska, Ada and Pande, Santosh},
 doi = {10.1145/3016078.2851175},
 issn = {0362-1340},
 issue_date = {August 2016},
 journal = {SIGPLAN Not.},
 keyword = {irregular applications, scheduling, work-stealing},
 link = {http://doi.acm.org/10.1145/3016078.2851175},
 month = {feb},
 number = {8},
 numpages = {2},
 pages = {37:1--37:2},
 publisher = {ACM},
 title = {Efficient Distributed Workstealing via Matchmaking},
 volume = {51},
 year = {2016}
}


@article{DeMatteis:2016:KCR:3016078.2851148,
 abstract = {This paper addresses the problem of designing scaling strategies for elastic data stream processing. Elasticity allows applications to rapidly change their configuration on-the-fly (e.g., the amount of used resources) in response to dynamic workload fluctuations. In this work we face this problem by adopting the Model Predictive Control technique, a control-theoretic method aimed at finding the optimal application configuration along a limited prediction horizon in the future by solving an online optimization problem. Our control strategies are designed to address latency constraints, using Queueing Theory models, and energy consumption by changing the number of used cores and the CPU frequency through the Dynamic Voltage and Frequency Scaling (DVFS) support available in the modern multicore CPUs. The proactive capabilities, in addition to the latency- and energy-awareness, represent the novel features of our approach. To validate our methodology, we develop a thorough set of experiments on a high-frequency trading application. The results demonstrate the high-degree of flexibility and configurability of our approach, and show the effectiveness of our elastic scaling strategies compared with existing state-of-the-art techniques used in similar scenarios.},
 acmid = {2851148},
 address = {New York, NY, USA},
 articleno = {13},
 author = {De Matteis, Tiziano and Mencagli, Gabriele},
 doi = {10.1145/3016078.2851148},
 issn = {0362-1340},
 issue_date = {August 2016},
 journal = {SIGPLAN Not.},
 keyword = {DVFS, data stream processing, elasticity, model predictive control, multicore programming},
 link = {http://doi.acm.org/10.1145/3016078.2851148},
 month = {feb},
 number = {8},
 numpages = {12},
 pages = {13:1--13:12},
 publisher = {ACM},
 title = {Keep Calm and React with Foresight: Strategies for Low-latency and Energy-efficient Elastic Data Stream Processing},
 volume = {51},
 year = {2016}
}


@inproceedings{Kannan:2016:HPA:2851141.2851152,
 abstract = {Non-negative matrix factorization (NMF) is the problem of determining two non-negative low rank factors W and H, for the given input matrix A, such that A &ap; WH. NMF is a useful tool for many applications in different domains such as topic modeling in text mining, background separation in video analysis, and community detection in social networks. Despite its popularity in the data mining community, there is a lack of efficient distributed algorithms to solve the problem for big data sets. We propose a high-performance distributed-memory parallel algorithm that computes the factorization by iteratively solving alternating non-negative least squares (NLS) subproblems for W and H. It maintains the data and factor matrices in memory (distributed across processors), uses MPI for interprocessor communication, and, in the dense case, provably minimizes communication costs (under mild assumptions). As opposed to previous implementations, our algorithm is also flexible: (1) it performs well for both dense and sparse matrices, and (2) it allows the user to choose any one of the multiple algorithms for solving the updates to low rank factors W and H within the alternating iterations. We demonstrate the scalability of our algorithm and compare it with baseline implementations, showing significant performance improvements.},
 acmid = {2851152},
 address = {New York, NY, USA},
 articleno = {9},
 author = {Kannan, Ramakrishnan and Ballard, Grey and Park, Haesun},
 booktitle = {Proceedings of the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
 doi = {10.1145/2851141.2851152},
 isbn = {978-1-4503-4092-2},
 link = {http://doi.acm.org/10.1145/2851141.2851152},
 location = {Barcelona, Spain},
 numpages = {11},
 pages = {9:1--9:11},
 publisher = {ACM},
 series = {PPoPP '16},
 title = {A High-performance Parallel Algorithm for Nonnegative Matrix Factorization},
 year = {2016}
}


@inproceedings{Dice:2016:RTL:2851141.2851162,
 abstract = {Transactional lock elision (TLE) is a well-known technique that exploits hardware transactional memory (HTM) to introduce concurrency into lock-based software. It achieves that by attempting to execute a critical section protected by a lock in an atomic hardware transaction, reverting to the lock if these attempts fail. One significant drawback of TLE is that it disables hardware speculation once there is a thread running under lock. In this paper we present two algorithms that rely on existing compiler support for transactional programs and allow threads to speculate concurrently on HTM along with a thread holding the lock. We demonstrate the benefit of our algorithms over TLE and other related approaches with an in-depth analysis of a number of benchmarks and a wide range of workloads, including an AVL tree-based micro-benchmark and ccTSA, a real sequence assembler application.},
 acmid = {2851162},
 address = {New York, NY, USA},
 articleno = {19},
 author = {Dice, Dave and Kogan, Alex and Lev, Yossi},
 booktitle = {Proceedings of the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
 doi = {10.1145/2851141.2851162},
 isbn = {978-1-4503-4092-2},
 keyword = {concurrency, hardware transactional memory, transactional lock elision},
 link = {http://doi.acm.org/10.1145/2851141.2851162},
 location = {Barcelona, Spain},
 numpages = {12},
 pages = {19:1--19:12},
 publisher = {ACM},
 series = {PPoPP '16},
 title = {Refined Transactional Lock Elision},
 year = {2016}
}


@article{Ritson:2016:BWM:3016078.2851150,
 abstract = {To achieve good multi-core performance, modern microprocessors have weak memory models, rather than enforce sequential consistency. This gives the programmer a wide scope for choosing exactly how to implement various aspects of inter-thread communication through the system's shared memory. However, these choices come with both semantic and performance consequences, often in tension with each other. In this paper, we focus on the performance side, and define techniques for evaluating the impact of various choices in using weak memory models, such as where to put fences, and which fences to use. We make no attempt to judge certain strategies as best or most efficient, and instead provide the techniques that will allow the programmer to understand the performance implications when identifying and resolving any semantic/performance trade-offs. In particular, our technique supports the reasoned selection of macrobenchmarks to use in investigating trade-offs in using weak memory models. We demonstrate our technique on both synthetic benchmarks and real-world applications for the Linux Kernel and OpenJDK Hotspot Virtual Machine on the ARMv8 and POWERv7 architectures.},
 acmid = {2851150},
 address = {New York, NY, USA},
 articleno = {24},
 author = {Ritson, Carl G. and Owens, Scott},
 doi = {10.1145/3016078.2851150},
 issn = {0362-1340},
 issue_date = {August 2016},
 journal = {SIGPLAN Not.},
 keyword = {benchmarking, concurrency, memory models, performance},
 link = {http://doi.acm.org/10.1145/3016078.2851150},
 month = {feb},
 number = {8},
 numpages = {11},
 pages = {24:1--24:11},
 publisher = {ACM},
 title = {Benchmarking Weak Memory Models},
 volume = {51},
 year = {2016}
}


@inproceedings{Farooqui:2016:AWI:2851141.2851194,
 abstract = {Recent integrated CPU-GPU processors like Intel's Broadwell and AMD's Kaveri support hardware CPU-GPU shared virtual memory, atomic operations, and memory coherency. This enables fine-grained CPU-GPU work-stealing, but architectural differences between the CPU and GPU hurt the performance of traditionally-implemented work-stealing on such processors. These architectural differences include different clock frequencies, atomic operation costs, and cache and shared memory latencies. This paper describes a preliminary implementation of our work-stealing scheduler, Libra, which includes techniques to deal with these architectural differences in integrated CPU-GPU processors. Libra's affinity-aware techniques achieve significant performance gains over classically-implemented work-stealing. We show preliminary results using a diverse set of nine regular and irregular workloads running on an Intel Broadwell Core-M processor. Libra currently achieves up to a 2× performance improvement over classical work-stealing, with a 20% average improvement.},
 acmid = {2851194},
 address = {New York, NY, USA},
 articleno = {30},
 author = {Farooqui, Naila and Barik, Rajkishore and Lewis, Brian T. and Shpeisman, Tatiana and Schwan, Karsten},
 booktitle = {Proceedings of the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
 doi = {10.1145/2851141.2851194},
 isbn = {978-1-4503-4092-2},
 link = {http://doi.acm.org/10.1145/2851141.2851194},
 location = {Barcelona, Spain},
 numpages = {2},
 pages = {30:1--30:2},
 publisher = {ACM},
 series = {PPoPP '16},
 title = {Affinity-aware Work-stealing for Integrated CPU-GPU Processors},
 year = {2016}
}


@article{Wang:2016:APG:3016078.2851154,
 abstract = {Betweenness centrality (BC) is an important metrics in graph analysis which indicates critical vertices in large-scale networks based on shortest path enumeration. Typically, a BC algorithm constructs a shortest-path DAG for each vertex to calculate its BC score. However, for emerging real-world graphs, even the state-of-the-art BC algorithm will introduce a number of redundancies, as suggested by the existence of articulation points. Articulation points imply some common sub-DAGs in the DAGs for different vertices, but existing algorithms do not leverage such information and miss the optimization opportunity. We propose a redundancy elimination approach, which identifies the common sub-DAGs shared between the DAGs for different vertices. Our approach leverages the articulation points and reuses the results of the common sub-DAGs in calculating the BC scores, which eliminates redundant computations. We implemented the approach as an algorithm with two-level parallelism and evaluated it on a multicore platform. Compared to the state-of-the-art implementation using shared memory, our approach achieves an average speedup of 4.6x across a variety of real-world graphs, with the traversal rates up to 45 ~ 2400 MTEPS (Millions of Traversed Edges per Second).},
 acmid = {2851154},
 address = {New York, NY, USA},
 articleno = {7},
 author = {Wang, Lei and Yang, Fan and Zhuang, Liangji and Cui, Huimin and Lv, Fang and Feng, Xiaobing},
 doi = {10.1145/3016078.2851154},
 issn = {0362-1340},
 issue_date = {August 2016},
 journal = {SIGPLAN Not.},
 keyword = {betweenness centrality, parallelism, partial redundancy elimination},
 link = {http://doi.acm.org/10.1145/3016078.2851154},
 month = {feb},
 number = {8},
 numpages = {13},
 pages = {7:1--7:13},
 publisher = {ACM},
 title = {Articulation Points Guided Redundancy Elimination for Betweenness Centrality},
 volume = {51},
 year = {2016}
}


@proceedings{Asenjo:2016:2851141,
 abstract = {It is our great pleasure to welcome you to the 21st ACM Symposium on Principles and Practice of Parallel Programming --- PPoPP'16, in Barcelona, Spain. For the first time, PPoPP is to be held in Europe and we hope you enjoy the history, culture, cuisine and cosmopolitan atmosphere of the thriving and spectacular city of Barcelona. PPoPP is the leading forum for work on all aspects of parallel programming, including foundational and theoretical aspects, techniques, tools, and practical experiences. Given the rise of parallel architectures into the consumer market (desktops, laptops, and mobile devices), we made an effort to attract work that addresses new parallel workloads, techniques and tools that attempt to improve the productivity of parallel programming, and work towards improved synergy with such emerging architectures.},
 address = {New York, NY, USA},
 isbn = {978-1-4503-4092-2},
 location = {Barcelona, Spain},
 publisher = {ACM},
 title = {PPoPP '16: Proceedings of the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
 year = {2016}
}


@article{Kalikar:2016:DNM:3016078.2851164,
 abstract = {We present efficient locking mechanisms for hierarchical data structures. Several applications work on an abstract hierarchy of objects, and a parallel execution on this hierarchy necessitates synchronization across workers operating on different parts of the hierarchy. Existing synchronization mechanisms are either too coarse, too inefficient, or too ad hoc, resulting in reduced or unpredictable amount of concurrency. We propose a new locking approach based on the structural properties of the underlying hierarchy. We show that the developed techniques are efficient even when the hierarchy is an arbitrary graph, and are applicable even when the hierarchy involves mutation. Theoretically, we present our approach as a locking-cost-minimizing instance of a generic algebraic model of synchronization for hierarchical data structures. Using STMBench7, we illustrate considerable reduction in the locking cost, resulting in an average throughput improvement of 42%.},
 acmid = {2851164},
 address = {New York, NY, USA},
 articleno = {23},
 author = {Kalikar, Saurabh and Nasre, Rupesh},
 doi = {10.1145/3016078.2851164},
 issn = {0362-1340},
 issue_date = {August 2016},
 journal = {SIGPLAN Not.},
 keyword = {dominators, graphs, hierarchical data structure, locking, object graphs, synchronization, trees},
 link = {http://doi.acm.org/10.1145/3016078.2851164},
 month = {feb},
 number = {8},
 numpages = {12},
 pages = {23:1--23:12},
 publisher = {ACM},
 title = {DomLock: A New Multi-granularity Locking Technique for Hierarchies},
 volume = {51},
 year = {2016}
}


@inproceedings{Machado:2016:PCD:2851141.2851149,
 abstract = {Concurrency bugs that stem from schedule-dependent branches are hard to understand and debug, because their root causes imply not only different event orderings, but also changes in the control-flow between failing and non-failing executions. We present Cortex: a system that helps exposing and understanding concurrency bugs that result from schedule-dependent branches, without relying on information from failing executions. Cortex preemptively exposes failing executions by perturbing the order of events and control-flow behavior in non-failing schedules from production runs of a program. By leveraging this information from production runs, Cortex synthesizes executions to guide the search for failing schedules. Production-guided search helps cope with the large execution search space by targeting failing executions that are similar to observed non-failing executions. Evaluation on popular benchmarks shows that Cortex is able to expose failing schedules with only a few perturbations to non-failing executions, and takes a practical amount of time.},
 acmid = {2851149},
 address = {New York, NY, USA},
 articleno = {29},
 author = {Machado, Nuno and Lucia, Brandon and Rodrigues, Lu\'{\i}s},
 booktitle = {Proceedings of the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
 doi = {10.1145/2851141.2851149},
 isbn = {978-1-4503-4092-2},
 link = {http://doi.acm.org/10.1145/2851141.2851149},
 location = {Barcelona, Spain},
 numpages = {12},
 pages = {29:1--29:12},
 publisher = {ACM},
 series = {PPoPP '16},
 title = {Production-guided Concurrency Debugging},
 year = {2016}
}


@article{Gindraud:2016:ICM:3016078.2851195,
 abstract = {The shared memory model helps parallel programming productivity, but it also has a high hardware cost and imposes scalability constraints. Ultimately, higher performance will use distributed memories, which scales better but requires programmers to manually transfer data between local memories, which is a complex task. Distributed memories are also more energy efficient than shared memories, and are used in a family of embedded computing solutions called multi processor system on chip (MPSoC).},
 acmid = {2851195},
 address = {New York, NY, USA},
 articleno = {31},
 author = {Gindraud, Fran\c{c}ois and Rastello, Fabrice and Cohen, Albert and Broquedis, Fran\c{c}ois},
 doi = {10.1145/3016078.2851195},
 issn = {0362-1340},
 issue_date = {August 2016},
 journal = {SIGPLAN Not.},
 keyword = {distributed shared memory, global address space, memory allocation},
 link = {http://doi.acm.org/10.1145/3016078.2851195},
 month = {feb},
 number = {8},
 numpages = {2},
 pages = {31:1--31:2},
 publisher = {ACM},
 title = {An Interval Constrained Memory Allocator for the Givy GAS Runtime},
 volume = {51},
 year = {2016}
}


@article{Prades:2016:CAX:3016078.2851181,
 abstract = {Many data centers currently use virtual machines (VMs) to achieve a more efficient usage of hardware resources. However, current virtualization solutions, such as Xen, do not easily provide graphics processing unit (GPU) accelerators to applications running in the virtualized domain with the flexibility usually required in data centers (i.e., managing virtual GPU instances and concurrently sharing them among several VMs). Remote GPU virtualization frameworks such as the rCUDA solution may address this problem. In this work we analyze the use of the rCUDA framework to accelerate scientific applications running inside Xen VMs. Results show that the use of the rCUDA framework is a feasible approach, featuring a very low overhead if an InfiniBand fabric is already present in the cluster.},
 acmid = {2851181},
 address = {New York, NY, USA},
 articleno = {35},
 author = {Prades, Javier and Rea\~{n}o, Carlos and Silla, Federico},
 doi = {10.1145/3016078.2851181},
 issn = {0362-1340},
 issue_date = {August 2016},
 journal = {SIGPLAN Not.},
 keyword = {CUDA, HPC, InfiniBand, rCUDA, virtualization, xen},
 link = {http://doi.acm.org/10.1145/3016078.2851181},
 month = {feb},
 number = {8},
 numpages = {2},
 pages = {35:1--35:2},
 publisher = {ACM},
 title = {CUDA Acceleration for Xen Virtual Machines in Infiniband Clusters with rCUDA},
 volume = {51},
 year = {2016}
}


@article{Chatzopoulos:2016:EES:3016078.2851159,
 abstract = {This paper presents ESTIMA, an easy-to-use tool for extrapolating the scalability of in-memory applications. ESTIMA is designed to perform a simple, yet important task: given the performance of an application on a small machine with a handful of cores, ESTIMA extrapolates its scalability to a larger machine with more cores, while requiring minimum input from the user. The key idea underlying ESTIMA is the use of stalled cycles (e.g. cycles that the processor spends waiting for various events, such as cache misses or waiting on a lock). ESTIMA measures stalled cycles on a few cores and extrapolates them to more cores, estimating the amount of waiting in the system. ESTIMA can be effectively used to predict the scalability of in-memory applications. For instance, using measurements of memcached and SQLite on a desktop machine, we obtain accurate predictions of their scalability on a server. Our extensive evaluation on a large number of in-memory benchmarks shows that ESTIMA has generally low prediction errors.},
 acmid = {2851159},
 address = {New York, NY, USA},
 articleno = {27},
 author = {Chatzopoulos, Georgios and Dragojevi\'{c}, Aleksandar and Guerraoui, Rachid},
 doi = {10.1145/3016078.2851159},
 issn = {0362-1340},
 issue_date = {August 2016},
 journal = {SIGPLAN Not.},
 link = {http://doi.acm.org/10.1145/3016078.2851159},
 month = {feb},
 number = {8},
 numpages = {11},
 pages = {27:1--27:11},
 publisher = {ACM},
 title = {ESTIMA: Extrapolating Scalability of In-memory Applications},
 volume = {51},
 year = {2016}
}


@article{Zhang:2016:SAN:3016078.2851176,
 abstract = {Scalable locking is a key building block for scalable multi-threaded software. Its performance is especially critical in multi-socket, multi-core machines with non-uniform memory access (NUMA). Previous schemes such as local locking and remote locking only perform well under a certain level of contention, and often require non-trivial tuning for a particular configuration. Besides, for large NUMA systems, because of unmanaged lock server's nomination, current distance-first NUMA policies cannot perform satisfactorily. In this work, we propose SANL, a locking scheme that can deliver high performance under various contention levels by adaptively switching between the local and the remote lock scheme. Furthermore, we introduce a new NUMA policy for the remote lock that jointly considers node distances and server utilization when choosing lock servers. A comparison with seven representative locking schemes shows that SANL outperforms the others in most contention situations. In one group test, SANL is 3.7 times faster than RCL lock and 17 times faster than POSIX mutex.},
 acmid = {2851176},
 address = {New York, NY, USA},
 articleno = {50},
 author = {Zhang, Mingzhe and Lau, Francis C. M. and Wang, Cho-Li and Cheng, Luwei and Chen, Haibo},
 doi = {10.1145/3016078.2851176},
 issn = {0362-1340},
 issue_date = {August 2016},
 journal = {SIGPLAN Not.},
 link = {http://doi.acm.org/10.1145/3016078.2851176},
 month = {feb},
 number = {8},
 numpages = {2},
 pages = {50:1--50:2},
 publisher = {ACM},
 title = {Scalable Adaptive NUMA-aware Lock: Combining Local Locking and Remote Locking for Efficient Concurrency},
 volume = {51},
 year = {2016}
}


@article{Mohamedin:2016:DNC:3016078.2851189,
 abstract = {NUMA architectures posed the challenge of rethinking parallel applications due to the non-homogeneity introduced by their design, and their real benefits are limited to the characteristics of the particular workload. We name as partitionable transactional workloads such workloads that may be able to exploit the distributed nature of NUMA, such as transactional workloads where data and accesses can be easily partitioned among the so called NUMA zones. However, in case those workloads require the synchronization on shared data, we have to face the issue of exploiting the NUMA architecture also in the concurrency control for their transactions. Therefore in this paper we present a NUMA-aware concurrency control for transactional memory that we designed for promoting scalability in scenarios where both the transactional workload is prone to scale, and the characteristics of the underlying memory model are inherently non-uniform, such as NUMA architectures.},
 acmid = {2851189},
 address = {New York, NY, USA},
 articleno = {45},
 author = {Mohamedin, Mohamed and Palmieri, Roberto and Peluso, Sebastiano and Ravindran, Binoy},
 doi = {10.1145/3016078.2851189},
 issn = {0362-1340},
 issue_date = {August 2016},
 journal = {SIGPLAN Not.},
 keyword = {NUMA, scalability, transactional memory},
 link = {http://doi.acm.org/10.1145/3016078.2851189},
 month = {feb},
 number = {8},
 numpages = {2},
 pages = {45:1--45:2},
 publisher = {ACM},
 title = {On Designing NUMA-aware Concurrency Control for Scalable Transactional Memory},
 volume = {51},
 year = {2016}
}


@article{Bloemen:2016:MOS:3016078.2851161,
 abstract = {The main advantages of Tarjan's strongly connected component (SCC) algorithm are its linear time complexity and ability to return SCCs on-the-fly, while traversing or even generating the graph. Until now, most parallel SCC algorithms sacrifice both: they run in quadratic worst-case time and/or require the full graph in advance. The current paper presents a novel parallel, on-the-fly SCC algorithm. It preserves the linear-time property by letting workers explore the graph randomly while carefully communicating partially completed SCCs. We prove that this strategy is correct. For efficiently communicating partial SCCs, we develop a concurrent, iterable disjoint set structure (combining the union-find data structure with a cyclic list). We demonstrate scalability on a 64-core machine using 75 real-world graphs (from model checking and explicit data graphs), synthetic graphs (combinations of trees, cycles and linear graphs), and random graphs. Previous work did not show speedups for graphs containing a large SCC. We observe that our parallel algorithm is typically 10-30× faster compared to Tarjan's algorithm for graphs containing a large SCC. Comparable performance (with respect to the current state-of-the-art) is obtained for graphs containing many small SCCs.},
 acmid = {2851161},
 address = {New York, NY, USA},
 articleno = {8},
 author = {Bloemen, Vincent and Laarman, Alfons and van de Pol, Jaco},
 doi = {10.1145/3016078.2851161},
 issn = {0362-1340},
 issue_date = {August 2016},
 journal = {SIGPLAN Not.},
 keyword = {SCC, algorithm, depth-first search, digraph, graph, keywords strongly connected components, multi-core, parallel, union-find},
 link = {http://doi.acm.org/10.1145/3016078.2851161},
 month = {feb},
 number = {8},
 numpages = {12},
 pages = {8:1--8:12},
 publisher = {ACM},
 title = {Multi-core On-the-fly SCC Decomposition},
 volume = {51},
 year = {2016}
}


@article{Wang:2016:MGM:3016078.2851160,
 abstract = {The MCS lock is one of the most prevalent queuing locks. It provides fair scheduling and high performance on massively parallel systems. However, the MCS lock mandates a bring-your-own-context policy: each lock user must provide an additional context (i.e., a queue node) to interact with the lock. This paper proposes MCSg, a variant of the MCS lock that relaxes this restriction. Our key observation is that not all lock users are created equal. We analyzed how locks are used in massively-parallel modern systems, such as NUMA-aware operating systems and databases. We found that such systems often have a small number of "regular" code paths that enter the lock very frequently. Such code paths are the primary beneficiary of the high scalability of MCS locks. However, there are also many "guest" code paths that infrequently enter the lock and do not need the same degree of fairness to access the lock (e.g., background tasks that only run periodically with lower priority). These guest users, which are typically spread out in various modules of the software, prefer context-free locks, such as ticket locks. MCSg provides these guests a context-free interface while regular users still enjoy the benefits provided by MCS. It can also be used as a drop-in replacement of MCS for more advanced locks, such as cohort locking. We also propose MCSg++, an extended version of MCSg, which avoids guest starvation and non-FIFO behaviors that might happen with MCSg. Our evaluation using microbenchmarks and the TPC-C database benchmark on a 16-socket, 240-core server shows that both MCSg and MCSg++ preserve the benefits of MCS for regular users while providing a context-free interface for guests.},
 acmid = {2851160},
 address = {New York, NY, USA},
 articleno = {21},
 author = {Wang, Tianzheng and Chabbi, Milind and Kimura, Hideaki},
 doi = {10.1145/3016078.2851160},
 issn = {0362-1340},
 issue_date = {August 2016},
 journal = {SIGPLAN Not.},
 keyword = {MCS, fairness, latency, locking API, queued locks, scalability, spin locks, throughput},
 link = {http://doi.acm.org/10.1145/3016078.2851160},
 month = {feb},
 number = {8},
 numpages = {12},
 pages = {21:1--21:12},
 publisher = {ACM},
 title = {Be My Guest: MCS Lock Now Welcomes Guests},
 volume = {51},
 year = {2016}
}


@inproceedings{Newton:2016:PTH:2851141.2851142,
 abstract = {Given the sophistication of recent type systems, unification-based type-checking and inference can be a time-consuming phase of compilation---especially when union types are combined with subtyping. It is natural to consider improving performance through parallelism, but these algorithms are challenging to parallelize due to complicated control structure and difficulties representing data in a way that is both efficient and supports concurrency. We provide techniques that address these problems based on the LVish approach to deterministic-by-default parallel programming. We extend LVish with Saturating LVars, the first LVars implemented to release memory during the object's lifetime. Our design allows us to achieve a parallel speedup on worst-case (exponential) inputs of Hindley-Milner inference, and on the Typed Racket type-checking algorithm, which yields up an 8.46× parallel speedup on 14 cores for type-checking examples drawn from the Racket repository.},
 acmid = {2851142},
 address = {New York, NY, USA},
 articleno = {6},
 author = {Newton, Ryan R. and A\u{g}acan, \"{O}mer S. and Fogg, Peter and Tobin-Hochstadt, Sam},
 booktitle = {Proceedings of the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
 doi = {10.1145/2851141.2851142},
 isbn = {978-1-4503-4092-2},
 link = {http://doi.acm.org/10.1145/2851141.2851142},
 location = {Barcelona, Spain},
 numpages = {12},
 pages = {6:1--6:12},
 publisher = {ACM},
 series = {PPoPP '16},
 title = {Parallel Type-checking with Haskell Using Saturating LVars and Stream Generators},
 year = {2016}
}


@article{Kurt:2016:USR:3016078.2851180,
 abstract = {Models based on task graphs that operate on single-assignment data are attractive in several ways, but also require nuanced algorithms for scheduling and memory management for efficient execution. In this paper, we consider memory-efficient dynamic scheduling of task graphs, and present a novel approach for dynamically recycling the memory locations assigned to data items as they are produced by tasks.},
 acmid = {2851180},
 address = {New York, NY, USA},
 articleno = {54},
 author = {Kurt, Mehmet Can and Ren, Bin and Krishnamoorthy, Sriram and Agrawal, Gagan},
 doi = {10.1145/3016078.2851180},
 issn = {0362-1340},
 issue_date = {August 2016},
 journal = {SIGPLAN Not.},
 keyword = {dynamic task graphs, memory management},
 link = {http://doi.acm.org/10.1145/3016078.2851180},
 month = {feb},
 number = {8},
 numpages = {2},
 pages = {54:1--54:2},
 publisher = {ACM},
 title = {User-assisted Storage Reuse Determination for Dynamic Task Graphs},
 volume = {51},
 year = {2016}
}


@inproceedings{Chowdhury:2016:AAD:2851141.2851167,
 abstract = {We present AUTOGEN---an algorithm that for a wide class of dynamic programming (DP) problems automatically discovers highly efficient cache-oblivious parallel recursive divide-and-conquer algorithms from inefficient iterative descriptions of DP recurrences. AUTOGEN analyzes the set of DP table locations accessed by the iterative algorithm when run on a DP table of small size, and automatically identifies a recursive access pattern and a corresponding provably correct recursive algorithm for solving the DP recurrence. We use AUTOGEN to autodiscover efficient algorithms for several well-known problems. Our experimental results show that several autodiscovered algorithms significantly outperform parallel looping and tiled loop-based algorithms. Also these algorithms are less sensitive to fluctuations of memory and bandwidth compared with their looping counterparts, and their running times and energy profiles remain relatively more stable. To the best of our knowledge, AUTOGEN is the first algorithm that can automatically discover new nontrivial divide-and-conquer algorithms.},
 acmid = {2851167},
 address = {New York, NY, USA},
 articleno = {10},
 author = {Chowdhury, Rezaul and Ganapathi, Pramod and Tithi, Jesmin Jahan and Bachmeier, Charles and Kuszmaul, Bradley C. and Leiserson, Charles E. and Solar-Lezama, Armando and Tang, Yuan},
 booktitle = {Proceedings of the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
 doi = {10.1145/2851141.2851167},
 isbn = {978-1-4503-4092-2},
 keyword = {AutoGen, automatic discovery, cache-adaptive, cache-efficient, cache-oblivious, divide-and-conquer, dynamic programming, energy-efficient, parallel, recursive},
 link = {http://doi.acm.org/10.1145/2851141.2851167},
 location = {Barcelona, Spain},
 numpages = {12},
 pages = {10:1--10:12},
 publisher = {ACM},
 series = {PPoPP '16},
 title = {AUTOGEN: Automatic Discovery of Cache-oblivious Parallel Recursive Algorithms for Solving Dynamic Programs},
 year = {2016}
}


@inproceedings{Ramalhete:2016:TME:2851141.2851171,
 abstract = {Several basic mutual exclusion lock algorithms are known, with one of the simplest being the Ticket Lock. We present a new mutual exclusion lock with properties similar to the Ticket Lock but using atomic_exchange() instead of atomic_fetch_add() that can be more efficient on systems without a native instruction for atomic_fetch_add(), or in which the native instruction for atomic_exchange() is faster than the one for atomic_fetch_add(). Similarly to the Ticket Lock, our lock has small memory foot print, is extremely simple, respects FIFO order, and provides starvation freedom in architectures that implement atomic_exchange() as a single instruction, like x86.},
 acmid = {2851171},
 address = {New York, NY, USA},
 articleno = {52},
 author = {Ramalhete, Pedro and Correia, Andreia},
 booktitle = {Proceedings of the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
 doi = {10.1145/2851141.2851171},
 isbn = {978-1-4503-4092-2},
 keyword = {locks, mutual exclusion, ticket lock},
 link = {http://doi.acm.org/10.1145/2851141.2851171},
 location = {Barcelona, Spain},
 numpages = {2},
 pages = {52:1--52:2},
 publisher = {ACM},
 series = {PPoPP '16},
 title = {Tidex: A Mutual Exclusion Lock},
 year = {2016}
}


@inproceedings{Merrill:2016:MSM:2851141.2851190,
 abstract = {We present a perfectly balanced, "merge-based" parallel method for computing sparse matrix-vector products (SpMV). Our algorithm operates directly upon the Compressed Sparse Row (CSR) sparse matrix format, a predominant in-memory representation for general-purpose sparse linear algebra computations. Our CsrMV performs an equitable multi-partitioning of the input dataset, ensuring that no single thread can be overwhelmed by assignment to (a) arbitrarily-long rows or (b) an arbitrarily-large number of zero-length rows. This parallel decomposition requires neither offline preprocessing nor specialized/ancillary data formats. We evaluate our method on both CPU and GPU microarchitecture across an enormous corpus of diverse real world matrix datasets. We show that traditional CsrMV methods are inconsistent performers subject to order-of-magnitude slowdowns, whereas the performance response of our method is substantially impervious to row-length heterogeneity.},
 acmid = {2851190},
 address = {New York, NY, USA},
 articleno = {43},
 author = {Merrill, Duane and Garland, Michael},
 booktitle = {Proceedings of the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
 doi = {10.1145/2851141.2851190},
 isbn = {978-1-4503-4092-2},
 keyword = {GPU, SpMV, merge, merge-path, parallel decomposition, segmented reduction, sparse graph, sparse matrix},
 link = {http://doi.acm.org/10.1145/2851141.2851190},
 location = {Barcelona, Spain},
 numpages = {2},
 pages = {43:1--43:2},
 publisher = {ACM},
 series = {PPoPP '16},
 title = {Merge-based Sparse Matrix-vector Multiplication (SpMV) Using the CSR Storage Format},
 year = {2016}
}


@inproceedings{Ashkiani:2016:GM:2851141.2851169,
 abstract = {Multisplit is a broadly useful parallel primitive that permutes its input data into contiguous buckets or bins, where the function that categorizes an element into a bucket is provided by the programmer. Due to the lack of an efficient multisplit on GPUs, programmers often choose to implement multisplit with a sort. However, sort does more work than necessary to implement multisplit, and is thus inefficient. In this work, we provide a parallel model and multiple implementations for the multisplit problem. Our principal focus is multisplit for a small number of buckets. In our implementations, we exploit the computational hierarchy of the GPU to perform most of the work locally, with minimal usage of global operations. We also use warp-synchronous programming models to avoid branch divergence and reduce memory usage, as well as hierarchical reordering of input elements to achieve better coalescing of global memory accesses. On an NVIDIA K40c GPU, for key-only (key-value) multisplit, we demonstrate a 3.0-6.7x (4.4-8.0x) speedup over radix sort, and achieve a peak throughput of 10.0 G keys/s.},
 acmid = {2851169},
 address = {New York, NY, USA},
 articleno = {12},
 author = {Ashkiani, Saman and Davidson, Andrew and Meyer, Ulrich and Owens, John D.},
 booktitle = {Proceedings of the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
 doi = {10.1145/2851141.2851169},
 isbn = {978-1-4503-4092-2},
 link = {http://doi.acm.org/10.1145/2851141.2851169},
 location = {Barcelona, Spain},
 numpages = {13},
 pages = {12:1--12:13},
 publisher = {ACM},
 series = {PPoPP '16},
 title = {GPU Multisplit},
 year = {2016}
}


@inproceedings{Denniston:2016:DH:2851141.2851157,
 abstract = {Many image processing tasks are naturally expressed as a pipeline of small computational kernels known as stencils. Halide is a popular domain-specific language and compiler designed to implement image processing algorithms. Halide uses simple language constructs to express what to compute and a separate scheduling co-language for expressing when and where to perform the computation. This approach has demonstrated performance comparable to or better than hand-optimized code. Until now, however, Halide has been restricted to parallel shared memory execution, limiting its performance for memory-bandwidth-bound pipelines or large-scale image processing tasks. We present an extension to Halide to support distributed-memory parallel execution of complex stencil pipelines. These extensions compose with the existing scheduling constructs in Halide, allowing expression of complex computation and communication strategies. Existing Halide applications can be distributed with minimal changes, allowing programmers to explore the tradeoff between recomputation and communication with little effort. Approximately 10 new of lines code are needed even for a 200 line, 99 stage application. On nine image processing benchmarks, our extensions give up to a 1.4× speedup on a single node over regular multithreaded execution with the same number of cores, by mitigating the effects of non-uniform memory access. The distributed benchmarks achieve up to 18× speedup on a 16 node testing machine and up to 57× speedup on 64 nodes of the NERSC Cori supercomputer.},
 acmid = {2851157},
 address = {New York, NY, USA},
 articleno = {5},
 author = {Denniston, Tyler and Kamil, Shoaib and Amarasinghe, Saman},
 booktitle = {Proceedings of the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
 doi = {10.1145/2851141.2851157},
 isbn = {978-1-4503-4092-2},
 keyword = {distributed memory, image processing, stencils},
 link = {http://doi.acm.org/10.1145/2851141.2851157},
 location = {Barcelona, Spain},
 numpages = {12},
 pages = {5:1--5:12},
 publisher = {ACM},
 series = {PPoPP '16},
 title = {Distributed Halide},
 year = {2016}
}


@article{Maier:2016:CHT:3016078.2851188,
 abstract = {Concurrent hash tables are one of the most important concurrent data structures with numerous applications. Since hash table accesses can dominate the execution time of the overall application, we need implementations that achieve good speedup. Unfortunately, currently available concurrent hashing libraries turn out to be far away from this requirement in particular when contention on some elements occurs. Our starting point for better performing data structures is a fast and simple lock-free concurrent hash table based on linear probing that is limited to word-sized key-value types and does not support dynamic size adaptation. We explain how to lift these limitations in a provably scalable way and demonstrate that dynamic growing has a performance overhead comparable to the same generalization in sequential hash tables. We perform extensive experiments comparing the performance of our implementations with six of the most widely used concurrent hash tables. Ours are considerably faster than the best algorithms with similar restrictions and an order of magnitude faster than the best more general tables. In some extreme cases, the difference even approaches four orders of magnitude.},
 acmid = {2851188},
 address = {New York, NY, USA},
 articleno = {34},
 author = {Maier, Tobias and Sanders, Peter and Dementiev, Roman},
 doi = {10.1145/3016078.2851188},
 issn = {0362-1340},
 issue_date = {August 2016},
 journal = {SIGPLAN Not.},
 keyword = {concurrency, experimental, lock-freedom},
 link = {http://doi.acm.org/10.1145/3016078.2851188},
 month = {feb},
 number = {8},
 numpages = {2},
 pages = {34:1--34:2},
 publisher = {ACM},
 title = {Concurrent Hash Tables: Fast and General?(!)},
 volume = {51},
 year = {2016}
}


@inproceedings{Luo:2016:DCO:2851141.2851182,
 abstract = {Memory performance is one essential factor for tapping into the full potential of the massive parallelism of GPU. It has motivated some recent efforts in GPU cache modeling. This paper presents a new data-centric way to model the performance of a system with heterogeneous memory resources. The new model is composable, meaning it can predict the performance difference due to placing data differently by profiling the execution just once.},
 acmid = {2851182},
 address = {New York, NY, USA},
 articleno = {38},
 author = {Luo, Hao and Chen, Guoyang and Li, Pengcheng and Ding, Chen and Shen, Xipeng},
 booktitle = {Proceedings of the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
 doi = {10.1145/2851141.2851182},
 isbn = {978-1-4503-4092-2},
 keyword = {footprint, locality metrics, locality modeling},
 link = {http://doi.acm.org/10.1145/2851141.2851182},
 location = {Barcelona, Spain},
 numpages = {2},
 pages = {38:1--38:2},
 publisher = {ACM},
 series = {PPoPP '16},
 title = {Data-centric Combinatorial Optimization of Parallel Code},
 year = {2016}
}


@proceedings{Cohen:2015:2688500,
 abstract = {
                  An abstract is not available.
              },
 address = {New York, NY, USA},
 isbn = {978-1-4503-3205-7},
 location = {San Francisco, CA, USA},
 publisher = {ACM},
 title = {PPoPP 2015: Proceedings of the 20th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
 year = {2015}
}


@article{Narayanaswamy:2016:VCA:3016078.2851165,
 abstract = {Modern shared memory multiprocessors permit reordering of memory operations for performance reasons. These reorderings are often a source of subtle bugs in programs written for such architectures. Traditional approaches to verify weak memory programs often rely on interleaving semantics, which is prone to state space explosion, and thus severely limits the scalability of the analysis. In recent times, there has been a renewed interest in modelling dynamic executions of weak memory programs using partial orders. However, such an approach typically requires ad-hoc mechanisms to correctly capture the data and control-flow choices/conflicts present in real-world programs. In this work, we propose a novel, conflict-aware, composable, truly concurrent semantics for programs written using C/C++ for modern weak memory architectures. We exploit our symbolic semantics based on general event structures to build an efficient decision procedure that detects assertion violations in bounded multi-threaded programs. Using a large, representative set of benchmarks, we show that our conflict-aware semantics outperforms the state-of-the-art partial-order based approaches.},
 acmid = {2851165},
 address = {New York, NY, USA},
 articleno = {25},
 author = {Narayanaswamy, Ganesh and Joshi, Saurabh and Kroening, Daniel},
 doi = {10.1145/3016078.2851165},
 issn = {0362-1340},
 issue_date = {August 2016},
 journal = {SIGPLAN Not.},
 keyword = {concurrency, software, verification, weak consistency models},
 link = {http://doi.acm.org/10.1145/3016078.2851165},
 month = {feb},
 number = {8},
 numpages = {12},
 pages = {25:1--25:12},
 publisher = {ACM},
 title = {The Virtues of Conflict: Analysing Modern Concurrency},
 volume = {51},
 year = {2016}
}


@inproceedings{Cruz:2016:DCG:2851141.2851153,
 abstract = {Declarative programming has been hailed as a promising approach to parallel programming since it makes it easier to reason about programs while hiding the implementation details of parallelism from the programmer. However, its advantage is also its disadvantage as it leaves the programmer with no straightforward way to optimize programs for performance. In this paper, we introduce Coordinated Linear Meld (CLM), a concurrent forward-chaining linear logic programming language, with a declarative way to coordinate the execution of parallel programs allowing the programmer to specify arbitrary scheduling and data partitioning policies. Our approach allows the programmer to write graph-based declarative programs and then optionally to use coordination to fine-tune parallel performance. In this paper we specify the set of coordination facts, discuss their implementation in a parallel virtual machine, and show---through example---how they can be used to optimize parallel execution. We compare the performance of CLM programs against the original uncoordinated Linear Meld and several other frameworks.},
 acmid = {2851153},
 address = {New York, NY, USA},
 articleno = {4},
 author = {Cruz, Flavio and Rocha, Ricardo and Goldstein, Seth Copen},
 booktitle = {Proceedings of the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
 doi = {10.1145/2851141.2851153},
 isbn = {978-1-4503-4092-2},
 keyword = {linear logic, parallel programming},
 link = {http://doi.acm.org/10.1145/2851141.2851153},
 location = {Barcelona, Spain},
 numpages = {12},
 pages = {4:1--4:12},
 publisher = {ACM},
 series = {PPoPP '16},
 title = {Declarative Coordination of Graph-based Parallel Programs},
 year = {2016}
}


@article{Agrawal:2016:EAE:3016078.2851144,
 abstract = {Similarity search finds the most similar matches in an object collection for a given query; making it an important problem across a wide range of disciplines such as web search, image recognition and protein sequencing. Practical implementations of High Dimensional Similarity Search (HDSS) search across billions of possible solutions for multiple queries in real time, making its performance and efficiency a significant challenge. Existing clusters and datacenters use commercial multicore hardware to perform search, which may not provide the optimal performance and performance per Watt. This work explores the performance, power and cost benefits of using throughput accelerators like GPUs to perform similarity search for query cohorts even under tight deadlines. We propose optimized implementations of similarity search for both the host and the accelerator. Augmenting existing Xeon servers with accelerators results in a 3× improvement in throughput per machine, resulting in a more than 2.5× reduction in cost of ownership, even for discounted Xeon servers. Replacing a Xeon based cluster with an accelerator based cluster for similarity search reduces the total cost of ownership by more than 6× to 16× while consuming significantly less power than an ARM based cluster.},
 acmid = {2851144},
 address = {New York, NY, USA},
 articleno = {3},
 author = {Agrawal, Sandeep R. and Dee, Christopher M. and Lebeck, Alvin R.},
 doi = {10.1145/3016078.2851144},
 issn = {0362-1340},
 issue_date = {August 2016},
 journal = {SIGPLAN Not.},
 keyword = {GPGPU, energy efficiency, high throughput, total cost of ownership},
 link = {http://doi.acm.org/10.1145/3016078.2851144},
 month = {feb},
 number = {8},
 numpages = {12},
 pages = {3:1--3:12},
 publisher = {ACM},
 title = {Exploiting Accelerators for Efficient High Dimensional Similarity Search},
 volume = {51},
 year = {2016}
}


@inproceedings{Umar:2016:EPF:2851141.2851186,
 abstract = {Recent research has suggested that improving fine-grained data-locality is one of the main approaches to improving energy efficiency and performance. However, no previous research has investigated the effect of the approach on these metrices in the case of concurrent data structures. This paper investigates how fine-grained data locality influences energy efficiency and performance in concurrent search trees, a crucial data structure that is widely used in several important systems. We conduct a set of experiments on three lock-based concurrent search trees: DeltaTree, a portable fine-grained locality-aware concurrent search tree; CBTree, a coarse-grained locality-aware B+tree; and BST-TK, a locality-oblivious concurrent search tree. We run the experiments on a commodity x86 platform and an embedded ARM platform. The experimental results show that DeltaTree has 13--25% better energy efficiency and 10--22% more operations/second on the x86 and ARM platforms, respectively. The results confirm that portable fine-grained locality can improve energy efficiency and performance in concurrent search trees.},
 acmid = {2851186},
 address = {New York, NY, USA},
 articleno = {36},
 author = {Umar, Ibrahim and Anshus, Otto J. and Ha, Phuong H.},
 booktitle = {Proceedings of the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
 doi = {10.1145/2851141.2851186},
 isbn = {978-1-4503-4092-2},
 keyword = {concurrent algorithms, data locality, energy efficient computing systems, memory systems, multicore processors, performance evaluation, power analysis},
 link = {http://doi.acm.org/10.1145/2851141.2851186},
 location = {Barcelona, Spain},
 numpages = {2},
 pages = {36:1--36:2},
 publisher = {ACM},
 series = {PPoPP '16},
 title = {Effect of Portable Fine-grained Locality on Energy Efficiency and Performance in Concurrent Search Trees},
 year = {2016}
}


