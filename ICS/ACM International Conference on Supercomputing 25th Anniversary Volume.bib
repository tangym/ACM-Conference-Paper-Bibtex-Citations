@inproceedings{Lv:2014:ARS:2591635.2591663,
 abstract = {Does academic research impact popular open-source peer-to-peer systems? Looking back, we find that the simple technique "expanding ring" does find its use in Gnutella and many other contexts.},
 acmid = {2591663},
 address = {New York, NY, USA},
 author = {Lv, Qin and Cao, Pei and Cohen, Edith and Li, Kai and Shenker, Scott},
 booktitle = {ACM International Conference on Supercomputing 25th Anniversary Volume},
 doi = {10.1145/2591635.2591663},
 isbn = {978-1-4503-2840-1},
 keyword = {scalable query algorithms, unstructured peer-to-peer systems},
 link = {http://doi.acm.org/10.1145/2591635.2591663},
 location = {Munich, Germany},
 numpages = {19},
 pages = {64--82},
 publisher = {ACM},
 title = {Author Retrospective for Search and Replication in Unstructured Peer-to-peer Networks},
 year = {2014}
}


@inproceedings{Feautrier:1988:AE:2591635.2667159,
 abstract = {A common problem in restructuring programs for vector or parallel execution is the suppression of false dependencies which originate in the reuse of the same memory cell for unrelated values. The method is simple and well understood in the case of scalars. This paper gives the general solution for the case of arrays. The expansion is done in two steps: first, modify all definitions of the offending array in order to obtain the single assignment property. Then, reconstruct the original data flow by adapting all uses of the array. This is done with the help of a new algorithm for solving parametric integer programs. The technique is quite general and may be used for other purposes, including program checking, collecting array predicates, etc.},
 acmid = {2667159},
 address = {New York, NY, USA},
 author = {Feautrier, P.},
 booktitle = {ACM International Conference on Supercomputing 25th Anniversary Volume},
 doi = {10.1145/2591635.2667159},
 isbn = {978-1-4503-2840-1},
 link = {http://doi.acm.org/10.1145/2591635.2667159},
 location = {Munich, Germany},
 numpages = {13},
 pages = {99--111},
 publisher = {ACM},
 title = {Array Expansion},
 year = {2014}
}


@inproceedings{Ramirez:1999:STC:2591635.2667175,
 abstract = {
                  An abstract is not available.
              },
 acmid = {2667175},
 address = {New York, NY, USA},
 author = {Ram\'{\i}rez, Alex and Larriba-Pey, Josep-L. and Navarro, Carlos and Torrellas, Josep and Valero, Mateo},
 booktitle = {ACM International Conference on Supercomputing 25th Anniversary Volume},
 doi = {10.1145/2591635.2667175},
 isbn = {978-1-4503-2840-1},
 link = {http://doi.acm.org/10.1145/2591635.2667175},
 location = {Munich, Germany},
 numpages = {8},
 pages = {261--268},
 publisher = {ACM},
 title = {Software Trace Cache},
 year = {2014}
}


@inproceedings{Irigoin:2014:ARS:2591635.2591645,
 abstract = {The PIPS project was started in 1988 to investigate the automatic detection of medium- and large-grain parallelism in scientific programs thanks to summarization techniques based on convex array regions. By 1992 the PIPS system had reached its original goals, but it has morphed into a comprehensive, open-source platform still in use today. What were the key scientific and engineering decisions that made this possible in spite of some inevitable shortcomings?},
 acmid = {2591645},
 address = {New York, NY, USA},
 author = {Irigoin, Francois and Jouvelot, Pierre and Triolet, Remi},
 booktitle = {ACM International Conference on Supercomputing 25th Anniversary Volume},
 doi = {10.1145/2591635.2591645},
 isbn = {978-1-4503-2840-1},
 keyword = {automatic parallelization, interprocedural analysis},
 link = {http://doi.acm.org/10.1145/2591635.2591645},
 location = {Munich, Germany},
 numpages = {3},
 pages = {12--14},
 publisher = {ACM},
 title = {Author Retrospective for Semantical Interprocedural Parallelization: An Overview of the PIPS Project},
 year = {2014}
}


@inproceedings{Gonzalez:1995:DCM:2591635.2667170,
 abstract = {
                  An abstract is not available.
              },
 acmid = {2667170},
 address = {New York, NY, USA},
 author = {Gonz\'{a}lez, Antonio and Aliagas, Carlos and Valero, Mateo},
 booktitle = {ACM International Conference on Supercomputing 25th Anniversary Volume},
 doi = {10.1145/2591635.2667170},
 isbn = {978-1-4503-2840-1},
 link = {http://doi.acm.org/10.1145/2591635.2667170},
 location = {Munich, Germany},
 numpages = {10},
 pages = {217--226},
 publisher = {ACM},
 title = {A Data Cache with Multiple Caching Strategies Tuned to Different Types of Locality},
 year = {2014}
}


@inproceedings{Allen:1988:OPA:2591635.2667156,
 abstract = {
                  An abstract is not available.
              },
 acmid = {2667156},
 address = {New York, NY, USA},
 author = {Allen, F. and Burke, M. and Charles, P. and Cytron, R. and Ferrante, J.},
 booktitle = {ACM International Conference on Supercomputing 25th Anniversary Volume},
 doi = {10.1145/2591635.2667156},
 isbn = {978-1-4503-2840-1},
 link = {http://doi.acm.org/10.1145/2591635.2667156},
 location = {Munich, Germany},
 numpages = {1},
 pages = {86--86},
 publisher = {ACM},
 title = {An Overview of the PTRAN Analysis System for Multiprocessing},
 year = {2014}
}


@inproceedings{Ebcioglu:1989:GRP:2591635.2667160,
 abstract = {This paper presents a new approach to resource-constrained compiler extraction of fine-grain parallelism, targeted towards VLIW supercomputers, and in particular, the IBM VLIW (Very Large Instruction Word) processor. The algorithms described integrate resource limitations into Percolation Scheduling—a global parallelization technique—to deal with resource constraints, without sacrificing the generality and completeness of Percolation Scheduling in the process. This is in sharp contrast with previous approaches which either applied only to conditional-free code, or drastically limited the parallelization process by imposing relatively local heuristic resource constraints early in the scheduling process.},
 acmid = {2667160},
 address = {New York, NY, USA},
 author = {Ebcioglu, Kemal and Nicolau, Alexandru},
 booktitle = {ACM International Conference on Supercomputing 25th Anniversary Volume},
 doi = {10.1145/2591635.2667160},
 isbn = {978-1-4503-2840-1},
 link = {http://doi.acm.org/10.1145/2591635.2667160},
 location = {Munich, Germany},
 numpages = {10},
 pages = {112--121},
 publisher = {ACM},
 title = {A Global Resource-constrained Parallelization Technique},
 year = {2014}
}


@inproceedings{Goodman:2014:ARC:2591635.2591640,
 abstract = {In 1987 we were working at the University of Wisconsin-Madison with Jim Smith, J. T. Hsieh, Koujuch Liou and Andrew Pleszkun on PIPE [4], an unorthodox 'decoupled access-execute processor.' The driving innovation of PIPE was the separation of instructions dealing with memory through a separate and independent instruction stream racing ahead, initiating load and store instructions in-order, synchronized only with other instructions through architectural queues. The queues eliminated the need for allocating and managing temporary registers. While the problems of register allocation and instruction scheduling were traditionally treated independently, the genesis of this paper came while developing an optimizing compiler for PIPE, with the insight that in a conventional architecture these two closely related problems have conflicting goals that could be played off against each other to maximize benefit: Code scheduling uses as many temporary registers as possible to achieve a higher level of instruction-level parallelism (ILP), while register allocation minimizes the use of temporary registers so that more data items could be allocated to the available registers. As we gained more experience in optimizing for parallel programs, we came to understand that such conflicting goals were commonplace in the trade-off between parallelism and locality [2]. This work introduced the concept of the DAG-driven register allocation. We defined two terms, width and height of a DAG, in the context of code scheduling. The width of a DAG is the maximal number of mutually independent nodes requiring a destination (i.e. target) register and the height of a DAG is the length of its critical path. The shape of the DAG can be changed due to register allocation. For example, while the width can be reduced by reusing registers, the height could be increased. The purpose of DAG-driven register allocation is to minimize the height while keeping the width smaller than the number of available registers.},
 acmid = {2591640},
 address = {New York, NY, USA},
 author = {Goodman, James R. and Hsu, Wei Chung},
 booktitle = {ACM International Conference on Supercomputing 25th Anniversary Volume},
 doi = {10.1145/2591635.2591640},
 isbn = {978-1-4503-2840-1},
 link = {http://doi.acm.org/10.1145/2591635.2591640},
 location = {Munich, Germany},
 numpages = {2},
 pages = {4--5},
 publisher = {ACM},
 title = {Author Retrospective for Code Scheduling and Register Allocation in Large Basic Blocks},
 year = {2014}
}


@inproceedings{Dundas:1997:IDC:2591635.2667173,
 abstract = {
                  An abstract is not available.
              },
 acmid = {2667173},
 address = {New York, NY, USA},
 author = {Dundas, James and Mudge, Trevor},
 booktitle = {ACM International Conference on Supercomputing 25th Anniversary Volume},
 doi = {10.1145/2591635.2667173},
 isbn = {978-1-4503-2840-1},
 link = {http://doi.acm.org/10.1145/2591635.2667173},
 location = {Munich, Germany},
 numpages = {8},
 pages = {245--252},
 publisher = {ACM},
 title = {Improving Data Cache Performance by Pre-executing Instructions Under a Cache Miss},
 year = {2014}
}


@inproceedings{Olukotun:1999:IPS:2591635.2667177,
 abstract = {
                  An abstract is not available.
              },
 acmid = {2667177},
 address = {New York, NY, USA},
 author = {Olukotun, Kunle and Hammond, Lance and Willey, Mark},
 booktitle = {ACM International Conference on Supercomputing 25th Anniversary Volume},
 doi = {10.1145/2591635.2667177},
 isbn = {978-1-4503-2840-1},
 keyword = {chip multiprocessor, data speculation, feedback-driven optimization, multithreading, parallel programming, performance evaluation},
 link = {http://doi.acm.org/10.1145/2591635.2667177},
 location = {Munich, Germany},
 numpages = {10},
 pages = {277--286},
 publisher = {ACM},
 title = {Improving the Performance of Speculatively Parallel Applications on the Hydra CMP},
 year = {2014}
}


@inproceedings{Eichenberger:1995:OMS:2591635.2667171,
 abstract = {
                  An abstract is not available.
              },
 acmid = {2667171},
 address = {New York, NY, USA},
 author = {Eichenberger, Alexandre E. and Davidson, Edward S. and Abraham, Santosh G.},
 booktitle = {ACM International Conference on Supercomputing 25th Anniversary Volume},
 doi = {10.1145/2591635.2667171},
 isbn = {978-1-4503-2840-1},
 link = {http://doi.acm.org/10.1145/2591635.2667171},
 location = {Munich, Germany},
 numpages = {10},
 pages = {227--236},
 publisher = {ACM},
 title = {Optimum Modulo Schedules for Minimum Register Requirements},
 year = {2014}
}


@inproceedings{Clauss:1996:CSL:2591635.2667172,
 abstract = {
                  An abstract is not available.
              },
 acmid = {2667172},
 address = {New York, NY, USA},
 author = {Clauss, Philippe},
 booktitle = {ACM International Conference on Supercomputing 25th Anniversary Volume},
 doi = {10.1145/2591635.2667172},
 isbn = {978-1-4503-2840-1},
 link = {http://doi.acm.org/10.1145/2591635.2667172},
 location = {Munich, Germany},
 numpages = {8},
 pages = {237--244},
 publisher = {ACM},
 title = {Counting Solutions to Linear and Nonlinear Constraints Through Ehrhart Polynomials: Applications to Analyze and Transform Scientific Programs},
 year = {2014}
}


@inproceedings{Suh:2001:ACM:2591635.2667181,
 abstract = {An accurate, tractable, analytic cache model for time-shared systems is presented, which estimates the overall cache miss-rate of a multiprocessing system with any cache size and time quanta. The input to the model consists of the isolated miss-rate curves for each process, the time quanta for each of the executing processes, and the total cache size. The output is the overall miss-rate. Trace-driven simulations demonstrate that the estimated miss-rate is very accurate. Since the model provides a fast and accurate way to estimate the effect of context switching, it is useful for both understanding the effect of context switching on caches and optimizing cache performance for time-shared systems. A cache partitioning mechanism is also presented and is shown to improve the cache miss-rate up to 25% over the normal LRU replacement policy.},
 acmid = {2667181},
 address = {New York, NY, USA},
 author = {Suh, G. Edward and Devadas, Srinivas and Rudolph, Larry},
 booktitle = {ACM International Conference on Supercomputing 25th Anniversary Volume},
 doi = {10.1145/2591635.2667181},
 isbn = {978-1-4503-2840-1},
 link = {http://doi.acm.org/10.1145/2591635.2667181},
 location = {Munich, Germany},
 numpages = {12},
 pages = {323--334},
 publisher = {ACM},
 title = {Analytical Cache Models with Applications to Cache Partitioning},
 year = {2014}
}


@inproceedings{Hartley:2008:BIA:2591635.2667189,
 abstract = {We are currently witnessing the emergence of two paradigms in parallel computing: streaming processing and multi-core CPUs. Represented by solid commercial products widely available in commodity PCs, GPUs and multi-core CPUs bring together an unprecedented combination of high performance at low cost. The scientific computing community needs to keep pace with application models and middleware which scale efficiently to hundreds of internal processing units. The purpose of the work we present here is twofold: first, a cooperative environment is designed so that both parallel models can coexist and complement one another. Second, beyond the parallelism of multiple internal cores, further parallelism is introduced when multiple CPU sockets, multiple GPUs, and multiple nodes are combined within a unique multi-processor platform which exceeds 10 TFLOPS when using 16 nodes. We illustrate our cooperative parallelization approach by implementing a large-scale, biomedical image analysis application which contains a number of assorted kernels including typical streaming operators, co-occurrence matrices, convolutions, and histograms. Experimental results are compared among different implementation strategies and almost linear speed-up is achieved when all coexisting methods in CPUs and GPUs are combined.},
 acmid = {2667189},
 address = {New York, NY, USA},
 author = {Hartley, Timothy D.R. and Catalyurek, Umit and Ruiz, Antonio and Igual, Francisco and Mayo, Rafael and Ujaldon, Manuel},
 booktitle = {ACM International Conference on Supercomputing 25th Anniversary Volume},
 doi = {10.1145/2591635.2667189},
 isbn = {978-1-4503-2840-1},
 keyword = {biomedical image analysis, cuda programming, graphics processors, high performance computing, multicore cpus, multiprocessors},
 link = {http://doi.acm.org/10.1145/2591635.2667189},
 location = {Munich, Germany},
 numpages = {11},
 pages = {413--423},
 publisher = {ACM},
 title = {Biomedical Image Analysis on a Cooperative Cluster of GPUs and Multicores},
 year = {2014}
}


@inproceedings{Yu:2000:ARP:2591635.2667180,
 abstract = {In this paper, we propose to adapt parallelizing transformations, more specifically, reduction parallelizations, to the actual reference pattern executed by a loop, i.e., to the particular input data and dynamic phase of a program. More precisely we will show how, after validating a reduction at run-time (when this is not possible at compile time) we can dynamically characterize its reference pattern and choose the most appropriate method for parallelizing it. For this purpose, we develop a library of parallel reduction algorithms, including both previously known and novel schemes, which includes algorithms specialized for different classes of access behavior. In particular, each algorithm in our library has identified strengths related to specific reference pattern characteristics, which are matched, at run-time, with measured characteristics of the actual reference pattern. The matching of algorithm to reference pattern is performed using a decision-tree based selection scheme. The contribution of this work consists in new optimizations for reduction parallelization and in the introduction of a new approach to the optimization of irregular applications: Characteristic based customization.},
 acmid = {2667180},
 address = {New York, NY, USA},
 author = {Yu, Hao and Rauchwerger, Lawrence},
 booktitle = {ACM International Conference on Supercomputing 25th Anniversary Volume},
 doi = {10.1145/2591635.2667180},
 isbn = {978-1-4503-2840-1},
 link = {http://doi.acm.org/10.1145/2591635.2667180},
 location = {Munich, Germany},
 numpages = {12},
 pages = {311--322},
 publisher = {ACM},
 title = {Adaptive Reduction Parallelization Techniques},
 year = {2014}
}


@inproceedings{Cytron:2014:ARP:2591635.2591638,
 abstract = {The PTRAN (Parallel Translator) system at IBM had as its goal the analysis and optimization of sequential programs for parallel architectures. In this paper, we give our perspective on what has changed since PTRAN, and what is still relevant.},
 acmid = {2591638},
 address = {New York, NY, USA},
 author = {Cytron, Ron K. and Ferrante, Jeanne and Allen, Fran and Burke, Michael and Charles, Philippe},
 booktitle = {ACM International Conference on Supercomputing 25th Anniversary Volume},
 doi = {10.1145/2591635.2591638},
 isbn = {978-1-4503-2840-1},
 keyword = {automatic parallelization, control dependence, data dependence, interprocedural analysis, program analysis, program dependence graph, program optimization., program restructuring},
 link = {http://doi.acm.org/10.1145/2591635.2591638},
 location = {Munich, Germany},
 numpages = {3},
 pages = {1--3},
 publisher = {ACM},
 title = {Author Retrospective for PTRAN's Analysis and Optimization Techniques},
 year = {2014}
}


@inproceedings{Li:2014:ARA:2591635.2591648,
 abstract = {Array privatization, as a program transform to increase the opportunity for loop parallelization, was introduced at a time when numerical programs dominated the parallel computing world. Today, with parallel processors becoming ubiquitous, the computer industry faces a new challenge, i.e. how to best utilize hardware parallelism for the next generation of main-stream applications. The author re-examines the technique of array privatization both in its historical context and in view of new developments in recent years.},
 acmid = {2591648},
 address = {New York, NY, USA},
 author = {Li, Zhiyuan},
 booktitle = {ACM International Conference on Supercomputing 25th Anniversary Volume},
 doi = {10.1145/2591635.2591648},
 isbn = {978-1-4503-2840-1},
 keyword = {loops, memory allocation, program parallelization},
 link = {http://doi.acm.org/10.1145/2591635.2591648},
 location = {Munich, Germany},
 numpages = {3},
 pages = {21--23},
 publisher = {ACM},
 title = {Author's Retrospective for Array Privatization for Parallel Execution of Loops},
 year = {2014}
}


@inproceedings{Clauss:2014:ARC:2591635.2591654,
 abstract = {Ehrhart polynomials are amazing mathematical objects that I discovered in the early 90's and that have many applications in computer science. This retrospective relates my personal and scientific experience regarding this research work, my meeting with the mathematician Eugène Ehrhart, and addresses some extensions and perspectives of further developments. Original paper: http://dx.doi.org/10.1145/237578.237617},
 acmid = {2591654},
 address = {New York, NY, USA},
 author = {Clauss, Philippe},
 booktitle = {ACM International Conference on Supercomputing 25th Anniversary Volume},
 doi = {10.1145/2591635.2591654},
 isbn = {978-1-4503-2840-1},
 keyword = {ehrhart polynomials, integer points, nested loops, polyhedral model, polyhedron, program analysis and optimization},
 link = {http://doi.acm.org/10.1145/2591635.2591654},
 location = {Munich, Germany},
 numpages = {3},
 pages = {37--39},
 publisher = {ACM},
 title = {Author Retrospective for Counting Solutions to Linear and Nonlinear Constraints Through Ehrhart Polynomials: Applications to Analyze and Transform Scientific Programs},
 year = {2014}
}


@inproceedings{Goodman:1988:CSR:2591635.2667158,
 abstract = {We discuss the issues about the interdependency between code scheduling and register allocation. We present two methods as solutions: (1) an integrated code scheduling technique; and (2) a DAG-driven register allocator. The integrated code scheduling method combines two scheduling techniques—one to reduce pipeline delays and the other to minimize register usage—into a single phase. By keeping track of the number of available registers, the scheduler can choose the appropriate scheduling technique to schedule a better code sequence. The DAG-driven register allocator uses a dependency graph to assist in assigning registers; it introduces much less extra dependency than does an ordinary register allocator. For large basic blocks, both approaches were shown to generate more efficient code sequences than conventional techniques in the simulations.},
 acmid = {2667158},
 address = {New York, NY, USA},
 author = {Goodman, J. R. and Hsu, W.-C.},
 booktitle = {ACM International Conference on Supercomputing 25th Anniversary Volume},
 doi = {10.1145/2591635.2667158},
 isbn = {978-1-4503-2840-1},
 link = {http://doi.acm.org/10.1145/2591635.2667158},
 location = {Munich, Germany},
 numpages = {11},
 pages = {88--98},
 publisher = {ACM},
 title = {Code Scheduling and Register Allocation in Large Basic Blocks},
 year = {2014}
}


@inproceedings{Alverson:1990:TCS:2591635.2667161,
 abstract = {
                  An abstract is not available.
              },
 acmid = {2667161},
 address = {New York, NY, USA},
 author = {Alverson, Robert and Callahan, David and Cummings, Daniel and Koblenz, Brian and Porterfield, Allan and Smith, Burton},
 booktitle = {ACM International Conference on Supercomputing 25th Anniversary Volume},
 doi = {10.1145/2591635.2667161},
 isbn = {978-1-4503-2840-1},
 link = {http://doi.acm.org/10.1145/2591635.2667161},
 location = {Munich, Germany},
 numpages = {6},
 pages = {122--127},
 publisher = {ACM},
 title = {The Tera Computer System},
 year = {2014}
}


@inproceedings{Gornish:1990:CDP:2591635.2667162,
 abstract = {Memory hierarchies are used by multiprocessor systems to reduce large memory access times. It is necessary to automatically manage such a hierarchy, to obtain effective memory utilization. In this paper, we discuss the various issues involved in obtaining an optimal memory management strategy for a memory hierarchy. We present an algorithm for finding the earliest point in a program that a block of data can be prefetched. This determination is based on the control and data dependencies in the program. Such a method is an integral part of more general memory management algorithms. We demonstrate our method's potential by using static analysis to estimate the performance improvement afforded by our prefetching strategy and to analyze the reference patterns in a set of Fortran benchmarks. We also study the effectiveness of prefetching in a realistic shared-memory system using an RTL-level simulator and real codes. This differs from previous studies by considering prefetching benefits in the presence of network contention.},
 acmid = {2667162},
 address = {New York, NY, USA},
 author = {Gornish, Edward H. and Granston, Elana D. and Veidenbaum, Alexander V.},
 booktitle = {ACM International Conference on Supercomputing 25th Anniversary Volume},
 doi = {10.1145/2591635.2667162},
 isbn = {978-1-4503-2840-1},
 link = {http://doi.acm.org/10.1145/2591635.2667162},
 location = {Munich, Germany},
 numpages = {15},
 pages = {128--142},
 publisher = {ACM},
 title = {Compiler-directed Data Prefetching in Multiprocessors with Memory Hierarchies},
 year = {2014}
}


@inproceedings{Kennedy:1992:OPD:2591635.2667164,
 abstract = {Previous research has used program transformation to introduce parallelism and to exploit data locality. Unfortunately, these two objectives have usually been considered independently. This work explores the trade-offs between effectively utilizing parallelism and memory hierarchy on shared-memory multiprocessors. We present a simple, but surprisingly accurate, memory model to determine cache line reuse from both multiple accesses to the same memory location and from consecutive memory access. The model is used in memory optimizing and loop parallelization algorithms that effectively exploit data locality and parallelism in concert. We demonstrate the efficacy of this approach with very encouraging experimental results.},
 acmid = {2667164},
 address = {New York, NY, USA},
 author = {Kennedy, Ken and McKinley, Kathryn S.},
 booktitle = {ACM International Conference on Supercomputing 25th Anniversary Volume},
 doi = {10.1145/2591635.2667164},
 isbn = {978-1-4503-2840-1},
 link = {http://doi.acm.org/10.1145/2591635.2667164},
 location = {Munich, Germany},
 numpages = {12},
 pages = {151--162},
 publisher = {ACM},
 title = {Optimizing for Parallelism and Data Locality},
 year = {2014}
}


@inproceedings{Crowley:2000:CPA:2591635.2667178,
 abstract = {The rapid advancements of networking technology have boosted potential bandwidth to the point that the cabling is no longer the bottleneck. Rather, the bottlenecks lie at the crossing points, the nodes of the network, where data traffic is intercepted or forwarded. As a result, there has been tremendous interest in speeding those nodes, making the equipment run faster by means of specialized chips to handle data trafficking. The Network Processor is the blanket name thrown over such chips in their varied forms. To date, no performance data exist to aid in the decision of what processor architecture to use in next generation network processor. Our goal is to remedy this situation. In this study, we characterize both the application workloads that network processors need to support as well as emerging applications that we anticipate may be supported in the future. Then, we consider the performance of three sample benchmarks drawn from these workloads on several state-of-the-art processor architectures, including: an aggressive, out-of-order, speculative super-scalar processor, a fine-grained multithreaded processor, a single chip multiprocessor, and a simultaneous multithreaded processor (SMT). The network interface environment is simulated in detail, and our results indicate that SMT is the architecture best suited to this environment.},
 acmid = {2667178},
 address = {New York, NY, USA},
 author = {Crowley, Patrick and Fluczynski, Marc E. and Baer, Jean-Loup and Bershad, Brian N.},
 booktitle = {ACM International Conference on Supercomputing 25th Anniversary Volume},
 doi = {10.1145/2591635.2667178},
 isbn = {978-1-4503-2840-1},
 link = {http://doi.acm.org/10.1145/2591635.2667178},
 location = {Munich, Germany},
 numpages = {12},
 pages = {287--298},
 publisher = {ACM},
 title = {Characterizing Processor Architectures for Programmable Network Interfaces},
 year = {2014}
}


@inproceedings{Bilmes:2014:ARO:2591635.2591656,
 abstract = {PHiPAC was an early attempt to improve software performance by searching in a large design space of possible implementations to find the best one. At the time, in the early 1990s, the most efficient numerical linear algebra libraries were carefully hand tuned for specific microarchitectures and compilers, and were often written in assembly language. This allowed very precise tuning of an algorithm to the specifics of a current platform, and provided great opportunity for high efficiency. The prevailing thought at the time was that such an approach was necessary to produce near-peak performance. On the other hand, this approach was brittle, and required great human effort to try each code variant, and so only a tiny subset of the possible code design points could be explored. Worse, given the combined complexities of the compiler and microarchitecture, it was difficult to predict which code variants would be worth the implementation effort. PHiPAC circumvented this effort by using code generators that could easily generate a vast assortment of very different points within a design space, and even across very different design spaces altogether. By following a set of carefully crafted coding guidelines, the generated code was reasonably efficient for any point in the design space. To search the design space, PHiPAC took a rather naive but effective approach. Due to the human-designed and deterministic nature of computing systems, one might reasonably think that smart modeling of the microprocessor and compiler would be sufficient to predict, without performing any timing, the optimal point for a given algorithm. But the combination of an optimizing compiler and a dynamically scheduled microarchitecture with multiple levels of cache and prefetching results in a very complex, highly non-linear system. Small changes in the code can cause large changes in performance, thwarting "smart" search algorithms that attempt to build models of the performance surface. PHiPAC instead used massive, diverse, offline, and only loosely informed grid or randomized search, where each point was evaluated by empirically measuring the code running on the machine, which succeeded due to its indifference to the shape of the performance surface.},
 acmid = {2591656},
 address = {New York, NY, USA},
 author = {Bilmes, Jeff and Asanovic, Krste and Chin, Chee-Whye and Demmel, Jim},
 booktitle = {ACM International Conference on Supercomputing 25th Anniversary Volume},
 doi = {10.1145/2591635.2591656},
 isbn = {978-1-4503-2840-1},
 link = {http://doi.acm.org/10.1145/2591635.2591656},
 location = {Munich, Germany},
 numpages = {3},
 pages = {42--44},
 publisher = {ACM},
 title = {Author Retrospective for Optimizing Matrix Multiply Using PHiPAC: A Portable High-performance ANSI C Coding Methodology},
 year = {2014}
}


@proceedings{Banerjee:2014:2591635,
 abstract = {The International Conference on Supercomputing (ICS) was born in 1987 in Athens, Greece. Some of us have been associated with this conference since its very beginning, and we take great joy and pride in seeing it become successful and now pass the quarter century mark. A decision was made to celebrate the silver jubilee by publishing a 25th Anniversary Volume consisting of some of the most important papers presented at the conference over the years, together with the authors' retrospectives. The ideal would have been to pick one paper from each of the 25 years that had the greatest impact, and making sure that no more than one paper was from the same author or set of authors. I am happy to report that we did not follow these rules too rigidly and did not rely blindly on citation counts of papers. The distinguished committee that made the final decisions was a group of former ICS program committee chairs: Eduard Ayguade, Kyle Gallivan, Avi Mendelson, Alex Nicolau, Constantine Polychronopoulos, Mateo Valero, Alex Veidenbaum, Harry Wijshoff and myself. The committee selected 35 papers after extensive discussion. These papers and their authors' retrospectives appear in this volume. Several selected papers are included without a retrospective; their authors chose not to write one or had a very tough work schedule and could not do it in time. Only 32 of the selected papers are reprinted in this volume however, because ACM does not hold the copyright to papers from ICS'87.},
 address = {New York, NY, USA},
 editor = {Banerjee, Utpal},
 isbn = {978-1-4503-2840-1},
 location = {Munich, Germany},
 publisher = {ACM},
 title = {ACM International Conference on Supercomputing 25th Anniversary Volume},
 year = {2014}
}


@inproceedings{Kubiatowicz:2014:ARA:2591635.2591650,
 abstract = {The MIT Alewife project, launched in the Spring of 1988, comprised a dynamic group of researchers who designed and implemented the Alewife multiprocessor [1]. One of the most important and unexpected outcomes of this project was the message-passing interface described in "Anatomy of a Message in the Alewife Multiprocessor," selected for this Retrospective. This interface was essential to the Alewife OS and runtime systems, was important for I/O, and enabled a new and innovative cache-coherence protocol, called Limit-LESS [3]. It also enabled direct comparisons between shared memory and message-passing [4]. Original paper: http://dx.doi.org/10.1145/165939.165970.},
 acmid = {2591650},
 address = {New York, NY, USA},
 author = {Kubiatowicz, John},
 booktitle = {ACM International Conference on Supercomputing 25th Anniversary Volume},
 doi = {10.1145/2591635.2591650},
 isbn = {978-1-4503-2840-1},
 keyword = {message-passing interfaces, parallel computing},
 link = {http://doi.acm.org/10.1145/2591635.2591650},
 location = {Munich, Germany},
 numpages = {3},
 pages = {26--28},
 publisher = {ACM},
 title = {Author Retrospective for Anatomy of a Message in the Alewife Multiprocessor},
 year = {2014}
}


@inproceedings{Huh:2005:NSF:2591635.2667186,
 abstract = {We propose an organization for the on-chip memory system of a chip multiprocessor, in which 16 processors share a 16MB pool of 256 L2 cache banks. The L2 cache is organized as a non-uniform cache architecture (NUCA) array with a switched network embedded in it for high performance. We show that this organization can support the spectrum of degrees of sharing: unshared, in which each processor has a private portion of the cache, thus reducing hit latency, completely shared, in which every processor shares the entire cache, thus minimizing misses, and every point in between. We find the optimal degree of sharing for a number of cache bank mapping policies, and also evaluate a per-application cache partitioning strategy. We conclude that a static NUCA organization with sharing degrees of two or four work best across a suite of commercial and scientific parallel workloads. We also demonstrate that migratory, dynamic NUCA approaches improve performance significantly for a subset of the workloads at the cost of increased power consumption and complexity, especially as per-application cache partitioning strategies are applied.},
 acmid = {2667186},
 address = {New York, NY, USA},
 author = {Huh, Jaehyuk and Kim, Changkyu and Shafi, Hazim and Zhang, Lixin and Burger, Doug and Keckler, Stephen W.},
 booktitle = {ACM International Conference on Supercomputing 25th Anniversary Volume},
 doi = {10.1145/2591635.2667186},
 isbn = {978-1-4503-2840-1},
 keyword = {cache sharing, chip-multiprocessor, non-uniform cache architecture},
 link = {http://doi.acm.org/10.1145/2591635.2667186},
 location = {Munich, Germany},
 numpages = {10},
 pages = {380--389},
 publisher = {ACM},
 title = {A NUCA Substrate for Flexible CMP Cache Sharing},
 year = {2014}
}


@inproceedings{Mellor-Crummey:2014:ARC:2591635.2591651,
 abstract = {Compilers for data-parallel languages use data distribution specifications to guide code generation for distributed-memory machines. Our 1994 paper described how to generate efficient code for programs that employ block-cyclic data distributions. In subsequent work at Rice University, Darte, Mellor-Crummey, Fowler, and Chavarría-Miranda developed a more general form of block-cyclic partitionings known as generalized multipartitionings. Generalized multipartitionings provide some additional balance properties that make them useful for parallelizing computations that solve recurrences along spatial dimensions. These partitionings were subsequently implemented in Rice University's dHPF compiler for High Performance Fortran. In the years since, the field has changed in many ways; however, variants of block-cyclic data distributions are still used today by modern parallel programming models, algorithms, and compilers. Original paper: http://dx.doi.org/10.1145/181181.181572},
 acmid = {2591651},
 address = {New York, NY, USA},
 author = {Mellor-Crummey, John and Hiranandani, Seema and Sethi, Ajay},
 booktitle = {ACM International Conference on Supercomputing 25th Anniversary Volume},
 doi = {10.1145/2591635.2591651},
 isbn = {978-1-4503-2840-1},
 keyword = {compilers, data distributions, data-parallel languages},
 link = {http://doi.acm.org/10.1145/2591635.2591651},
 location = {Munich, Germany},
 numpages = {3},
 pages = {29--31},
 publisher = {ACM},
 title = {Author Retrospective: Compilation Techniques for Block-cyclic Distributions},
 year = {2014}
}


@inproceedings{Yang:1992:PST:2591635.2667165,
 abstract = {We describe a parallel programming tool for scheduling static task graphs and generating the appropriate target code for message passing MIMD architectures. The computational complexity of the system is almost linear to the size of the task graph and preliminary experiments show performance comparable to the "best" hand-written programs.},
 acmid = {2667165},
 address = {New York, NY, USA},
 author = {Yang, Tao and Gerasoulis, Apostolos},
 booktitle = {ACM International Conference on Supercomputing 25th Anniversary Volume},
 doi = {10.1145/2591635.2667165},
 isbn = {978-1-4503-2840-1},
 link = {http://doi.acm.org/10.1145/2591635.2667165},
 location = {Munich, Germany},
 numpages = {10},
 pages = {163--172},
 publisher = {ACM},
 title = {PYRROS: Static Task Scheduling and Code Generation for Message Passing Multiprocessors},
 year = {2014}
}


@proceedings{Malony:2013:2464996,
 abstract = {Welcome to the 27th ACM International Conference on Supercomputing (ICS), the oldest and longest running conference on high-performance computing. ICS is well known as the premier technical forum where researchers present their latest results and share with colleagues their perspectives on the state-of-the-art in the field in a focused, intimate environment. ICS 2013 continues this strong tradition with an exceptional technical program, thought-provoking keynote addresses, interesting workshops and tutorials, and opportunities for student participation. The conference rotates between the United States and international locations in Europe and Japan. This year ICS is taking place in beautiful Eugene, Oregon, the "Emerald City" of the Willamette Valley and the home of the University of Oregon. It is my pleasure to serve as the General Chair for ICS 2013 and I hope that you will find the meeting to be a rich and rewarding experience.},
 address = {New York, NY, USA},
 isbn = {978-1-4503-2130-3},
 location = {Eugene, Oregon, USA},
 note = {415131},
 publisher = {ACM},
 title = {ICS '13: Proceedings of the 27th International ACM Conference on International Conference on Supercomputing},
 year = {2013}
}


@inproceedings{Chang:2007:CCP:2591635.2667188,
 abstract = {This paper presents Cooperative Cache Partitioning (CCP) to allocate cache resources among threads concurrently running on CMPs. Unlike cache partitioning schemes that use a single spatial partition repeatedly throughout a stable program phase, CCP resolves cache contention with multiple time-sharing partitions. Timesharing cache resources among partitions allows each thrashing thread to speed up dramatically in at least one partition by unfairly shrinking other threads' capacity allocations, while improving fairness by giving different partitions equal chance to execute. Quality-of-Service (QoS) is guaranteed over the long term by orchestrating the shrink and expansion of each thread's capacity across partitions to bound the average slowdown. Time-sharing based cache partitioning is further integrated with CMP cooperative caching [6] to exploit the benefits of LRU-based latency optimizations, which leads to a simplified partitioning algorithm and better performance for workloads that do not benefit from cache partitioning. We evaluate the effectiveness of CCP by simulating a 4-core CMP running all combinations of 7 representative SPEC2000 benchmarks. For workloads that can benefit from cache partitioning, CCP achieves up to 60%, and on average 12%, better performance than the exhaustive search of optimal static partitions. Overall, CCP provides the best results on almost all evaluation metrics for different cache sizes.},
 acmid = {2667188},
 address = {New York, NY, USA},
 author = {Chang, Jichuan and Sohi, Gurindar S.},
 booktitle = {ACM International Conference on Supercomputing 25th Anniversary Volume},
 doi = {10.1145/2591635.2667188},
 isbn = {978-1-4503-2840-1},
 keyword = {CMP, QoS, cooperative cache partitioning, fairness, multiple time-sharing partitions},
 link = {http://doi.acm.org/10.1145/2591635.2667188},
 location = {Munich, Germany},
 numpages = {11},
 pages = {402--412},
 publisher = {ACM},
 title = {Cooperative Cache Partitioning for Chip Multiprocessors},
 year = {2014}
}


@inproceedings{Mudge:2014:ARI:2591635.2591655,
 abstract = {The paper introduces and evaluates a technique, referred to as runahead, that prefetches instructions and data into the L1 caches on cache misses. The results of the experiments reported in the paper show that the CPI of a simple in-order pipeline could be reduced by 30%. The principle advantage of runahead over many other prefetching strategies, is twofold: 1) it prefetches both instructions and date; and 2) it is able to create non-sequential data prefetches. Original Paper: http://dx.doi.org/10.1145/263580.263597.},
 acmid = {2591655},
 address = {New York, NY, USA},
 author = {Mudge, Trevor},
 booktitle = {ACM International Conference on Supercomputing 25th Anniversary Volume},
 doi = {10.1145/2591635.2591655},
 isbn = {978-1-4503-2840-1},
 keyword = {prefetching, runahead, speculation},
 link = {http://doi.acm.org/10.1145/2591635.2591655},
 location = {Munich, Germany},
 numpages = {2},
 pages = {40--41},
 publisher = {ACM},
 title = {Author Retrospective Improving Data Cache Performance by Pre-executing Instructions Under a Cache Miss},
 year = {2014}
}


@inproceedings{Dally:2014:ARD:2591635.2591668,
 abstract = {In the eight years that have passed since we published "Design Tradeoffs for Tiled CMP On-Chip Networks," on-chip interconnection networks have become pervasive, as semiconductor scaling has allowed increasing numbers of processor cores and components to be integrated on a chip. The contributions of the paper, in particular the detailed technology models and the use of channel slicing and concentration to improve efficiency, remain as valid today as they were when we began the research that led to the paper. The network-on-chip (NoC) design methodology presented in the paper is widely used today. In this retrospective, we review the more significant contributions of the original paper, and comment on the impact it had on subsequent research.},
 acmid = {2591668},
 address = {New York, NY, USA},
 author = {Dally, William J. and Balfour, James},
 booktitle = {ACM International Conference on Supercomputing 25th Anniversary Volume},
 doi = {10.1145/2591635.2591668},
 isbn = {978-1-4503-2840-1},
 keyword = {area modeling, energy modeling, network topology, networks-on-chip},
 link = {http://doi.acm.org/10.1145/2591635.2591668},
 location = {Munich, Germany},
 numpages = {3},
 pages = {77--79},
 publisher = {ACM},
 title = {Author Retrospective for Design Tradeoffs for Tiled CMP On-chip Networks},
 year = {2014}
}


@inproceedings{Hartley:2014:ARB:2591635.2591670,
 abstract = {The last six years has seen Moore's Law continue to produce incredible gains in computational power. Indeed, the November, 2007 list of the top ten fastest supercomputers in the world contained no machines with acceleration of any kind. The same list six years later has four of the ten fastest supercomputers using accelerators, including the top two machines [3] . And the ten greenest supercomputers in November 2013 are using NVIDIA GPUs [1]. This heterogeneity of processing units is present from the top of the computational power spectrum down to the bottom, where GPU hardware coupled with novel CUDA and OpenCL programming paradigms have brought significant performance gains to consumer-grade applications. Our paper was one of a number of papers riding the wave of greater programming flexibility and computational capability of accelerators in general, and NVIDIA GPUs in particular. This wave has only increased in size, and probably marks the end of the significant computational performance in section points for general-purpose silicon processors. Advances in networking technology and field-programmability are waiting in the wings for their turn to change the landscape of computing. But for now, we look back at our paper and its peers, and examine their impact.},
 acmid = {2591670},
 address = {New York, NY, USA},
 author = {Hartley, Timothy D.R. and Catalyurek, Umit V. and Ruiz, Antonio and Igual, Fancisco and Mayo, Rafael and Ujaldon, Manuel},
 booktitle = {ACM International Conference on Supercomputing 25th Anniversary Volume},
 doi = {10.1145/2591635.2591670},
 isbn = {978-1-4503-2840-1},
 keyword = {cuda, gpu},
 link = {http://doi.acm.org/10.1145/2591635.2591670},
 location = {Munich, Germany},
 numpages = {3},
 pages = {82--84},
 publisher = {ACM},
 title = {Author's Retrospective for Biomedical Image Analysis on a Cooperative Cluster of Gpus and Multicores},
 year = {2014}
}


@inproceedings{Spring:1998:ALS:2591635.2667176,
 abstract = {
                  An abstract is not available.
              },
 acmid = {2667176},
 address = {New York, NY, USA},
 author = {Spring, Neil and Wolski, Rich},
 booktitle = {ACM International Conference on Supercomputing 25th Anniversary Volume},
 doi = {10.1145/2591635.2667176},
 isbn = {978-1-4503-2840-1},
 link = {http://doi.acm.org/10.1145/2591635.2667176},
 location = {Munich, Germany},
 numpages = {8},
 pages = {269--276},
 publisher = {ACM},
 title = {Application Level Scheduling of Gene Sequence Comparison on Metacomputers},
 year = {2014}
}


@inproceedings{Gornish:2014:ARC:2591635.2591644,
 abstract = {The ideas of compiler-generated prefetching were first proposed in the late 80's. Two main research directions were pursued, both addressed prefetching for loop-based applications containing arrays with regular access patterns. Our approach assumed a hardware prefetcher controlled by software and prefetching into a separate prefetch buffer and finding the earliest point such a prefetch can possibly be initiated. The other approach proposed to prefetch into a cache using a prefetch instruction. This retrospective examines the decisions made at the time and their applicability in today's high-performance processors.},
 acmid = {2591644},
 address = {New York, NY, USA},
 author = {Gornish, Eddie and Granston, Elana and Veidenbaum, Alexander V.},
 booktitle = {ACM International Conference on Supercomputing 25th Anniversary Volume},
 doi = {10.1145/2591635.2591644},
 isbn = {978-1-4503-2840-1},
 keyword = {caches, compiler, interconnection network, multi-processor, prefetching},
 link = {http://doi.acm.org/10.1145/2591635.2591644},
 location = {Munich, Germany},
 numpages = {3},
 pages = {9--11},
 publisher = {ACM},
 title = {Author Retrospective for Compiler-directed Data Prefetching in Multiprocessors with Memory Hierarchies},
 year = {2014}
}


@inproceedings{Lv:2002:SRU:2591635.2667182,
 abstract = {Decentralized and unstructured peer-to-peer networks such as Gnutella are attractive for certain applications because they require no centralized directories and no precise control over network topology or data placement. However, the flooding-based query algorithm used in Gnutella does not scale; each query generates a large amount of traffic and large systems quickly become overwhelmed by the query-induced load. This paper explores, through simulation, various alternatives to Gnutella's query algorithm, data replication strategy, and network topology. We propose a query algorithm based on multiple random walks that resolves queries almost as quickly as Gnutella's flooding method while reducing the network traffic by two orders of magnitude in many cases. We also present simulation results on a distributed replication strategy proposed in [8]. Finally, we find that among the various network topologies we consider, uniform random graphs yield the best performance.},
 acmid = {2667182},
 address = {New York, NY, USA},
 author = {Lv, Qin and Cao, Pei and Cohen, Edith and Li, Kai and Shenker, Scott},
 booktitle = {ACM International Conference on Supercomputing 25th Anniversary Volume},
 doi = {10.1145/2591635.2667182},
 isbn = {978-1-4503-2840-1},
 keyword = {peer-to-peer, replication, search, unstructured},
 link = {http://doi.acm.org/10.1145/2591635.2667182},
 location = {Munich, Germany},
 numpages = {12},
 pages = {335--346},
 publisher = {ACM},
 title = {Search and Replication in Unstructured Peer-to-peer Networks},
 year = {2014}
}


@inproceedings{Suh:2014:ARA:2591635.2591665,
 abstract = {AEGIS is a single-chip secure processor that can be used to protect the integrity and confidentiality of an application program from both physical and software attacks. We briefly describe the history behind this architecture and its key features, discuss main observations and lessons from the project, and list limitations of AEGIS and how recent research addresses them.},
 acmid = {2591665},
 address = {New York, NY, USA},
 author = {Suh, G. Edward and Fletcher, Christopher and Clarke, Dwaine and Gassend, Blaise and van Dijk, Marten and Devadas, Srinivas},
 booktitle = {ACM International Conference on Supercomputing 25th Anniversary Volume},
 doi = {10.1145/2591635.2591665},
 isbn = {978-1-4503-2840-1},
 keyword = {certified execution, secure processors, software licensing},
 link = {http://doi.acm.org/10.1145/2591635.2591665},
 location = {Munich, Germany},
 numpages = {3},
 pages = {68--70},
 publisher = {ACM},
 title = {Author Retrospective AEGIS: Architecture for Tamper-evident and Tamper-resistant Processing},
 year = {2014}
}


@inproceedings{Li:1992:APP:2591635.2667166,
 abstract = {In recent experiments, array privatization played a critical role in successful parallelization of several real programs. This paper presents compiler algorithms for the program analysis for this transformation. The paper also addresses issues in the implementation.},
 acmid = {2667166},
 address = {New York, NY, USA},
 author = {Li, Zhiyuan},
 booktitle = {ACM International Conference on Supercomputing 25th Anniversary Volume},
 doi = {10.1145/2591635.2667166},
 isbn = {978-1-4503-2840-1},
 link = {http://doi.acm.org/10.1145/2591635.2667166},
 location = {Munich, Germany},
 numpages = {10},
 pages = {173--182},
 publisher = {ACM},
 title = {Array Privatization for Parallel Execution of Loops},
 year = {2014}
}


@inproceedings{Peir:2002:BFC:2591635.2667183,
 abstract = {A processor must know a load instruction's latency to schedule the load's dependent instructions at the correct time. Unfortunately, modern processors do not know this latency until well after the dependent instructions should have been scheduled to avoid pipeline bubbles between themselves and the load. One solution to this problem is to predict the load's latency, by predicting whether the load will hit or miss in the data cache. Existing cache hit/miss predictors, however, can only correctly predict about 50% of cache misses.This paper introduces a new hit/miss predictor that uses a Bloom Filter to identify cache misses early in the pipeline. This early identification of cache misses allows the processor to more accurately schedule instructions that are dependent on loads and to more precisely prefetch data into the cache. Simulations using a modified SimpleScalar model show that the proposed Bloom Filter is nearly perfect, with a prediction accuracy greater than 99% for the SPECint2000 benchmarks. IPC (Instructions Per Cycle) performance improved by 19% over a processor that delayed the scheduling of instructions dependent on a load until the load latency was known, and by 6% and 7% over a processor that always predicted a load would hit the cache and with a counter-based hit/miss predictor respectively. This IPC reaches 99.7% of the IPC of a processor with perfect scheduling.},
 acmid = {2667183},
 address = {New York, NY, USA},
 author = {Peir, Jih-Kwon and Lai, Shih-Chang and Lu, Shih-Lien and Stark, Jared and Lai, Konrad},
 booktitle = {ACM International Conference on Supercomputing 25th Anniversary Volume},
 doi = {10.1145/2591635.2667183},
 isbn = {978-1-4503-2840-1},
 keyword = {bloom filter, data cache, data prefetching, data speculation, instruction scheduling},
 link = {http://doi.acm.org/10.1145/2591635.2667183},
 location = {Munich, Germany},
 numpages = {10},
 pages = {347--356},
 publisher = {ACM},
 title = {Bloom Filtering Cache Misses for Accurate Data Speculation and Prefetching},
 year = {2014}
}


@inproceedings{Callahan:1988:AIS:2591635.2667157,
 abstract = {
                  An abstract is not available.
              },
 acmid = {2667157},
 address = {New York, NY, USA},
 author = {Callahan, D. and Kennedy, K.},
 booktitle = {ACM International Conference on Supercomputing 25th Anniversary Volume},
 doi = {10.1145/2591635.2667157},
 isbn = {978-1-4503-2840-1},
 link = {http://doi.acm.org/10.1145/2591635.2667157},
 location = {Munich, Germany},
 numpages = {1},
 pages = {87--87},
 publisher = {ACM},
 title = {Analysis of Interprocedural Side Effects in a Parallel Programming Environment},
 year = {2014}
}


@inproceedings{Crowley:2014:ARC:2591635.2591659,
 abstract = {Our investigation into architectural support for networking tasks began in late 1998, and it began with something of a coincidence. Fiuczynski was an active graduate student participant in the SPIN project at UW [2,3]. He had been writing a SPIN driver for a high-speed network interface card (NIC), and visited Crowley (at the time, a graduate student investigating the relationships between processor & memory architectures and desktop computer applications [4]) one afternoon to discuss the NIC's processor. Had either Fiuczynksi or Crowley been out of the office on that particular day, it is not clear that this collaboration would have emerged. As it happened, both were in the office that day and the NIC processor was very interesting: a dual-core MIPS-like processor built specifically for the NIC. In ensuing discussions, it became clear that-despite the simplicity of its instruction set and microarchitecture-the NIC processor was as a whole more interesting than the much larger and complex x86 processor of the host system. It was more interesting because, as a dual-core processor, it was different and in no way an incremental improvement upon the deeply pipelined CPUs that dominated computing at the time. (They still dominate PCs and servers, of course, but mobile, graphics and embedded chips represent a huge swath of relevant computing platforms today.) It represented evidence of a combination of commercial and technical justification for designing a new and different processor to handle networking tasks.},
 acmid = {2591659},
 address = {New York, NY, USA},
 author = {Crowley, Patrick},
 booktitle = {ACM International Conference on Supercomputing 25th Anniversary Volume},
 doi = {10.1145/2591635.2591659},
 isbn = {978-1-4503-2840-1},
 keyword = {network processor, networking systems, processor architecture},
 link = {http://doi.acm.org/10.1145/2591635.2591659},
 location = {Munich, Germany},
 numpages = {2},
 pages = {54--55},
 publisher = {ACM},
 title = {Author Retrospective for Characterizing Processor Architectures for Programmable Network Interfaces},
 year = {2014}
}


@inproceedings{Feautrier:2014:ARA:2591635.2591641,
 abstract = {In the late 1980's, I was recovering from a long stint as the manager of Paris University computing facility (yes, there were still computing facilities, and PCs were just coming of age) and I was teaching old fashioned automatic parallelization to postgraduate students. I had heard of scalar expansion and privatization, and it was a natural question whether it could be extended to arrays. I first concocted partial solutions - for instance, valid only for the innermost loop - and then decided to try for the general case: converting array accesses to single assignment form. I soon realized that this implied finding the source, or last write before a given read, and that the solution must be a function of the position of the read in the temporal execution of the program. It was obvious that this could not be done for arbitrary complex programs, hence I specified a set of restriction: the polyhedral model. I also introduced the execution order, now known as the 'happens-before' relation. Finding the last write then became an integer programming problem with some unfamiliar features: lexicographic order took the place of the economic function, the problem had to be solved exactly, and the coordinates of the read operation were acting as parameters. Hence, I had first to build PIP (a parametric integer programming tool [2]) before solving my problem. PIP was developed on a 80286 PC, using Borland TurboC and LeLisp. It then took me about two years to have an improved form of the ICS paper published by a journal [3]. Here, the emphasis was more on single assignment conversion and its use for program comprehension. I also formalized a comparison algorithm, which is needed when there are several potential sources. But it was not until [4] that I managed to prove its termination. Meanwhile, the ICS paper had attracted attention from the other side of the Atlantic. Most important was Bill Pugh's work [6], in which the problem was reformulated in term of affine relations, and solved by Bill's own linear programming tool, Omega. I remember that we exchanged our benchmarks, and found that our results were equivalent. An early example of reproducible research!!},
 acmid = {2591641},
 address = {New York, NY, USA},
 author = {Feautrier, Paul},
 booktitle = {ACM International Conference on Supercomputing 25th Anniversary Volume},
 doi = {10.1145/2591635.2591641},
 isbn = {978-1-4503-2840-1},
 link = {http://doi.acm.org/10.1145/2591635.2591641},
 location = {Munich, Germany},
 numpages = {1},
 pages = {6--6},
 publisher = {ACM},
 title = {Author Retrospective for Array Expansion, Array Shrinking, or There and Back Again},
 year = {2014}
}


@inproceedings{Suh:2014:ARA:2591635.2591662,
 abstract = {This paper presents the author retrospective on the analytical cache modeling work published in the 2001 International Conference on Supercomputing (ICS). We summarize the history of the work, revisit primary observations and lessons that we learned from the modeling effort, and also briefly describe follow-up work to show how the research direction evolved over time. Original Paper: http://dx.doi.org/10.1145/377792.377797},
 acmid = {2591662},
 address = {New York, NY, USA},
 author = {Suh, G. Edward and Kurian, George and Devadas, Srinivas and Rudolph, Larry},
 booktitle = {ACM International Conference on Supercomputing 25th Anniversary Volume},
 doi = {10.1145/2591635.2591662},
 isbn = {978-1-4503-2840-1},
 keyword = {cache models, cache partitioning, process scheduling},
 link = {http://doi.acm.org/10.1145/2591635.2591662},
 location = {Munich, Germany},
 numpages = {3},
 pages = {61--63},
 publisher = {ACM},
 title = {Author Retrospective for Analytical Cache Models with Applications to Cache Partitioning},
 year = {2014}
}


@inproceedings{Eichenberger:2014:ARO:2591635.2591653,
 abstract = {
                  An abstract is not available.
              },
 acmid = {2591653},
 address = {New York, NY, USA},
 author = {Eichenberger, Alexandre E. and Davidson, Edward S. and Abraham, Santosh G.},
 booktitle = {ACM International Conference on Supercomputing 25th Anniversary Volume},
 doi = {10.1145/2591635.2591653},
 isbn = {978-1-4503-2840-1},
 link = {http://doi.acm.org/10.1145/2591635.2591653},
 location = {Munich, Germany},
 numpages = {2},
 pages = {35--36},
 publisher = {ACM},
 title = {Author Retrospective for Optimum Modulo Schedules for Minimum Register Requirements},
 year = {2014}
}


@inproceedings{Yeh:1993:IIF:2591635.2667167,
 abstract = {
                  An abstract is not available.
              },
 acmid = {2667167},
 address = {New York, NY, USA},
 author = {Yeh, Tse-Yu and Marr, Deborah T. and Patt, Yale N.},
 booktitle = {ACM International Conference on Supercomputing 25th Anniversary Volume},
 doi = {10.1145/2591635.2667167},
 isbn = {978-1-4503-2840-1},
 link = {http://doi.acm.org/10.1145/2591635.2667167},
 location = {Munich, Germany},
 numpages = {10},
 pages = {183--192},
 publisher = {ACM},
 title = {Increasing the Instruction Fetch Rate via Multiple Branch Prediction and a Branch Address Cache},
 year = {2014}
}


@inproceedings{Hiranandani:1994:CTB:2591635.2667169,
 abstract = {Compilers for data-parallel languages such as Fortran D and High-Performance Fortran use data alignment and distribution specifications as the basis for translating programs for execution on MIMD distributed-memory machines. This paper describes techniques for generating efficient code for programs that use block-cyclic distributions. These techniques can be applied to programs with symbolic loop bounds, symbolic array dimensions, and loops with non-unit strides. We present algorithms for computing the data elements that need to be communicated among processors both for loops with unit and non-unit strides, a linear-time algorithm for computing the memory access sequence for loops with non-unit strides, and experimental results for a hand-compiled test case using block-cyclic distributions},
 acmid = {2667169},
 address = {New York, NY, USA},
 author = {Hiranandani, Seema and Kennedy, Ken and Mellor-Crummey, John and Sethi, Ajay},
 booktitle = {ACM International Conference on Supercomputing 25th Anniversary Volume},
 doi = {10.1145/2591635.2667169},
 isbn = {978-1-4503-2840-1},
 link = {http://doi.acm.org/10.1145/2591635.2667169},
 location = {Munich, Germany},
 numpages = {12},
 pages = {205--216},
 publisher = {ACM},
 title = {Compilation Techniques for Block-cyclic Distributions},
 year = {2014}
}


@inproceedings{Pingali:2014:ARS:2591635.2591660,
 abstract = {This paper describes a technique for transforming imperfectly-nested loops to enhance locality of reference. The key idea is to embed the iteration space of every statement in the loop nest into a special iteration space called the product space. The product space is interpreted as a perfectly-nested loop that is transformed to enhance locality and enable tiling; after that, fully permutable loops are tiled, and a new imperfectly-nested loop with better locality than the original one is generated. Experimental results show that this approach is effective in locality enhancement for dense matrix factorization codes and relaxation codes, among others.},
 acmid = {2591660},
 address = {New York, NY, USA},
 author = {Pingali, Keshav},
 booktitle = {ACM International Conference on Supercomputing 25th Anniversary Volume},
 doi = {10.1145/2591635.2591660},
 isbn = {978-1-4503-2840-1},
 keyword = {imperfectly-nested loops, locality enhancement, restructuring compilers},
 link = {http://doi.acm.org/10.1145/2591635.2591660},
 location = {Munich, Germany},
 numpages = {3},
 pages = {56--58},
 publisher = {ACM},
 title = {Author Retrospective for Synthesizing Transformations for Locality Enhancement of Imperfectly-nested Loop Nests},
 year = {2014}
}


@inproceedings{Ramirez:2014:ARS:2591635.2594508,
 abstract = {In superscalar processors, capable of issuing and executing multiple instructions per cycle, fetch performance represents an upper bound to the overall processor performance. Unless there is some form of instruction re-use mechanism, you cannot execute instructions faster than you can fetch them.  Instruction Level Parallelism, represented by wide issue out oforder superscalar processors, was the trending topic during the end of the 90's and early 2000's. It is indeed the most promising way to continue improving processor performance in a way that does not impact application development, unlike current multicore architectures which require parallelizing the applications (a process that is still far from being automated in the general case). Widening superscalar processor issue was the promise of neverending improvements to single thread performance, as identified by Yale N. Patt et al. in the 1997 special issue of IEEE Computer about "Billion transistor processors" [1]. However, instruction fetch performance is limited by the control flow of the program. The basic fetch stage implementation can read instructions from a single cache line, starting from the current fetch address and up to the next control flow instruction. That is one basic block per cycle at most. Given that the typical basic block size in SPEC integer benchmarks is 4-6 instructions, fetch performance was limited to those same 4-6 instructions per cycle, making 8-wide and 16-wide superscalar processors impractical. It became imperative to find mechanisms to fetch more than 8 instructions per cycle, and that meant fetching more than one basic block per cycle.},
 acmid = {2594508},
 address = {New York, NY, USA},
 author = {Ramirez, Alex and Falcon, Ayose J. and Santana, Oliverio J. and Valero, Mateo},
 booktitle = {ACM International Conference on Supercomputing 25th Anniversary Volume},
 doi = {10.1145/2591635.2594508},
 isbn = {978-1-4503-2840-1},
 keyword = {binary translation, ilp, instruction fetch, superscalar processors},
 link = {http://doi.acm.org/10.1145/2591635.2594508},
 location = {Munich, Germany},
 numpages = {3},
 pages = {45--47},
 publisher = {ACM},
 title = {Author Retrospective for Software Trace Cache},
 year = {2014}
}


@inproceedings{Kubiatowicz:1993:AMA:2591635.2667168,
 abstract = {Shared-memory provides a uniform and attractive mechanism for communication. For efficiency, it is often implemented with a layer of interpretive hardware on top of a message-passing communications network. This interpretive layer is responsible for data location, data movement, and cache coherence. It uses patterns of communication that benefit common programming styles, but which are only heuristics. This suggests that certain styles of communication may benefit from direct access to the underlying communications substrate. The Alewife machine, a shared-memory multiprocessor being built at MIT, provides such an interface. The interface is an integral part of the shared memory implementation and affords direct, user-level access to the network queues, supports an efficient DMA mechanism, and includes fast trap handling for message reception. This paper discusses the design and implementation of the Alewife message-passing interface and addresses the issues and advantages of using such an interface to complement hardware-synthesized shared memory.},
 acmid = {2667168},
 address = {New York, NY, USA},
 author = {Kubiatowicz, John and Agarwal, Anant},
 booktitle = {ACM International Conference on Supercomputing 25th Anniversary Volume},
 doi = {10.1145/2591635.2667168},
 isbn = {978-1-4503-2840-1},
 link = {http://doi.acm.org/10.1145/2591635.2667168},
 location = {Munich, Germany},
 numpages = {12},
 pages = {193--204},
 publisher = {ACM},
 title = {Anatomy of a Message in the Alewife Multiprocessor},
 year = {2014}
}


@inproceedings{McKinley:2014:ARO:2591635.2591646,
 abstract = {Today there is an urgent need for algorithms, programming language systems and tools, and hardware that deliver on the potential of parallelism due to the end of Dennard scaling. This work (from my PhD dissertation, supervised by Ken Kennedy) was one of the early papers to optimize for and experimentally explore the tension between data locality and parallelism on shared memory machines. A key result was that false sharing of cache lines between processors with local caches on separate chips was disastrous to the performance and scaling of applications. This retrospective includes a short personal tour through the history of parallel computing, a discussion of locality and parallelism modeling versus a polyhedral formulation of optimizing dense matrix codes, and how this problem is still relevant to compilers today. I end with a short memorial to my deceased co-author and advisor Ken Kennedy.},
 acmid = {2591646},
 address = {New York, NY, USA},
 author = {McKinley, Kathryn S.},
 booktitle = {ACM International Conference on Supercomputing 25th Anniversary Volume},
 doi = {10.1145/2591635.2591646},
 isbn = {978-1-4503-2840-1},
 keyword = {false sharing, locality, loop transformations, parallelism},
 link = {http://doi.acm.org/10.1145/2591635.2591646},
 location = {Munich, Germany},
 numpages = {3},
 pages = {15--17},
 publisher = {ACM},
 title = {Author Retrospective for Optimizing for Parallelism and Data Locality},
 year = {2014}
}


@inproceedings{Yang:2014:ARP:2591635.2591647,
 abstract = {Given a program with annotated task parallelism represented as a directed acyclic graph (DAG), the PYRROS project was focused on fast DAG scheduling, code generation and runtime execution on distributed memory architectures. PYRROS scheduling goes through several processing stages including clustering of tasks, cluster mapping, and task execution ordering. Since the publication of the PYRROS project, there have been new advancements in the area of DAG scheduling algorithms, the use of DAG scheduling for irregular and large-scale computation, and software system development with annotated task parallelism on modern parallel and cloud architectures. This retrospective describes our experience from this project and the follow-up work, and reviews representative papers related to DAG scheduling published in the last decade.},
 acmid = {2591647},
 address = {New York, NY, USA},
 author = {Yang, Tao and Gerasoulis, Apostolos},
 booktitle = {ACM International Conference on Supercomputing 25th Anniversary Volume},
 doi = {10.1145/2591635.2591647},
 isbn = {978-1-4503-2840-1},
 keyword = {dag, parallel processing, scheduling, task graph},
 link = {http://doi.acm.org/10.1145/2591635.2591647},
 location = {Munich, Germany},
 numpages = {3},
 pages = {18--20},
 publisher = {ACM},
 title = {Author Retrospective for PYRROS: Static Task Scheduling and Code Generation for Message Passing Multiprocessors},
 year = {2014}
}


@inproceedings{Gonzalez:2014:ARD:2591635.2591652,
 abstract = {In this paper we present a retrospective on our paper published in ICS 1995, which to best of our knowledge was the first paper that introduced the concept of a cache memory with multiple subcaches, each tuned for a different type of locality. In this retrospective, we summarize the main ideas of the original paper and outline some of the later work that exploited similar ideas and could have been influenced by our original paper, including two actual industrial microprocessors. DOI: http://dx.doi.org/10.1145/224538.224622},
 acmid = {2591652},
 address = {New York, NY, USA},
 author = {Gonz\'{a}lez, Antonio and Aliagas, Carlos},
 booktitle = {ACM International Conference on Supercomputing 25th Anniversary Volume},
 doi = {10.1145/2591635.2591652},
 isbn = {978-1-4503-2840-1},
 link = {http://doi.acm.org/10.1145/2591635.2591652},
 location = {Munich, Germany},
 numpages = {3},
 pages = {32--34},
 publisher = {ACM},
 title = {Author Retrospective for the Dual Data Cache},
 year = {2014}
}


@inproceedings{Ebcioglu:2014:ARG:2591635.2591642,
 abstract = {Looking back over the last 25 years, we will reflect below on our 1989 paper, "A global resource-constrained parallelization technique" published in the Third International Conference on Supercomputing, held in Crete, June 1989.},
 acmid = {2591642},
 address = {New York, NY, USA},
 author = {Ebcio\u{g}lu, Kemal and Nicolau, Alexandru},
 booktitle = {ACM International Conference on Supercomputing 25th Anniversary Volume},
 doi = {10.1145/2591635.2591642},
 isbn = {978-1-4503-2840-1},
 link = {http://doi.acm.org/10.1145/2591635.2591642},
 location = {Munich, Germany},
 numpages = {2},
 pages = {7--8},
 publisher = {ACM},
 title = {Author Retrospective for a Global Resource-constrained Parallelization Technique},
 year = {2014}
}


@inproceedings{Yu:2014:ARA:2591635.2591661,
 abstract = {Modern applications are dynamic and input dependent and algorithm performance is input and environment sensitive. This potential mismatch between algorithmic choice and performance is exacerbated in the case of parallel programs because the penalty for less than optimal locality is grows with the size of the machine. Reductions, e.g., map-reduce are one of the most important algorithms used in parallel codes are also input sensitive. This led us to develop an adaptive framework that used a statistical method to learn how to select the best algorithm for every execution instance. We applied it to parallel reduction algorithm selection. The importance of better reduction methods as well as adaptive selection methods has only increased since the time this paper was first published. Original paper:http://dx.doi.org/10.1145/335231.335238},
 acmid = {2591661},
 address = {New York, NY, USA},
 author = {Yu, Hao and Rauchwerger, Lawrence},
 booktitle = {ACM International Conference on Supercomputing 25th Anniversary Volume},
 doi = {10.1145/2591635.2591661},
 isbn = {978-1-4503-2840-1},
 keyword = {adaptive algorithms, machine learning, matrix multiplication, parallel algorithms, reductions, sorting},
 link = {http://doi.acm.org/10.1145/2591635.2591661},
 location = {Munich, Germany},
 numpages = {2},
 pages = {59--60},
 publisher = {ACM},
 title = {Author Retrospective for Adaptive Reduction Parallelization Techniques},
 year = {2014}
}


@inproceedings{Huh:2014:ARN:2591635.2591667,
 abstract = {In 2005, as chip multiprocessors started to appear widely, it became possible for the on-chip cores to share the last-level cache. At the time, architects either considered the last-level cache to be divided into per-core private segments, or wholly shared. The shared cache utilized the capacity more efficiency but suffered from high, uniform latencies. This paper proposed a new direction: allowing the caches to be non-uniform, with a varying number of processors sharing each section of the cache. Sharing degree, the number of cores sharing a last-level cache, determines the level of replication in on-chip caches and also affects the capacity and latency for each shared cache. Building on our previous work that introduced non-uniform cache architectures (NUCA), this study explored the design space for shared multi-core caches, focusing on the effect of sharing degree. Our observation of a per-application optimal sharing degree led to a static NUCA design with a reconfigurable sharing degree. This work in multicore NUCA cache architectures has been influential in contemporary systems, including the level-3 cache in the IBM Power 7 and Power 8 processors.},
 acmid = {2591667},
 address = {New York, NY, USA},
 author = {Huh, Jaehyuk and Kim, Changkyu and Shafi, Hazim and Zhang, Lixin and Burger, Doug and Keckler, Stephen W.},
 booktitle = {ACM International Conference on Supercomputing 25th Anniversary Volume},
 doi = {10.1145/2591635.2591667},
 isbn = {978-1-4503-2840-1},
 keyword = {multicore processors, nuca design, sharing degree},
 link = {http://doi.acm.org/10.1145/2591635.2591667},
 location = {Munich, Germany},
 numpages = {3},
 pages = {74--76},
 publisher = {ACM},
 title = {Author Retrospective for a NUCA Substrate for Flexible CMP Cache Sharing},
 year = {2014}
}


@inproceedings{Pinheiro:2004:ECT:2591635.2667185,
 abstract = {In this paper, we study energy conservation techniques for disk array-based network servers. First, we introduce a new conservation technique, called Popular Data Concentration (PDC), that migrates frequently accessed data to a subset of the disks. The goal is to skew the load towards a few of the disks, so that others can be transitioned to low-power modes. Next, we introduce a user-level file server that takes advantage of PDC. In the context of this server, we compare PDC to the Massive Array of Idle Disks (MAID). Using a validated simulator, we evaluate these techniques for conventional and two-speed disks and a wide range of parameters. Our results for conventional disks show that PDC and MAID can only conserve energy when the load on the server is extremely low. When two-speed disks are used, both PDC and MAID can conserve significant energy with only a small fraction of delayed requests. Overall, we find that PDC achieves more consistent and robust energy savings than MAID.},
 acmid = {2667185},
 address = {New York, NY, USA},
 author = {Pinheiro, Eduardo and Bianchini, Ricardo},
 booktitle = {ACM International Conference on Supercomputing 25th Anniversary Volume},
 doi = {10.1145/2591635.2667185},
 isbn = {978-1-4503-2840-1},
 keyword = {disk power, energy conservation, network servers},
 link = {http://doi.acm.org/10.1145/2591635.2667185},
 location = {Munich, Germany},
 numpages = {11},
 pages = {369--379},
 publisher = {ACM},
 title = {Energy Conservation Techniques for Disk Array-based Servers},
 year = {2014}
}


@inproceedings{Chang:2014:ARC:2591635.2591669,
 abstract = {In this paper, we reflect on our experiences and the lessons learned in designing and evaluating the cooperative cache partitioning technique for chip multiprocessors.},
 acmid = {2591669},
 address = {New York, NY, USA},
 author = {Chang, Jichuan and Sohi, Gurindar S.},
 booktitle = {ACM International Conference on Supercomputing 25th Anniversary Volume},
 doi = {10.1145/2591635.2591669},
 isbn = {978-1-4503-2840-1},
 keyword = {algorithms, design, management, performance},
 link = {http://doi.acm.org/10.1145/2591635.2591669},
 location = {Munich, Germany},
 numpages = {2},
 pages = {80--81},
 publisher = {ACM},
 title = {Author Retrospective for Cooperative Cache Partitioning for Chip Multiprocessors},
 year = {2014}
}


@inproceedings{Bilmes:1997:OMM:2591635.2667174,
 abstract = {
                  An abstract is not available.
              },
 acmid = {2667174},
 address = {New York, NY, USA},
 author = {Bilmes, Jeff and Asanovic, Krste and Chin, Chee-Whye and Demmel, Jim},
 booktitle = {ACM International Conference on Supercomputing 25th Anniversary Volume},
 doi = {10.1145/2591635.2667174},
 isbn = {978-1-4503-2840-1},
 link = {http://doi.acm.org/10.1145/2591635.2667174},
 location = {Munich, Germany},
 numpages = {8},
 pages = {253--260},
 publisher = {ACM},
 title = {Optimizing Matrix Multiply Using PHiPAC: A Portable, High-performance, ANSI C Coding Methodology},
 year = {2014}
}


@inproceedings{Yeh:2014:ARI:2591635.2591649,
 abstract = {"Increasing the Instruction Fetch Rate viaMultiple Branch Prediction and a Branch Address Cache" was the first paper to propose a highly accurate hardware mechanism for predicting and fetching multiple non-contiguous basic blocks using leading-edge aggressive branch predictors of the time. Prior to this paper, the methods to increase fetch bandwidth relied on software and compiler mechanisms to increase the size of the basic blocks themselves. The publication of our paper inspired an explosion of research to further improve the accuracy of multiple branch prediction, reduce complexity of fetching multiple basic blocks, or increase the fetch bandwidth in other ways. Our HPS research group defied conventional wisdom in a 1991 ISCA paper [1] that demonstrated instruction level parallelism (IPC) can be greater than two even for non-scientific workloads. This flew in the face of numerous "proofs" at the time. We showed that there was enough parallelism for an aggressive superscalar out-of-order execution engine to exploit to dramatically improve performance, provided that the penalties caused by incorrect branch predictions, cache misses, and TLB misses could be improved.},
 acmid = {2591649},
 address = {New York, NY, USA},
 author = {Yeh, Tse-Yu and Marr, Deborah T. and Patt, Yale N.},
 booktitle = {ACM International Conference on Supercomputing 25th Anniversary Volume},
 doi = {10.1145/2591635.2591649},
 isbn = {978-1-4503-2840-1},
 keyword = {branch prediction},
 link = {http://doi.acm.org/10.1145/2591635.2591649},
 location = {Munich, Germany},
 numpages = {2},
 pages = {24--25},
 publisher = {ACM},
 title = {Author Retrospective for Increasing the Instruction Fetch Rate via Multiple Branch Prediction and a Branch Address Cache},
 year = {2014}
}


@inproceedings{Wolski:2014:ARD:2591635.2591657,
 abstract = {This paper attempts, retrospectively, to frame results originally published in the 1998 edition of the ACM International Conference on Supercomputing in the present context. The article entitled "Application Level Scheduling of Gene Sequence Comparison on Metacomputers" described the use of on-line performance prediction to adapt, dynamically, the execution schedule of a gene sequencing code to fluctuating performance conditions in the wide area. This retrospective discusses those results in light of recent technological developments in large-scale, wide area distributed computing.},
 acmid = {2591657},
 address = {New York, NY, USA},
 author = {Wolski, Rich},
 booktitle = {ACM International Conference on Supercomputing 25th Anniversary Volume},
 doi = {10.1145/2591635.2591657},
 isbn = {978-1-4503-2840-1},
 keyword = {distributed systems},
 link = {http://doi.acm.org/10.1145/2591635.2591657},
 location = {Munich, Germany},
 numpages = {3},
 pages = {48--50},
 publisher = {ACM},
 title = {Author Retrospective of Dynamic Application Scheduling Using On-line Analytics: Then and Now},
 year = {2014}
}


@inproceedings{Olukotun:2014:ARI:2591635.2591658,
 abstract = {Our 1999 paper described how to use hardware with thread-level speculation (TLS) support to effectively parallelize a number of serial application benchmarks with minimal programmer intervention required. The ability of TLS hardware to allow programmers to parallelize code almost arbitrarily and then performance tune afterwards, based on feedback supplied by the TLS system, provided significant improvements to programmer productivity and made parallel programming much less error-prone. Since this paper appeared, we investigated other hardware variations that could provide similar benefits in terms of programmer productivity, such as ones based on an extension of transactional memory. Unfortunately, these concepts have not been implemented on any real systems. As a result, there is still an opportunity to implement schemes like the ones that we described in this paper in order to ease parallel programming in future systems dramatically.},
 acmid = {2591658},
 address = {New York, NY, USA},
 author = {Olukotun, Kunle and Hammond, Lance and Willey, Mark},
 booktitle = {ACM International Conference on Supercomputing 25th Anniversary Volume},
 doi = {10.1145/2591635.2591658},
 isbn = {978-1-4503-2840-1},
 keyword = {chip multiprocessor (cmp), parallel programming, retrospective, thread-level speculation (tls), transactional memory},
 link = {http://doi.acm.org/10.1145/2591635.2591658},
 location = {Munich, Germany},
 numpages = {3},
 pages = {51--53},
 publisher = {ACM},
 title = {Author's Retrospective for: Improving the Performance of Speculatively Parallel Applications on the Hydra CMP},
 year = {2014}
}


@inproceedings{Suh:2003:AAT:2591635.2667184,
 abstract = {We describe the architecture for a single-chip AEGIS processor which can be used to build computing systems secure against both physical and software attacks. Our architecture assumes that all components external to the processor, such as memory, are untrusted. We show two different implementations. In the first case, the core functionality of the operating system is trusted and implemented in a security kernel. We also describe a variant implementation assuming an untrusted operating system. AEGIS provides users with tamper-evident, authenticated environments in which any physical or software tampering by an adversary is guaranteed to be detected, and private and authenticated tamper-resistant environments where additionally the adversary is unable to obtain any information about software or data by tampering with, or otherwise observing, system operation. AEGIS enables many applications, such as commercial grid computing, secure mobile agents, software licensing, and digital rights management.Preliminary simulation results indicate that the overhead of security mechanisms in AEGIS is reasonable.},
 acmid = {2667184},
 address = {New York, NY, USA},
 author = {Suh, G. Edward and Clarke, Dwaine and Gassend, Blaise and van Dijk, Marten and Devadas, Srinivas},
 booktitle = {ACM International Conference on Supercomputing 25th Anniversary Volume},
 doi = {10.1145/2591635.2667184},
 isbn = {978-1-4503-2840-1},
 keyword = {certified execution, secure processors, software licensing},
 link = {http://doi.acm.org/10.1145/2591635.2667184},
 location = {Munich, Germany},
 numpages = {12},
 pages = {357--368},
 publisher = {ACM},
 title = {AEGIS: Architecture for Tamper-evident and Tamper-resistant Processing},
 year = {2014}
}


@inproceedings{Peir:2014:ARB:2591635.2591664,
 abstract = {In this paper, we provide the authors? retrospective analysis of the paper "Bloom Filtering Cache Misses for Accurate Data Speculative and Prefetching" which was published in the proceedings of 2002 International Conference on Supercomputing. DOI: http://dx.doi.org/10.1145/514191.514219},
 acmid = {2591664},
 address = {New York, NY, USA},
 author = {Peir, Jih-Kwon and Lai, Shih-Chang Kevin and Lu, Shih-Lien and Stark, Jared and Lai, Konrad},
 booktitle = {ACM International Conference on Supercomputing 25th Anniversary Volume},
 doi = {10.1145/2591635.2591664},
 isbn = {978-1-4503-2840-1},
 keyword = {bloom filter, cache hit/miss prediction, data speculation, instruction scheduling, prefetching},
 link = {http://doi.acm.org/10.1145/2591635.2591664},
 location = {Munich, Germany},
 numpages = {3},
 pages = {65--67},
 publisher = {ACM},
 title = {Author Retrospective for Bloom Filtering Cache Misses for Accurate Data Speculation and Prefetching},
 year = {2014}
}


@inproceedings{Irigoin:1991:SIP:2591635.2667163,
 abstract = {
                  An abstract is not available.
              },
 acmid = {2667163},
 address = {New York, NY, USA},
 author = {Irigoin, Fran\c{c}ois and Jouvelot, Pierre and Triolet, R{\'e}mi},
 booktitle = {ACM International Conference on Supercomputing 25th Anniversary Volume},
 doi = {10.1145/2591635.2667163},
 isbn = {978-1-4503-2840-1},
 link = {http://doi.acm.org/10.1145/2591635.2667163},
 location = {Munich, Germany},
 numpages = {8},
 pages = {143--150},
 publisher = {ACM},
 title = {Semantical Interprocedural Parallelization: An Overview of the PIPS Project},
 year = {2014}
}


@proceedings{Bode:2014:2597652,
 abstract = {Welcome to the 28th ACM International Conference on Supercomputing (ICS), the oldest and longest running conference on high-performance computing. ICS is a premier forum for researchers to present and discuss latest results and perspectives on the state-of-the-art in supercomputing with their colleagues. ICS 2014 continues the strong focus on excellent technical presentations, motivating keynote addresses, and a small collection of carefully selected workshops and tutorials. The conference follows a cycle of four years visiting twice the United States, once Europe and Asia. This year ICS is taking place in Munich, the center of Bavaria in Germany. The venue is the Bavarian Academy of Sciences with a long tradition going back to 1759. The Academy is running the Leibniz Supercomputer Centre with its 3 Petaflops SuperMUC system. The success of ICS 2014 is the result of an outstanding team of people. We want to thank the organizing committee at Technische Universität München and the Leibniz Supercomputer Centre for their dedication in setting up and running the conference. The excellent technical program was brought together by the PC chair Per Stenström and the area co-chairs Barton P. Miller, Lawrence Rauchwerger and Martin Schulz. Thanks to the huge reviewing effort of the program committee and the extended review committee, 34 excellent papers were selected from over 160 submissions. We would also like to thank the workshops chair Bernd Mohr, the tutorials chair Michael Bader and the poster chair Erwin Laure for broadening the scope of ICS 2014 with additional activities. The publicity chair Beniamino Di Martino put a lot of effort into attracting submissions and participants to the conference. Shajulin Benedict cooperated with ACM-Sheridan Proceedings Service in the publication of the proceedings. Josef Weidendorfer, Herbert Huber and Carsten Trinitis worked closely together in the local organization team taking care of the local logistics, the sponsoring and the financial accounting. Finally, we thank the steering committee and especially its chair Alex Veidenbaum for giving us the opportunity to run this prestigious conference in Munich. The continued sponsorship of ACM SIGARCH for ICS is the basis for 28 events of this conference series and for even more to come. We also want to thank the industrial sponsors of this year's conference enabling us to have enjoyable and fruitful days in Munich. Finally, our thanks go to all the authors of papers submitted to the conference for taking the effort in presenting their latest research results. Without their contributions, this conference would not be such a stimulating meeting.},
 address = {New York, NY, USA},
 isbn = {978-1-4503-2642-1},
 location = {Munich, Germany},
 note = {104148},
 publisher = {ACM},
 title = {ICS '14: Proceedings of the 28th ACM International Conference on Supercomputing},
 year = {2014}
}


@inproceedings{Gannon:1988:SCL:2591635.2667155,
 abstract = {
                  An abstract is not available.
              },
 acmid = {2667155},
 address = {New York, NY, USA},
 author = {Gannon, Dennis and Jalby, William and Gallivan, Kyle},
 booktitle = {ACM International Conference on Supercomputing 25th Anniversary Volume},
 doi = {10.1145/2591635.2667155},
 isbn = {978-1-4503-2840-1},
 link = {http://doi.acm.org/10.1145/2591635.2667155},
 location = {Munich, Germany},
 numpages = {1},
 pages = {85--85},
 publisher = {ACM},
 title = {Strategies for Cache and Local Memory Management by Global Program Transformation},
 year = {2014}
}


@inproceedings{Balfour:2006:DTT:2591635.2667187,
 abstract = {We develop detailed area and energy models for on-chip interconnection networks and describe tradeoffs in the design of efficient networks for tiled chip multiprocessors. Using these detailed models we investigate how aspects of the network architecture including topology, channel width, routing strategy, and buffer size affect performance and impact area and energy efficiency. We simulate the performance of a variety of on-chip networks designed for tiled chip multiprocessors implemented in an advanced VLSI process and compare area and energy efficiencies estimated from our models. We demonstrate that the introduction of a second parallel network can increase performance while improving efficiency, and evaluate different strategies for distributing traffic over the subnetworks. Drawing on insights from our analysis, we present a concentrated mesh topology with replicated subnetworks and express channels which provides a 24% improvement in area efficiency and a 48% improvement in energy efficiency over other networks evaluated in this study.},
 acmid = {2667187},
 address = {New York, NY, USA},
 author = {Balfour, James and Dally, William J.},
 booktitle = {ACM International Conference on Supercomputing 25th Anniversary Volume},
 doi = {10.1145/2591635.2667187},
 isbn = {978-1-4503-2840-1},
 link = {http://doi.acm.org/10.1145/2591635.2667187},
 location = {Munich, Germany},
 numpages = {12},
 pages = {390--401},
 publisher = {ACM},
 title = {Design Tradeoffs for Tiled CMP On-chip Networks},
 year = {2014}
}


@inproceedings{Pinheiro:2014:ARE:2591635.2591666,
 abstract = {This is a retrospective on our original paper titled "Energy Conservation Techniques for Disk Array-based Servers", which was published in the Proceedings of the International Conference on Supercomputing in 2004. Original paper: http://dx.doi.org/10.1145/2591635.2591666},
 acmid = {2591666},
 address = {New York, NY, USA},
 author = {Pinheiro, Eduardo and Bianchini, Ricardo},
 booktitle = {ACM International Conference on Supercomputing 25th Anniversary Volume},
 doi = {10.1145/2591635.2591666},
 isbn = {978-1-4503-2840-1},
 keyword = {disk arrays, disk power, energy conservation},
 link = {http://doi.acm.org/10.1145/2591635.2591666},
 location = {Munich, Germany},
 numpages = {3},
 pages = {71--73},
 publisher = {ACM},
 title = {Author Retrospective on Energy Conservation Techniques for Disk Array-based Servers},
 year = {2014}
}


@inproceedings{Ahmed:2000:STL:2591635.2667179,
 abstract = {We present an approach for synthesizing transformations to enhance locality in imperfectly-nested loops. The key idea is to embed the iteration space of every statement in a loop nest into a special iteration space called the product space. The product space can be viewed as a perfectly-nested loop nest, so embedding generalizes techniques like code sinking and loop fusion that are used in ad hoc ways in current compilers to produce perfectly-nested loops from imperfectly-nested ones. In contrast to these ad hoc techniques however, our embeddings are chosen carefully to enhance locality. The product space is then transformed further to enhance locality, after which fully permutable loops are tiled, and code is generated. We evaluate the effectiveness of this approach for dense numerical linear algebra benchmarks, relaxation codes, and the tomcatv code from the SPEC benchmarks.},
 acmid = {2667179},
 address = {New York, NY, USA},
 author = {Ahmed, Nawaaz and Mateev, Nikolay and Pingali, Keshav},
 booktitle = {ACM International Conference on Supercomputing 25th Anniversary Volume},
 doi = {10.1145/2591635.2667179},
 isbn = {978-1-4503-2840-1},
 link = {http://doi.acm.org/10.1145/2591635.2667179},
 location = {Munich, Germany},
 numpages = {12},
 pages = {299--310},
 publisher = {ACM},
 title = {Synthesizing Transformations for Locality Enhancement of Imperfectly-nested Loop Nests},
 year = {2014}
}


