@inproceedings{Rodrigues:2016:ETS:2935334.2935376,
 abstract = {Touch-enabled devices have a growing variety of screen sizes; however, there is little knowledge on the effect of key size on non-visual text-entry performance. We conducted a user study with 12 blind participants to investigate how non-visual input performance varies with four QWERTY keyboard sizes (ranging from 15mm to 2.5mm). This paper presents an analysis of typing performance and touch behaviors discussing its implications for future research. Our findings show that there is an upper limit to the benefits of larger target sizes between 10mm and 15mm. Input speed decreases from 4.5 to 2.4 words per minute (WPM) for targets sizes below 10mm. The smallest size was deemed unusable by participants even though performance was in par with previous work.},
 acmid = {2935376},
 address = {New York, NY, USA},
 author = {Rodrigues, Andr{\'e} and Nicolau, Hugo and Montague, Kyle and Carri\c{c}o, Lu\'{\i}s and Guerreiro, Tiago},
 booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
 doi = {10.1145/2935334.2935376},
 isbn = {978-1-4503-4408-1},
 keyword = {blind, key size, performance, text-entry, touchscreen},
 link = {http://doi.acm.org/10.1145/2935334.2935376},
 location = {Florence, Italy},
 numpages = {6},
 pages = {47--52},
 publisher = {ACM},
 series = {MobileHCI '16},
 title = {Effect of Target Size on Non-visual Text-entry},
 year = {2016}
}


@inproceedings{Bardot:2016:TVU:2935334.2935342,
 abstract = {Tactile raised-line maps are paper maps widely used by visually impaired people. We designed a mobile technique, based on hand tracking and a smartwatch, in order to leverage pervasive access to virtual maps. We use the smartwatch to render localized text-to-speech and vibratory feedback during hand exploration, but also to provide filtering functions activated by swipe gestures. We conducted a first study to compare the usability of a raised-line map with three virtual maps (plain, with filter, with filter and grid). The results show that virtual maps are usable, and that adding a filter, or a filter and a grid, significantly speeds up data exploration and selection. The results of a following case study showed that visually impaired users were able to achieve a complex task with the device, i.e. finding spatial correlations between two sets of data.},
 acmid = {2935342},
 address = {New York, NY, USA},
 author = {Bardot, Sandra and Serrano, Marcos and Jouffrais, Christophe},
 booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
 doi = {10.1145/2935334.2935342},
 isbn = {978-1-4503-4408-1},
 keyword = {geospatial data, map exploration, visually impaired users, wearable devices},
 link = {http://doi.acm.org/10.1145/2935334.2935342},
 location = {Florence, Italy},
 numpages = {12},
 pages = {100--111},
 publisher = {ACM},
 series = {MobileHCI '16},
 title = {From Tactile to Virtual: Using a Smartwatch to Improve Spatial Map Exploration for Visually Impaired Users},
 year = {2016}
}


@inproceedings{Sankar:2016:SCC:2935334.2935337,
 abstract = {We present an interactive system to capture CAD-like 3D models of indoor scenes, on a mobile device. To overcome sensory and computational limitations of the mobile platform, we employ an in situ, semi-automated approach and harness the user's high-level knowledge of the scene to assist the reconstruction and modeling algorithms. The modeling proceeds in two stages: (1) The user captures the 3D shape and dimensions of the room. (2) The user then uses voice commands and an augmented reality sketching interface to insert objects of interest, such as furniture, artwork, doors and windows. Our system recognizes the sketches and add a corresponding 3D model into the scene at the appropriate location. The key contributions of this work are the design of a multi-modal user interface to effectively capture the user's semantic understanding of the scene and the underlying algorithms that process the input to produce useful reconstructions.},
 acmid = {2935337},
 address = {New York, NY, USA},
 author = {Sankar, Aditya and Seitz, Steven M.},
 booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
 doi = {10.1145/2935334.2935337},
 isbn = {978-1-4503-4408-1},
 keyword = {algorithms, design, human factors, interactive 3D modeling, mobile augmented reality, sketching},
 link = {http://doi.acm.org/10.1145/2935334.2935337},
 location = {Florence, Italy},
 numpages = {11},
 pages = {233--243},
 publisher = {ACM},
 series = {MobileHCI '16},
 title = {In Situ CAD Capture},
 year = {2016}
}


@inproceedings{Henze:2016:STL:2935334.2935381,
 abstract = {Devices with touchscreens have an inherent latency. When a user's finger drags an object across the screen the object follows with a latency of around 100ms for current devices. Previous work showed that latencies down to 25ms reduce users' performance and that even 10ms latency is noticeable. In this paper we demonstrate an approach that reduces latency using a predictive model. Extrapolating the finger's movement we predict where the finger will be in the next moment. Comparing different prediction approaches we show for three different tasks that prediction using neural networks is more precise than linear and polynomial extrapolation. Furthermore, we show through a Fitts' Law dragging experiment that reducing touch latency can significantly increases users' performance. As the approach is software-based it can easily be integrated into existing mobile applications and systems.},
 acmid = {2935381},
 address = {New York, NY, USA},
 author = {Henze, Niels and Funk, Markus and Shirazi, Alireza Sahami},
 booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
 doi = {10.1145/2935334.2935381},
 isbn = {978-1-4503-4408-1},
 keyword = {lag, latency, prediction, touch input, touchscreen},
 link = {http://doi.acm.org/10.1145/2935334.2935381},
 location = {Florence, Italy},
 numpages = {8},
 pages = {434--441},
 publisher = {ACM},
 series = {MobileHCI '16},
 title = {Software-reduced Touchscreen Latency},
 year = {2016}
}


@inproceedings{Corbett:2016:ISA:2935334.2935386,
 abstract = {Voice interactions on mobile phones are most often used to augment or supplement touch based interactions for users' convenience. However, for people with limited hand dexterity caused by various forms of motor-impairments voice interactions can have a significant impact and in some cases even enable independent interaction with a mobile device for the first time. For these users, a Mobile Voice User Interface (M-VUI), which allows for completely hands-free, voice only interaction would provide a high level of accessibility and independence. Implementing such a system requires research to address long standing usability challenges introduced by voice interactions that negatively affect user experience due to difficulty learning and discovering voice commands. In this paper we address these concerns reporting on research conducted to improve the visibility and learnability of voice commands of a M-VUI application being developed on the Android platform. Our research confirmed long standing challenges with voice interactions while exploring several methods for improving the onboarding and learning experience. Based on our findings we offer a set of implications for the design of M-VUIs.},
 acmid = {2935386},
 address = {New York, NY, USA},
 author = {Corbett, Eric and Weber, Astrid},
 booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
 doi = {10.1145/2935334.2935386},
 isbn = {978-1-4503-4408-1},
 keyword = {accessibility, universal voice control, voice user interfaces},
 link = {http://doi.acm.org/10.1145/2935334.2935386},
 location = {Florence, Italy},
 numpages = {11},
 pages = {72--82},
 publisher = {ACM},
 series = {MobileHCI '16},
 title = {What Can I Say?: Addressing User Experience Challenges of a Mobile Voice User Interface for Accessibility},
 year = {2016}
}


@inproceedings{Shao:2016:SSK:2935334.2935336,
 abstract = {The rise of smartwatches calls for efficient, convenient and suitable text input methods for these small computers. The minuscule size of these screens brings many challenges on how to interact with these devices. Keyboard design requires optimization for these small screens to provide a good user experience and fast text entry method on these devices. We introduce SwipeKey, a text input method that uses swipe directions to allow multiple inputs per button and thus allows for an increase in the effective size of input buttons. We have conducted thorough experiments optimizing SwipeKey to create a fast, low-error, and easy to learn soft keyboard for smartwatches. These benefits result from having a keyboard that emphasizes the use of swipe motions. Our user study results show that with a specific combination of swipe directions and corresponding button size, SwipeKey users achieved a speed of 11 in words per minute (WPM), a 53% improvement from baseline (7.2 in WPM) and dramatically decreased character error rate (CER) from the baseline of 10% down to 3.4%.},
 acmid = {2935336},
 address = {New York, NY, USA},
 author = {Shao, Yuan-Fu and Chang-Ogimoto, Masatoshi and Pointner, Reinhard and Lin, Yu-Chih and Wu, Chen-Ting and Chen, Mike},
 booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
 doi = {10.1145/2935334.2935336},
 isbn = {978-1-4503-4408-1},
 keyword = {SwipeKey, input, keyboard design, smartwatch, text entry},
 link = {http://doi.acm.org/10.1145/2935334.2935336},
 location = {Florence, Italy},
 numpages = {12},
 pages = {60--71},
 publisher = {ACM},
 series = {MobileHCI '16},
 title = {SwipeKey: A Swipe-based Keyboard Design for Smartwatches},
 year = {2016}
}


@inproceedings{Micallef:2016:TEA:2935334.2935338,
 abstract = {A majority of Stroke survivors have an arm impairment (up to 80%), which persists over the long term (>12 months). Physiotherapy experts believe that a rehabilitation Aide-Memoire could help these patients [25]. Hence, we designed, with the input of physiotherapists, Stroke experts and former Stroke patients, the Aide-Memoire Stroke (AIMS) App to help them remember to exercise more frequently. We evaluated its use in a controlled field evaluation on a smartphone, tablet and smartwatch. Since one of the main features of the app is to remind Stroke survivors to exercise we also investigated reminder modalities (i.e., visual, vibrate, audio, speech). One key finding is that Stroke survivors opted for a combination of modalities to remind them to conduct their exercises. Also, Stroke survivors seem to prefer smartphones compared to other mobile devices due to their ease of use, usability, familiarity and being easier to handle with one arm.},
 acmid = {2935338},
 address = {New York, NY, USA},
 author = {Micallef, Nicholas and Baillie, Lynne and Uzor, Stephen},
 booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
 doi = {10.1145/2935334.2935338},
 isbn = {978-1-4503-4408-1},
 keyword = {exercise app, mobile devices, reminder modalities, stroke, user design, user studies, wearables},
 link = {http://doi.acm.org/10.1145/2935334.2935338},
 location = {Florence, Italy},
 numpages = {12},
 pages = {112--123},
 publisher = {ACM},
 series = {MobileHCI '16},
 title = {Time to Exercise!: An Aide-memoire Stroke App for Post-stroke Arm Rehabilitation},
 year = {2016}
}


@inproceedings{Guo:2016:ETN:2935334.2935345,
 abstract = {Because smartwatches are worn on the wrist, they do not require users to hold the device, leaving at least one hand free to engage in other activities. Unfortunately, this benefit is thwarted by the typical interaction model of smartwatches; for interactions beyond glancing at information or using speech, users must utilize their other hand to manipulate a touchscreen and/or hardware buttons. In order to enable no-touch, wrist-only smartwatch interactions so that users can, for example, hold a cup of coffee while controlling their device, we explore two tilt-based interaction techniques for menu selection and navigation: AnglePoint, which directly maps the position of a virtual pointer to the tilt angle of the smartwatch, and ObjectPoint, which objectifies the underlying virtual pointer as an object imbued with a physics model. In a user study, we found that participants were able to perform menu selection and continuous selection of menu items as well as navigation through a menu hierarchy more quickly and accurately with ObjectPoint, even though previous research on tilt for other mobile devices suggested that AnglePoint would be more effective. We provide an explanation of our results and discuss the implications for more "hands-free" smartwatch interactions.},
 acmid = {2935345},
 address = {New York, NY, USA},
 author = {Guo, Anhong and Paek, Tim},
 booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
 doi = {10.1145/2935334.2935345},
 isbn = {978-1-4503-4408-1},
 keyword = {IMU sensors, interaction techniques, mobile sensing, smartwatch, tilt-based interaction, wearable devices},
 link = {http://doi.acm.org/10.1145/2935334.2935345},
 location = {Florence, Italy},
 numpages = {12},
 pages = {17--28},
 publisher = {ACM},
 series = {MobileHCI '16},
 title = {Exploring Tilt for No-touch, Wrist-only Interactions on Smartwatches},
 year = {2016}
}


@inproceedings{Rosales:2016:SAO:2935334.2935363,
 abstract = {It is well-documented that ICT are designed mostly with young users in mind. In addition, most studies about smartphone use do not include older people or even consider age differences. Consequently, little is known about how to design smartphone apps taking older people's interests into account. We have used a mixed-method approach with an intergenerational perspective to approach this topic. First, we track the smartphone activities of 238 panelists. Second, we conduct an online survey (382 respondents). Third, we document the experiences of a group of older people in a smartphone learning club. We have found specific media consumption and communication patterns among older individuals: for example, at home they are more prone to jumping between devices for ergonomic reasons, thus, cross-device interactions are key for this group. We discuss the relevance of intergenerational studies in counterbalancing the spread of age stereotypes and identifying alternative adoption trends.},
 acmid = {2935363},
 address = {New York, NY, USA},
 author = {Rosales, Andrea and Fern\'{a}ndez-Ard\`{e}vol, Mireia},
 booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
 doi = {10.1145/2935334.2935363},
 isbn = {978-1-4503-4408-1},
 keyword = {age stereotypes, ageism, cross-device, log data, mixed-methods, older people, smartphone, tracking, use},
 link = {http://doi.acm.org/10.1145/2935334.2935363},
 location = {Florence, Italy},
 numpages = {13},
 pages = {491--503},
 publisher = {ACM},
 series = {MobileHCI '16},
 title = {Smartphones, Apps and Older People's Interests: From a Generational Perspective},
 year = {2016}
}


@inproceedings{Vatavu:2016:DVU:2935334.2935364,
 abstract = {We investigate in this work users' perceptions of interacting with invisible, zero-weight digital matter for smart mobile scenarios. To this end, we introduce the concept of a digital vibron as vibrational manifestation of a digital object located outside its container device. We exemplify gesture-based interactions for digital vibrons and show how thinking about interactions in terms of digital vibrons can lead to new interactive experiences in the physical-digital space. We present the results of a user study that showed high scores of users' perceived experience, usability, and desirability, and we discuss users' preferences for vibration patterns to inform the design of vibrotactile feedback for digital vibrons. We hope that this work will inspire researchers and practitioners to further explore and develop digital vibrons to design localized vibrotactile feedback for digital objects outside their smart devices toward new interactive experiences in the physical-digital space.},
 acmid = {2935364},
 address = {New York, NY, USA},
 author = {Vatavu, Radu-Daniel and Mossel, Annette and Sch\"{o}nauer, Christian},
 booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
 doi = {10.1145/2935334.2935364},
 isbn = {978-1-4503-4408-1},
 keyword = {digital matter, elicitation study, evaluation, feedback, gestures, smart device, touch, user study, vibrons, vibrotactile},
 link = {http://doi.acm.org/10.1145/2935334.2935364},
 location = {Florence, Italy},
 numpages = {10},
 pages = {217--226},
 publisher = {ACM},
 series = {MobileHCI '16},
 title = {Digital Vibrons: Understanding Users' Perceptions of Interacting with Invisible, Zero-weight Matter},
 year = {2016}
}


@inproceedings{Hsiu:2016:NSF:2935334.2935362,
 abstract = {Force sensing has been widely used for bringing the touch from binary to multiple states, creating new abilities on surface interactions. However, prior proposed force sensing techniques mainly focus on enabling force-applied gestures on certain devices. This paper presents Nail+, a technique using fingernail deformation to enable force touch sensing interactions on everyday rigid surfaces. Our prototype, 3x3 0.2mm strain sensor array mounted on a fingernail, was implemented and conducted with a 12-participant study for evaluating the feasibility of this sensing approach. Result showed that the accuracy for sensing normal and force-applied tapping and swiping can achieve 84.67% on average. We finally proposed two example applications using Nail+ prototype for controlling the interfaces of head-mounted display (HMD) devices and remote screens.},
 acmid = {2935362},
 address = {New York, NY, USA},
 author = {Hsiu, Min-Chieh and Wang, Chiuan and Huang, Da-Yuan and Lin, Jhe-Wei and Lin, Yu-Chih and Yang, De-Nian and Hung, Yi-ping and Chen, Mike},
 booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
 doi = {10.1145/2935334.2935362},
 isbn = {978-1-4503-4408-1},
 keyword = {fingernail, force sensing, nail deformation, natural user interface (NUI), strain gauges},
 link = {http://doi.acm.org/10.1145/2935334.2935362},
 location = {Florence, Italy},
 numpages = {6},
 pages = {1--6},
 publisher = {ACM},
 series = {MobileHCI '16},
 title = {Nail+: Sensing Fingernail Deformation to Detect Finger Force Touch Interactions on Rigid Surfaces},
 year = {2016}
}


@inproceedings{Ahmetovic:2016:NNC:2935334.2935361,
 abstract = {Turn-by-turn navigation is a useful paradigm for assisting people with visual impairments during mobility as it reduces the cognitive load of having to simultaneously sense, localize and plan. To realize such a system, it is necessary to be able to automatically localize the user with sufficient accuracy, provide timely and efficient instructions and have the ability to easily deploy the system to new spaces. We propose a smartphone-based system that provides turn-by-turn navigation assistance based on accurate real-time localization over large spaces. In addition to basic navigation capabilities, our system also informs the user about nearby points-of-interest (POI) and accessibility issues (e.g., stairs ahead). After deploying the system on a university campus across several indoor and outdoor areas, we evaluated it with six blind subjects and showed that our system is capable of guiding visually impaired users in complex and unfamiliar environments.},
 acmid = {2935361},
 address = {New York, NY, USA},
 author = {Ahmetovic, Dragan and Gleason, Cole and Ruan, Chengxiong and Kitani, Kris and Takagi, Hironobu and Asakawa, Chieko},
 booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
 doi = {10.1145/2935334.2935361},
 isbn = {978-1-4503-4408-1},
 keyword = {assistive technologies, bluetooth low-energy beacons, localization, navigation assistance, real world accessibility, turn-by-turn navigation, visual impairments},
 link = {http://doi.acm.org/10.1145/2935334.2935361},
 location = {Florence, Italy},
 numpages = {10},
 pages = {90--99},
 publisher = {ACM},
 series = {MobileHCI '16},
 title = {NavCog: A Navigational Cognitive Assistant for the Blind},
 year = {2016}
}


@inproceedings{Fitzpatrick:2016:PPP:2935334.2935369,
 abstract = {Social media platforms and mobile applications increasingly include geographic features and services. While previous research has looked into how people perceive, interpret, and act on information available about a person, the spatial self, an individual's display of mobility through space for identity performance, is underexplored, especially in encounters with strangers. Strangers themselves offer a unique potential for exploring relational contexts and how those may relate to interpreting and reacting to the spatial self. We ran a 3 (map: personal, social, and task) x 3 (relationship: date, friend, coworker) x 2 (gender of participant: female, male) laboratory experiment with a mixed model design to see if and how the spatial self affects interest in future interaction. We find that maps, relationship, and gender all affect the ways in which people interpret and act on expressing interest in an individual. We discuss theoretical and design implications of how spatial selves affect this process.},
 acmid = {2935369},
 address = {New York, NY, USA},
 author = {Fitzpatrick, Colin and Birnholtz, Jeremy and Gergle, Darren},
 booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
 doi = {10.1145/2935334.2935369},
 isbn = {978-1-4503-4408-1},
 keyword = {check-ins, experiment, impression formation, location, logistic regression, relationships, spatial self},
 link = {http://doi.acm.org/10.1145/2935334.2935369},
 location = {Florence, Italy},
 numpages = {11},
 pages = {295--305},
 publisher = {ACM},
 series = {MobileHCI '16},
 title = {People, Places, and Perceptions: Effects of Location Check-in Awareness on Impressions of Strangers},
 year = {2016}
}


@inproceedings{Matviienko:2016:NIA:2935334.2935359,
 abstract = {Car navigation systems typically combine multiple output modalities; for example, GPS-based navigation aids show a real-time map, or feature spoken prompts indicating upcoming maneuvers. However, the drawback of graphical navigation displays is that drivers have to explicitly glance at them, which can distract from a situation on the road. To decrease driver distraction while driving with a navigation system, we explore the use of ambient light as a navigation aid in the car, in order to shift navigation aids to the periphery of human attention. We investigated this by conducting studies in a driving simulator, where we found that drivers spent significantly less time glancing at the ambient light navigation aid than on a GUI navigation display. Moreover, ambient light-based navigation was perceived to be easy to use and understand, and preferred over traditional GUI navigation displays. We discuss the implications of these outcomes on automotive personal navigation devices.},
 acmid = {2935359},
 address = {New York, NY, USA},
 author = {Matviienko, Andrii and L\"{o}cken, Andreas and El Ali, Abdallah and Heuten, Wilko and Boll, Susanne},
 booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
 doi = {10.1145/2935334.2935359},
 isbn = {978-1-4503-4408-1},
 keyword = {ambient displays, light-based navigation, navigation, peripheral visualization},
 link = {http://doi.acm.org/10.1145/2935334.2935359},
 location = {Florence, Italy},
 numpages = {12},
 pages = {283--294},
 publisher = {ACM},
 series = {MobileHCI '16},
 title = {NaviLight: Investigating Ambient Light Displays for Turn-by-turn Navigation in Cars},
 year = {2016}
}


@inproceedings{Das:2016:EFP:2935334.2935349,
 abstract = {Effective use of personal data is a core utility of modern smartphones. On Android, several challenges make developing compelling personal data applications difficult. First, personal data is stored in isolated silos. Thus, relationships between data from different providers are missing, data must be queried by source of origin rather than meaning and the persistence of different types of data differ greatly. Second, interfaces to these data are inconsistent and complex. In turn, developers are forced to interleave SQL with Java boilerplate, resulting in error-prone code that does not generalize. Our solution is Epistenet: a toolkit that (1) unifies the storage and treatment of mobile personal data; (2) preserves relationships between disparate data; (3) allows for expressive queries based on the meaning of data rather than its source of origin (e.g., one can query for all communications with John while at the park); and, (4) provides a simple, native query interface to facilitate development.},
 acmid = {2935349},
 address = {New York, NY, USA},
 author = {Das, Sauvik and Wiese, Jason and Hong, Jason I.},
 booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
 doi = {10.1145/2935334.2935349},
 isbn = {978-1-4503-4408-1},
 link = {http://doi.acm.org/10.1145/2935334.2935349},
 location = {Florence, Italy},
 numpages = {10},
 pages = {244--253},
 publisher = {ACM},
 series = {MobileHCI '16},
 title = {Epistenet: Facilitating Programmatic Access \&\#38; Processing of Semantically Related Mobile Personal Data},
 year = {2016}
}


@inproceedings{Saidi:2016:IES:2935334.2935341,
 abstract = {While several techniques offer more than one detailed view in Overview+Detail (O+D) interfaces, the optimal number of detailed views has not been investigated. But the answer is not trivial: using a single detailed view offers a larger display size but only allows a sequential exploration of the overview; using several detailed views reduces the size of each view but allows a parallel exploration of the overview. In this paper we investigate the benefits of splitting the detailed view in O+D interfaces for working with very large graphs. We implemented an O+D interface where the overview is displayed on a large screen while 1, 2 or 4 split views are displayed on a tactile tablet. We experimentally evaluated the effect of the number of split views according to the number of nodes to connect. Using 4 split views is better than 1 and 2 for working on more than 2 nodes.},
 acmid = {2935341},
 address = {New York, NY, USA},
 author = {Saidi, Houssem and Serrano, Marcos and Dubois, Emmanuel},
 booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
 doi = {10.1145/2935334.2935341},
 isbn = {978-1-4503-4408-1},
 keyword = {graph, interaction techniques, multi-device, multi-surface, multi-view, overview and detail},
 link = {http://doi.acm.org/10.1145/2935334.2935341},
 location = {Florence, Italy},
 numpages = {5},
 pages = {180--184},
 publisher = {ACM},
 series = {MobileHCI '16},
 title = {Investigating the Effects of Splitting Detailed Views in Overview+Detail Interfaces},
 year = {2016}
}


@inproceedings{Hernandez:2016:WED:2935334.2935340,
 abstract = {The Experience Sampling Method is widely used for collecting self-report responses from people in natural settings. While most traditional approaches rely on using a phone to trigger prompts and record information, wearable devices now offer new opportunities that may improve this method. This research quantitatively and qualitatively studies the experience sampling process on head-worn and wrist-worn wearable devices, and compares them to the traditional "smartphone in the pocket." To enable this work, we designed and implemented a custom application to provide similar prompts across the three types of devices and evaluated it with 15 individuals for five days (75 days total), in the context of real-life stress measurement. We found significant differences in response times across devices, and captured tradeoffs in interaction types, screen size, and device familiarity that can affect both users' experience and the reports made by users.},
 acmid = {2935340},
 address = {New York, NY, USA},
 author = {Hernandez, Javier and McDuff, Daniel and Infante, Christian and Maes, Pattie and Quigley, Karen and Picard, Rosalind},
 booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
 doi = {10.1145/2935334.2935340},
 isbn = {978-1-4503-4408-1},
 keyword = {Google Glass, ecological momentary assessment, experience sampling method, smart-eyewear, smartphone, smartwatch, wearable devices},
 link = {http://doi.acm.org/10.1145/2935334.2935340},
 location = {Florence, Italy},
 numpages = {11},
 pages = {195--205},
 publisher = {ACM},
 series = {MobileHCI '16},
 title = {Wearable ESM: Differences in the Experience Sampling Method Across Wearable Devices},
 year = {2016}
}


@inproceedings{Paay:2016:DAY:2935334.2935378,
 abstract = {Discovering activities in the city around you can be difficult with traditional search engines unless you know what you are looking for. Searching for inspiration on things to do requires a more open-ended and explorative approach. We introduce transitory search as a dynamic way of uncovering information about activities in the city around you that allows the user to start from a vague idea of what they are interested in, and iteratively modify their search using slider continuums to discover best-fit results. We present the design of a smartphone app exemplifying the idea of transitory search and give results from a lab evaluation and a 4-week field deployment involving 15 people in two different cities. Our findings indicate that transitory search on a mobile device both supports discovering activities in the city and more interestingly helps users reflect on and shape their preferences in situ. We also found that ambiguous slider continuums work well as people happily form and refine individual interpretations of them.},
 acmid = {2935378},
 address = {New York, NY, USA},
 author = {Paay, Jeni and Kjeldskov, Jesper and Skov, Mikael B. and Nielsen, Per M. and Pearce, Jon},
 booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
 doi = {10.1145/2935334.2935378},
 isbn = {978-1-4503-4408-1},
 keyword = {event finding, explorative search, sliders, transitory search},
 link = {http://doi.acm.org/10.1145/2935334.2935378},
 location = {Florence, Italy},
 numpages = {7},
 pages = {387--393},
 publisher = {ACM},
 series = {MobileHCI '16},
 title = {Discovering Activities in Your City Using Transitory Search},
 year = {2016}
}


@inproceedings{Fechner:2016:FCE:2935334.2935379,
 abstract = {A key problem in the area of citizen engagement is to make people aware of opportunities to participate and to motivate them to take action. We propose an approach that uses geofences and proactive notifications on mobile devices to raise citizen awareness of engagement opportunities in situ and to trigger the exploration of these opportunities. Notifications are automatically triggered in the near vicinity of engagement opportunities based on space, time, and user preferences. We conducted two user studies to investigate our approach. A field-based study revealed specific usage patterns and motivational aspects of the situated discovery of engagement opportunities. A lab-based comparison study investigated the pragmatic and hedonistic qualities of our application. Results indicate that users prefer to be informed in situ even when they do not necessarily interact with notifications straight away.},
 acmid = {2935379},
 address = {New York, NY, USA},
 author = {Fechner, Thore and Schlarmann, Dominik and Kray, Christian},
 booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
 doi = {10.1145/2935334.2935379},
 isbn = {978-1-4503-4408-1},
 keyword = {citizen engagement, geofencing, location-based service, notification},
 link = {http://doi.acm.org/10.1145/2935334.2935379},
 location = {Florence, Italy},
 numpages = {12},
 pages = {353--364},
 publisher = {ACM},
 series = {MobileHCI '16},
 title = {Facilitating Citizen Engagement in Situ: Assessing the Impact of Pro-active Geofenced Notifications},
 year = {2016}
}


@inproceedings{Hsieh:2016:NES:2935334.2935358,
 abstract = {This paper investigates the feasibility of using a nail-mounted array of tactors, NailTactors, as an eyes-free output device. By rim-attached eccentric-rotating-mass (ERM) vibrators to artificial nails, miniature high-resolution tactile displays were realized as an eyes-free output device. To understand how to deliver rich signals to users for valid signal perception, three user studies were conducted. The results suggest that users can not only recognized absolute and relative directional cues, but also recognized numerical characters in EdgeWrite format with an overall 89% recognition rate. Experiments also identified the optimal placement of ERM actuators for maximizing information transfer.},
 acmid = {2935358},
 address = {New York, NY, USA},
 author = {Hsieh, Meng-Ju and Liang, Rong-Hao and Chen, Bing-Yu},
 booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
 doi = {10.1145/2935334.2935358},
 isbn = {978-1-4503-4408-1},
 keyword = {always-available output, nail-mounted device, tactor array},
 link = {http://doi.acm.org/10.1145/2935334.2935358},
 location = {Florence, Italy},
 numpages = {6},
 pages = {29--34},
 publisher = {ACM},
 series = {MobileHCI '16},
 title = {NailTactors: Eyes-free Spatial Output Using a Nail-mounted Tactor Array},
 year = {2016}
}


@inproceedings{Heller:2016:WEC:2935334.2935365,
 abstract = {Mobile audio augmented reality systems (MAARS) simulate virtual audio sources in a physical space via headphones. While 20 years ago, these required expensive sensing and rendering equipment, the necessary technology has become widely available. Smartphones have become capable to run high-fidelity spatial audio rendering algorithms, and modern sensors can provide rich data to the rendering process. Combined, these constitute an inexpensive, powerful platform for audio augmented reality. We evaluated the practical limitations of currently available off-the-shelf hardware using a voice sample in a lab experiment. State of the art motion sensors provide multiple degrees of freedom, including pitch and roll angles instead of yaw only. Since our rendering algorithm is also capable of including this richer sensor data in terms of source elevation, we also measured its impact on sound localization. Results show that mobile audio augmented reality systems achieve the same horizontal resolution as stationary systems. We found that including pitch and roll angles did not significantly improve the users' localization performance.},
 acmid = {2935365},
 address = {New York, NY, USA},
 author = {Heller, Florian and Jevanesan, Jayan and Dietrich, Pascal and Borchers, Jan},
 booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
 doi = {10.1145/2935334.2935365},
 isbn = {978-1-4503-4408-1},
 keyword = {audio augmented reality, mobile devices, navigation, spatial audio, virtual audio spaces},
 link = {http://doi.acm.org/10.1145/2935334.2935365},
 location = {Florence, Italy},
 numpages = {5},
 pages = {278--282},
 publisher = {ACM},
 series = {MobileHCI '16},
 title = {Where Are We?: Evaluating the Current Rendering Fidelity of Mobile Audio Augmented Reality Systems},
 year = {2016}
}


@proceedings{Paterno:2016:2935334,
 abstract = {MobileHCI brings together people from diverse backgrounds and areas of expertise to provide a truly multidisciplinary forum. Academics, hardware and software developers, designers and practitioners alike can discuss challenges encountered on different frontiers of mobility, as well as potential solutions that will advance the field. The conference covers both academic and industry research, ranging from fundamental interaction models and techniques to social and cultural aspects of everyday life with mobile devices and services.},
 address = {New York, NY, USA},
 isbn = {978-1-4503-4408-1},
 location = {Florence, Italy},
 publisher = {ACM},
 title = {MobileHCI '16: Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
 year = {2016}
}


@inproceedings{Grussenmeyer:2016:FUH:2935334.2935367,
 abstract = {In order to navigate through the world, people who are blind and visually impaired typically use maps through either textual directions or tactile printouts. However, visual maps on a touchscreen are not accessible to this population. Two prototypes were designed to test users' ability to trace graphical lines and directions through maps on a touchscreen using haptic feedback from an Android smart watch and tablet. With the first prototype, we show that blind and visually impaired users had lower threshold than sighted users for determining the distance between two lines on a touchscreen, suggesting their enhanced ability to form representations of spatial distance from tactile vibrational cues. With the second prototype, we show that it is feasible for blind and visually impaired users to follow directions through graphical maps based on vibrational cues. We believe these results show that our prototypes have the potential to be effective in real-world applications.},
 acmid = {2935367},
 address = {New York, NY, USA},
 author = {Grussenmeyer, William and Garcia, Jesel and Jiang, Fang},
 booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
 doi = {10.1145/2935334.2935367},
 isbn = {978-1-4503-4408-1},
 keyword = {accessibility, blind, haptic feedback, touchscreen, visually impaired, wearable computing},
 link = {http://doi.acm.org/10.1145/2935334.2935367},
 location = {Florence, Italy},
 numpages = {7},
 pages = {83--89},
 publisher = {ACM},
 series = {MobileHCI '16},
 title = {Feasibility of Using Haptic Directions Through Maps with a Tablet and Smart Watch for People Who Are Blind and Visually Impaired},
 year = {2016}
}


@proceedings{Paterno:2016:2957265,
 abstract = {MobileHCI brings together people from diverse backgrounds and areas of expertise to provide a truly multidisciplinary forum. Academics, hardware and software developers, designers and practitioners alike can discuss challenges encountered on different frontiers of mobility, as well as potential solutions that will advance the field. The conference covers both academic and industry research, ranging from fundamental interaction models and techniques to social and cultural aspects of everyday life with mobile technologies.},
 address = {New York, NY, USA},
 isbn = {978-1-4503-4413-5},
 location = {Florence, Italy},
 publisher = {ACM},
 title = {MobileHCI '16: Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
 year = {2016}
}


@inproceedings{ElAli:2016:TLP:2935334.2935352,
 abstract = {While HCI for development (HCI4D) research has typically focused on technological practices of poor and low-literate communities, little research has addressed how technology literate individuals living in a poor infrastructure environment use technology. Our work fills this gap by focusing on Lebanon, a country with longstanding political instability, and the wayfinding issues there stemming from missing street signs and names, a poor road infrastructure, and a non-standardized addressing system. We examine the relationship between technology literate individuals' navigation and direction giving strategies and their usage of current digital navigation aids. Drawing on an interview study (N=12) and a web survey (N=85), our findings show that while these individuals rely on mapping services and WhatsApp's share location feature to aid wayfinding, many technical and cultural problems persist that are currently resolved through social querying. We discuss our results in light of problems that any map user encounters in poor infrastructure environments.},
 acmid = {2935352},
 address = {New York, NY, USA},
 author = {El Ali, Abdallah and Bachour, Khaled and Heuten, Wilko and Boll, Susanne},
 booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
 doi = {10.1145/2935334.2935352},
 isbn = {978-1-4503-4408-1},
 keyword = {HCI4D, ICT4D, Lebanon, addressing, giving directions, mapping services, mobile, navigation, strategies, wayfinding},
 link = {http://doi.acm.org/10.1145/2935334.2935352},
 location = {Florence, Italy},
 numpages = {12},
 pages = {266--277},
 publisher = {ACM},
 series = {MobileHCI '16},
 title = {Technology Literacy in Poor Infrastructure Environments: Characterizing Wayfinding Strategies in Lebanon},
 year = {2016}
}


@inproceedings{Franjcic:2016:WIM:2935334.2935374,
 abstract = {Motion tracking systems are gaining popularity and have a number of applications in research, entertainment, and arts. These systems must be calibrated before use. This process requires extensive user effort to determine a 3D coordinate system with acceptable accuracy. Usually, this is achieved by rapidly manipulating a calibration device (e.g. a calibration wand) in a volume for a set amount of time. While this is a complex spatial input task, improving the user experience of calibration inspired little research. This paper presents the design, implementation, and evaluation of WAVI --- a prototype device mounted on a calibration wand to jointly provide visual and tactile feedback during the calibration process. We conducted a user study that showed that the device significantly increases calibration quality without increasing user effort. Based on our experiences with WAVI, we present new insights for improving motion tracking calibration and complex spatial input.},
 acmid = {2935374},
 address = {New York, NY, USA},
 author = {Franjcic, Zlatko and Wo\'{z}niak, Pawe\l W. and Kasparavi\v{c}i\={u}te, Gabriele and Fjeld, Morten},
 booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
 doi = {10.1145/2935334.2935374},
 isbn = {978-1-4503-4408-1},
 keyword = {calibration, motion tracking, prototyping, spatial input},
 link = {http://doi.acm.org/10.1145/2935334.2935374},
 location = {Florence, Italy},
 numpages = {12},
 pages = {254--265},
 publisher = {ACM},
 series = {MobileHCI '16},
 title = {WAVI: Improving Motion Capture Calibration Using Haptic and Visual Feedback},
 year = {2016}
}


@inproceedings{Tomlinson:2016:TWI:2935334.2935390,
 abstract = {As ubiquitous as weather is in our daily lives, individuals with vision impairments endure poorly designed user experiences when attempting to check the weather on their mobile devices. This is primarily caused by a mismatch between the visually based information layout on screen and the order in which a screen reader, such as TalkBack or VoiceOver, presents the information to users with visual impairments. Additionally, any image or icon included on the screen presents no information to the user if they are not able to see it. Therefore we created the Accessible Weather App to run on Android and integrate with the TalkBack accessibility feature that is already available on the operating system. We also included a set of auditory weather icons which use sound, rather than visuals, to convey current weather conditions to users in a fast and pleasant way. This paper discusses the process for determining what features the users' would want and require, as well as our methodology for evaluating the beta version of our app.},
 acmid = {2935390},
 address = {New York, NY, USA},
 author = {Tomlinson, Brianna J. and Schuett, Jonathan H. and Shortridge, Woodbury and Chandran, Jehoshaph and Walker, Bruce N.},
 booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
 doi = {10.1145/2935334.2935390},
 isbn = {978-1-4503-4408-1},
 keyword = {VoiceOver, accessibility, app, assistive technology, blind, sonification, talkback, visually impaired, weather},
 link = {http://doi.acm.org/10.1145/2935334.2935390},
 location = {Florence, Italy},
 numpages = {10},
 pages = {377--386},
 publisher = {ACM},
 series = {MobileHCI '16},
 title = {Talkin' About the Weather: Incorporating TalkBack Functionality and Sonifications for Accessible App Design},
 year = {2016}
}


@inproceedings{Akhadov:2016:MBR:2935334.2935372,
 abstract = {With current digital cameras and smartphones, taking photos and videos has never been easier. However, it is still difficult to take a photo of a brief action at the right time. In addition, editing captured videos, such as modifying the playback speed of some parts of a video, remains a time consuming task. In this work we investigate how the motion sensors embedded in mobile devices, such as smartphones, can facilitate camera control. In particular, we show two families of applications: automatic camera trigger control for jump photos and automatic playback speed control (video speed ramping) for action videos. Our approach uses joint devices: a remote camera takes a photo or a video of the scene and it is controlled by the motion sensor of a mobile device, either during or after recording. This allows casual users to achieve visually appealing effects with little effort, even for self portraits.},
 acmid = {2935372},
 address = {New York, NY, USA},
 author = {Akhadov, Sabir and Lancelle, Marcel and Bazin, Jean-Charles and Gross, Markus},
 booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
 doi = {10.1145/2935334.2935372},
 isbn = {978-1-4503-4408-1},
 keyword = {camera trigger, jump photo, motion sensor, remote control, video speed ramping},
 link = {http://doi.acm.org/10.1145/2935334.2935372},
 location = {Florence, Italy},
 numpages = {6},
 pages = {428--433},
 publisher = {ACM},
 series = {MobileHCI '16},
 title = {Motion Based Remote Camera Control with Mobile Devices},
 year = {2016}
}


@inproceedings{Oppermann:2016:PAE:2935334.2935368,
 abstract = {This paper reports on a study of AREEF, a multi-player Underwater Augmented Reality (UWAR) experience for swimming pools. Using off-the-shelf components combined with a custom made waterproof case and an innovative game concept, AREEF puts computer game technology to use for recreational and educational purposes in and under water. After an experience overview, we present evidence gained from a user-centred design-process including a pilot study with 3 kids and a final evaluation with 36 kids. Our discussion covers technical findings regarding marker placement, tracking, and device handling, as well as design related issues like virtual object placement and the need for extremely obvious user interaction and feedback when staging a mobile underwater experience.},
 acmid = {2935368},
 address = {New York, NY, USA},
 author = {Oppermann, Leif and Blum, Lisa and Shekow, Marius},
 booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
 doi = {10.1145/2935334.2935368},
 isbn = {978-1-4503-4408-1},
 keyword = {augmented reality, exertion, games, mobile, underwater, virtual environments},
 link = {http://doi.acm.org/10.1145/2935334.2935368},
 location = {Florence, Italy},
 numpages = {11},
 pages = {330--340},
 publisher = {ACM},
 series = {MobileHCI '16},
 title = {Playing on AREEF: Evaluation of an Underwater Augmented Reality Game for Kids},
 year = {2016}
}


@inproceedings{Hakobyan:2016:LEA:2935334.2935356,
 abstract = {Ongoing advances in technology are increasing the scope for enhancing and supporting older adults' daily living. The digital divide between older and younger adults raises concerns, however, about the suitability of technological solutions for older adults, especially for those with impairments. Taking older adults with Age-Related Macular Degeneration (AMD) as a case study, we used user-centred and participatory design approaches to develop an assistive mobile app for self-monitoring their intake of food [12,13]. In this paper we report on findings of a longitudinal field evaluation of our app that was conducted to investigate how it was received and adopted by older adults with AMD and its impact on their lives. Demonstrating the benefit of applying inclusive design methods for technology for older adults, our findings reveal how the use of the app raises participants' awareness and facilitates self-monitoring of diet, encourages positive (diet) behaviour change, and encourages learning.},
 acmid = {2935356},
 address = {New York, NY, USA},
 author = {Hakobyan, Lilit and Lumsden, Jo and Shaw, Rachel and O'Sullivan, Dympna},
 booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
 doi = {10.1145/2935334.2935356},
 isbn = {978-1-4503-4408-1},
 keyword = {age-related macular degeneration (AMD), assistive technology, diet diary, health behaviour change, mobile apps, older adults, user-centred design (UCD)},
 link = {http://doi.acm.org/10.1145/2935334.2935356},
 location = {Florence, Italy},
 numpages = {11},
 pages = {124--134},
 publisher = {ACM},
 series = {MobileHCI '16},
 title = {A Longitudinal Evaluation of the Acceptability and Impact of a Diet Diary App for Older Adults with Age-related Macular Degeneration},
 year = {2016}
}


@inproceedings{Wiese:2016:SYN:2935334.2935388,
 abstract = {Friends, family and colleagues at work may repeatedly observe how their peers unlock their smartphones. These "insiders" may combine multiple partial observations to form a hypothesis of a target's secret. This changing landscape requires that we update the methods used to assess the security of unlocking mechanisms against human shoulder surfing attacks. In our paper, we introduce a methodology to study shoulder surfing risks in the insider threat model. Our methodology dissects the authentication process into minimal observations by humans. Further processing is based on simulations. The outcome is an estimate of the number of observations needed to break a mechanism. The flexibility of this approach benefits the design of new mechanisms. We demonstrate the application of our methodology by performing an analysis of the SwiPIN scheme published at CHI 2015. Our results indicate that SwiPIN can be defeated reliably by a majority of the population with as few as 6 to 11 observations.},
 acmid = {2935388},
 address = {New York, NY, USA},
 author = {Wiese, Oliver and Roth, Volker},
 booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
 doi = {10.1145/2935334.2935388},
 isbn = {978-1-4503-4408-1},
 keyword = {authentication, insider threats, mobile devices, shoulder surfing},
 link = {http://doi.acm.org/10.1145/2935334.2935388},
 location = {Florence, Italy},
 numpages = {12},
 pages = {453--464},
 publisher = {ACM},
 series = {MobileHCI '16},
 title = {See You Next Time: A Model for Modern Shoulder Surfers},
 year = {2016}
}


@inproceedings{Nasim:2016:UCL:2935334.2935350,
 abstract = {In this measurement study, we analyze whether mobile phone users exhibit temporal regularity in their mobile communication. To this end, we collected a mobile phone usage dataset from a developing country -- Pakistan. The data consists of 783 users and 229, 450 communication events. We found a number of interesting patterns both at the aggregate level and at dyadic level in the data. Some interesting results include: the number of calls to different alters consistently follow the rank-size rule; a communication event between an ego-alter(user-contact) pair greatly increases the chances of another communication event; certain ego-alter pairs tend to communicate more over weekends; ego-alter pairs exhibit autocorrelation in various time quantum. Identifying such idiosyncrasies in the ego-alter communication can help improve the calling experience of smartphone users by automatically (smartly) sorting the call log without any manual intervention.},
 acmid = {2935350},
 address = {New York, NY, USA},
 author = {Nasim, Mehwish and Rextin, Aimal and Khan, Numair and Malik, Muhammad Muddassir},
 booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
 doi = {10.1145/2935334.2935350},
 isbn = {978-1-4503-4408-1},
 keyword = {call logs, mobile phones, temporal patterns, time series},
 link = {http://doi.acm.org/10.1145/2935334.2935350},
 location = {Florence, Italy},
 numpages = {8},
 pages = {483--490},
 publisher = {ACM},
 series = {MobileHCI '16},
 title = {Understanding Call Logs of Smartphone Users for Making Future Calls},
 year = {2016}
}


@inproceedings{Colley:2016:CCA:2935334.2935384,
 abstract = {Mobile devices are currently the most commonly used platform to experience Augmented Reality (AR). Nevertheless, they typically provide a less than ideal ergonomic experience, requiring the user to operate them with arms raised. In this paper we evaluate how to improve the ergonomics of AR experiences by modifying the angle between the mobile device's camera and its display. Whereas current mobile device cameras point out vertically from the back cover, we modify the camera angle to be 0, 45 and 90 degrees. In addition, we also investigate the use of the smartwatch as an AR browser form factor. Key findings are, that whilst the current approximately see-through configuration provides the fastest task completion times, a camera offset angle of 45° provides reduced task load and was preferred by users. When comparing different form factors and screen sizes, the smartwatch format was found to be unsuitable for AR browsing use.},
 acmid = {2935384},
 address = {New York, NY, USA},
 author = {Colley, Ashley and Van Vlaenderen, Wouter and Sch\"{o}ning, Johannes and H\"{a}kkil\"{a}, Jonna},
 booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
 doi = {10.1145/2935334.2935384},
 isbn = {978-1-4503-4408-1},
 keyword = {augmented reality, augmented reality browsers, magic lens interaction, mobile devices, smartwatches},
 link = {http://doi.acm.org/10.1145/2935334.2935384},
 location = {Florence, Italy},
 numpages = {11},
 pages = {442--452},
 publisher = {ACM},
 series = {MobileHCI '16},
 title = {Changing the Camera-to-screen Angle to Improve AR Browser Usage},
 year = {2016}
}


@inproceedings{Kim:2016:AMT:2935334.2935380,
 abstract = {Mobile technologies offer the potential for enhanced healthcare, especially by supporting self-management of chronic care. For these technologies to impact chronic care, they need to work for older adults, because the majority of people with chronic conditions are older. A major challenge remains: integrating the appropriate use of such technologies into the lives of older adults. We investigated how older adults would accept mobile technologies by interviewing two groups of older adults (technology adopters and non-adopters who aged 60+) about their experiences and perspectives to mobile technologies. Our preliminary results indicate that there is an additional phase, the intention to learn, and three relating factors, self-efficacy, conversion readiness, and peer support, that significantly influence the acceptance of mobile technologies among the participants, but are not represented in the existing models. With these findings, we propose a tentative theoretical model that extends the existing theories to explain the ways in which our participants came to accept mobile technologies. Future work should investigate the validity of the proposed model by testing our findings against younger people.},
 acmid = {2935380},
 address = {New York, NY, USA},
 author = {Kim, Sunyoung and Gajos, Krzysztof Z. and Muller, Michael and Grosz, Barbara J.},
 booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
 doi = {10.1145/2935334.2935380},
 isbn = {978-1-4503-4408-1},
 keyword = {aging, digital health, healthcare technology, mobile technology adoption, older adults},
 link = {http://doi.acm.org/10.1145/2935334.2935380},
 location = {Florence, Italy},
 numpages = {11},
 pages = {147--157},
 publisher = {ACM},
 series = {MobileHCI '16},
 title = {Acceptance of Mobile Technology by Older Adults: A Preliminary Study},
 year = {2016}
}


@inproceedings{Huang:2016:CCI:2935334.2935387,
 abstract = {Participatory sensing systems use people and their smartphones as a sensing infrastructure, and getting people to make contributions remains a critical challenge. Little work details how system designers should combine different interactions to increase coverage of service location. Tiramisu, a participatory sensing system, invites transit riders to crowdsource real-time arrival information by sharing location traces when they commute. We extended this system with a new feature that allows riders at stops to "spot" buses passing by. To better understand the impact of this new feature, we conducted an observational log analysis, examining changes in coverage and user behavior before and after the new feature. Following the addition of the spotting feature, participants' contributions increased coverage (the number of trips with real-time data) by 98%, and they used the app more than twice as much. The addition of the spotting feature was also followed by a significant increase of trace contributions.},
 acmid = {2935387},
 address = {New York, NY, USA},
 author = {Huang, Yun and Zimmerman, John and Tomasic, Anthony and Steinfeld, Aaron},
 booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
 doi = {10.1145/2935334.2935387},
 isbn = {978-1-4503-4408-1},
 keyword = {coverage of service location, mobile crowdsourcing, participatory sensing, user contribution},
 link = {http://doi.acm.org/10.1145/2935334.2935387},
 location = {Florence, Italy},
 numpages = {12},
 pages = {365--376},
 publisher = {ACM},
 series = {MobileHCI '16},
 title = {Combining Contribution Interactions to Increase Coverage in Mobile Participatory Sensing Systems},
 year = {2016}
}


@inproceedings{Jarusriboonchai:2016:LTP:2935334.2935385,
 abstract = {Mobile phones have become common tools for photography. Despite the fact that photos are social artifacts, mobile phones afford the act of photo taking only as an individual activity. Photo taking that involves more than one photographer has been envisioned to create positive outcomes and experiences. We implemented this vision with mobile camera phones, exploring how this would influence photo taking practices and experiences. We conducted a user study where altogether 22 participants (11 pairs) were using a novel mobile photography method based on asymmetrical interaction abilities, comparing that with two traditional methods. We present the collaborative practices emerged in different photography methods and report user experience findings particularly with regard to enforced collaboration in mobile photo taking. The results highlight benefits and positive experiences in collaborative photo taking. We discuss lessons learned and point out design implications that come into play when designing for mobile collocated collaboration.},
 acmid = {2935385},
 address = {New York, NY, USA},
 author = {Jarusriboonchai, Pradthana and Olsson, Thomas and Lyckvi, Sus Lundgren and V\"{a}\"{a}n\"{a}nen, Kaisa},
 booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
 doi = {10.1145/2935334.2935385},
 isbn = {978-1-4503-4408-1},
 keyword = {asymmetry, collaboration, collocated interaction, design research, digital photography, photo taking, user experience, user study},
 link = {http://doi.acm.org/10.1145/2935334.2935385},
 location = {Florence, Italy},
 numpages = {12},
 pages = {529--540},
 publisher = {ACM},
 series = {MobileHCI '16},
 title = {Let's Take Photos Together: Exploring Asymmetrical Interaction Abilities on Mobile Camera Phones},
 year = {2016}
}


@inproceedings{Carter:2016:BMM:2935334.2935355,
 abstract = {Most teleconferencing tools treat users in distributed meetings monolithically: all participants are meant to be interconnected in more-or-less the same manner. In practice, people connect to meetings in different contexts, sometimes sitting in front of a laptop or tablet giving their full attention, but at other times mobile and concurrently involved in other tasks or as a liminal participant in a larger group meeting. In this paper, we present the design and evaluation of two applications, MixMeetWear and MixMeetMate, to help users in non-standard contexts flexibly participate in meetings.},
 acmid = {2935355},
 address = {New York, NY, USA},
 author = {Carter, Scott and Marlow, Jennifer and Komori, Aki and M\"{a}kel\"{a}, Ville},
 booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
 doi = {10.1145/2935334.2935355},
 isbn = {978-1-4503-4408-1},
 keyword = {mobile, teleconferencing, wearable},
 link = {http://doi.acm.org/10.1145/2935334.2935355},
 location = {Florence, Italy},
 numpages = {11},
 pages = {407--417},
 publisher = {ACM},
 series = {MobileHCI '16},
 title = {Bringing Mobile into Meetings: Enhancing Distributed Meeting Participation on Smartwatches and Mobile Phones},
 year = {2016}
}


@inproceedings{Pfeiffer:2016:LYB:2935334.2935348,
 abstract = {Electrical muscle stimulation (EMS) is a promising wearable haptic output technology as it can be miniaturized considerably and delivers a wide range of haptic output. However, prototyping EMS applications is challenging. It requires detailed knowledge and skills about hardware, software, and physiological characteristics. To simplify prototyping with EMS in mobile and wearable situations we present the Let Your Body Move toolkit. It consists of (1) a hardware control module with Bluetooth communication that uses off-the-shelf EMS devices as signal generators, (2) a simple communications protocol to connect mobile devices, and (3) a set of control applications as starting points for EMS prototyping. We describe EMS-specific parameters, electrode placements on the skin, and user calibration. The toolkit was evaluated in a workshop with 10 researchers in haptics. The results show that the toolkit allows to quickly generate non-trivial prototypes. The hardware schematics and software components are available as open source software.},
 acmid = {2935348},
 address = {New York, NY, USA},
 author = {Pfeiffer, Max and Duente, Tim and Rohs, Michael},
 booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
 doi = {10.1145/2935334.2935348},
 isbn = {978-1-4503-4408-1},
 keyword = {EMS, electrical muscle stimulation, force feedback, haptic feedback, mobile, prototyping, toolkit, wearable},
 link = {http://doi.acm.org/10.1145/2935334.2935348},
 location = {Florence, Italy},
 numpages = {10},
 pages = {418--427},
 publisher = {ACM},
 series = {MobileHCI '16},
 title = {Let Your Body Move: A Prototyping Toolkit for Wearable Force Feedback with Electrical Muscle Stimulation},
 year = {2016}
}


@inproceedings{Suzuki:2016:FSC:2935334.2935357,
 abstract = {We present a concept of using a movable background to navigate a caret on small mobile devices. The standard approach to selecting text on mobile devices is to directly touch the location on the text that a user wants to select. This is problematic because the user's finger hides the area to select. Our concept is to use a movable background to navigate the caret. Users place a caret by tapping on the screen and then move the background by touching and dragging. In this method, the caret is fixed on the screen and the user drags the background text to navigate the caret. We compared our technique with the iPhone's default UI and found that even though participants were using our technique for the first time, average task completion time was not different or even faster than Default UI in the case of the small font size and got a significantly higher usability score than Default UI.},
 acmid = {2935357},
 address = {New York, NY, USA},
 author = {Suzuki, Kenji and Okabe, Kazumasa and Sakamoto, Ryuuki and Sakamoto, Daisuke},
 booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
 doi = {10.1145/2935334.2935357},
 isbn = {978-1-4503-4408-1},
 keyword = {caret, mobile device, movable background, text selection},
 link = {http://doi.acm.org/10.1145/2935334.2935357},
 location = {Florence, Italy},
 numpages = {5},
 pages = {478--482},
 publisher = {ACM},
 series = {MobileHCI '16},
 title = {Fix and Slide: Caret Navigation with Movable Background},
 year = {2016}
}


@inproceedings{Schulze:2016:CCH:2935334.2935347,
 abstract = {We explore if and how identifying the character of face-to-face conversations can help manage notifications on smartphones so that they become less disruptive. We show that the social dimensions depth/importance and formality/goal orientation of a conversation are strong indicators of receptiveness. Furthermore, we find that there are types of conversation, e.g. small talk, in which individuals are even more receptive to notifications than in situations without any verbal social interaction at all. This refutes the assumption currently found in the literature that the occurrence of a conversation is a strong predictor of unavailability. We demonstrate a system that tracks conversations in which the user is engaged and that analyzes speech in terms of embedded affective and social cues. Eventually, we find that information of either kind, derived from audio, improves the accuracy of personal notification preference models substantially.},
 acmid = {2935347},
 address = {New York, NY, USA},
 author = {Schulze, Florian and Groh, Georg},
 booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
 doi = {10.1145/2935334.2935347},
 isbn = {978-1-4503-4408-1},
 keyword = {character of conversation, conversation-awareness, interruptibility, mobile notification management},
 link = {http://doi.acm.org/10.1145/2935334.2935347},
 location = {Florence, Italy},
 numpages = {11},
 pages = {518--528},
 publisher = {ACM},
 series = {MobileHCI '16},
 title = {Conversational Context Helps Improve Mobile Notification Management},
 year = {2016}
}


@inproceedings{Sarangapani:2016:VMP:2935334.2935354,
 abstract = {Cross-cultural learning has gained increased interest and importance within school curricula in recent years. Schools are using technology to accumulate resources for cross-cultural learning, which has predominantly been pre-prepared videos, documentaries, photos and textual information available online. In this paper we describe the engagement with video technology on mobile smartphones by three migrant families who were tasked with developing cross-cultural resources over the course of six weeks. The resources developed were then used as a learning resource in a classroom and feedback was taken from the teacher. Our study has established that mobile phones particularly smartphones are an accessible, evocative and affordable avenue to aid in the development of cross-cultural resources alongside building stronger parental engagement in schools. The study contributes an expansion of knowledge in research areas that seek to use video technology on mobile phones to build cross-cultural resources for learning and strengthen home-school and school-home communication.},
 acmid = {2935354},
 address = {New York, NY, USA},
 author = {Sarangapani, Vidya and Kharrufa, Ahmed and Balaam, Madeline and Leat, David and Wright, Pete},
 booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
 doi = {10.1145/2935334.2935354},
 isbn = {978-1-4503-4408-1},
 keyword = {community collaboration, cross-cultural, education, learning, mediator, video technology},
 link = {http://doi.acm.org/10.1145/2935334.2935354},
 location = {Florence, Italy},
 numpages = {12},
 pages = {341--352},
 publisher = {ACM},
 series = {MobileHCI '16},
 title = {Virtual.Cultural.Collaboration: Mobile Phones, Video Technology, and Cross-cultural Learning},
 year = {2016}
}


@inproceedings{Murnane:2016:MMA:2935334.2935383,
 abstract = {Our body clock causes considerable variations in our behavioral, mental, and physical processes, including alertness, throughout the day. While much research has studied technology usage patterns, the potential impact of underlying biological processes on these patterns is under-explored. Using data from 20 participants over 40 days, this paper presents the first study to connect patterns of mobile application usage with these contributing biological factors. Among other results, we find that usage patterns vary for individuals with different body clock types, that usage correlates with rhythms of alertness, that app use features such as duration and switching can distinguish periods of low and high alertness, and that app use reflects sleep interruptions as well as sleep duration. We conclude by discussing how our findings inform the design of biologically-friendly technology that can better support personal rhythms of performance.},
 acmid = {2935383},
 address = {New York, NY, USA},
 author = {Murnane, Elizabeth L. and Abdullah, Saeed and Matthews, Mark and Kay, Matthew and Kientz, Julie A. and Choudhury, Tanzeem and Gay, Geri and Cosley, Dan},
 booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
 doi = {10.1145/2935334.2935383},
 isbn = {978-1-4503-4408-1},
 keyword = {alertness, circadian rhythms, mobile app use, sleep},
 link = {http://doi.acm.org/10.1145/2935334.2935383},
 location = {Florence, Italy},
 numpages = {13},
 pages = {465--477},
 publisher = {ACM},
 series = {MobileHCI '16},
 title = {Mobile Manifestations of Alertness: Connecting Biological Rhythms with Patterns of Smartphone App Use},
 year = {2016}
}


@inproceedings{Cramer:2016:SFE:2935334.2935370,
 abstract = {Emojis are an extremely common occurrence in mobile communications, but their meaning is open to interpretation. We investigate motivations for their usage in mobile messaging in the US. This study asked 228 participants for the last time that they used one or more emojis in a conversational message, and collected that message, along with a description of the emojis' intended meaning and function. We discuss functional distinctions between: adding additional emotional or situational meaning, adjusting tone, making a message more engaging to the recipient, conversation management, and relationship maintenance. We discuss lexical placement within messages, as well as social practices. We show that the social and linguistic function of emojis are complex and varied, and that supporting emojis can facilitate important conversational functions.},
 acmid = {2935370},
 address = {New York, NY, USA},
 author = {Cramer, Henriette and de Juan, Paloma and Tetreault, Joel},
 booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
 doi = {10.1145/2935334.2935370},
 isbn = {978-1-4503-4408-1},
 keyword = {emojis, messaging, mobile},
 link = {http://doi.acm.org/10.1145/2935334.2935370},
 location = {Florence, Italy},
 numpages = {6},
 pages = {504--509},
 publisher = {ACM},
 series = {MobileHCI '16},
 title = {Sender-intended Functions of Emojis in US Messaging},
 year = {2016}
}


@inproceedings{Wenig:2016:SBI:2935334.2935373,
 abstract = {Providing pedestrian navigation instructions on small screens is a challenging task due to limited screen space. As image-based approaches for navigation have been successfully proven to outperform map-based navigation on mobile devices, we propose to bring image-based navigation to smartwatches. We contribute a straightforward pipeline to easily create image-based indoor navigation instructions that allow users to freely navigate in indoor environments without any localization infrastructure and with minimal user input on the smartwatch. In a user study, we show that our approach outperforms the current state-of-the art application in terms of task completion time, perceived task load and perceived usability. In addition, we did not find an indication that there is a need to provide explicit directional instructions for image-based navigation on small screens.},
 acmid = {2935373},
 address = {New York, NY, USA},
 author = {Wenig, Dirk and Steenbergen, Alexander and Sch\"{o}ning, Johannes and Hecht, Brent and Malaka, Rainer},
 booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
 doi = {10.1145/2935334.2935373},
 isbn = {978-1-4503-4408-1},
 keyword = {cartography, mobile maps, pedestrian navigation, smartwatches, stripe maps},
 link = {http://doi.acm.org/10.1145/2935334.2935373},
 location = {Florence, Italy},
 numpages = {7},
 pages = {400--406},
 publisher = {ACM},
 series = {MobileHCI '16},
 title = {ScrollingHome: Bringing Image-based Indoor Navigation to Smartwatches},
 year = {2016}
}


@inproceedings{Dalvi:2016:PRH:2935334.2935366,
 abstract = {As part of an ongoing standardization effort, we were asked to evaluate Marathi text input mechanisms on smartphones. We undertook a between-subject longitudinal evaluation of four existing keyboards with 153 novice users who participated for 31 sessions each, spread over 3--4 weeks. In this paper, we present the empirical results of the performance of these keyboards and discuss them with respect to their designs. We found that keyboards with logical layouts performed marginally better than keyboards with partially frequency-based layouts. Results also showed that users performed poorly on keyboards that have word prediction features in comparison with keyboards that don't have prediction features while typing Marathi. We speculate that this difference in performance is related to a "cognitive toll" that the users pay to use word prediction. We identify several directions for future research.},
 acmid = {2935366},
 address = {New York, NY, USA},
 author = {Dalvi, Girish and Ahire, Shashank and Emmadi, Nagraj and Joshi, Manjiri and Joshi, Anirudha and Ghosh, Sanjay and Ghone, Prasad and Parmar, Narendra},
 booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
 doi = {10.1145/2935334.2935366},
 isbn = {978-1-4503-4408-1},
 keyword = {Indian languages, text input, text input evaluation, text prediction},
 link = {http://doi.acm.org/10.1145/2935334.2935366},
 location = {Florence, Italy},
 numpages = {12},
 pages = {35--46},
 publisher = {ACM},
 series = {MobileHCI '16},
 title = {Does Prediction Really Help in Marathi Text Input?: Empirical Analysis of a Longitudinal Study},
 year = {2016}
}


@inproceedings{Geurts:2016:BBB:2935334.2935377,
 abstract = {Persons that suffered from a cardiac disease are often recommended to integrate a sufficient level of physical exercise in their daily life. Initially, cardiac rehabilitation takes place in a closely monitored setting in a hospital or a rehabilitation center. Sustaining the effort once the patient has left the ambulatory, supervised environment is a challenge, and drop-out rates are high. Emerging approaches such as telemonitoring and telerehabilitation have been proven to show the potential to support the cardiac patient in adhering to the advised physical exercise. However, most telerehabilitation solutions only support a limited range of physical exercise, such as step-counting during walking. We propose BoB (Back on Bike), a mobile application that guides cardiac patients while cycling. Design choices are explained according to three pillars: ease of use, reduce fear, and direct and indirect motivation. In this paper, we report the results from a field study with cardiac patients.},
 acmid = {2935377},
 address = {New York, NY, USA},
 author = {Geurts, Eva and Haesen, Mieke and Dendale, Paul and Luyten, Kris and Coninx, Karin},
 booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
 doi = {10.1145/2935334.2935377},
 isbn = {978-1-4503-4408-1},
 keyword = {cardiac rehabilitation, mobile application, persuasion, prevention, remote monitoring, user experience},
 link = {http://doi.acm.org/10.1145/2935334.2935377},
 location = {Florence, Italy},
 numpages = {12},
 pages = {135--146},
 publisher = {ACM},
 series = {MobileHCI '16},
 title = {Back on Bike: The BoB Mobile Cycling App for Secondary Prevention in Cardiac Patients},
 year = {2016}
}


@inproceedings{Jokela:2016:NGB:2935334.2935346,
 abstract = {As wearable devices become more popular, situations where there are multiple persons present with such devices will become commonplace. In these situations, wearable devices could support collaborative tasks and experiences between co-located persons through multi-user applications. We present an elicitation study that gathers from end users interaction methods for wearable devices for two common tasks in co-located interaction: group binding and cross-display object movement. We report a total of 154 methods collected from 30 participants. We categorize the methods based on the metaphor and modality of interaction, and discuss the strengths and weaknesses of each category based on qualitative and quantitative feedback given by the participants.},
 acmid = {2935346},
 address = {New York, NY, USA},
 author = {Jokela, Tero and Rezaei, Parisa Pour and V\"{a}\"{a}n\"{a}nen, Kaisa},
 booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
 doi = {10.1145/2935334.2935346},
 isbn = {978-1-4503-4408-1},
 keyword = {co-located interaction, cross-display object movement, device ecosystem binding, elicitation study, group association, guessability study, multi-device user interfaces, pairing, smartglasses, smartwatches, wearable devices},
 link = {http://doi.acm.org/10.1145/2935334.2935346},
 location = {Florence, Italy},
 numpages = {11},
 pages = {206--216},
 publisher = {ACM},
 series = {MobileHCI '16},
 title = {Natural Group Binding and Cross-display Object Movement Methods for Wearable Devices},
 year = {2016}
}


@inproceedings{Sadana:2016:GMV:2935334.2935391,
 abstract = {We present Gesture Morpher, a tool for prototyping and testing multi-touch interactions based on video recordings of target application behaviors, e.g., a sequence of screenshots recorded by a screen capture tool. Gesture Morpher extracts continuous behaviors from video recordings, such as transformations of UI content, and suggests a set of multi-touch interactions that are suitable for achieving these behaviors. Designers can easily test different interactions on a touch device with visual response that is automatically synthesized from the video recording, all without any programming. We discuss a range of multi-touch interaction scenarios Gesture Morpher supports, our method for extracting continuous interaction behaviors from video recordings, and techniques for associating touch-input with the output effect extracted from the videos.},
 acmid = {2935391},
 address = {New York, NY, USA},
 author = {Sadana, Ramik and Li, Yang},
 booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
 doi = {10.1145/2935334.2935391},
 isbn = {978-1-4503-4408-1},
 keyword = {multi-touch, rapid prototyping, simulation, video analysis},
 link = {http://doi.acm.org/10.1145/2935334.2935391},
 location = {Florence, Italy},
 numpages = {6},
 pages = {227--232},
 publisher = {ACM},
 series = {MobileHCI '16},
 title = {Gesture Morpher: Video-based Retargeting of Multi-touch Interactions},
 year = {2016}
}


@inproceedings{Su:2016:UTT:2935334.2935339,
 abstract = {In this work we introduce 2D-Dragger, a unified touch-based target acquisition technique that enables easy access to small targets in dense regions or distant targets on screens of various sizes. The effective width of a target is constant with our tool, allowing a fixed scale of finger movement for capturing a new target. Our tool is thus insensitive to the distribution and size of the selectable targets, and consistently works well for screens of different sizes, from mobile to wall-sized screens. Our user studies show that overall 2D-Dragger performs the best compared to the state-of-the-art techniques for selecting both near and distant targets of various sizes in different densities.},
 acmid = {2935339},
 address = {New York, NY, USA},
 author = {Su, Qingkun and Au, Oscar Kin-Chung and Xu, Pengfei and Fu, Hongbo and Tai, Chiew-Lan},
 booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
 doi = {10.1145/2935334.2935339},
 isbn = {978-1-4503-4408-1},
 keyword = {accessibility, effective width, target acquisition, touch input},
 link = {http://doi.acm.org/10.1145/2935334.2935339},
 location = {Florence, Italy},
 numpages = {10},
 pages = {170--179},
 publisher = {ACM},
 series = {MobileHCI '16},
 title = {2D-Dragger: Unified Touch-based Target Acquisition with Constant Effective Width},
 year = {2016}
}


@inproceedings{Pohl:2016:EEE:2935334.2935382,
 abstract = {Current soft keyboards for emoji entry all present emoji in the same way: in long lists, spread over several categories. While categories limit the number of emoji in each individual list, the overall number is still so large, that emoji entry is a challenging task. The task takes particularly long if users pick the wrong category when searching for an emoji. Instead, we propose a new zooming keyboard for emoji entry. Here, users can see all emoji at once, aiding in building spatial memory where related emoji are to be found. We compare our zooming emoji keyboard against the Google keyboard and find that our keyboard allows for 18% faster emoji entry, reducing the required time for one emoji from 15.6 s to 12.7 s. A preliminary longitudinal evaluation with three participants showed that emoji entry time over the duration of the study improved at up to 60 % to a final average of 7.5 s.},
 acmid = {2935382},
 address = {New York, NY, USA},
 author = {Pohl, Henning and Stanke, Dennis and Rohs, Michael},
 booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
 doi = {10.1145/2935334.2935382},
 isbn = {978-1-4503-4408-1},
 keyword = {emoji, interaction technique, mobile input, soft keyboard, spatial memory, text entry, zooming user interfaces},
 link = {http://doi.acm.org/10.1145/2935334.2935382},
 location = {Florence, Italy},
 numpages = {8},
 pages = {510--517},
 publisher = {ACM},
 series = {MobileHCI '16},
 title = {EmojiZoom: Emoji Entry via Large Overview Maps \&\#128516;\&\#128269;},
 year = {2016}
}


@inproceedings{Corsten:2016:UBP:2935334.2935371,
 abstract = {Using a smartphone touchscreen to control apps mirrored to a distant display is hard, since the user cannot see where she is touching while looking at the distant screen. Tactile landmarks at the back of the phone can mitigate this problem, especially in landscape mode [3]: By moving a finger across these landmarks, the user can haptically estimate the finger position in proportion to the touchscreen. Upon pinching the thumb resting above the touchscreen towards that finger at the back, the finger position is transferred to the front and registered as a touch. However, despite proprioception, this technique leads to a shift between back and front position, denoted as pinch error. We investigated this error using different target locations, device thicknesses, and tilt angles to derive target sizes that can be acquired at a 96% success rate.},
 acmid = {2935371},
 address = {New York, NY, USA},
 author = {Corsten, Christian and Link, Andreas and Karrer, Thorsten and Borchers, Jan},
 booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
 doi = {10.1145/2935334.2935371},
 isbn = {978-1-4503-4408-1},
 keyword = {back-of-device, eyes-free, pinch error, proprioception},
 link = {http://doi.acm.org/10.1145/2935334.2935371},
 location = {Florence, Italy},
 numpages = {5},
 pages = {185--189},
 publisher = {ACM},
 series = {MobileHCI '16},
 title = {Understanding Back-to-front Pinching for Eyes-free Mobile Touch Input},
 year = {2016}
}


@inproceedings{Feuchtner:2016:IPP:2935334.2935335,
 abstract = {We present a quantitative and qualitative analysis of interruptions of interaction with a public display game, and explore the use of a manual pause mode in this scenario. In previous public display installations we observed users frequently interrupting their interaction. To explore ways of supporting such behavior, we implemented a gesture controlled multiuser game with four pausing techniques. We evaluated them in a field study analyzing 704 users and found that our pausing techniques were eagerly explored, but rarely used with the intention to pause the game. Our study shows that interactions with public displays are considerably intermissive, and that users mostly interrupt interaction to socialize and mainly approach public displays in groups. We conclude that, as a typical characteristic of public display interaction, interruptions deserve consideration. However, manual pause modes are not well suited for games on public displays. Instead, interruptions should be implicitly supported by the application design.},
 acmid = {2935335},
 address = {New York, NY, USA},
 author = {Feuchtner, Tiare and Walter, Robert and M\"{u}ller, J\"{o}rg},
 booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
 doi = {10.1145/2935334.2935335},
 isbn = {978-1-4503-4408-1},
 keyword = {games, gesture interaction, interruption, pause mode, public displays, user studies},
 link = {http://doi.acm.org/10.1145/2935334.2935335},
 location = {Florence, Italy},
 numpages = {12},
 pages = {306--317},
 publisher = {ACM},
 series = {MobileHCI '16},
 title = {Interruption and Pausing of Public Display Games},
 year = {2016}
}


@inproceedings{Lyons:2016:VPI:2935334.2935344,
 abstract = {As a new generation of smartwatches enters the market, one common use is for displaying information such as notifications. While some content might warrant immediately interrupting a user, there is also information that might be important to display yet less urgent. It would be useful to show this content on the watch but not immediately draw the user's attention away from their primary task. In this paper, we investigate how fast three visual parameters draw a user's attention. In particular, we present data from a smartwatch user study where we examine the size, frequency, and color of a visual prompt and the associated impact on reaction time. We find statistically significant differences for size and frequency where smaller and slower result in the less immediate reactions. We also present reaction time distributions that a designer can use to tailor expected notification response times to match their content.},
 acmid = {2935344},
 address = {New York, NY, USA},
 author = {Lyons, Kent},
 booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
 doi = {10.1145/2935334.2935344},
 isbn = {978-1-4503-4408-1},
 keyword = {notification, reaction time, smartwatch, user study},
 link = {http://doi.acm.org/10.1145/2935334.2935344},
 location = {Florence, Italy},
 numpages = {5},
 pages = {190--194},
 publisher = {ACM},
 series = {MobileHCI '16},
 title = {Visual Parameters Impacting Reaction Times on Smartwatches},
 year = {2016}
}


@inproceedings{Yeo:2016:WPT:2935334.2935375,
 abstract = {The screen size of a smartwatch provides limited space to enable expressive multi-touch input, resulting in a markedly difficult and limited experience. We present WatchMI: Watch Movement Input that enhances touch interaction on a smartwatch to support continuous pressure touch, twist, pan gestures and their combinations. Our novel approach relies on software that analyzes, in real-time, the data from a built-in Inertial Measurement Unit (IMU) in order to determine with great accuracy and different levels of granularity the actions performed by the user, without requiring additional hardware or modification of the watch. We report the results of an evaluation with the system, and demonstrate that the three proposed input interfaces are accurate, noise-resistant, easy to use and can be deployed on a variety of smartwatches. We then showcase the potential of this work with seven different applications including, map navigation, an alarm clock, a music player, pan gesture recognition, text entry, file explorer and controlling remote devices or a game character.},
 acmid = {2935375},
 address = {New York, NY, USA},
 author = {Yeo, Hui-Shyong and Lee, Juyoung and Bianchi, Andrea and Quigley, Aaron},
 booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
 doi = {10.1145/2935334.2935375},
 isbn = {978-1-4503-4408-1},
 keyword = {rich touch, small screen, smart watch, wearable devices},
 link = {http://doi.acm.org/10.1145/2935334.2935375},
 location = {Florence, Italy},
 numpages = {6},
 pages = {394--399},
 publisher = {ACM},
 series = {MobileHCI '16},
 title = {WatchMI: Pressure Touch, Twist and Pan Gesture Input on Unmodified Smartwatches},
 year = {2016}
}


@inproceedings{Mottelson:2016:IMD:2935334.2935360,
 abstract = {The small displays of smartwatches make text entry difficult and time consuming. While text entry rates can be increased, this continues to occur at the expense of available screen display space. Soft keyboards can easily use half the display space of tiny-screened devices. To combat this problem, we present Invisiboard: an invisible text entry method using the entire display for both text entry and display simultaneously. Invisiboard combines a numberpad-like layout with swipe gestures. This maximizes input target size, provides a familiar layout, and maximizes display space. Through this, Invisiboard achieves entry rates comparable or even faster than an existing research baseline. A user study with 12 participants writing 3264 words revealed an entry rate of 10.6 Words Per Minute (WPM) after 30 minutes, 7% faster than ZoomBoard. Furthermore, with nominal training, some participants demonstrated entry rates of over 30 WPM.},
 acmid = {2935360},
 address = {New York, NY, USA},
 author = {Mottelson, Aske and Larsen, Christoffer and Lyderik, Mikkel and Strohmeier, Paul and Knibbe, Jarrod},
 booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
 doi = {10.1145/2935334.2935360},
 isbn = {978-1-4503-4408-1},
 keyword = {smartwatch, swipe, text entry, wearables},
 link = {http://doi.acm.org/10.1145/2935334.2935360},
 location = {Florence, Italy},
 numpages = {7},
 pages = {53--59},
 publisher = {ACM},
 series = {MobileHCI '16},
 title = {Invisiboard: Maximizing Display and Input Space with a Full Screen Text Entry Method for Smartwatches},
 year = {2016}
}


@inproceedings{Pohl:2016:SSN:2935334.2935351,
 abstract = {With the increasing popularity of smartwatches over the last years, there has been a substantial interest in novel input methods for such small devices. However, feedback modalities for smartwatches have not seen the same level of interest. This is surprising, as one of the primary function of smartwatches is their use for notifications. It is the interrupting nature of current notifications on smartwatches that has also drawn some of the more critical responses to them. Here, we present a subtle notification mechanism for smartwatches that uses light scattering in a wearer's skin as a feedback modality. This does not disrupt the wearer in the same way as vibration feedback and also connects more naturally with the user's body.},
 acmid = {2935351},
 address = {New York, NY, USA},
 author = {Pohl, Henning and Medrek, Justyna and Rohs, Michael},
 booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
 doi = {10.1145/2935334.2935351},
 isbn = {978-1-4503-4408-1},
 keyword = {in the wild, indirect illumination, notifications, wearables},
 link = {http://doi.acm.org/10.1145/2935334.2935351},
 location = {Florence, Italy},
 numpages = {10},
 pages = {7--16},
 publisher = {ACM},
 series = {MobileHCI '16},
 title = {ScatterWatch: Subtle Notifications via Indirect Illumination Scattered in the Skin},
 year = {2016}
}


@inproceedings{Tong:2016:YSY:2935334.2935343,
 abstract = {Mobile devices offer great opportunities in the field of collaborative learning. They are especially interesting in their ability to provide digital information while still supporting social interactions between group members, which are essential elements of coordinated and shared activities. However, in truly mobile conditions, e.g. outdoors, the high variability of groups spatial configurations can potentially modify coordination mechanisms. We designed and tested an orienteering mobile learning game to better understand how device use shaped collaboration in highly mobile conditions. The study involved four groups of three students all equipped with tablets. We focused our analysis on the relationship between participants' arrangements (F-formations), their device usage and coordination mechanisms (i.e. awareness, regulation, information sharing, and discussion). Our results emphasize the importance of considering the transitions between arrangements more than F-formations per se. We discuss the implications of these findings for the design and analysis of mobile collaborative activities.},
 acmid = {2935343},
 address = {New York, NY, USA},
 author = {Tong, Lili and Serna, Audrey and Pageaud, Simon and George, S{\'e}bastien and Tabard, Aur{\'e}lien},
 booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
 doi = {10.1145/2935334.2935343},
 isbn = {978-1-4503-4408-1},
 keyword = {F-formation, collaboration, collaboration dynamics, coordination, group regulation, mobile learning, ubiquitous computing},
 link = {http://doi.acm.org/10.1145/2935334.2935343},
 location = {Florence, Italy},
 numpages = {12},
 pages = {318--329},
 publisher = {ACM},
 series = {MobileHCI '16},
 title = {It's Not How You Stand, It's How You Move: F-formations and Collaboration Dynamics in a Mobile Learning Game},
 year = {2016}
}


@inproceedings{Ashbrook:2016:BET:2935334.2935389,
 abstract = {We present Bitey, a subtle, wearable device for enabling input via tooth clicks. Based on a bone-conduction microphone worn just above the ears, Bitey recognizes the click sounds from up to five different pairs of teeth, allowing fully hands-free interface control. We explore the space of tooth input and show that Bitey allows for a high degree of accuracy in distinguishing between different tooth clicks, with up to 94% accuracy under laboratory conditions for five different tooth pairs. Finally, we illustrate Bitey's potential through two demonstration applications: a list navigation and selection interface and a keyboard input method.},
 acmid = {2935389},
 address = {New York, NY, USA},
 author = {Ashbrook, Daniel and Tejada, Carlos and Mehta, Dhwanit and Jiminez, Anthony and Muralitharam, Goudam and Gajendra, Sangeeta and Tallents, Ross},
 booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
 doi = {10.1145/2935334.2935389},
 isbn = {978-1-4503-4408-1},
 keyword = {audio interfaces, bio-acoustics, gestures, subtle interfaces, tooth input, wearable computing},
 link = {http://doi.acm.org/10.1145/2935334.2935389},
 location = {Florence, Italy},
 numpages = {12},
 pages = {158--169},
 publisher = {ACM},
 series = {MobileHCI '16},
 title = {Bitey: An Exploration of Tooth Click Gestures for Hands-free User Interface Control},
 year = {2016}
}


