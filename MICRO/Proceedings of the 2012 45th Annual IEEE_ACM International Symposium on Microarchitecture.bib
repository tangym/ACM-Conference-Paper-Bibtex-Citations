@inproceedings{2012:AI:2457472.2457521,
 abstract = {
                  An abstract is not available.
              },
 acmid = {2457521},
 address = {Washington, DC, USA},
 booktitle = {Proceedings of the 2012 45th Annual IEEE/ACM International Symposium on Microarchitecture},
 doi = {10.1109/MICRO.2012.50},
 isbn = {978-0-7695-4924-8},
 key = {$\!\!$},
 link = {http://dx.doi.org/10.1109/MICRO.2012.50},
 location = {Vancouver, B.C., CANADA},
 numpages = {2},
 pages = {473--474},
 publisher = {IEEE Computer Society},
 series = {MICRO-45},
 title = {Author Index},
 year = {2012}
}


@inproceedings{Hardy:2012:PVA:2457472.2457485,
 abstract = {This paper presents a first-order analytical model for determining the performance degradation caused by permanently faulty cells in architectural and non-architectural arrays. We refer to this degradation as the performance vulnerability factor (PVF). The study assumes a future where cache blocks with faulty cells are disabled resulting in less cache capacity and extra misses while faulty predictor cells are still used but cause additional mispredictions. For a given program run, random probability of permanent cell failure, and processor configuration, the model can rapidly provide the expected PVF as well as lower and upper PVF probability distribution bounds for an individual array or array combination. The model is used to predict the PVF for the three predictors and the last level cache, used in this study, for a wide range of cell failure rates. The analysis reveals that for cell failure rate of up to 1.5e-6 the expected PVF is very small. For higher failure rates the expected PVF grows noticeably mostly due to the extra misses in the last level cache. The expected PVF of the predictors remains small even at high failure rates but the PVF distribution reveals cases of significant performance degradation with a non-negligible probability. These results suggest that designers of future processors can leverage trade-offs between PVF and reliability to sustain area, performance and energy scaling. The paper demonstrates this approach by exploring the implications of different cell size on yield and PVF.},
 acmid = {2457485},
 address = {Washington, DC, USA},
 author = {Hardy, Damien and Sideris, Isidoros and Ladas, Nikolas and Sazeides, Yiannakis},
 booktitle = {Proceedings of the 2012 45th Annual IEEE/ACM International Symposium on Microarchitecture},
 doi = {10.1109/MICRO.2012.14},
 isbn = {978-0-7695-4924-8},
 link = {http://dx.doi.org/10.1109/MICRO.2012.14},
 location = {Vancouver, B.C., CANADA},
 numpages = {12},
 pages = {48--59},
 publisher = {IEEE Computer Society},
 series = {MICRO-45},
 title = {The Performance Vulnerability of Architectural and Non-architectural Arrays to Permanent Faults},
 year = {2012}
}


@inproceedings{Qureshi:2012:FLT:2457472.2457502,
 abstract = {This paper analyzes the design trade-offs in architecting large-scale DRAM caches. Prior research, including the recent work from Loh and Hill, have organized DRAM caches similar to conventional caches. In this paper, we contend that some of the basic design decisions typically made for conventional caches (such as serialization of tag and data access, large associativity, and update of replacement state) are detrimental to the performance of DRAM caches, as they exacerbate the already high hit latency. We show that higher performance can be obtained by optimizing the DRAM cache architecture first for latency, and then for hit rate. We propose a latency-optimized cache architecture, called Alloy Cache, that eliminates the delay due to tag serialization by streaming tag and data together in a single burst. We also propose a simple and highly effective Memory Access Predictor that incurs a storage overhead of 96 bytes per core and a latency of 1 cycle. It helps service cache misses faster without the need to wait for a cache miss detection in the common case. Our evaluations show that our latency-optimized cache design significantly outperforms both the recent proposal from Loh and Hill, as well as an impractical SRAM Tag-Store design that incurs an unacceptable overhead of several tens of megabytes. On average, the proposal from Loh and Hill provides 8.7% performance improvement, the "idealized" SRAM Tag design provides 24%, and our simple latency-optimized design provides 35%.},
 acmid = {2457502},
 address = {Washington, DC, USA},
 author = {Qureshi, Moinuddin K. and Loh, Gabe H.},
 booktitle = {Proceedings of the 2012 45th Annual IEEE/ACM International Symposium on Microarchitecture},
 doi = {10.1109/MICRO.2012.30},
 isbn = {978-0-7695-4924-8},
 keyword = {DRAM Cache, Stacked Memory, Memory Access Predictor},
 link = {http://dx.doi.org/10.1109/MICRO.2012.30},
 location = {Vancouver, B.C., CANADA},
 numpages = {12},
 pages = {235--246},
 publisher = {IEEE Computer Society},
 series = {MICRO-45},
 title = {Fundamental Latency Trade-off in Architecting DRAM Caches: Outperforming Impractical SRAM-Tags with a Simple and Practical Design},
 year = {2012}
}


@inproceedings{2012:TPI:2457472.2457474,
 abstract = {
                  An abstract is not available.
              },
 acmid = {2457474},
 address = {Washington, DC, USA},
 booktitle = {Proceedings of the 2012 45th Annual IEEE/ACM International Symposium on Microarchitecture},
 doi = {10.1109/MICRO.2012.1},
 isbn = {978-0-7695-4924-8},
 key = {$\!\!$},
 link = {http://dx.doi.org/10.1109/MICRO.2012.1},
 location = {Vancouver, B.C., CANADA},
 pages = {i--},
 publisher = {IEEE Computer Society},
 series = {MICRO-45},
 title = {Title Page I},
 year = {2012}
}


@inproceedings{2012:CP:2457472.2457476,
 abstract = {
                  An abstract is not available.
              },
 acmid = {2457476},
 address = {Washington, DC, USA},
 booktitle = {Proceedings of the 2012 45th Annual IEEE/ACM International Symposium on Microarchitecture},
 doi = {10.1109/MICRO.2012.3},
 isbn = {978-0-7695-4924-8},
 key = {$\!\!$},
 link = {http://dx.doi.org/10.1109/MICRO.2012.3},
 location = {Vancouver, B.C., CANADA},
 pages = {iv--},
 publisher = {IEEE Computer Society},
 series = {MICRO-45},
 title = {Copyright Page},
 year = {2012}
}


@inproceedings{Alisafaee:2012:SCT:2457472.2457510,
 abstract = {Chip-multiprocessors require a coherence directory to track data sharing and order accesses to the shared data. Scaling coherence directories to support a large number of cores is challenging due to excessive area requirements of the directories. The state-of-the-art proposals reduce the directory size by not keeping coherence information for private data. These approaches are useful for workloads that have predominantly private data, but are not applicable to workloads with shared data. We observe that data are not actively shared by multiple cores. In workloads with a shared dataset, although each core accesses the whole data, the chance that multiple cores access the same piece of data at the same time is low. Based on this observation we design a Spatiotemporal Coherence Tracking scheme that drastically reduces the directory size without sacrificing performance. The proposed directory scheme uses dual-grain tracking and switches between the granularities whenever possible to save the area. It dynamically detects spatial regions of data that are privately accessed by one core over a time period and for those regions, increases coherence tracking granularity from block-level to region-level. Our experimental results show that the proposed approach can reduce the baseline sparse directory size by at least 75% across a variety of commercial and scientific workloads, while sacrificing only 1% of performance. Using our approach, the directory can be under-provisioned to have fewer entries than the number of cache blocks that are being tracked.},
 acmid = {2457510},
 address = {Washington, DC, USA},
 author = {Alisafaee, Mohammad},
 booktitle = {Proceedings of the 2012 45th Annual IEEE/ACM International Symposium on Microarchitecture},
 doi = {10.1109/MICRO.2012.39},
 isbn = {978-0-7695-4924-8},
 link = {http://dx.doi.org/10.1109/MICRO.2012.39},
 location = {Vancouver, B.C., CANADA},
 numpages = {10},
 pages = {341--350},
 publisher = {IEEE Computer Society},
 series = {MICRO-45},
 title = {Spatiotemporal Coherence Tracking},
 year = {2012}
}


@inproceedings{Wang:2012:SSA:2457472.2457517,
 abstract = {Traditional alias analysis is expensive and ineffective for dynamic optimizations. In practice, dynamic optimization systems perform memory optimizations speculatively, and rely on hardware, such as alias registers, to detect memory aliases at runtime. Existing hardware alias detection schemes either cannot scale up to a large number of alias registers or may introduce false positives. Order-based alias detection overcomes the limitations. However, it brings considerable challenges as how software can efficiently manage the alias register queue and impose restrictions on optimizations. In this paper, we present SMARQ, a Software-Managed Alias Register Queue, which manages the alias register queue efficiently and supports more aggressive speculative optimizations. We conducted experiments with a dynamic optimization system on a VLIW processor that has 64 alias registers. The experiments on a suite of SPECFP2000 benchmarks show that SMARQ improves the overall performance by 39% as compared to the case without hardware alias detection. By scaling up to a large number (from 16 to 64) of alias registers, SMARQ improves performance by 10%. Compared to a technique with false positives (similar to Itanium), SMARQ improves performance by 13%. To reduce the chance of alias register overflow, the novel alias register allocation algorithm in SMARQ reduces the alias register working set by 74% as compared to a straightforward alias register allocation based on program order.},
 acmid = {2457517},
 address = {Washington, DC, USA},
 author = {Wang, Cheng and Wu, Youfeng and Rong, Hongbo and Park, Hyunchul},
 booktitle = {Proceedings of the 2012 45th Annual IEEE/ACM International Symposium on Microarchitecture},
 doi = {10.1109/MICRO.2012.46},
 isbn = {978-0-7695-4924-8},
 keyword = {dynamic optimization, alias register, hardware alias detection, speculation},
 link = {http://dx.doi.org/10.1109/MICRO.2012.46},
 location = {Vancouver, B.C., CANADA},
 numpages = {12},
 pages = {425--436},
 publisher = {IEEE Computer Society},
 series = {MICRO-45},
 title = {SMARQ: Software-Managed Alias Register Queue for Dynamic Optimizations},
 year = {2012}
}


@inproceedings{2012:CA:2457472.2457473,
 abstract = {
                  An abstract is not available.
              },
 acmid = {2457473},
 address = {Washington, DC, USA},
 booktitle = {Proceedings of the 2012 45th Annual IEEE/ACM International Symposium on Microarchitecture},
 doi = {10.1109/MICRO.2012.53},
 isbn = {978-0-7695-4924-8},
 key = {$\!\!$},
 link = {http://dx.doi.org/10.1109/MICRO.2012.53},
 location = {Vancouver, B.C., CANADA},
 pages = {C1--},
 publisher = {IEEE Computer Society},
 series = {MICRO-45},
 title = {Cover Art},
 year = {2012}
}


@inproceedings{Jeon:2012:WLE:2457472.2457484,
 abstract = {General purpose graphics processing units (GPGPUs) are feature rich GPUs that provide general purpose computing ability with massive number of parallel threads. The massive parallelism combined with programmability made GPGPUs the most attractive choice in supercomputing centers. Unsurprisingly, most of the GPGPU-based studies have been focusing on performance improvement leveraging GPGPU's high degree of parallelism. However, for many scientific applications that commonly run on supercomputers, program correctness is as important as performance. Few soft or hard errors could lead to corrupt results and can potentially waste days or even months of computing effort. In this research we exploit unique architectural characteristics of GPGPUs to propose a light weight error detection method, called Warped Dual Modular Redundancy (Warped-DMR). Warped-DMR detects errors in computation by relying on opportunistic spatial and temporal dual-modular execution of code. Warped-DMR is light weight because it exploits the underutilized parallelism in GPGPU computing for error detection. Error detection spans both within a warp as well as between warps, called intra-warp and inter-warp DMR, respectively. Warped-DMR achieves 96% error coverage while incurring a worst-case 16% performance overhead without extra execution units or programmer's effort.},
 acmid = {2457484},
 address = {Washington, DC, USA},
 author = {Jeon, Hyeran and Annavaram, Murali},
 booktitle = {Proceedings of the 2012 45th Annual IEEE/ACM International Symposium on Microarchitecture},
 doi = {10.1109/MICRO.2012.13},
 isbn = {978-0-7695-4924-8},
 keyword = {GPGPU, DMR, Reliable computing},
 link = {http://dx.doi.org/10.1109/MICRO.2012.13},
 location = {Vancouver, B.C., CANADA},
 numpages = {11},
 pages = {37--47},
 publisher = {IEEE Computer Society},
 series = {MICRO-45},
 title = {Warped-DMR: Light-weight Error Detection for GPGPU},
 year = {2012}
}


@inproceedings{Lukefahr:2012:CCP:2457472.2457508,
 abstract = {Heterogeneous multicore systems -- comprised of multiple cores with varying capabilities, performance, and energy characteristics --have emerged as a promising approach to increasing energy efficiency. Such systems reduce energy consumption by identifying phase changes in an application and migrating execution to the most efficient core that meets its current performance requirements. However, due to the overhead of switching between cores, migration opportunities are limited to coarse-grained phases (hundreds of millions of instructions), reducing the potential to exploit energy efficient cores. We propose Composite Cores, an architecture that reduces switching overheads by bringing the notion of heterogeneity within a single core. The proposed architecture pairs big and little compute µEngines that together can achieve high performance and energy efficiency. By sharing much of the architectural state between the µEngines, the switching overhead can be reduced to near zero, enabling fine-grained switching and increasing the opportunities to utilize the little µEngine without sacrificing performance. An intelligent controller switches between the µEngines to maximize energy efficiency while constraining performance loss to a configurable bound. We evaluate Composite Cores using cycle accurate micro architectural simulations and a detailed power model. Results show that, on average, the controller is able to map 25% of the execution to the little µEngine, achieving an 18% energy savings while limiting performance loss to 5%.},
 acmid = {2457508},
 address = {Washington, DC, USA},
 author = {Lukefahr, Andrew and Padmanabha, Shruti and Das, Reetuparna and Sleiman, Faissal M. and Dreslinski, Ronald and Wenisch, Thomas F. and Mahlke, Scott},
 booktitle = {Proceedings of the 2012 45th Annual IEEE/ACM International Symposium on Microarchitecture},
 doi = {10.1109/MICRO.2012.37},
 isbn = {978-0-7695-4924-8},
 keyword = {heterogeneous architecture, core microarchitecure, split pipelines, reactive controller},
 link = {http://dx.doi.org/10.1109/MICRO.2012.37},
 location = {Vancouver, B.C., CANADA},
 numpages = {12},
 pages = {317--328},
 publisher = {IEEE Computer Society},
 series = {MICRO-45},
 title = {Composite Cores: Pushing Heterogeneity Into a Core},
 year = {2012}
}


@inproceedings{Jiang:2012:FFP:2457472.2457481,
 abstract = {As a promising nonvolatile memory technology, Phase Change Memory (PCM) has many advantages over traditional DRAM. Multi-level Cell PCM (MLC) has the benefit of increased memory capacity with low fabrication cost. Due to high per-cell write power and long write latency, MLC PCM requires careful power management to ensure write reliability. Unfortunately, existing power management schemes applied to MLC PCM result in low write throughput and large performance degradation. In this paper, we propose Fine-grained write Power Budgeting (FPB) for MLC PCM. We first identify two major problems for MLC write operations: (i) managing write power without consideration of the iterative write process used by MLC is overly pessimistic, (ii) a heavily written (hot) chip may block the memory from accepting further writes due to chip power restrictions, although most chips may be available. To address these problems, we propose two FPB schemes. First, FPB-IPM observes a global power budget and regulates power across write iterations according to the step-down power demand of each iteration. Second, FPB-GCP integrates a global charge pump on a DIMM to boost power for hot PCM chips while staying within the global power budget. Our experimental results show that these techniques achieve significant improvement on write throughput and system performance. Our schemes also interact positively with PCM effective read latency reduction techniques, such as write cancellation, write pausing and write truncation.},
 acmid = {2457481},
 address = {Washington, DC, USA},
 author = {Jiang, Lei and Zhang, Youtao and Childers, Bruce R. and Yang, Jun},
 booktitle = {Proceedings of the 2012 45th Annual IEEE/ACM International Symposium on Microarchitecture},
 doi = {10.1109/MICRO.2012.10},
 isbn = {978-0-7695-4924-8},
 keyword = {Multiple Level Cell, Phase Change Memory, Power Budget},
 link = {http://dx.doi.org/10.1109/MICRO.2012.10},
 location = {Vancouver, B.C., CANADA},
 numpages = {12},
 pages = {1--12},
 publisher = {IEEE Computer Society},
 series = {MICRO-45},
 title = {FPB: Fine-grained Power Budgeting to Improve Write Throughput of Multi-level Cell Phase Change Memory},
 year = {2012}
}


@inproceedings{Kim:2012:AST:2457472.2457500,
 abstract = {Sudden variations in current (large di/dt) can lead to significant power supply voltage droops and timing errors in modern microprocessors. Several papers discuss the complexity involved with developing test programs, also known as stress marks, to stress the system. Authors of these papers produced tools and methodologies to generate stress marks automatically using techniques such as integer linear programming or genetic algorithms. However, nearly all of the previous work took place in the context of single-core systems, and results were collected and analyzed using cycle-level simulators. In this paper, we measure and analyze di/dt issues on state-of-the-art multi-core x86 systems using real hardware rather than simulators. We build on an existing single-core stress mark generation tool to develop an Automated DI/dT stress mark generation framework, referred to as AUDIT, to generate di/dt stress marks quickly and effectively for multicore systems. We showcase AUDIT's capabilities to adjust to micro architectural and architectural changes. We also present a dithering algorithm to address thread alignment issues on multi-core processors. We compare standard benchmarks, existing di/dt stress marks, and AUDIT-generated stress marks executing on multi-threaded, multi-core systems with complex out-of-order pipelines. Finally, we show how stress analysis using simulators may lead to flawed insights about di/dt issues.},
 acmid = {2457500},
 address = {Washington, DC, USA},
 author = {Kim, Youngtaek and John, Lizy Kurian and Pant, Sanjay and Manne, Srilatha and Schulte, Michael and Bircher, W. Lloyd and Govindan, Madhu S.  Sibi},
 booktitle = {Proceedings of the 2012 45th Annual IEEE/ACM International Symposium on Microarchitecture},
 doi = {10.1109/MICRO.2012.28},
 isbn = {978-0-7695-4924-8},
 keyword = {di/dt, inductive noise, stressmark generation, voltage droop, power distribution network, low power, genetic algorithm, hardware measurement},
 link = {http://dx.doi.org/10.1109/MICRO.2012.28},
 location = {Vancouver, B.C., CANADA},
 numpages = {12},
 pages = {212--223},
 publisher = {IEEE Computer Society},
 series = {MICRO-45},
 title = {AUDIT: Stress Testing the Automatic Way},
 year = {2012}
}


@inproceedings{Rogers:2012:CWS:2457472.2457487,
 abstract = {This paper studies the effects of hardware thread scheduling on cache management in GPUs. We propose Cache-Conscious Wave front Scheduling (CCWS), an adaptive hardware mechanism that makes use of a novel intra-wave front locality detector to capture locality that is lost by other schedulers due to excessive contention for cache capacity. In contrast to improvements in the replacement policy that can better tolerate difficult access patterns, CCWS shapes the access pattern to avoid thrashing the shared L1. We show that CCWS can outperform any replacement scheme by evaluating against the Belady-optimal policy. Our evaluation demonstrates that cache efficiency and preservation of intra-wave front locality become more important as GPU computing expands beyond use in high performance computing. At an estimated cost of 0.17% total chip area, CCWS reduces the number of threads actively issued on a core when appropriate. This leads to an average 25% fewer L1 data cache misses which results in a harmonic mean 24% performance improvement over previously proposed scheduling policies across a diverse selection of cache-sensitive workloads.},
 acmid = {2457487},
 address = {Washington, DC, USA},
 author = {Rogers, Timothy G. and O'Connor, Mike and Aamodt, Tor M.},
 booktitle = {Proceedings of the 2012 45th Annual IEEE/ACM International Symposium on Microarchitecture},
 doi = {10.1109/MICRO.2012.16},
 isbn = {978-0-7695-4924-8},
 link = {http://dx.doi.org/10.1109/MICRO.2012.16},
 location = {Vancouver, B.C., CANADA},
 numpages = {12},
 pages = {72--83},
 publisher = {IEEE Computer Society},
 series = {MICRO-45},
 title = {Cache-Conscious Wavefront Scheduling},
 year = {2012}
}


@inproceedings{Kumar:2012:AAB:2457472.2457513,
 abstract = {The fixed geometries of current cache designs do not adapt to the working set requirements of modern applications, causing significant inefficiency. The short block lifetimes and moderate spatial locality exhibited by many applications result in only a few words in the block being touched prior to eviction. Unused words occupy between 17 -- 80% of a 64K L1 cache and between 1% -- 79% of a 1MB private LLC. This effectively shrinks the cache size, increases miss rate, and wastes on-chip bandwidth. Scaling limitations of wires mean that unused-word transfers comprise a large fraction (11%) of on-chip cache hierarchy energy consumption. We propose Amoeba-Cache, a design that supports a variable number of cache blocks, each of a different granularity. Amoeba-Cache employs a novel organization that completely eliminates the tag array, treating the storage array as uniform and morph able between tags and data. This enables the cache to harvest space from unused words in blocks for additional tag storage, thereby supporting a variable number of tags (and correspondingly, blocks). Amoeba-Cache adjusts individual cache line granularities according to the spatial locality in the application. It adapts to the appropriate granularity both for different data objects in an application as well as for different phases of access to the same data. Overall, compared to a fixed granularity cache, the Amoeba-Cache reduces miss rate on average (geometric mean) by 18% at the L1 level and by 18% at the L2 level and reduces L1 -- L2 miss bandwidth by ?46%. Correspondingly, Amoeba-Cache reduces on-chip memory hierarchy energy by as much as 36% (mcf) and improves performance by as much as 50% (art).},
 acmid = {2457513},
 address = {Washington, DC, USA},
 author = {Kumar, Snehasish and Zhao, Hongzhou and Shriraman, Arrvindh and Matthews, Eric and Dwarkadas, Sandhya and Shannon, Lesley},
 booktitle = {Proceedings of the 2012 45th Annual IEEE/ACM International Symposium on Microarchitecture},
 doi = {10.1109/MICRO.2012.42},
 isbn = {978-0-7695-4924-8},
 keyword = {Cache architecture, Energy efficiency, Adaptive granularity, Memory system, Amoeba-Cache},
 link = {http://dx.doi.org/10.1109/MICRO.2012.42},
 location = {Vancouver, B.C., CANADA},
 numpages = {13},
 pages = {376--388},
 publisher = {IEEE Computer Society},
 series = {MICRO-45},
 title = {Amoeba-Cache: Adaptive Blocks for Eliminating Waste in the Memory Hierarchy},
 year = {2012}
}


@inproceedings{Malladi:2012:RDP:2457472.2457492,
 abstract = {We re-think DRAM power modes by modeling and characterizing inter-arrival times for memory requests to determine the properties an ideal power mode should have. This analysis indicates that even the most responsive of today's power modes are rarely used. Up to 88% of memory is spent idling in an active mode. This analysis indicates that power modes must have much shorter exit latencies than they have today. Wake-up latencies less than 100ns are ideal. To address these challenges, we present MemBlaze, an architecture with DRAMs and links that are capable of fast power up, which provides more opportunities to power down memories. By eliminating DRAM chip timing circuitry, a key contributor to power up latency, and by shifting timing responsibility to the controller, MemBlaze permits data transfers immediately after wake-up and reduces energy per transfer by 50% with no performance impact. Alternatively, in scenarios where DRAM timing circuitry must remain, we explore mechanisms to accommodate DRAMs that power up with less than perfect interface timing. We present MemCorrect which detects timing errors while MemDrowsy lowers transfer rates and widens sampling margins to accommodate timing uncertainty in situations where the interface circuitry must recalibrate after exit from power down state. Combined, MemCorrect and MemDrowsy still reduce energy per transfer by 50% but incur modest (e.g., 10%) performance penalties.},
 acmid = {2457492},
 address = {Washington, DC, USA},
 author = {Malladi, Krishna T. and Shaeffer, Ian and Gopalakrishnan, Liji and Lo, David and Lee, Benjamin C. and Horowitz, Mark},
 booktitle = {Proceedings of the 2012 45th Annual IEEE/ACM International Symposium on Microarchitecture},
 doi = {10.1109/MICRO.2012.21},
 isbn = {978-0-7695-4924-8},
 keyword = {DRAM, Main memory, Energy Efficiency, Circuit-architecture, Data centers, Cloud systems, Emerging technologies, Memory systems, System architecture, Workload characterization},
 link = {http://dx.doi.org/10.1109/MICRO.2012.21},
 location = {Vancouver, B.C., CANADA},
 numpages = {12},
 pages = {131--142},
 publisher = {IEEE Computer Society},
 series = {MICRO-45},
 title = {Rethinking DRAM Power Modes for Energy Proportionality},
 year = {2012}
}


@inproceedings{Khubaib:2012:MEM:2457472.2457498,
 abstract = {Several researchers have recognized in recent years that today's workloads require a micro architecture that can handle single-threaded code at high performance, and multi-threaded code at high throughput, while consuming no more energy than is necessary. This paper proposes Morph Core, a unique approach to satisfying these competing requirements, by starting with a traditional high performance out-of-order core and making minimal changes that can transform it into a highly-threaded in-order SMT core when necessary. The result is a micro architecture that outperforms an aggressive 4-way SMT out-of-order core, "medium" out-of-order cores, small in-order cores, and Core Fusion. Compared to a 2-way SMT out-of-order core, Morph Core increases performance by 10% and reduces energy-delay-squared product by 22%.},
 acmid = {2457498},
 address = {Washington, DC, USA},
 author = {Khubaib and Suleman, M. Aater and Hashemi, Milad and Wilkerson, Chris and Patt, Yale N.},
 booktitle = {Proceedings of the 2012 45th Annual IEEE/ACM International Symposium on Microarchitecture},
 doi = {10.1109/MICRO.2012.36},
 isbn = {978-0-7695-4924-8},
 link = {http://dx.doi.org/10.1109/MICRO.2012.36},
 location = {Vancouver, B.C., CANADA},
 numpages = {12},
 pages = {305--316},
 publisher = {IEEE Computer Society},
 series = {MICRO-45},
 title = {MorphCore: An Energy-Efficient Microarchitecture for High Performance ILP and High Throughput TLP},
 year = {2012}
}


@proceedings{Farrens:2013:2540708,
 abstract = {The program includes 39 papers selected from 239 submissions. The program committee (PC) of 40 distinguished experts used a two-round review process. In the first round, all papers received 2 reviews from PC members and one review from an external expert. The 169 papers with at least one review with a positive score for overall merit were promoted to the second round. In the second round, papers received at least one review from a PC member and one review from an external expert, for a minimum total of 5 reviews per paper. Nevertheless, we solicited up to 7 reviews for some papers in order to provide additional expert opinions for the selection process. Overall, 363 reviewers submitted 1,105 paper reviews.},
 address = {New York, NY, USA},
 isbn = {978-1-4503-2638-4},
 location = {Davis, California},
 publisher = {ACM},
 title = {MICRO-46: Proceedings of the 46th Annual IEEE/ACM International Symposium on Microarchitecture},
 year = {2013}
}


@inproceedings{Sheikh:2012:CD:2457472.2457509,
 abstract = {Mobile and PC/server class processor companies continue to roll out flagship core micro architectures that are faster than their predecessors. Meanwhile placing more cores on a chip coupled with constant supply voltage puts per-core energy consumption at a premium. Hence, the challenge is to find future micro architecture optimizations that not only increase performance but also conserve energy. Eliminating branch mispredictions--which waste both time and energy--is valuable in this respect. We first explore the control-flow landscape by characterizing mispredictions in four benchmark suites. We find that a third of mispredictions-per-1K-instructions (MPKI) come from what we call separable branches: branches with large control-dependent regions (not suitable for if-conversion), whose backward slices do not depend on their control-dependent instructions or have only a short dependence. We propose control-flow decoupling (CFD) to eradicate mispredictions of separable branches. The idea is to separate the loop containing the branch into two loops: the first contains only the branch's predicate computation and the second contains the branch and its control-dependent instructions. The first loop communicates branch outcomes to the second loop through an architectural queue. Micro architecturally, the queue resides in the fetch unit to drive timely, non-speculative fetching or skipping of successive dynamic instances of the control-dependent region. Either the programmer or compiler can transform a loop for CFD, and we evaluate both. On a micro architecture configured similar to Intel's Sandy Bridge core, CFD increases performance by up to 43%, and reduces energy consumption by up to 41%. Moreover, for some applications, CFD is a necessary catalyst for future complexity-effective large-window architectures to tolerate memory latency.},
 acmid = {2457509},
 address = {Washington, DC, USA},
 author = {Sheikh, Rami and Tuck, James and Rotenberg, Eric},
 booktitle = {Proceedings of the 2012 45th Annual IEEE/ACM International Symposium on Microarchitecture},
 doi = {10.1109/MICRO.2012.38},
 isbn = {978-0-7695-4924-8},
 keyword = {branch prediction, predication, pre-execution, superscalar processor, hardware/software codesign, ISA extensions},
 link = {http://dx.doi.org/10.1109/MICRO.2012.38},
 location = {Vancouver, B.C., CANADA},
 numpages = {12},
 pages = {329--340},
 publisher = {IEEE Computer Society},
 series = {MICRO-45},
 title = {Control-Flow Decoupling},
 year = {2012}
}


@proceedings{Carro:2011:2155620,
 abstract = {This year, we present to you a technical program consisting of 44 technical papers, selected from among 209 submissions by a Program Committee composed of 43 active researchers and practitioners in our field. The challenge of selecting which papers to accept among the 209 submissions was significant. To support this selection process, we solicited at least four reviews for each paper, and the vast majority of the papers had five reviews. A typical paper was reviewed by three program committee members and two other experts in our field. Overall, 1018 reviews were generated (on average, 4.9 reviews per paper), 629 of which came from the program committee members and 389 from experts outside of the program committee. For all papers but one, at least two of the reviewers indicated that they are at least Knowledgeable: know most if not all of the relevant work, understand the problem very well, and for 81% of the papers the average level of expertise among its reviewers was at least Knowledgeable. This year we tried a new approach to how author responses to reviews are handled - we asked both PC and external reviewers to read all of the reviews and the authors responses, and provide a separate post-rebuttal score that will be used to decide which papers will be discussed in the program committee meeting. We are especially grateful to our external reviewers for doing this - some of them reviewed and then graded four papers, which makes their workload about 25% of that assigned to the average PC member. We introduced a set of detailed formatting rules that made it possible to set a reasonable limit of 28 pages for submissions. Our goal was to reduce the variance in the amount of work each PC member and each reviewer had to perform while encouraging authors to succinctly present their work.},
 address = {New York, NY, USA},
 isbn = {978-1-4503-1053-6},
 location = {Porto Alegre, Brazil},
 publisher = {ACM},
 title = {MICRO-44: Proceedings of the 44th Annual IEEE/ACM International Symposium on Microarchitecture},
 year = {2011}
}


@inproceedings{Bertran:2012:SEC:2457472.2457499,
 abstract = {Microprocessor-based systems today are composed of multi-core, multi-threaded processors with complex cache hierarchies and gigabytes of main memory. Accurate characterization of such a system, through predictive pre-silicon modeling and/or diagnostic post silicon measurement based analysis are increasingly cumbersome and error prone. This is especially true of energy-related characterization studies. In this paper, we take the position that automated micro-benchmarks generated with particular objectives in mind hold the key to obtaining accurate energy-related characterization. As such, we first present a flexible micro-benchmark generation framework (MicroProbe) that is used to probe complex multi-core/multithreaded systems with a variety and range of energy-related queries in mind. We then present experimental results centered around an IBM POWER7 CMP/SMT system to demonstrate how the systematically generated micro-benchmarks can be used to answer three specific queries: (a) How to project application-specific (and if needed, phase-specific) power consumption with component-wise breakdowns? (b) How to measure energy-per-instruction (EPI) values for the target machine? (c) How to bound the worst-case (maximum) power consumption in order to determine safe, but practical (i.e. affordable) packaging or cooling solutions? The solution approaches to the above problems are all new. Hardware measurement based analysis shows superior power projection accuracy (with error margins of less than 2.3% across SPEC CPU2006) as well as maxpower stressing capability (with 10.7% increase in processor power over the very worst-case power seen during the execution of SPEC CPU2006 applications).},
 acmid = {2457499},
 address = {Washington, DC, USA},
 author = {Bertran, Ramon and Buyuktosunoglu, Alper and Gupta, Meeta S. and Gonzalez, Marc and Bose, Pradip},
 booktitle = {Proceedings of the 2012 45th Annual IEEE/ACM International Symposium on Microarchitecture},
 doi = {10.1109/MICRO.2012.27},
 isbn = {978-0-7695-4924-8},
 keyword = {automated micro-benchmarks, counter-based power models, energy per instruction, max-power stressmark},
 link = {http://dx.doi.org/10.1109/MICRO.2012.27},
 location = {Vancouver, B.C., CANADA},
 numpages = {13},
 pages = {199--211},
 publisher = {IEEE Computer Society},
 series = {MICRO-45},
 title = {Systematic Energy Characterization of CMP/SMT Processor Systems via Automated Micro-Benchmarks},
 year = {2012}
}


@inproceedings{Sim:2012:MDC:2457472.2457503,
 abstract = {Die-stacking technology allows conventional DRAM to be integrated with processors. While numerous opportunities to make use of such stacked DRAM exist, one promising way is to use it as a large cache. Although previous studies show that DRAM caches can deliver performance benefits, there remain inefficiencies as well as significant hardware costs for auxiliary structures. This paper presents two innovations that exploit the bursty nature of memory requests to streamline the DRAM cache. The first is a low-cost Hit-Miss Predictor (HMP) that virtually eliminates the hardware overhead of the previously proposed multi-megabyte Miss Map structure. The second is a Self-Balancing Dispatch (SBD) mechanism that dynamically sends some requests to the off-chip memory even though the request may have hit in the die-stacked DRAM cache. This makes effective use of otherwise idle off-chip bandwidth when the DRAM cache is servicing a burst of cache hits. These techniques, however, are hampered by dirty (modified) data in the DRAM cache. To ensure correctness in the presence of dirty data in the cache, the HMP must verify that a block predicted as a miss is not actually present, otherwise the dirty block must be provided. This verification process can add latency, especially when DRAM cache banks are busy. In a similar vein, SBD cannot redirect requests to off-chip memory when a dirty copy of the block exists in the DRAM cache. To relax these constraints, we introduce a hybrid write policy for the cache that simultaneously supports write-through and write-back policies for different pages. Only a limited number of pages are permitted to operate in a write-back mode at one time, thereby bounding the amount of dirty data in the DRAM cache. By keeping the majority of the DRAM cache clean, most HMP predictions do not need to be verified, and the self balancing dispatch has more opportunities to redistribute requests (i.e., only requests to the limited number of dirty pages must go to the DRAM cache to maintain correctness). Our proposed techniques improve performance compared to the Miss Map-based DRAM cache approach while simultaneously eliminating the costly Miss Map structure.},
 acmid = {2457503},
 address = {Washington, DC, USA},
 author = {Sim, Jaewoong and Loh, Gabriel H. and Kim, Hyesoon and O'Connor, Mike and Thottethodi, Mithuna},
 booktitle = {Proceedings of the 2012 45th Annual IEEE/ACM International Symposium on Microarchitecture},
 doi = {10.1109/MICRO.2012.31},
 isbn = {978-0-7695-4924-8},
 link = {http://dx.doi.org/10.1109/MICRO.2012.31},
 location = {Vancouver, B.C., CANADA},
 numpages = {11},
 pages = {247--257},
 publisher = {IEEE Computer Society},
 series = {MICRO-45},
 title = {A Mostly-Clean DRAM Cache for Effective Hit Speculation and Self-Balancing Dispatch},
 year = {2012}
}


@inproceedings{Wu:2012:KWA:2457472.2457490,
 abstract = {Data warehousing applications represent an emerging application arena that requires the processing of relational queries and computations over massive amounts of data. Modern general purpose GPUs are high bandwidth architectures that potentially offer substantial improvements in throughput for these applications. However, there are significant challenges that arise due to the overheads of data movement through the memory hierarchy and between the GPU and host CPU. This paper proposes data movement optimizations to address these challenges. Inspired in part by loop fusion optimizations in the scientific computing community, we propose kernel fusion as a basis for data movement optimizations. Kernel fusion fuses the code bodies of two GPU kernels to i) reduce data footprint to cut down data movement throughout GPU and CPU memory hierarchy, and ii) enlarge compiler optimization scope. We classify producer consumer dependences between compute kernels into three types, i) fine-grained thread-to-thread dependences, ii) medium-grained thread block dependences, and iii) coarse-grained kernel dependences. Based on this classification, we propose a compiler framework, Kernel Weaver, that can automatically fuse relational algebra operators thereby eliminating redundant data movement. The experiments on NVIDIA Fermi platforms demonstrate that kernel fusion achieves 2.89x speedup in GPU computation and a 2.35x speedup in PCIe transfer time on average across the micro-benchmarks tested. We present key insights, lessons learned, measurements from our compiler implementation, and opportunities for further improvements.},
 acmid = {2457490},
 address = {Washington, DC, USA},
 author = {Wu, Haicheng and Diamos, Gregory and Cadambi, Srihari and Yalamanchili, Sudhakar},
 booktitle = {Proceedings of the 2012 45th Annual IEEE/ACM International Symposium on Microarchitecture},
 doi = {10.1109/MICRO.2012.19},
 isbn = {978-0-7695-4924-8},
 keyword = {Database, GPU, Compiler Optimization},
 link = {http://dx.doi.org/10.1109/MICRO.2012.19},
 location = {Vancouver, B.C., CANADA},
 numpages = {12},
 pages = {107--118},
 publisher = {IEEE Computer Society},
 series = {MICRO-45},
 title = {Kernel Weaver: Automatically Fusing Database Primitives for Efficient GPU Computation},
 year = {2012}
}


@inproceedings{2012:REV:2457472.2457480,
 abstract = {
                  An abstract is not available.
              },
 acmid = {2457480},
 address = {Washington, DC, USA},
 booktitle = {Proceedings of the 2012 45th Annual IEEE/ACM International Symposium on Microarchitecture},
 doi = {10.1109/MICRO.2012.52},
 isbn = {978-0-7695-4924-8},
 key = {$\!\!$},
 link = {http://dx.doi.org/10.1109/MICRO.2012.52},
 location = {Vancouver, B.C., CANADA},
 pages = {xvii--xix},
 publisher = {IEEE Computer Society},
 series = {MICRO-45},
 title = {Reviewers},
 year = {2012}
}


@inproceedings{Jacobi:2012:TMA:2457472.2457483,
 abstract = {We present the introduction of transactional memory into the next generation IBM System z CPU. We first describe the instruction-set architecture features, including requirements for enterprise-class software RAS. We then describe the implementation in the IBM zEnterprise EC12 (zEC12) microprocessor generation, focusing on how transactional memory can be embedded into the existing cache design and multiprocessor shared-memory infrastructure. We explain practical reasons behind our choices. The zEC12 system is available since September 2012.},
 acmid = {2457483},
 address = {Washington, DC, USA},
 author = {Jacobi, Christian and Slegel, Timothy and Greiner, Dan},
 booktitle = {Proceedings of the 2012 45th Annual IEEE/ACM International Symposium on Microarchitecture},
 doi = {10.1109/MICRO.2012.12},
 isbn = {978-0-7695-4924-8},
 link = {http://dx.doi.org/10.1109/MICRO.2012.12},
 location = {Vancouver, B.C., CANADA},
 numpages = {12},
 pages = {25--36},
 publisher = {IEEE Computer Society},
 series = {MICRO-45},
 title = {Transactional Memory Architecture and Implementation for IBM System Z},
 year = {2012}
}


@inproceedings{Sharifi:2012:AEM:2457472.2457507,
 abstract = {To achieve high performance in emerging multicores, it is crucial to reduce the number of memory accesses that suffer from very high latencies. However, this should be done with care as improving latency of an access can worsen the latency of another as a result of resource sharing. Therefore, the goal should be to balance latencies of memory accesses issued by an application in an execution phase, while ensuring a low average latency value. Targeting Network-on-Chip (NoC) based multicores, we propose two network prioritization schemes that can cooperatively improve performance by reducing end-to-end memory access latencies. Our first scheme prioritizes memory response messages such that, in a given period of time, messages of an application that experience higher latencies than the average message latency for that application are expedited and a more uniform memory latency pattern is achieved. Our second scheme prioritizes the request messages that are destined for idle memory banks over others, with the goal of improving bank utilization and preventing long queues from being built in front of the memory banks. These two network prioritization-based optimizations together lead to uniform memory access latencies with a low average value. Our experiments with a 4x8 mesh network-based multicore show that, when applied together, our schemes can achieve 15%, 10% and 13% performance improvement on memory intensive, memory non-intensive, and mixed multiprogrammed workloads, respectively.},
 acmid = {2457507},
 address = {Washington, DC, USA},
 author = {Sharifi, Akbar and Kultursay, Emre and Kandemir, Mahmut and Das, Chita R.},
 booktitle = {Proceedings of the 2012 45th Annual IEEE/ACM International Symposium on Microarchitecture},
 doi = {10.1109/MICRO.2012.35},
 isbn = {978-0-7695-4924-8},
 link = {http://dx.doi.org/10.1109/MICRO.2012.35},
 location = {Vancouver, B.C., CANADA},
 numpages = {11},
 pages = {294--304},
 publisher = {IEEE Computer Society},
 series = {MICRO-45},
 title = {Addressing End-to-End Memory Access Latency in NoC-Based Multicores},
 year = {2012}
}


@inproceedings{Deng:2012:CCC:2457472.2457494,
 abstract = {Recent work has introduced memory system dynamic voltage and frequency scaling (DVFS), and has suggested that balanced scaling of both CPU and the memory system is the most promising approach for conserving energy in server systems. In this paper, we first demonstrate that CPU and memory system DVFS often conflict when performed independently by separate controllers. In response, we propose Co Scale, the first method for effectively coordinating these mechanisms under performance constraints. Co Scale relies on execution profiling of each core via (existing and new) performance counters, and models of core and memory performance and power consumption. Co Scale explores the set of possible frequency settings in such a way that it efficiently minimizes the full-system energy consumption within the performance bound. Our results demonstrate that, by effectively coordinating CPU and memory power management, Co Scale conserves a significant amount of system energy compared to existing approaches, while consistently remaining within the prescribed performance bounds. The results also show that Co Scale conserves almost as much system energy as an offline, idealized approach.},
 acmid = {2457494},
 address = {Washington, DC, USA},
 author = {Deng, Qingyuan and Meisner, David and Bhattacharjee, Abhishek and Wenisch, Thomas F. and Bianchini, Ricardo},
 booktitle = {Proceedings of the 2012 45th Annual IEEE/ACM International Symposium on Microarchitecture},
 doi = {10.1109/MICRO.2012.22},
 isbn = {978-0-7695-4924-8},
 keyword = {energy conservation, dynamic voltage and frequency scaling, coordination},
 link = {http://dx.doi.org/10.1109/MICRO.2012.22},
 location = {Vancouver, B.C., CANADA},
 numpages = {12},
 pages = {143--154},
 publisher = {IEEE Computer Society},
 series = {MICRO-45},
 title = {CoScale: Coordinating CPU and Memory System DVFS in Server Systems},
 year = {2012}
}


@inproceedings{2012:PI:2457472.2457522,
 abstract = {
                  An abstract is not available.
              },
 acmid = {2457522},
 address = {Washington, DC, USA},
 booktitle = {Proceedings of the 2012 45th Annual IEEE/ACM International Symposium on Microarchitecture},
 doi = {10.1109/MICRO.2012.51},
 isbn = {978-0-7695-4924-8},
 key = {$\!\!$},
 link = {http://dx.doi.org/10.1109/MICRO.2012.51},
 location = {Vancouver, B.C., CANADA},
 pages = {476--},
 publisher = {IEEE Computer Society},
 series = {MICRO-45},
 title = {Publisher's Information},
 year = {2012}
}


@inproceedings{Gebhart:2012:UPC:2457472.2457489,
 abstract = {Modern throughput processors such as GPUs employ thousands of threads to drive high-bandwidth, long-latency memory systems. These threads require substantial on-chip storage for registers, cache, and scratchpad memory. Existing designs hard-partition this local storage, fixing the capacities of these structures at design time. We evaluate modern GPU workloads and find that they have widely varying capacity needs across these different functions. Therefore, we propose a unified local memory which can dynamically change the partitioning among registers, cache, and scratchpad on a per-application basis. The tuning that this flexibility enables improves both performance and energy consumption, and broadens the scope of applications that can be efficiently executed on GPUs. Compared to a hard-partitioned design, we show that unified local memory provides a performance benefit as high as 71% along with an energy reduction up to 33%.},
 acmid = {2457489},
 address = {Washington, DC, USA},
 author = {Gebhart, Mark and Keckler, Stephen W. and Khailany, Brucek and Krashinsky, Ronny and Dally, William J.},
 booktitle = {Proceedings of the 2012 45th Annual IEEE/ACM International Symposium on Microarchitecture},
 doi = {10.1109/MICRO.2012.18},
 isbn = {978-0-7695-4924-8},
 link = {http://dx.doi.org/10.1109/MICRO.2012.18},
 location = {Vancouver, B.C., CANADA},
 numpages = {11},
 pages = {96--106},
 publisher = {IEEE Computer Society},
 series = {MICRO-45},
 title = {Unifying Primary Cache, Scratch, and Register File Memories in a Throughput Processor},
 year = {2012}
}


@inproceedings{Ketterlin:2012:PDA:2457472.2457518,
 abstract = {This paper describes a tool using one or more executions of a sequential program to detect parallel portions of the program. The tool, called Par wiz, uses dynamic binary instrumentation, targets various forms of parallelism, and suggests distinct parallelization actions, ranging from simple directive tagging to elaborate loop transformations. The first part of the paper details the link between the program's static structures (like routines and loops), the memory accesses performed by the program, and the dependencies that are used to highlight potential parallelism. This part also describes the instrumentation involved, and the general architecture of the system. The second part of the paper puts the framework into action. The first study focuses on loop parallelism, targeting OpenMP parallel-for directives, including privatization when necessary. The second study is an adaptation of a well-known vectorization technique based on a slightly richer dependence description, where the tool suggests an elaborate loop transformation. The third study views loops as a graph of (hopefully lightly) dependent iterations. The third part of the paper explains how the overall cost of data-dependence profiling can be reduced. This cost has two major causes: first, instrumenting memory accesses slows down the program, and second, turning memory accesses into dependence graphs consumes processing time. Par wiz uses static analysis of the original (binary) program to provide data at a coarser level, moving from individual accesses to complete loops whenever possible, thereby reducing the impact of both sources of inefficiency.},
 acmid = {2457518},
 address = {Washington, DC, USA},
 author = {Ketterlin, Alain and Clauss, Philippe},
 booktitle = {Proceedings of the 2012 45th Annual IEEE/ACM International Symposium on Microarchitecture},
 doi = {10.1109/MICRO.2012.47},
 isbn = {978-0-7695-4924-8},
 link = {http://dx.doi.org/10.1109/MICRO.2012.47},
 location = {Vancouver, B.C., CANADA},
 numpages = {12},
 pages = {437--448},
 publisher = {IEEE Computer Society},
 series = {MICRO-45},
 title = {Profiling Data-Dependence to Assist Parallelization: Framework, Scope, and Optimization},
 year = {2012}
}


@inproceedings{Wu:2012:IMD:2457472.2457516,
 abstract = {Diverse software and heterogeneous hardware pose new challenges in systems and architecture management. Managers benefit from improving introspective capabilities, which provide data across a spectrum of platforms, from software and data center profilers to performance counters and canary circuits. Despite this wealth of data, management has become more difficult as sophisticated decisions are demanded. To address these challenges, we present modeling strategies for integrated hardware-software analysis. These strategies include (i) identifying shared software behavior, (ii) quantifying that behavior in a portable, micro architecture-independent manner, inferring generalized trends with statistical regression models, and (iv) automatically constructing/updating these models as new software profiles are obtained. Models produced by these strategies are accurate for general SPEC2006 applications with median errors of 8-10%. Predicted and actual performance are strongly correlated with coefficients greater than 0.9. Moreover, when we exploit application semantics and domain-specific software parameters, model accuracy improves and model complexity falls. In a case study for sparse linear algebra, we present models with 5% median error and new capabilities in coordinating hardware-software tuning.},
 acmid = {2457516},
 address = {Washington, DC, USA},
 author = {Wu, Weidan and Lee, Benjamin C.},
 booktitle = {Proceedings of the 2012 45th Annual IEEE/ACM International Symposium on Microarchitecture},
 doi = {10.1109/MICRO.2012.45},
 isbn = {978-0-7695-4924-8},
 link = {http://dx.doi.org/10.1109/MICRO.2012.45},
 location = {Vancouver, B.C., CANADA},
 numpages = {12},
 pages = {413--424},
 publisher = {IEEE Computer Society},
 series = {MICRO-45},
 title = {Inferred Models for Dynamic and Sparse Hardware-Software Spaces},
 year = {2012}
}


@inproceedings{Lunteren:2012:DPW:2457472.2457520,
 abstract = {A growing number of applications rely on fast pattern matching to scan data in real-time for security and analytics purposes. The RegX accelerator in the IBM Power Edge of Network (PowerEN) processor supports these applications using a combination of fast programmable state machines and simple processing units to scan data streams against thousands of regular-expression patterns at state-of-the-art Ethernet link speeds. RegX employs a special rule cache and includes several new micro-architectural features that enable various instruction dispatch and execution options for the processing units. The architecture applies RISC philosophy to special-purpose computing: hardware provides fast, simple primitives, typically performed in a single cycle, which are exploited by an intelligent compiler and system software for high performance. This approach provides the flexibility required to achieve good performance across a wide range of workloads. As implemented in the PowerEN processor, the accelerator achieves a theoretical peak scan rate of 73.6 Gbit/s, and a measured scan rate of about 15 to 40 Gbit/s for typical intrusion detection workloads.},
 acmid = {2457520},
 address = {Washington, DC, USA},
 author = {Lunteren, Jan Van and Hagleitner, Christoph and Heil, Timothy and Biran, Giora and Shvadron, Uzi and Atasu, Kubilay},
 booktitle = {Proceedings of the 2012 45th Annual IEEE/ACM International Symposium on Microarchitecture},
 doi = {10.1109/MICRO.2012.49},
 isbn = {978-0-7695-4924-8},
 link = {http://dx.doi.org/10.1109/MICRO.2012.49},
 location = {Vancouver, B.C., CANADA},
 numpages = {12},
 pages = {461--472},
 publisher = {IEEE Computer Society},
 series = {MICRO-45},
 title = {Designing a Programmable Wire-Speed Regular-Expression Matching Accelerator},
 year = {2012}
}


@inproceedings{Pham:2012:CCL:2457472.2457504,
 abstract = {Translation Look aside Buffers (TLBs) are critical to system performance, particularly as applications demand larger working sets and with the adoption of virtualization. Architectural support for super pages has previously been proposed to improve TLB performance. By allocating contiguous physical pages to contiguous virtual pages, the operating system (OS) constructs super pages which need just one TLB entry rather than the hundreds required for the constituent base pages. While this greatly reduces TLB misses, these gains are often offset by the implementation difficulties of generating and managing ample contiguity for super pages. We show, however, that basic OS memory allocation mechanisms such as buddy allocators and memory compaction naturally assign contiguous physical pages to contiguous virtual pages. Our real-system experiments show that while usually insufficient for super pages, these intermediate levels of contiguity exist under various system conditions and even under high load. In response, we propose Coalesced Large-Reach TLBs (CoLT), which leverage this intermediate contiguity to coalesce multiple virtual-to-physical page translations into single TLB entries. We show that CoLT implementations eliminate 40\% to 58\% of TLB misses on average, improving performance by 14\%. Overall, we demonstrate that the OS naturally generates page allocation contiguity. CoLT exploits this contiguity to eliminate TLB misses for next-generation, big-data applications with low-overhead implementations.},
 acmid = {2457504},
 address = {Washington, DC, USA},
 author = {Pham, Binh and Vaidyanathan, Viswanathan and Jaleel, Aamer and Bhattacharjee, Abhishek},
 booktitle = {Proceedings of the 2012 45th Annual IEEE/ACM International Symposium on Microarchitecture},
 doi = {10.1109/MICRO.2012.32},
 isbn = {978-0-7695-4924-8},
 link = {http://dx.doi.org/10.1109/MICRO.2012.32},
 location = {Vancouver, B.C., CANADA},
 numpages = {12},
 pages = {258--269},
 publisher = {IEEE Computer Society},
 series = {MICRO-45},
 title = {CoLT: Coalesced Large-Reach TLBs},
 year = {2012}
}


@inproceedings{Duong:2012:ICM:2457472.2457514,
 abstract = {Cache management policies such as replacement, bypass, or shared cache partitioning have been relying on data reuse behavior to predict the future. This paper proposes a new way to use dynamic reuse distances to further improve such policies. A new replacement policy is proposed which prevents replacing a cache line until a certain number of accesses to its cache set, called a Protecting Distance (PD). The policy protects a cache line long enough for it to be reused, but not beyond that to avoid cache pollution. This can be combined with a bypass mechanism that also relies on dynamic reuse analysis to bypass lines with less expected reuse. A miss fetch is bypassed if there are no unprotected lines. A hit rate model based on dynamic reuse history is proposed and the PD that maximizes the hit rate is dynamically computed. The PD is recomputed periodically to track a program's memory access behavior and phases. Next, a new multi-core cache partitioning policy is proposed using the concept of protection. It manages lifetimes of lines from different cores (threads) in such a way that the overall hit rate is maximized. The average per-thread lifetime is reduced by decreasing the thread's PD. The single-core PD-based replacement policy with bypass achieves an average speedup of 4.2% over the DIP policy, while the average speedups over DIP are 1.5% for dynamic RRIP (DRRIP) and 1.6% for sampling dead-block prediction (SDP). The 16-core PD-based partitioning policy improves the average weighted IPC by 5.2%, throughput by 6.4% and fairness by 9.9% over thread-aware DRRIP (TA-DRRIP). The required hardware is evaluated and the overhead is shown to be manageable.},
 acmid = {2457514},
 address = {Washington, DC, USA},
 author = {Duong, Nam and Zhao, Dali and Kim, Taesu and Cammarota, Rosario and Valero, Mateo and Veidenbaum, Alexander V.},
 booktitle = {Proceedings of the 2012 45th Annual IEEE/ACM International Symposium on Microarchitecture},
 doi = {10.1109/MICRO.2012.43},
 isbn = {978-0-7695-4924-8},
 keyword = {protecting distance, reuse distance distribution, cache management, replacement policy, bypass policy, partitioning policy, cache pollution, hit rate model},
 link = {https://doi.org/10.1109/MICRO.2012.43},
 location = {Vancouver, B.C., CANADA},
 numpages = {12},
 pages = {389--400},
 publisher = {IEEE Computer Society},
 series = {MICRO-45},
 title = {Improving Cache Management Policies Using Dynamic Reuse Distances},
 year = {2012}
}


@inproceedings{2012:TPI:2457472.2457475,
 abstract = {
                  An abstract is not available.
              },
 acmid = {2457475},
 address = {Washington, DC, USA},
 booktitle = {Proceedings of the 2012 45th Annual IEEE/ACM International Symposium on Microarchitecture},
 doi = {10.1109/MICRO.2012.2},
 isbn = {978-0-7695-4924-8},
 key = {$\!\!$},
 link = {http://dx.doi.org/10.1109/MICRO.2012.2},
 location = {Vancouver, B.C., CANADA},
 pages = {iii--},
 publisher = {IEEE Computer Society},
 series = {MICRO-45},
 title = {Title Page Iii},
 year = {2012}
}


@inproceedings{2012:MPC:2457472.2457478,
 abstract = {
                  An abstract is not available.
              },
 acmid = {2457478},
 address = {Washington, DC, USA},
 booktitle = {Proceedings of the 2012 45th Annual IEEE/ACM International Symposium on Microarchitecture},
 doi = {10.1109/MICRO.2012.6},
 isbn = {978-0-7695-4924-8},
 key = {$\!\!$},
 link = {http://dx.doi.org/10.1109/MICRO.2012.6},
 location = {Vancouver, B.C., CANADA},
 pages = {x--xii},
 publisher = {IEEE Computer Society},
 series = {MICRO-45},
 title = {Message from the Program Chair},
 year = {2012}
}


@inproceedings{Miftakhutdinov:2012:PPI:2457472.2457493,
 abstract = {Dynamic voltage and frequency scaling (DVFS) can make modern processors more power and energy efficient if we can accurately predict the effect of frequency scaling on processor performance. State-of-the-art DVFS performance predictors, however, fail to accurately predict performance when confronted with realistic memory systems. We propose CRIT+BW, the first DVFS performance predictor designed for realistic memory systems. In particular, CRIT+BW takes into account both variable memory access latency and performance effects of prefetching. When evaluated with a realistic memory system, DVFS realizes 65% of potential energy savings when using CRIT+BW, compared to less than 34% when using previously proposed DVFS performance predictors.},
 acmid = {2457493},
 address = {Washington, DC, USA},
 author = {Miftakhutdinov, Rustam and Ebrahimi, Eiman and Patt, Yale N.},
 booktitle = {Proceedings of the 2012 45th Annual IEEE/ACM International Symposium on Microarchitecture},
 doi = {10.1109/MICRO.2012.23},
 isbn = {978-0-7695-4924-8},
 link = {http://dx.doi.org/10.1109/MICRO.2012.23},
 location = {Vancouver, B.C., CANADA},
 numpages = {11},
 pages = {155--165},
 publisher = {IEEE Computer Society},
 series = {MICRO-45},
 title = {Predicting Performance Impact of DVFS for Realistic Memory Systems},
 year = {2012}
}


@inproceedings{Muzahid:2012:VHS:2457472.2457512,
 abstract = {Past work has focused on detecting data races as proxies for Sequential Consistency (SC) violations. However, most data races do not violate SC. In addition, lock-free data structures and synchronization libraries sometimes explicitly employ data races but rely on SC semantics for correctness. Consequently, to uncover SC violations, we need to develop a more precise technique. This paper presents Vulcan, the first hardware scheme to precisely detect SC violations at runtime, in programs running on a relaxed-consistency machine. The scheme leverages cache coherence protocol transactions to dynamically detect cycles in memory access orders across threads. When one such cycle is about to occur, an exception is triggered. For the conditions considered in this paper and with enough hardware, Vulcan suffers neither false positives nor false negatives. In addition, Vulcan induces negligible execution overhead, requires no help from the software, and only takes as input the program executable. Experimental results show that Vulcan detects three new SC violation bugs in the Pthread and Crypt libraries, and in the fmm code from SPLASH-2. Moreover, Vulcan's negligible execution overhead makes it suitable for on-the-fly use.},
 acmid = {2457512},
 address = {Washington, DC, USA},
 author = {Muzahid, Abdullah and Qi, Shanxiang and Torrellas, Josep},
 booktitle = {Proceedings of the 2012 45th Annual IEEE/ACM International Symposium on Microarchitecture},
 doi = {10.1109/MICRO.2012.41},
 isbn = {978-0-7695-4924-8},
 keyword = {Parallel program, Memory model, Sequential consistency violation, Bug},
 link = {http://dx.doi.org/10.1109/MICRO.2012.41},
 location = {Vancouver, B.C., CANADA},
 numpages = {13},
 pages = {363--375},
 publisher = {IEEE Computer Society},
 series = {MICRO-45},
 title = {Vulcan: Hardware Support for Detecting Sequential Consistency Violations Dynamically},
 year = {2012}
}


@inproceedings{Wong:2012:KSE:2457472.2457491,
 abstract = {Server energy proportionality has been improving over the past several years. Many components in a system, such as CPU, memory and disk, have been achieving good energy proportionality behavior. Using a wide range of server power data from the published SPEC power data we show that the overall system energy proportionality has reached 80%. We present two novel metrics, linear deviation and proportionality gap, that provide insights into accurately quantifying energy proportionality. Using these metrics we show that energy proportionality improvements are not uniform across various server utilization levels. In particular, the energy proportionality of even a highly proportional server suffers significantly at non-zero but low utilizations. We propose to tackle the lack of energy proportionality at low utilization using server-level heterogeneity. We present Knight Shift, a server-level heterogenous server architecture that introduces an active low power mode, through the addition of a tightly-coupled compute node called the Knight, enabling two energy-efficient operating regions. We evaluated Knight Shift against a variety of real-world data center workloads using a combination of prototyping and simulation, showing up to 75% energy savings with tail latency bounded by the latency of the Knight and up to 14% improvement to Performance per TCO dollar spent.},
 acmid = {2457491},
 address = {Washington, DC, USA},
 author = {Wong, Daniel and Annavaram, Murali},
 booktitle = {Proceedings of the 2012 45th Annual IEEE/ACM International Symposium on Microarchitecture},
 doi = {10.1109/MICRO.2012.20},
 isbn = {978-0-7695-4924-8},
 link = {http://dx.doi.org/10.1109/MICRO.2012.20},
 location = {Vancouver, B.C., CANADA},
 numpages = {12},
 pages = {119--130},
 publisher = {IEEE Computer Society},
 series = {MICRO-45},
 title = {KnightShift: Scaling the Energy Proportionality Wall Through Server-Level Heterogeneity},
 year = {2012}
}


@inproceedings{2012:ERC:2457472.2457479,
 abstract = {
                  An abstract is not available.
              },
 acmid = {2457479},
 address = {Washington, DC, USA},
 booktitle = {Proceedings of the 2012 45th Annual IEEE/ACM International Symposium on Microarchitecture},
 doi = {10.1109/MICRO.2012.9},
 isbn = {978-0-7695-4924-8},
 key = {$\!\!$},
 link = {http://dx.doi.org/10.1109/MICRO.2012.9},
 location = {Vancouver, B.C., CANADA},
 pages = {xv--xvi},
 publisher = {IEEE Computer Society},
 series = {MICRO-45},
 title = {External Review Committee},
 year = {2012}
}


@inproceedings{Chatterjee:2012:LHD:2457472.2457482,
 abstract = {The DRAM main memory system in modern servers is largely homogeneous. In recent years, DRAM manufacturers have produced chips with vastly differing latency and energy characteristics. This provides the opportunity to build a heterogeneous main memory system where different parts of the address space can yield different latencies and energy per access. The limited prior work in this area has explored smart placement of pages with high activities. In this paper, we propose a novel alternative to exploit DRAM heterogeneity. We observe that the critical word in a cache line can be easily recognized beforehand and placed in a low-latency region of the main memory. Other non-critical words of the cache line can be placed in a low-energy region. We design an architecture that has low complexity and that can accelerate the transfer of the critical word by tens of cycles. For our benchmark suite, we show an average performance improvement of 12.9% and an accompanying memory energy reduction of 15%.},
 acmid = {2457482},
 address = {Washington, DC, USA},
 author = {Chatterjee, Niladrish and Shevgoor, Manjunath and Balasubramonian, Rajeev and Davis, Al and Fang, Zhen and Illikkal, Ramesh and Iyer, Ravi},
 booktitle = {Proceedings of the 2012 45th Annual IEEE/ACM International Symposium on Microarchitecture},
 doi = {10.1109/MICRO.2012.11},
 isbn = {978-0-7695-4924-8},
 link = {http://dx.doi.org/10.1109/MICRO.2012.11},
 location = {Vancouver, B.C., CANADA},
 numpages = {12},
 pages = {13--24},
 publisher = {IEEE Computer Society},
 series = {MICRO-45},
 title = {Leveraging Heterogeneity in DRAM Main Memories to Accelerate Critical Word Access},
 year = {2012}
}


@inproceedings{Park:2012:LTS:2457472.2457488,
 abstract = {Mobile computing as exemplified by the smart phone has become an integral part of our daily lives. The next generation of these devices will be driven by providing an even richer user experience and compelling capabilities: higher definition multimedia, 3D graphics, augmented reality, games, and voice interfaces. To address these goals, the core computing capabilities of the smart phone must be scaled. However, the energy budgets are increasing at a much lower rate, requiring fundamental improvements in computing efficiency. SIMD accelerators offer the combination of high performance and low energy consumption through low control and interconnect overhead. However, SIMD accelerators are not a panacea. Many applications lack sufficient vector parallelism to effectively utilize a large number of SIMD lanes. Further, the use of symmetric hardware lanes leads to low utilization and high static power dissipation as SIMD width is scaled. To address these inefficiencies, this paper focuses on breaking two traditional rules of SIMD processing: homogeneity and static configuration. The Libra accelerator increases SIMD utility by blurring the divide between vector and instruction parallelism to support efficient execution of a wider range of loops, and it increases hardware utilization through the use of heterogeneous hardware across the SIMD lanes. Experimental results show that the 32-lane Libra outperforms traditional SIMD accelerators by an average of 1.58x performance improvement due to higher loop coverage with 29% less energy consumption through heterogeneous hardware.},
 acmid = {2457488},
 address = {Washington, DC, USA},
 author = {Park, Yongjun and Park, Jason Jong Kyu and Park, Hyunchul and Mahlke, Scott},
 booktitle = {Proceedings of the 2012 45th Annual IEEE/ACM International Symposium on Microarchitecture},
 doi = {10.1109/MICRO.2012.17},
 isbn = {978-0-7695-4924-8},
 keyword = {SIMD Architecture, Programmable Accelerator},
 link = {http://dx.doi.org/10.1109/MICRO.2012.17},
 location = {Vancouver, B.C., CANADA},
 numpages = {12},
 pages = {84--95},
 publisher = {IEEE Computer Society},
 series = {MICRO-45},
 title = {Libra: Tailoring SIMD Execution Using Heterogeneous Hardware and Dynamic Configurability},
 year = {2012}
}


@proceedings{2012:2457472,
 abstract = {
                  An abstract is not available.
              },
 address = {Washington, DC, USA},
 isbn = {978-0-7695-4924-8},
 issn = {1072-4451},
 key = {$\!\!$},
 location = {Vancouver, B.C., CANADA},
 publisher = {IEEE Computer Society},
 title = {MICRO-45: Proceedings of the 2012 45th Annual IEEE/ACM International Symposium on Microarchitecture},
 year = {2012}
}


@inproceedings{2012:MGC:2457472.2457477,
 abstract = {
                  An abstract is not available.
              },
 acmid = {2457477},
 address = {Washington, DC, USA},
 booktitle = {Proceedings of the 2012 45th Annual IEEE/ACM International Symposium on Microarchitecture},
 doi = {10.1109/MICRO.2012.5},
 isbn = {978-0-7695-4924-8},
 key = {$\!\!$},
 link = {http://dx.doi.org/10.1109/MICRO.2012.5},
 location = {Vancouver, B.C., CANADA},
 pages = {ix--},
 publisher = {IEEE Computer Society},
 series = {MICRO-45},
 title = {Message from the General Chair},
 year = {2012}
}


@inproceedings{Chen:2012:NND:2457472.2457505,
 abstract = {While power-gating is a promising technique to mitigate the increasing static power of a chip, a fundamental requirement is for the idle periods to be sufficiently long to compensate for the power-gating and performance overhead. On-chip routers are potentially good targets for power optimizations, but few works have explored effective ways of power-gating them due to the intrinsic dependence between the node and router--any packet (sent, received or forwarded) must wakeup the router before being transferred, thus breaking the potentially long idle period into fragmented intervals. Simulation shows that directly applying conventional power-gating techniques would cause frequent state-transitions and significant energy and performance overhead. In this paper, we propose NoRD (Node-Router Decoupling), a novel power-aware on-chip network approach that provides for power-gating bypass to decouple the node's ability for transferring packets from the powered-on/off status of the associated router, thereby maximizing the length of router idle periods. Full system evaluation using PARSEC benchmarks shows that the proposed approach can substantially reduce the number of state-transitions, completely hide wakeup latency from the critical path of packet transport and eliminate node-network disconnection problems. Compared to an optimized conventional power-gating technique applied to on-chip routers, NoRD can further reduce the router static energy by 29.9% and improve the average packet latency by 26.3%, with only 3% additional area overhead.},
 acmid = {2457505},
 address = {Washington, DC, USA},
 author = {Chen, Lizhong and Pinkston, Timothy M.},
 booktitle = {Proceedings of the 2012 45th Annual IEEE/ACM International Symposium on Microarchitecture},
 doi = {10.1109/MICRO.2012.33},
 isbn = {978-0-7695-4924-8},
 link = {http://dx.doi.org/10.1109/MICRO.2012.33},
 location = {Vancouver, B.C., CANADA},
 numpages = {12},
 pages = {270--281},
 publisher = {IEEE Computer Society},
 series = {MICRO-45},
 title = {NoRD: Node-Router Decoupling for Effective Power-gating of On-Chip Routers},
 year = {2012}
}


@inproceedings{Radojkovic:2012:KPS:2457472.2457515,
 abstract = {One of the greatest challenges in computer architecture is how to write efficient, portable, and correct software for multi-core processors. A promising approach is to expose more parallelism to the compiler, through the use of domain-specific languages. The compiler can then perform complex transformations that the programmer would otherwise have had to do. Many important applications related to audio and video encoding, software radio and signal processing have regular behavior that can be represented using a stream programming language. When written in such a language, a portable stream program can be automatically mapped by the stream compiler onto multicore hardware. One of the most difficult tasks of the stream compiler is partitioning the stream program into software threads. The choice of partition significantly affects performance, but finding the optimal partition is an NP-complete problem. This paper presents a method, based on Extreme Value theory (EVT), that statistically estimates the performance of the optimal partition. Knowing the optimal performance improves the evaluation of any partitioning algorithm, and it is the most important piece of information when deciding whether an existing algorithm should be enhanced. We use the method to evaluate a recently-published partitioning algorithm based on a heuristic. We further analyze how the statistical method is affected by the choice of sampling method, and we recommend how sampling should be done. Finally, since a heuristic-based algorithm may not always be available, the user may try to find a good partition by picking the best from a random sample. We analyze whether this approach is likely to find a good partition. To the best of our knowledge, this study is the first application of EVT to a graph partitioning problem.},
 acmid = {2457515},
 address = {Washington, DC, USA},
 author = {Radojkovic, Petar and Carpenter, Paul M. and Moreto, Miquel and Ramirez, Alex and Cazorla, Francisco J.},
 booktitle = {Proceedings of the 2012 45th Annual IEEE/ACM International Symposium on Microarchitecture},
 doi = {10.1109/MICRO.2012.44},
 isbn = {978-0-7695-4924-8},
 keyword = {Kernel partitioning, StremIt, Extreme Value Theory},
 link = {http://dx.doi.org/10.1109/MICRO.2012.44},
 location = {Vancouver, B.C., CANADA},
 numpages = {12},
 pages = {401--412},
 publisher = {IEEE Computer Society},
 series = {MICRO-45},
 title = {Kernel Partitioning of Streaming Applications: A Statistical Approach to an NP-complete Problem},
 year = {2012}
}


@inproceedings{Prodromou:2012:NOR:2457472.2457486,
 abstract = {The widespread proliferation of the Chip Multi-Processor (CMP) paradigm has cemented the criticality of the on-chip interconnection fabric. The Network-on-Chip (NoC) is becoming increasingly susceptible to emerging reliability threats. As technology feature sizes diminish into the nanoscale regime, reliability and process variability artifacts within the NoC start to become prominent. The need to detect the occurrence of faults at run-time is steadily becoming imperative. In this work, we propose NoCAlert, a comprehensive on-line and real-time fault detection mechanism that demonstrates 0% false negatives within the interconnect, for the fault model and stimulus set used in this study. Based on the concept of invariance checking, NoCAlert employs a group of lightweight micro-checker modules that collectively implement real-time hardware assertions. The checkers operate seamlessly and concurrently with normal NoC operation, thus eliminating the need for periodic, or triggered-based, self-testing. More importantly, 97% of the faults are detected instantaneously. Extensive cycle-accurate simulations in a 64-node CMP demonstrate the efficacy of the proposed technique. Finally, hardware synthesis results using commercial 65 nm technology libraries indicate minimal area and power overhead of 3% and less than 1%, respectively, and negligible impact on the router's critical path.},
 acmid = {2457486},
 address = {Washington, DC, USA},
 author = {Prodromou, Andreas and Panteli, Andreas and Nicopoulos, Chrysostomos and Sazeides, Yiannakis},
 booktitle = {Proceedings of the 2012 45th Annual IEEE/ACM International Symposium on Microarchitecture},
 doi = {10.1109/MICRO.2012.15},
 isbn = {978-0-7695-4924-8},
 link = {http://dx.doi.org/10.1109/MICRO.2012.15},
 location = {Vancouver, B.C., CANADA},
 numpages = {12},
 pages = {60--71},
 publisher = {IEEE Computer Society},
 series = {MICRO-45},
 title = {NoCAlert: An On-Line and Real-Time Fault Detection Mechanism for Network-on-Chip Architectures},
 year = {2012}
}


@inproceedings{Atta:2012:SSI:2457472.2457497,
 abstract = {Online transaction processing (OLTP) is at the core of many data center applications. OLTP workloads are known to have large instruction footprints that foil existing L1 instruction caches resulting in poor overall performance. Prefetching can reduce the impact of such instruction cache miss stalls, however, state-of-the-art solutions require large dedicated hardware tables on the order of 40KB in size. SLICC is a programmer transparent, low cost technique to minimize instruction cache misses when executing OLTP workloads. SLICC migrates threads, spreading their instruction footprint over several L1 caches. It exploits repetition within and across transactions, where a transaction's first iteration prefetches the instructions for subsequent iterations or similar subsequent transactions. SLICC reduces instruction misses by 58% on average for TPC-C and TPCE, thereby improving performance by 68%. When compared to a state-of-the-art prefetcher, and notwithstanding the increased storage overheads (42x as compared to SLICC), performance using SLICC is 21% higher for TPC-E and within 2% for TPC-C.},
 acmid = {2457497},
 address = {Washington, DC, USA},
 author = {Atta, Islam and Tozun, Pinar and Ailamaki, Anastasia and Moshovos, Andreas},
 booktitle = {Proceedings of the 2012 45th Annual IEEE/ACM International Symposium on Microarchitecture},
 doi = {10.1109/MICRO.2012.26},
 isbn = {978-0-7695-4924-8},
 keyword = {instruction cache, thread migration, OLTP},
 link = {http://dx.doi.org/10.1109/MICRO.2012.26},
 location = {Vancouver, B.C., CANADA},
 numpages = {11},
 pages = {188--198},
 publisher = {IEEE Computer Society},
 series = {MICRO-45},
 title = {SLICC: Self-Assembly of Instruction Cache Collectives for OLTP Workloads},
 year = {2012}
}


@inproceedings{Huang:2012:AFP:2457472.2457501,
 abstract = {There are not yet practical and accurate ways to directly measure core power in a microprocessor. This limits the granularity of measurement and control for computer power management. We overcome this limitation by presenting an accurate runtime per-core power proxy which closely estimates true core power. This enables new fine-grained microprocessor power management techniques at the core level. For example, cloud environments could manage and bill virtual machines for energy consumption associated with the core. The power model underlying our power proxy also enables energy-efficiency controllers to perform what-if analysis, instead of merely reacting to current conditions. We develop and validate a methodology for accurate power proxy training at both chip and core levels. Our implementation of power proxies uses on-chip logic in a high-performance multi-core processor and associated platform firmware. The power proxies account for full voltage and frequency ranges, as well as chip-to-chip process variations. For fixed clock frequency operation, a mean unsigned error of 1.8% for fine-grained 32ms samples across all workloads was achieved. For an interval of an entire workload, we achieve an average error of-0.2%. Similar results were achieved for voltage-scaling scenarios, too. We also present two sample applications of the power proxy: (1) per-core power billing for cloud computing services, and (2) simultaneous runtime energy saving comparisons among different power management policies without running each policy separately.},
 acmid = {2457501},
 address = {Washington, DC, USA},
 author = {Huang, Wei and Lefurgy, Charles and Kuk, William and Buyuktosunoglu, Alper and Floyd, Michael and Rajamani, Karthick and Allen-Ware, Malcolm and Brock, Bishop},
 booktitle = {Proceedings of the 2012 45th Annual IEEE/ACM International Symposium on Microarchitecture},
 doi = {10.1109/MICRO.2012.29},
 isbn = {978-0-7695-4924-8},
 link = {http://dx.doi.org/10.1109/MICRO.2012.29},
 location = {Vancouver, B.C., CANADA},
 numpages = {11},
 pages = {224--234},
 publisher = {IEEE Computer Society},
 series = {MICRO-45},
 title = {Accurate Fine-Grained Processor Power Proxies},
 year = {2012}
}


@inproceedings{Demetriades:2012:PCC:2457472.2457511,
 abstract = {Predicting target processors that a coherence request must be delivered to can improve the miss handling latency in shared memory systems. In directory coherence protocols, directly communicating with the predicted processors avoids costly indirection to the directory. In snooping protocols, prediction relaxes the high bandwidth requirements by replacing broadcast with multicast. In this work, we propose a new run-time coherence target prediction scheme that exploits the inherent correlation between synchronization points in a program and coherence communication. Our workload-driven analysis shows that by exposing synchronization points to hardware and tracking them at run time, we can simply and effectively track stable and repetitive communication patterns. Based on this observation, we build a predictor that can improve the miss latency of a directory protocol by 13%. Compared with existing address- and instruction-based prediction techniques, our predictor achieves comparable performance using substantially smaller power and storage overheads.},
 acmid = {2457511},
 address = {Washington, DC, USA},
 author = {Demetriades, Socrates and Cho, Sangyeun},
 booktitle = {Proceedings of the 2012 45th Annual IEEE/ACM International Symposium on Microarchitecture},
 doi = {10.1109/MICRO.2012.40},
 isbn = {978-0-7695-4924-8},
 keyword = {coherence prediction, destination set prediction, synchronization points},
 link = {http://dx.doi.org/10.1109/MICRO.2012.40},
 location = {Vancouver, B.C., CANADA},
 numpages = {12},
 pages = {351--362},
 publisher = {IEEE Computer Society},
 series = {MICRO-45},
 title = {Predicting Coherence Communication by Tracking Synchronization Points at Run Time},
 year = {2012}
}


@inproceedings{Esmaeilzadeh:2012:NAG:2457472.2457519,
 abstract = {This paper describes a learning-based approach to the acceleration of approximate programs. We describe the \emph{Parrot transformation}, a program transformation that selects and trains a neural network to mimic a region of imperative code. After the learning phase, the compiler replaces the original code with an invocation of a low-power accelerator called a \emph{neural processing unit} (NPU). The NPU is tightly coupled to the processor pipeline to accelerate small code regions. Since neural networks produce inherently approximate results, we define a programming model that allows programmers to identify approximable code regions -- code that can produce imprecise but acceptable results. Offloading approximable code regions to NPUs is faster and more energy efficient than executing the original code. For a set of diverse applications, NPU acceleration provides whole-application speedup of 2.3x and energy savings of 3.0x on average with quality loss of at most 9.6%.},
 acmid = {2457519},
 address = {Washington, DC, USA},
 author = {Esmaeilzadeh, Hadi and Sampson, Adrian and Ceze, Luis and Burger, Doug},
 booktitle = {Proceedings of the 2012 45th Annual IEEE/ACM International Symposium on Microarchitecture},
 doi = {10.1109/MICRO.2012.48},
 isbn = {978-0-7695-4924-8},
 keyword = {Approximate Computing, Neural Networks, Accelerator, Neural Processing Unit, NPU},
 link = {http://dx.doi.org/10.1109/MICRO.2012.48},
 location = {Vancouver, B.C., CANADA},
 numpages = {12},
 pages = {449--460},
 publisher = {IEEE Computer Society},
 series = {MICRO-45},
 title = {Neural Acceleration for General-Purpose Approximate Programs},
 year = {2012}
}


@inproceedings{Morris:2012:DRP:2457472.2457506,
 abstract = {As power dissipation in future Networks-on-Chips (NoCs) is projected to be a major bottleneck, researchers are actively engaged in developing alternate power-efficient technology solutions. Photonic interconnects is a disruptive technology solution that is capable of delivering the communication bandwidth at low power dissipation when the number of cores is scaled to large numbers. Similarly, 3D stacking is another interconnect technology solution that can lead to low energy/bit for communication. In this paper, we propose to combine photonic interconnects with 3D stacking to develop a scalable, reconfigurable, power-efficient and high-performance interconnect for future many-core systems, called R-3PO (Reconfigurable 3DPhotonic Networks-on-Chip). We propose to develop a multi-layer photonic interconnect that can dynamically reconfigure without system intervention and allocate channel bandwidth from less utilized links to more utilized communication links. In addition to improving performance, reconfiguration can re-allocate bandwidth around faulty channels, thereby increasing the resiliency of the architecture and gracefully degrading performance. For 64-core reconfigured network, our simulation results indicate that the performance can be further improved by 10%-25% for Splash-2, PARSEC and SPEC CPU2006 benchmarks, where as simulation results for 256-core chip indicate a performance improvement of more than 25% while saving 6%-36% energy when compared to state-of-the-art on-chip electrical and optical networks.},
 acmid = {2457506},
 address = {Washington, DC, USA},
 author = {Morris, Randy and Kodi, Avinash Karanth and Louri, Ahmed},
 booktitle = {Proceedings of the 2012 45th Annual IEEE/ACM International Symposium on Microarchitecture},
 doi = {10.1109/MICRO.2012.34},
 isbn = {978-0-7695-4924-8},
 keyword = {Networks-on-Chip (NoC), Photonic, Reconfiguration, Fault Tolerance},
 link = {http://dx.doi.org/10.1109/MICRO.2012.34},
 location = {Vancouver, B.C., CANADA},
 numpages = {12},
 pages = {282--293},
 publisher = {IEEE Computer Society},
 series = {MICRO-45},
 title = {Dynamic Reconfiguration of 3D Photonic Networks-on-Chip for Maximizing Performance and Improving Fault Tolerance},
 year = {2012}
}


@inproceedings{Hayes:2012:VED:2457472.2457495,
 abstract = {Database management systems (DBMS) have become an essential tool for industry and research and are often a significant component of data centres. As a result of this criticality, efficient execution of DBMS engines has become an important area of investigation. This work takes a top-down approach to accelerating decision support systems (DSS) on x86-64 microprocessors using vector ISA extensions. In the first step, a leading DSS DBMS is analysed for potential data-level parallelism. We discuss why the existing multimedia SIMD extensions (SSE/AVX) are not suitable for capturing this parallelism and propose a complementary instruction set reminiscent of classical vector architectures. The instruction set is implemented using unintrusive modifications to a modern x86-64 micro architecture tailored for DSS DBMS. The ISA and micro architecture are evaluated using a cycle-accurate x86-64 micro architectural simulator coupled with a highly-detailed memory simulator. We have found a single operator is responsible for 41% of total execution time for the TPC-H DSS benchmark. Our results show performance speedups between 1.94x and 4.56x for an implementation of this operator run with our proposed hardware modifications.},
 acmid = {2457495},
 address = {Washington, DC, USA},
 author = {Hayes, Timothy and Palomar, Oscar and Unsal, Osman and Cristal, Adrian and Valero, Mateo},
 booktitle = {Proceedings of the 2012 45th Annual IEEE/ACM International Symposium on Microarchitecture},
 doi = {10.1109/MICRO.2012.24},
 isbn = {978-0-7695-4924-8},
 keyword = {microarchitecture, database, dbms, decision, support, vector, simd, dlp, parallelism},
 link = {http://dx.doi.org/10.1109/MICRO.2012.24},
 location = {Vancouver, B.C., CANADA},
 numpages = {11},
 pages = {166--176},
 publisher = {IEEE Computer Society},
 series = {MICRO-45},
 title = {Vector Extensions for Decision Support DBMS Acceleration},
 year = {2012}
}


@inproceedings{Lotfi-Kamran:2012:NMS:2457472.2457496,
 abstract = {Scale-out server workloads benefit from many-core processor organizations that enable high throughput thanks to abundant request-level parallelism. A key characteristic of these workloads is the large instruction footprint that exceeds the capacity of private caches. While a shared last-level cache (LLC) can capture the instruction working set, it necessitates a low-latency interconnect fabric to minimize the core stall time on instruction fetches serviced by the LLC. Many-core processors with a mesh interconnect sacrifice performance on scale-out workloads due to NOC-induced delays. Low-diameter topologies can overcome the performance limitations of meshes through rich inter-node connectivity, but at a high area expense. To address the drawbacks of existing designs, this work introduces NOC-Out--a many-core processor organization that affords low LLC access delays at a small area cost. NOC-Out is tuned to accommodate the bilateral core-to-cache access pattern, characterized by minimal coherence activity and lack of inter-core communication, that is dominant in scale-out workloads. Optimizing for the bilateral access pattern, NOC-Out segregates cores and LLC banks into distinct network regions and reduces costly network connectivity by eliminating the majority of inter-core links. NOC-Out further simplifies the interconnect through the use of low-complexity tree-based topologies. A detailed evaluation targeting a 64-core CMP and a set of scale-out workloads reveals that NOC-Out improves system performance by 17% and reduces network area by 28% over a tiled mesh-based design. Compared to a design with a richly-connected flattened butterfly topology, NOC-Out reduces network area by 9x while matching the performance.},
 acmid = {2457496},
 address = {Washington, DC, USA},
 author = {Lotfi-Kamran, Pejman and Grot, Boris and Falsafi, Babak},
 booktitle = {Proceedings of the 2012 45th Annual IEEE/ACM International Symposium on Microarchitecture},
 doi = {10.1109/MICRO.2012.25},
 isbn = {978-0-7695-4924-8},
 link = {http://dx.doi.org/10.1109/MICRO.2012.25},
 location = {Vancouver, B.C., CANADA},
 numpages = {11},
 pages = {177--187},
 publisher = {IEEE Computer Society},
 series = {MICRO-45},
 title = {NOC-Out: Microarchitecting a Scale-Out Processor},
 year = {2012}
}


