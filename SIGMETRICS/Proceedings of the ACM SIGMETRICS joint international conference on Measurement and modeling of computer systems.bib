@inproceedings{Krevat:2011:AIL:1993744.1993788,
 abstract = {
                  An abstract is not available.
              },
 acmid = {1993788},
 address = {New York, NY, USA},
 author = {Krevat, Elie and Shiran, Tomer and Anderson, Eric and Tucek, Joseph and Wylie, Jay J. and Ganger, Gregory R.},
 booktitle = {Proceedings of the ACM SIGMETRICS Joint International Conference on Measurement and Modeling of Computer Systems},
 doi = {10.1145/1993744.1993788},
 isbn = {978-1-4503-0814-4},
 keyword = {cloud computing, data-intensive computing, efficiency, performance},
 link = {http://doi.acm.org/10.1145/1993744.1993788},
 location = {San Jose, California, USA},
 numpages = {2},
 pages = {125--126},
 publisher = {ACM},
 series = {SIGMETRICS '11},
 title = {Applying Idealized Lower-bound Runtime Models to Understand Inefficiencies in Data-intensive Computing},
 year = {2011}
}


@inproceedings{Sengupta:2011:CDC:1993744.1993782,
 abstract = {Large scale data centers are enabling the new era of Internet cloud computing. The computing platform in such data centers consists of low-cost commodity servers that, in large numbers and with software support, match the performance and reliability of expensive enterprise-class servers of yesterday, at a fraction of the cost. The network interconnect within the data center, however, has not seen the same scale of commoditization or dropping price points. Today's data centers use expensive enterprise-class networking equipment and associated best-practices that were not designed for the requirements of Internet-scale data center services -- they severely limit server-to-server network capacity, create fragmented pools of servers that do not allow any service to run on any server, and have poor reliability and utilization. The commoditization and redesign of data center networks to meet cloud computing requirements is the next frontier of innovation in the data center. Recent research in data center networks addresses many of these aspects involving both scale and commoditization. By creating large flat Layer 2 networks, data centers can provide the view of a flat unfragmented pool of servers to hosted services. By using traffic engineering methods (based on both oblivious and adaptive routing techniques) on specialized network topologies, the data center network can handle arbitrary and rapidly changing communication patterns between servers. By making data centers modular for incremental growth, the up-front investment in infrastructure can be reduced, thus increasing their economic feasibility. This is an exciting time to work in the data center networking area, as the industry is on the cusp of big changes, driven by the need to run Internet-scale services, enabled by the availability of low-cost commodity switches/routers, and fostered by creative and novel architectural innovations. What the Tutorial will cover: We will begin with an introduction to data centers for Internet/cloud services. We will survey several next-generation data center network designs that meet the criteria of allowing any service to run on any server in a flat unfragmented pool of servers and providing bandwidth guarantees for arbitrary communication patterns among servers (limited only by server line card rates). These span efforts from academia and industry research labs, including VL2, Portland, SEATTLE, Hedera, and BCube, and ongoing standardization activities like IEEE Data Center Ethernet (DCE) and IEEE TRILL. We will also cover other emerging aspects of data center networking like energy proportionality for greener data center networks.},
 acmid = {1993782},
 address = {New York, NY, USA},
 author = {Sengupta, Sudipta},
 booktitle = {Proceedings of the ACM SIGMETRICS Joint International Conference on Measurement and Modeling of Computer Systems},
 doi = {10.1145/1993744.1993782},
 isbn = {978-1-4503-0814-4},
 keyword = {cloud data centers, data center networks, data center traffic measurement, scalable commodity networking},
 link = {http://doi.acm.org/10.1145/1993744.1993782},
 location = {San Jose, California, USA},
 numpages = {2},
 pages = {355--356},
 publisher = {ACM},
 series = {SIGMETRICS '11},
 title = {Cloud Data Center Networks: Technologies, Trends, and Challenges},
 year = {2011}
}


@inproceedings{Ihm:2011:TUM:1993744.1993797,
 abstract = {As the nature of Web traffic evolves over time, we must update our understanding of underlying nature of today's Web, which is necessary to improve response time, understand caching effectiveness, and to design intermediary systems, such as firewalls, security analyzers, and reporting or management systems. In this paper, we analyze five years (2006-2010) of real Web traffic from a globally-distributed proxy system, which captures the browsing behavior of over 70,000 daily users from 187 countries. Using this data set, we examine major changes in Web traffic characteristics during this period, and also investigate the redundancy of this traffic, using both traditional object-level caching as well as content-based approaches.},
 acmid = {1993797},
 address = {New York, NY, USA},
 author = {Ihm, Sunghwan and Pai, Vivek S.},
 booktitle = {Proceedings of the ACM SIGMETRICS Joint International Conference on Measurement and Modeling of Computer Systems},
 doi = {10.1145/1993744.1993797},
 isbn = {978-1-4503-0814-4},
 keyword = {web caching, web traffic analysis},
 link = {http://doi.acm.org/10.1145/1993744.1993797},
 location = {San Jose, California, USA},
 numpages = {2},
 pages = {143--144},
 publisher = {ACM},
 series = {SIGMETRICS '11},
 title = {Towards Understanding Modern Web Traffic},
 year = {2011}
}


@inproceedings{Tsitsiklis:2011:PCD:1993744.1993759,
 abstract = {We propose and analyze a multi-server model that captures a performance trade-off between centralized and distributed processing. In our model, a fraction p of an available resource is deployed in a centralized manner (e.g., to serve a most loaded station) while the remaining fraction 1-p is allocated to local servers that can only serve requests addressed specifically to their respective stations. Using a fluid model approach, we demonstrate a surprising phase transition in steady-state delay, as p changes: in the limit of a large number of stations, and when any amount of centralization is available (p>0), the average queue length in steady state scales as log 1/1-p 1/1-λ when the traffic intensity λ goes to 1. This is exponentially smaller than the usual M/M/1-queue delay scaling of 1/1-λ, obtained when all resources are fully allocated to local stations (p=0). This indicates a strong qualitative impact of even a small degree of centralization. We prove convergence to a fluid limit, and characterize both the transient and steady-state behavior of the finite system, in the limit as the number of stations N goes to infinity. We show that the queue-length process converges to a unique fluid trajectory (over any finite time interval, as N → ∞), and that this fluid trajectory converges to a unique invariant state vI, for which a simple closed-form expression is obtained. We also show that the steady-state distribution of the N-server system concentrates on vI as N goes to infinity.},
 acmid = {1993759},
 address = {New York, NY, USA},
 author = {Tsitsiklis, John N. and Xu, Kuang},
 booktitle = {Proceedings of the ACM SIGMETRICS Joint International Conference on Measurement and Modeling of Computer Systems},
 doi = {10.1145/1993744.1993759},
 isbn = {978-1-4503-0814-4},
 keyword = {dynamic resource allocation, partial centralization, phase transition},
 link = {http://doi.acm.org/10.1145/1993744.1993759},
 location = {San Jose, California, USA},
 numpages = {12},
 pages = {161--172},
 publisher = {ACM},
 series = {SIGMETRICS '11},
 title = {On the Power of (Even a Little) Centralization in Distributed Processing},
 year = {2011}
}


@inproceedings{Rao:2011:SPV:1993744.1993790,
 abstract = {In this paper, we propose a distributed learning mechanism that facilitates self-adaptive virtual machines resource provisioning. We treat cloud resource allocation as a distributed learning task, in which each VM being a highly autonomous agent submits resource requests according to its own benefit. The mechanism evaluates the requests and replies with feedback. We develop a reinforcement learning algorithm with a highly efficient representation of experiences as the heart of the VM side learning engine. We prototype the mechanism and the distributed learning algorithm in an iBalloon system. Experiment results on a Xen-based cloud testbed demonstrate the effectiveness of iBalloon.},
 acmid = {1993790},
 address = {New York, NY, USA},
 author = {Rao, Jia and Bu, Xiangping and Wang, Kun and Xu, Cheng-Zhong},
 booktitle = {Proceedings of the ACM SIGMETRICS Joint International Conference on Measurement and Modeling of Computer Systems},
 doi = {10.1145/1993744.1993790},
 isbn = {978-1-4503-0814-4},
 keyword = {autonomic computing, cloud management, reinforcement learning},
 link = {http://doi.acm.org/10.1145/1993744.1993790},
 location = {San Jose, California, USA},
 numpages = {2},
 pages = {129--130},
 publisher = {ACM},
 series = {SIGMETRICS '11},
 title = {Self-adaptive Provisioning of Virtualized Resources in Cloud Computing},
 year = {2011}
}


@inproceedings{Chen:2011:AAN:1993744.1993802,
 abstract = {Branch taken rate and transition rate have been proposed as metrics to characterize the branch predictability. However, these two metrics may misclassify branches with regular history patterns as hard-to-predict branches, causing an inaccurate and ambiguous view of branch predictability. This study uses autocorrelation to analyze the branch history patterns and presents a new metric Degree of Pattern Irregularity (DPI) for branch classification. The proposed metric is evaluated with different branch predictors, and the results show that DPI significantly improves the quality and the accuracy of branch classification over traditional taken rate and transition rate.},
 acmid = {1993802},
 address = {New York, NY, USA},
 author = {Chen, Jian and John, Lizy Kurian},
 booktitle = {Proceedings of the ACM SIGMETRICS Joint International Conference on Measurement and Modeling of Computer Systems},
 doi = {10.1145/1993744.1993802},
 isbn = {978-1-4503-0814-4},
 keyword = {autocorrelation, branch characterization},
 link = {http://doi.acm.org/10.1145/1993744.1993802},
 location = {San Jose, California, USA},
 numpages = {2},
 pages = {153--154},
 publisher = {ACM},
 series = {SIGMETRICS '11},
 title = {Autocorrelation Analysis: A New and Improved Method for Measuring Branch Predictability},
 year = {2011}
}


@inproceedings{Lee:2011:SMV:1993744.1993793,
 abstract = {Threshold-based performance monitoring in large 3G networks is very challenging for two main factors: large network scale and dynamics in both time and spatial domains. There exists a fundamental tradeoff between the size of threshold settings and the alarm quality. In this paper, we propose a scalable monitoring solution, called threshold-compression that characterizes the tradeoff via intelligent threshold aggregation. The main insight behind our solution is to identify groups of network elements with similar threshold behaviors across location and time dimensions, thus forming spatial-temporal clusters and generating the associated compressed thresholds within the optimization framework. Our evaluations on a commercial 3G network have demonstrated the effectiveness of our threshold-compression solution, e.g., threshold setting reduction up to 90% within 10% false/miss alarms.},
 acmid = {1993793},
 address = {New York, NY, USA},
 author = {Lee, Suk-Bok and Pei, Dan and Hajiaghayi, MohammadTaghi and Pefkianakis, Ioannis and Lu, Songwu and Yan, He and Ge, Zihui and Yates, Jennifer and Kosseifi, Mario},
 booktitle = {Proceedings of the ACM SIGMETRICS Joint International Conference on Measurement and Modeling of Computer Systems},
 doi = {10.1145/1993744.1993793},
 isbn = {978-1-4503-0814-4},
 keyword = {3G network, monitoring},
 link = {http://doi.acm.org/10.1145/1993744.1993793},
 location = {San Jose, California, USA},
 numpages = {2},
 pages = {135--136},
 publisher = {ACM},
 series = {SIGMETRICS '11},
 title = {Scalable Monitoring via Threshold Compression in a Large Operational 3G Network},
 year = {2011}
}


@inproceedings{Kant:2011:CSB:1993744.1993795,
 abstract = {This paper introduces a closed-loop control algorithm to coordinate power management of memory ranks and thereby achieve power savings beyond independent rank power management while bounding the throughput degradation.},
 acmid = {1993795},
 address = {New York, NY, USA},
 author = {Kant, Krishna},
 booktitle = {Proceedings of the ACM SIGMETRICS Joint International Conference on Measurement and Modeling of Computer Systems},
 doi = {10.1145/1993744.1993795},
 isbn = {978-1-4503-0814-4},
 keyword = {DRAM, coordinated control, power management},
 link = {http://doi.acm.org/10.1145/1993744.1993795},
 location = {San Jose, California, USA},
 numpages = {2},
 pages = {139--140},
 publisher = {ACM},
 series = {SIGMETRICS '11},
 title = {A Control Scheme for Batching DRAM Requests to Improve Power Efficiency},
 year = {2011}
}


@inproceedings{Lee:2011:FLL:1993744.1993778,
 abstract = {Modern trading and cluster applications require microsecond latencies and almost no losses in data centers. This paper introduces an algorithm called FineComb that can estimate fine-grain end-to-end loss and latency measurements between edge routers in these data center networks. Such a mechanism can allow managers to distinguish between latencies and loss singularities caused by servers and those caused by the network. Compared to prior work, such as Lossy Difference Aggregator (LDA), that focused on switch-level latency measurements, the requirement of end-to-end latency measurements introduces the challenge of reordering that occurs commonly in IP networks due to churn. The problem is even more acute in switches across data center networks that employ multipath routing algorithms to exploit the inherent path diversity. Without proper care, a loss estimation algorithm can confound loss and reordering; further, any attempt to aggregate delay estimates in the presence of reordering results in severe errors. FineComb deals with these problems using order-agnostic packet digests and a simple new idea we call stash recovery. Our evaluation demonstrates that FineComb can provide orders of magnitude better accuracy in loss and delay estimates in the presence of reordering compared to LDA.},
 acmid = {1993778},
 address = {New York, NY, USA},
 author = {Lee, Myungjin and Goldberg, Sharon and Kompella, Ramana Rao and Varghese, George},
 booktitle = {Proceedings of the ACM SIGMETRICS Joint International Conference on Measurement and Modeling of Computer Systems},
 doi = {10.1145/1993744.1993778},
 isbn = {978-1-4503-0814-4},
 keyword = {latency, packet loss, passive measurement, reordering},
 link = {http://doi.acm.org/10.1145/1993744.1993778},
 location = {San Jose, California, USA},
 numpages = {12},
 pages = {329--340},
 publisher = {ACM},
 series = {SIGMETRICS '11},
 title = {Fine-grained Latency and Loss Measurements in the Presence of Reordering},
 year = {2011}
}


@inproceedings{Eibl:2011:FEE:1993744.1993786,
 abstract = {Recently, several researchers have proposed schemes for low-cost, low-power error detection in the processor core. In this work, we demonstrate that one particular scheme, an enhanced implementation of the Argus framework called Argus-2, is a viable option for industry adoption. Using an FPGA prototype, we experimentally evaluate Argus-2's ability to detect errors due to (a) all possible single stuck-at faults in a given core and (b) a statistically significant number of double stuck-at faults, including pairs of faults that are randomly located and pairs that are spatially correlated on the chip.},
 acmid = {1993786},
 address = {New York, NY, USA},
 author = {Eibl, Patrick J. and Meixner, Albert and Sorin, Daniel J.},
 booktitle = {Proceedings of the ACM SIGMETRICS Joint International Conference on Measurement and Modeling of Computer Systems},
 doi = {10.1145/1993744.1993786},
 isbn = {978-1-4503-0814-4},
 keyword = {computer architecture, dynamic verification, error detection},
 link = {http://doi.acm.org/10.1145/1993744.1993786},
 location = {San Jose, California, USA},
 numpages = {2},
 pages = {121--122},
 publisher = {ACM},
 series = {SIGMETRICS '11},
 title = {An FPGA-based Experimental Evaluation of Microprocessor Core Error Detection with Argus-2},
 year = {2011}
}


@inproceedings{Ribeiro:2011:CCR:1993744.1993801,
 abstract = {
                  An abstract is not available.
              },
 acmid = {1993801},
 address = {New York, NY, USA},
 author = {Ribeiro, Bruno and Figueiredo, Daniel and de Souza e Silva, Edmundo and Towsley, Don},
 booktitle = {Proceedings of the ACM SIGMETRICS Joint International Conference on Measurement and Modeling of Computer Systems},
 doi = {10.1145/1993744.1993801},
 isbn = {978-1-4503-0814-4},
 keyword = {continuous time random walk, dynamic graphs},
 link = {http://doi.acm.org/10.1145/1993744.1993801},
 location = {San Jose, California, USA},
 numpages = {2},
 pages = {151--152},
 publisher = {ACM},
 series = {SIGMETRICS '11},
 title = {Characterizing Continuous-time Random Walks on Dynamic Networks},
 year = {2011}
}


@inproceedings{Subhraveti:2011:RTP:1993744.1993757,
 abstract = {Software bugs that occur in production are often difficult to reproduce in the lab due to subtle differences in the application environment and nondeterminism. To address this problem, we present Transplay, a system that captures production software bugs into small per-bug recordings which are used to reproduce the bugs on a completely different operating system without access to any of the original software used in the production environment. Transplay introduces partial checkpointing, a new mechanism that efficiently captures the partial state necessary to reexecute just the last few moments of the application before it encountered a failure. The recorded state, which typically consists of a few megabytes of data, is used to replay the application without requiring the specific application binaries, libraries, support data, or the original execution environment. Transplay integrates with existing debuggers to provide standard debugging facilities to allow the user to examine the contents of variables and other program state at each source line of the application's replayed execution. We have implemented a Transplay prototype that can record unmodified Linux applications and replay them on different versions of Linux as well as Windows. Experiments with several applications including Apache and MySQL show that Transplay can reproduce real bugs and be used in production with modest recording overhead.},
 acmid = {1993757},
 address = {New York, NY, USA},
 author = {Subhraveti, Dinesh and Nieh, Jason},
 booktitle = {Proceedings of the ACM SIGMETRICS Joint International Conference on Measurement and Modeling of Computer Systems},
 doi = {10.1145/1993744.1993757},
 isbn = {978-1-4503-0814-4},
 keyword = {checkpoint-restart, record-replay, virtualization},
 link = {http://doi.acm.org/10.1145/1993744.1993757},
 location = {San Jose, California, USA},
 numpages = {12},
 pages = {109--120},
 publisher = {ACM},
 series = {SIGMETRICS '11},
 title = {Record and Transplay: Partial Checkpointing for Replay Debugging Across Heterogeneous Systems},
 year = {2011}
}


@inproceedings{Anandkumar:2011:TDS:1993744.1993774,
 abstract = {We consider the task of topology discovery of sparse random graphs using end-to-end random measurements (e.g., delay) between a subset of nodes, referred to as the participants. The rest of the nodes are hidden, and do not provide any information for topology discovery. We consider topology discovery under two routing models: (a) the participants exchange messages along the shortest paths and obtain end-to-end measurements, and (b) additionally, the participants exchange messages along the second shortest path. For scenario (a), our proposed algorithm results in a sub-linear edit-distance guarantee using a sub-linear number of uniformly selected participants. For scenario (b), we obtain a much stronger result, and show that we can achieve consistent reconstruction when a sub-linear number of uniformly selected nodes participate. This implies that accurate discovery of sparse random graphs is tractable using an extremely small number of participants. We finally obtain a lower bound on the number of participants required by any algorithm to reconstruct the original random graph up to a given edit distance. We also demonstrate that while consistent discovery is tractable for sparse random graphs using a small number of participants, in general, there are graphs which cannot be discovered by any algorithm even with a significant number of participants, and with the availability of end-to-end information along all the paths between the participants.},
 acmid = {1993774},
 address = {New York, NY, USA},
 author = {Anandkumar, Animashree and Hassidim, Avinatan and Kelner, Jonathan},
 booktitle = {Proceedings of the ACM SIGMETRICS Joint International Conference on Measurement and Modeling of Computer Systems},
 doi = {10.1145/1993744.1993774},
 isbn = {978-1-4503-0814-4},
 keyword = {end-to-end measurements, hidden nodes, quartet tests, sparse random graphs, topology discovery},
 link = {http://doi.acm.org/10.1145/1993744.1993774},
 location = {San Jose, California, USA},
 numpages = {12},
 pages = {293--304},
 publisher = {ACM},
 series = {SIGMETRICS '11},
 title = {Topology Discovery of Sparse Random Graphs with Few Participants},
 year = {2011}
}


@inproceedings{Zhou:2011:SOU:1993744.1993779,
 abstract = {Recent work on BitTorrent swarms has demonstrated that a bandwidth bottleneck at the seed can lead to the underutilization of the aggregate swarm capacity. Bandwidth underutilization also occurs naturally in mobile peer-to-peer swarms, as a mobile peer may not always be within the range of peers storing the content it desires. We argue in this paper that, in both cases, idle bandwidth can be exploited to allow content sharing across multiple swarms, thereby forming a universal swarm system. We propose a model for universal swarms that applies to a variety of peer-to-peer environments, both mobile and online. Through a fluid limit analysis, we demonstrate that universal swarms have significantly improved stability properties compared to individually autonomous swarms. In addition, by studying a swarm's stationary behavior, we identify content replication ratios across different swarms that minimize the average sojourn time in the system. We then propose a content exchange scheme between peers that leads to these optimal replication ratios, and study its convergence numerically.},
 acmid = {1993779},
 address = {New York, NY, USA},
 author = {Zhou, Xia and Ioannidis, Stratis and Massoulie, Laurent},
 booktitle = {Proceedings of the ACM SIGMETRICS Joint International Conference on Measurement and Modeling of Computer Systems},
 doi = {10.1145/1993744.1993779},
 isbn = {978-1-4503-0814-4},
 keyword = {content distribution, peer-to-peer networks, universal swarms},
 link = {http://doi.acm.org/10.1145/1993744.1993779},
 location = {San Jose, California, USA},
 numpages = {12},
 pages = {341--352},
 publisher = {ACM},
 series = {SIGMETRICS '11},
 title = {On the Stability and Optimality of Universal Swarms},
 year = {2011}
}


@proceedings{Misra:2010:1811039,
 abstract = {Welcome to ACM SIGMETRICS 2010! And to the city so nice, they named it twice. This year's conference continues the rich SIGMETRICS tradition of being the premier forum for the presentation of research on the measurement and modeling of computer systems. The technical program for 2010 features a set of outstanding papers that covers both theory and applications from a wide variety of areas, including dynamics and control, load balancing, measurement, network traffic, optimization, performance modeling and analysis, resource allocation, scheduling, sensor and wireless networks, and systems, among others. This year's call for papers attracted 184 submissions from all over the world (and a few beyond). The 90 member Technical Program Committee along with a selected group of external experts carefully considered all of the submissions with a total of 716 detailed reviews completed. The TPC meeting to select the final program was held on the campus of Columbia University in mid January, 2010. At the conclusion of the meeting, the committee had assembled a wonderful program composed of 29 papers and 20 posters, to be presented over three days at the conference. The quality of submissions was extremely high as reflected in the final technical program. Following the TPC meeting, a subcommittee had the pleasure (yet very difficult task) of selecting the best paper award winners. The paper entitled "Load Balancing via Randomized Local Search in Closed and Open Systems" by A. Ganesh, S. Lilienthal, D. Manjunath, A. Proutiere and F. Simatos received the Best Paper Award. The paper entitled "Distributed Sensor Network Localization from Local Connectivity: Performance Analysis for the HOP-TERRAIN Algorithm" by A. Karbasi and S. Oh received the Kenneth C. Sevcik Outstanding Student Paper Award. Please join us in congratulating the authors!},
 address = {New York, NY, USA},
 isbn = {978-1-4503-0038-4},
 location = {New York, New York, USA},
 publisher = {ACM},
 title = {SIGMETRICS '10: Proceedings of the ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems},
 year = {2010}
}


@inproceedings{Nguyen:2011:WPA:1993744.1993760,
 abstract = {We consider a weighted proportional allocation of resources that allows providers to discriminate usage of resources by users. This framework is a generalization of well-known proportional allocation by accommodating allocation of resources proportional to weighted bids or proportional to submitted bids but with weighted payments. We study a competition game where everyone is selfish: providers choose user discrimination weights aiming at maximizing their individual revenues while users choose their bids aiming at maximizing their individual payoffs. We analyze revenue and social welfare of this game. We find that the revenue is lower bounded by k/(k+1) times the revenue under standard price discrimination scheme, where a set of k users is excluded. For users with linear utility functions, we find that the social welfare is at least 1/(1+2/√3) of the maximum social welfare (approx. 46%) and that this bound is tight. We extend this efficiency result to a broad class of utility functions and multiple competing providers. We also describe an algorithm for adjusting discrimination weights by providers without a prior knowledge of user utility functions and establish convergence to equilibrium points of the competition game. Our results show that, in many cases, weighted proportional sharing achieves competitive revenue and social welfare, despite the fact that everyone is selfish.},
 acmid = {1993760},
 address = {New York, NY, USA},
 author = {Nguyen, Thanh and Vojnovic, Milan},
 booktitle = {Proceedings of the ACM SIGMETRICS Joint International Conference on Measurement and Modeling of Computer Systems},
 doi = {10.1145/1993744.1993760},
 isbn = {978-1-4503-0814-4},
 keyword = {auctions, proportional sharing, resource allocation},
 link = {http://doi.acm.org/10.1145/1993744.1993760},
 location = {San Jose, California, USA},
 numpages = {12},
 pages = {173--184},
 publisher = {ACM},
 series = {SIGMETRICS '11},
 title = {Weighted Proportional Allocation},
 year = {2011}
}


@inproceedings{Han:2011:PCB:1993744.1993789,
 abstract = {Despite the increasing interest in content bundling in BitTorrent systems, there are still few empirical studies on the bundling practice in real BitTorrent communities. In this paper, we conduct comprehensive measurements on one of the largest BitTorrent portals: The Pirate Bay. From the torrents data set collected for 38 days from April to May, 2010, we study how prevalent bundling is and how many files are bundled in a torrent, across different types of contents shared: Movie, Porn, TV, Music, Application, E-book, and Game.},
 acmid = {1993789},
 address = {New York, NY, USA},
 author = {Han, Jinyoung and Chung, Taejoong and Kim, Seungbae and Kwon, Ted Taekyoung and Kim, Hyun-chul and Choi, Yanghee},
 booktitle = {Proceedings of the ACM SIGMETRICS Joint International Conference on Measurement and Modeling of Computer Systems},
 doi = {10.1145/1993744.1993789},
 isbn = {978-1-4503-0814-4},
 keyword = {BitTorrent, content bundling, measurement},
 link = {http://doi.acm.org/10.1145/1993744.1993789},
 location = {San Jose, California, USA},
 numpages = {2},
 pages = {127--128},
 publisher = {ACM},
 series = {SIGMETRICS '11},
 title = {How Prevalent is Content Bundling in BitTorrent},
 year = {2011}
}


@inproceedings{Suh:2011:SEB:1993744.1993755,
 abstract = {The amount of charge stored in an SRAM cell shrinks rapidly with each technology generation thus increasingly exposing caches to soft errors. Benchmarking the FIT rate of caches due to soft errors is critical to evaluate the relative merits of a plethora of protection schemes that are being proposed to protect against soft errors. The benchmarking of cache reliability introduces a unique challenge as compared to internal processor storage structures, such as the load/store queue. In the case of internal processor structures the time a data bit resides in the structure is so short that it is generally safe to assume that no more than one soft error strike can occur. Thus the reliability of such structures is overwhelmingly dominated by single bit errors. By contrast, a memory block may reside for millions of cycles in a last level cache. In this case it is important to consider the impact of the spatial and temporal distribution of multiple errors within the lifetime of a cache block in the presence of error protection. This paper introduces a unified reliability benchmarking framework called PARMA (Precise Analytical Reliability Model for Architecture). PARMA is a rigorous analytical framework that accurately accounts for the distribution of multiple errors to measure the failure rate under any protection scheme. In a single simulation run PARMA provides a precise FIT rate (expected number of failures in one billion hours) measurement for storage structures where the effect of multiple errors cannot be neglected. We have implemented the PARMA framework on top of a cycle-accurate out-of-order processor simulator (sim-outorder) to benchmark L2 cache failure rates for a set of CPU 2000 benchmarks. The effectiveness of three protection schemes are compared in terms of L2 cache FIT rate: parity, word-level Single Error Correcting Double Error Detecting (SECDED) code and block-level SECDED. Exploiting the accuracy of PARMA, we demonstrate that current techniques to evaluate cache FIT rates in the presence of SECDED, such as accelerated fault injection simulations and first-principle derivations based on Architectural Vulnerability Factor (AVF), can overestimate FIT rates by vast amounts. Based on the insights gained during this research we also introduce a new approximate analytical model that can quickly and more accurately estimate cache FIT rate in the presence of SECDED.},
 acmid = {1993755},
 address = {New York, NY, USA},
 author = {Suh, Jinho and Manoochehri, Mehrtash and Annavaram, Murali and Dubois, Michel},
 booktitle = {Proceedings of the ACM SIGMETRICS Joint International Conference on Measurement and Modeling of Computer Systems},
 doi = {10.1145/1993744.1993755},
 isbn = {978-1-4503-0814-4},
 keyword = {cache, reliability, soft error},
 link = {http://doi.acm.org/10.1145/1993744.1993755},
 location = {San Jose, California, USA},
 numpages = {12},
 pages = {85--96},
 publisher = {ACM},
 series = {SIGMETRICS '11},
 title = {Soft Error Benchmarking of L2 Caches with PARMA},
 year = {2011}
}


@inproceedings{Aalto:2011:OTS:1993744.1993761,
 abstract = {We consider service systems where new jobs not only increase the load but also improve the service ability of such a system, cf. opportunistic scheduling gain in wireless systems. We study the optimal trade-off between the SRPT (Shortest Remaining Processing Time) discipline and opportunistic scheduling in the systems characterized by compact and symmetric capacity regions. The objective is to minimize the mean delay in a transient setting where all jobs are available at time 0 and no new jobs arrive thereafter. Our main result gives conditions under which the optimal rate vector does not depend on the sizes of the jobs as long as their order (in size) remains the same. In addition, it shows that in this case the optimal policy applies the SRPT principle serving the shortest job with the highest rate of the optimal rate vector, the second shortest with the second highest rate etc. We also give a recursive algorithm to determine both the optimal rate vector and the minimum mean delay. In some special cases, the rate vector, as well as the minimum mean delay, have even explicit expressions as demonstrated in the paper. For the general case, we derive both an upper bound and a lower bound of the minimum mean delay.},
 acmid = {1993761},
 address = {New York, NY, USA},
 author = {Aalto, Samuli and Penttinen, Aleksi and Lassila, Pasi and Osti, Prajwal},
 booktitle = {Proceedings of the ACM SIGMETRICS Joint International Conference on Measurement and Modeling of Computer Systems},
 doi = {10.1145/1993744.1993761},
 isbn = {978-1-4503-0814-4},
 keyword = {SRPT, capacity region, mean delay, opportunistic scheduling},
 link = {http://doi.acm.org/10.1145/1993744.1993761},
 location = {San Jose, California, USA},
 numpages = {12},
 pages = {185--196},
 publisher = {ACM},
 series = {SIGMETRICS '11},
 title = {On the Optimal Trade-off Between SRPT and Opportunistic Scheduling},
 year = {2011}
}


@inproceedings{Chen:2011:MPR:1993744.1993746,
 abstract = {The workloads in modern Chip-multiprocessors (CMP) are becoming increasingly diversified, creating different resource demands on hardware substrate. It is necessary to allocate hardware resources based on the needs of the workloads in order to improve system efficiency and/or ensure Quality-of-Service (QoS) at certain performance levels. Therefore, it is extremely important to identify the resource demand of the workload in terms of the performance and power efficiency. Existing models are inappropriate for estimating resource demands as they require either partial simulations or time-consuming training. This paper presents an integrated framework that is able to identify the single-resource or multi-resource demands on an array of hardware resources ranging from the issue width of the processor to the memory bandwidth. With an analytical model based on program inherent characteristics, this framework does not require any detailed simulation or training yet is still able to capture the performance trend of the program accurately. Our experiment shows that the proposed framework on average provides no larger than 8.6% error to any given performance target for multi-resource demand estimation. By using the proposed performance model, the framework identifies the multi-resource demands up to 40X faster compared to the state-of-the-art analytical model. The proposed framework can be applied in workload capacity planning, hardware resource adaptation as well as coordinated resource management for QoS in CMP systems.},
 acmid = {1993746},
 address = {New York, NY, USA},
 author = {Chen, Jian and John, Lizy Kurian and Kaseridis, Dimitris},
 booktitle = {Proceedings of the ACM SIGMETRICS Joint International Conference on Measurement and Modeling of Computer Systems},
 doi = {10.1145/1993744.1993746},
 isbn = {978-1-4503-0814-4},
 keyword = {microprocessor, performance modeling, program characteristics, resource demand},
 link = {http://doi.acm.org/10.1145/1993744.1993746},
 location = {San Jose, California, USA},
 numpages = {12},
 pages = {1--12},
 publisher = {ACM},
 series = {SIGMETRICS '11},
 title = {Modeling Program Resource Demand Using Inherent Program Characteristics},
 year = {2011}
}


@inproceedings{Sharifi:2011:MME:1993744.1993747,
 abstract = {Management of shared resources in emerging multicores for achieving predictable performance has received considerable attention in recent times. In general, almost all these approaches attempt to guarantee a certain level of performance QoS (weighted IPC, harmonic speedup, etc) by managing a single shared resource or at most a couple of interacting resources. A fundamental shortcoming of these approaches is the lack of coordination between these shared resources to satisfy a system level QoS. This is undesirable because providing end-to-end QoS in future multicores is essential for supporting wide-spread adoption of these architectures in virtualized servers and cloud computing systems. An initial step towards such an end-to-end QoS support in multicores is to ensure that at least the major computational and memory resources on-chip are managed efficiently in a coordinated fashion. In this paper, we propose METE, a platform for end-to-end on-chip resource management in multicore processors. Assuming that each application specifies a performance target/SLA, the main objective of METE is to dynamically provision sufficient on-chip resources to applications for achieving the specified targets. METE employs a feedback based system, designed as a Single-Input, Multiple-Output (SIMO) controller with an Auto-Regressive-Moving-Average (ARMA) model, to capture the behaviors of different applications. We evaluate a specific implementation of METE that manages cores, shared caches and off-chip bandwidth in an integrated manner on 8 and 16 core systems using a detailed full system simulator and workloads derived from the SPECOMP and SPECJBB multithreaded benchmarks. The collected results indicate that our proposed scheme is able to provision shared resources among co-runner applications dynamically over the course of execution, to provide end-to-end QoS and satisfy specified performance targets. Furthermore, the elegance of the control theory based multi-layer resource provisioning is in assuring QoS guarantees.},
 acmid = {1993747},
 address = {New York, NY, USA},
 author = {Sharifi, Akbar and Srikantaiah, Shekhar and Mishra, Asit K. and Kandemir, Mahmut and Das, Chita R.},
 booktitle = {Proceedings of the ACM SIGMETRICS Joint International Conference on Measurement and Modeling of Computer Systems},
 doi = {10.1145/1993744.1993747},
 isbn = {978-1-4503-0814-4},
 keyword = {control theory, end-to-end qos, resource management},
 link = {http://doi.acm.org/10.1145/1993744.1993747},
 location = {San Jose, California, USA},
 numpages = {12},
 pages = {13--24},
 publisher = {ACM},
 series = {SIGMETRICS '11},
 title = {METE: Meeting End-to-end QoS in Multicores Through System-wide Resource Management},
 year = {2011}
}


@inproceedings{Suchara:2011:NAJ:1993744.1993756,
 abstract = {Today's networks typically handle traffic engineering (e.g., tuning the routing-protocol parameters to optimize the flow of traffic) and failure recovery (e.g., pre-installed backup paths) independently. In this paper, we propose a unified way to balance load efficiently under a wide range of failure scenarios. Our architecture supports flexible splitting of traffic over multiple precomputed paths, with efficient path-level failure detection and automatic load balancing over the remaining paths. We propose two candidate solutions that differ in how the routers rebalance the load after a failure, leading to a trade-off between router complexity and load-balancing performance. We present and solve the optimization problems that compute the configuration state for each router. Our experiments with traffic measurements and topology data (including shared risks in the underlying transport network) from a large ISP identify a "sweet spot" that achieves near-optimal load balancing under a variety of failure scenarios, with a relatively small amount of state in the routers. We believe that our solution for joint traffic engineering and failure recovery will appeal to Internet Service Providers as well as the operators of data-center networks.},
 acmid = {1993756},
 address = {New York, NY, USA},
 author = {Suchara, Martin and Xu, Dahai and Doverspike, Robert and Johnson, David and Rexford, Jennifer},
 booktitle = {Proceedings of the ACM SIGMETRICS Joint International Conference on Measurement and Modeling of Computer Systems},
 doi = {10.1145/1993744.1993756},
 isbn = {978-1-4503-0814-4},
 keyword = {failure recovery, network architecture, optimization, simulation, traffic engineering},
 link = {http://doi.acm.org/10.1145/1993744.1993756},
 location = {San Jose, California, USA},
 numpages = {12},
 pages = {97--108},
 publisher = {ACM},
 series = {SIGMETRICS '11},
 title = {Network Architecture for Joint Failure Recovery and Traffic Engineering},
 year = {2011}
}


@inproceedings{Lam:2011:GRD:1993744.1993770,
 abstract = {Almost all geographic routing protocols have been designed for 2D. We present a novel geographic routing protocol, named MDT, for 2D, 3D, and higher dimensions with these properties: (i) guaranteed delivery for any connected graph of nodes and physical links, and (ii) low routing stretch from efficient forwarding of packets out of local minima. The guaranteed delivery property holds for node locations specified by accurate, inaccurate, or arbitrary coordinates. The MDT protocol suite includes a packet forwarding protocol together with protocols for nodes to construct and maintain a distributed MDT graph for routing. We present the performance of MDT protocols in 3D and 4D as well as performance comparisons of MDT routing versus representative geographic routing protocols for nodes in 2D and 3D. Experimental results show that MDT provides the lowest routing stretch in the comparisons. Furthermore, MDT protocols are specially designed to handle churn, i.e., dynamic topology changes due to addition and deletion of nodes and links. Experimental results show that MDT's routing success rate is close to 100% during churn and node states converge quickly to a correct MDT graph after churn.},
 acmid = {1993770},
 address = {New York, NY, USA},
 author = {Lam, Simon S. and Qian, Chen},
 booktitle = {Proceedings of the ACM SIGMETRICS Joint International Conference on Measurement and Modeling of Computer Systems},
 doi = {10.1145/1993744.1993770},
 isbn = {978-1-4503-0814-4},
 keyword = {Delaunay Triangulation, geographic routing},
 link = {http://doi.acm.org/10.1145/1993744.1993770},
 location = {San Jose, California, USA},
 numpages = {12},
 pages = {257--268},
 publisher = {ACM},
 series = {SIGMETRICS '11},
 title = {Geographic Routing in D-dimensional Spaces with Guaranteed Delivery and Low Stretch},
 year = {2011}
}


@inproceedings{Alizadeh:2011:SAQ:1993744.1993751,
 abstract = {Data Center Networks have recently caused much excitement in the industry and in the research community. They represent the convergence of networking, storage, computing and virtualization. This paper is concerned with the Quantized Congestion Notification (QCN) algorithm, developed for Layer 2 congestion management. QCN has recently been standardized as the IEEE 802.1Qau Ethernet Congestion Notification standard. We provide a stability analysis of QCN, especially in terms of its ability to utilize high capacity links in the shallow-buffered data center network environment. After a brief description of the QCN algorithm, we develop a delay-differential equation model for mathematically characterizing it. We analyze the model using a linearized approximation, obtaining stability margins as a function of algorithm parameters and network operating conditions. A second contribution of the paper is the articulation and analysis of the Averaging Principle (AP)---a new method for stabilizing control loops when lags increase. The AP is distinct from other well-known methods of feedback stabilization such as higher-order state feedback and lag-dependent gain adjustment. It turns out that the QCN and the BIC-TCP (and CUBIC) algorithms use the AP; we show that this enables them to be stable under large lags. The AP is also of independent interest since it applies to general control systems, not just congestion control systems.},
 acmid = {1993751},
 address = {New York, NY, USA},
 author = {Alizadeh, Mohammad and Kabbani, Abdul and Atikoglu, Berk and Prabhakar, Balaji},
 booktitle = {Proceedings of the ACM SIGMETRICS Joint International Conference on Measurement and Modeling of Computer Systems},
 doi = {10.1145/1993744.1993751},
 isbn = {978-1-4503-0814-4},
 keyword = {QCN, data center, ethernet, layer 2 congestion control},
 link = {http://doi.acm.org/10.1145/1993744.1993751},
 location = {San Jose, California, USA},
 numpages = {12},
 pages = {49--60},
 publisher = {ACM},
 series = {SIGMETRICS '11},
 title = {Stability Analysis of QCN: The Averaging Principle},
 year = {2011}
}


@inproceedings{Akella:2011:DIR:1993744.1993798,
 abstract = {We present the S4R supplemental routing system to address the constraints BGP places on ISPs and stub network alike. Technical soundness and economic viability are equal first class design requirements for S4R. In S4R, ISPs announce links connecting different parts of the Internet. ISPs can selfishly price their links to attract maximal amount of traffic. Stub networks can selfishly select paths that best meet their requirements at the lowest cost. We design a variety of practical algorithms for ISP and stub network response that strike a balance between accommodating selfishness of all participants and ensuring efficient and stable operation overall. We employ large scale simulations over realistic scenarios to show that S4R operates at a close-to-optimal state and that it encourages broad participation from stubs and ISPs.},
 acmid = {1993798},
 address = {New York, NY, USA},
 author = {Akella, Aditya and Chawla, Shuchi and Esquivel, Holly and Muthukrishnan, Chitra},
 booktitle = {Proceedings of the ACM SIGMETRICS Joint International Conference on Measurement and Modeling of Computer Systems},
 doi = {10.1145/1993744.1993798},
 isbn = {978-1-4503-0814-4},
 keyword = {inter-domain routing, selfishness},
 link = {http://doi.acm.org/10.1145/1993744.1993798},
 location = {San Jose, California, USA},
 numpages = {2},
 pages = {145--146},
 publisher = {ACM},
 series = {SIGMETRICS '11},
 title = {De-ossifying Internet Routing Through Intrinsic Support for End-network and ISP Selfishness},
 year = {2011}
}


@inproceedings{Singh:2011:IGM:1993744.1993803,
 abstract = {Current IP geoloation techniques can geolocate an IP address to a region approximately 700 square miles, roughly the size of a metropolitan area. We model geolocation as a pattern-recognition problem, and introduce techniques that geolocate addresses to within 5 miles inside a metropolitan area. We propose two complementary algorithms: The first algorithm, Pattern Based Geolocation (PBG), models the distribution of latencies to the target and compares it to those of the reference landmarks to resolve an address to within 5 miles in a metropolitan area. The second approach, Perturbation Augmented PBG (PAPBG), provides higher resolution by sending extra traffic in the network. While sending an aggregate of 600 Kbps extra traffic to 20 nodes for approximately 2 minutes, PAPBG geolocates addresses to within 3 miles.},
 acmid = {1993803},
 address = {New York, NY, USA},
 author = {Singh, Satinder Pal and Baden, Randolph and Lee, Choon and Bhattacharjee, Bobby and La, Richard and Shayman, Mark},
 booktitle = {Proceedings of the ACM SIGMETRICS Joint International Conference on Measurement and Modeling of Computer Systems},
 doi = {10.1145/1993744.1993803},
 isbn = {978-1-4503-0814-4},
 keyword = {geolocation, pattern recognition, perturbation},
 link = {http://doi.acm.org/10.1145/1993744.1993803},
 location = {San Jose, California, USA},
 numpages = {2},
 pages = {155--156},
 publisher = {ACM},
 series = {SIGMETRICS '11},
 title = {IP Geolocation in Metropolitan Areas},
 year = {2011}
}


@inproceedings{Liu:2011:GGL:1993744.1993767,
 abstract = {Energy expenditure has become a significant fraction of data center operating costs. Recently, "geographical load balancing" has been suggested to reduce energy cost by exploiting the electricity price differences across regions. However, this reduction of cost can paradoxically increase total energy use. This paper explores whether the geographical diversity of Internet-scale systems can additionally be used to provide environmental gains. Specifically, we explore whether geographical load balancing can encourage use of "green" renewable energy and reduce use of "brown" fossil fuel energy. We make two contributions. First, we derive two distributed algorithms for achieving optimal geographical load balancing. Second, we show that if electricity is dynamically priced in proportion to the instantaneous fraction of the total energy that is brown, then geographical load balancing provides significant reductions in brown energy use. However, the benefits depend strongly on the degree to which systems accept dynamic energy pricing and the form of pricing used.},
 acmid = {1993767},
 address = {New York, NY, USA},
 author = {Liu, Zhenhua and Lin, Minghong and Wierman, Adam and Low, Steven H. and Andrew, Lachlan L.H.},
 booktitle = {Proceedings of the ACM SIGMETRICS Joint International Conference on Measurement and Modeling of Computer Systems},
 doi = {10.1145/1993744.1993767},
 isbn = {978-1-4503-0814-4},
 keyword = {data centers, demand response, energy consumption, load balancing},
 link = {http://doi.acm.org/10.1145/1993744.1993767},
 location = {San Jose, California, USA},
 numpages = {12},
 pages = {233--244},
 publisher = {ACM},
 series = {SIGMETRICS '11},
 title = {Greening Geographical Load Balancing},
 year = {2011}
}


@inproceedings{Zhang:2011:SID:1993744.1993748,
 abstract = {Most of existing research on emerging multicore machines focus on parallelism extraction and architectural level optimizations. While these optimizations are critical, complementary approaches such as data locality enhancement can also bring significant benefits. Most of the previous data locality optimization techniques have been proposed and evaluated in the context of single core architectures. While one can expect these optimizations to be useful for multicore machines as well, multicores present further opportunities due to shared on-chip caches most of them accommodate. In order to optimize data locality targeting multicore machines however, the first step is to understand data reuse characteristics of multithreaded applications and potential benefits shared caches can bring. Motivated by these observations, we make the following contributions in this paper. First, we give a definition for inter-core data reuse and quantify it on multicores using a set of ten multithreaded application programs. Second, we show that neither on-chip cache hierarchies of current multicore architectures nor state-of-the-art (single-core centric) code/data optimizations exploit available inter-core data reuse in multithreaded applications. Third, we demonstrate that exploiting all available intercore reuse could boost overall application performance by around 21.3% on average, indicating that there is significant scope for optimization. However, we also show that trying to optimize for inter-core reuse aggressively without considering the impact of doing so on intra-core reuse can actually perform worse than optimizing for intra-core reuse alone. Finally, we present a novel, compiler-based data locality optimization strategy for multicores that balances both inter-core and intra-core reuse optimizations carefully to maximize benefits that can be extracted from shared caches. Our experiments with this strategy reveal that it is very effective in optimizing data locality in multicores.},
 acmid = {1993748},
 address = {New York, NY, USA},
 author = {Zhang, Yuanrui and Kandemir, Mahmut and Yemliha, Taylan},
 booktitle = {Proceedings of the ACM SIGMETRICS Joint International Conference on Measurement and Modeling of Computer Systems},
 doi = {10.1145/1993744.1993748},
 isbn = {978-1-4503-0814-4},
 keyword = {cache hierarchy-aware, computation mapping and scheduling, data reuse, multicores, shared cache},
 link = {http://doi.acm.org/10.1145/1993744.1993748},
 location = {San Jose, California, USA},
 numpages = {12},
 pages = {25--36},
 publisher = {ACM},
 series = {SIGMETRICS '11},
 title = {Studying Inter-core Data Reuse in Multicores},
 year = {2011}
}


@inproceedings{Gupta:2011:TMB:1993744.1993792,
 abstract = {We present a new tool to analyze three queueing systems which have defied exact analysis so far: (i) the classical M/G/k multi-server system, (ii) queueing systems with fluctuating arrival and service rates, and (iii) the M/G/1 round-robin queue. We argue that rather than looking for exact expressions for the mean response time as a function of the job size distribution, a more fruitful approach is to find distributions which minimize or maximize the mean response time given the first n moments of the job size distribution. We prove that for the M/G/k system in light traffic, and given n=2 and 3 moments, these 'extremal' distributions are given by principal representations of the moment sequence. Furthermore, if we restrict the distributions to lie in the class of Completely Monotone (CM) distributions, then for all the three queueing systems, for any n, the extremal distributions under the appropriate "light traffic" asymptotics are hyper-exponential distributions with finite number of phases. We conjecture that the property of extremality should be invariant to the system load, and thus our light traffic results should hold for general load as well.},
 acmid = {1993792},
 address = {New York, NY, USA},
 author = {Gupta, Varun and Osogami, Takayuki},
 booktitle = {Proceedings of the ACM SIGMETRICS Joint International Conference on Measurement and Modeling of Computer Systems},
 doi = {10.1145/1993744.1993792},
 isbn = {978-1-4503-0814-4},
 keyword = {light traffic analysis, m/g/k, markov-krein theorem, moments-based bounds, round-robin, tchebychef systems, time-varying load},
 link = {http://doi.acm.org/10.1145/1993744.1993792},
 location = {San Jose, California, USA},
 numpages = {2},
 pages = {133--134},
 publisher = {ACM},
 series = {SIGMETRICS '11},
 title = {Tight Moments-based Bounds for Queueing Systems},
 year = {2011}
}


@inproceedings{Cohen:2011:SSD:1993744.1993763,
 abstract = {The massive data streams observed in network monitoring, data processing and scientific studies are typically too large to store. For many applications over such data, we must obtain compact summaries of the stream. These summaries should allow accurate answering of post hoc queries with estimates which approximate the true answers over the original stream. The data often has an underlying structure which makes certain subset queries, in particular range queries, more relevant than arbitrary subsets. Applications such as access control, change detection, and heavy hitters typically involve subsets that are ranges or unions thereof. Random sampling is a natural summarization tool, being easy to implement and flexible to use. Known sampling methods are good for arbitrary queries but fail to optimize for the common case of range queries. Meanwhile, specialized summarization algorithms have been proposed for rangesum queries and related problems. These can outperform sampling giving fixed space resources, but lack its flexibility and simplicity. Particularly, their accuracy degrades when queries span multiple ranges. We define new stream sampling algorithms with a smooth and tunable trade-off between accuracy on range-sum queries and arbitrary subset-sum queries. The technical key is to relax requirements on the variance over all subsets to enable better performance on the ranges of interest. This boosts the accuracy on range queries while retaining the prime benefits of sampling, in particular flexibility and accuracy, with tail bounds guarantees. Our experimental study indicates that structure-aware summaries can drastically improve range-sum accuracy with respect to state-of-the-art stream sampling algorithms and outperform deterministic methods on range-sum queries and hierarchical heavy hitter queries.},
 acmid = {1993763},
 address = {New York, NY, USA},
 author = {Cohen, Edith and Cormode, Graham and Duffield, Nick},
 booktitle = {Proceedings of the ACM SIGMETRICS Joint International Conference on Measurement and Modeling of Computer Systems},
 doi = {10.1145/1993744.1993763},
 isbn = {978-1-4503-0814-4},
 keyword = {approximate query processing, data streams, structure-aware sampling, varopt},
 link = {http://doi.acm.org/10.1145/1993744.1993763},
 location = {San Jose, California, USA},
 numpages = {12},
 pages = {197--208},
 publisher = {ACM},
 series = {SIGMETRICS '11},
 title = {Structure-aware Sampling on Data Streams},
 year = {2011}
}


@inproceedings{Joseph:2011:SNM:1993744.1993752,
 abstract = {Multipath flow control has been proposed as a key way to improve the Internet's performance, reliability, and flexibility in supporting changing loads. Yet, at this point, there are very few tools to quantify the performance benefits; particularly in the context of a stochastic network supporting best effort flows, e.g., file transfers and web browsing sessions, where the metric of interest is transfer delay. This paper's focus is on developing analysis tools to evaluate flow-level performance and to support network design when multipath bandwidth allocation is based on proportional fairness. To overcome the analytical intractability of such systems we study closely related multipath approximations based on insensitive allocations such as balanced fairness. We obtain flow-level performance bounds on the mean per bit delay, exhibiting the role of resource pooling in the network, and use these to explore scenarios where increased path diversity need not result in high gains. While insightful these results are difficult to use to drive network design and capacity allocation. To that end, we study the large deviations for congestion events, i.e., accumulation of flows, in networks supporting multipath flow control. We show that such asymptotics are determined by certain critical resource pools, and study the sensitivity of congestion asymptotics to the pool's capacity and traffic loads. This suggests a disciplined approach to a capacity allocation problem in multipath networks based on a linear optimization problem.},
 acmid = {1993752},
 address = {New York, NY, USA},
 author = {Joseph, Vinay and de Veciana, Gustavo},
 booktitle = {Proceedings of the ACM SIGMETRICS Joint International Conference on Measurement and Modeling of Computer Systems},
 doi = {10.1145/1993744.1993752},
 isbn = {978-1-4503-0814-4},
 keyword = {balanced fair allocation, large deviations, multipath proportional fair allocation, network design, performance bounds, resource pools},
 link = {http://doi.acm.org/10.1145/1993744.1993752},
 location = {San Jose, California, USA},
 numpages = {12},
 pages = {61--72},
 publisher = {ACM},
 series = {SIGMETRICS '11},
 title = {Stochastic Networks with Multipath Flow Control: Impact of Resource Pools on Flow-level Performance and Network Congestion},
 year = {2011}
}


@inproceedings{Nguyen:2011:SP:1993744.1993769,
 abstract = {Source-controlled routing has been proposed as a way to improve flexibility of future network architectures, as well as simplifying the data plane. However, if a packet specifies its path, this precludes fast local re-routing within the network. We propose SlickPackets, a novel solution that allows packets to slip around failures by specifying alternate paths in their headers, in the form of compactly-encoded directed acyclic graphs. We show that this can be accomplished with reasonably small packet headers for real network topologies, and results in responsiveness to failures that is competitive with past approaches that require much more state within the network. Our approach thus enables fast failure response while preserving the benefits of source-controlled routing.},
 acmid = {1993769},
 address = {New York, NY, USA},
 author = {Nguyen, Giang T.K. and Agarwal, Rachit and Liu, Junda and Caesar, Matthew and Godfrey, P. Brighten and Shenker, Scott},
 booktitle = {Proceedings of the ACM SIGMETRICS Joint International Conference on Measurement and Modeling of Computer Systems},
 doi = {10.1145/1993744.1993769},
 isbn = {978-1-4503-0814-4},
 keyword = {failures, forwarding, reliability, routing},
 link = {http://doi.acm.org/10.1145/1993744.1993769},
 location = {San Jose, California, USA},
 numpages = {12},
 pages = {245--256},
 publisher = {ACM},
 series = {SIGMETRICS '11},
 title = {Slick Packets},
 year = {2011}
}


@inproceedings{Zhang:2011:RKD:1993744.1993787,
 abstract = {We study the role of Kullback-Leibler divergence in the framework of anomaly detection, where its abilities as a statistic underlying detection have never been investigated in depth. We give an in-principle analysis of network attack detection, showing explicitly attacks may be masked at minimal cost through 'camouflage'. We illustrate on both synthetic distributions and ones taken from real traffic.},
 acmid = {1993787},
 address = {New York, NY, USA},
 author = {Zhang, Lele and Veitch, Darryl and Ramamohanarao, Kotagiri},
 booktitle = {Proceedings of the ACM SIGMETRICS Joint International Conference on Measurement and Modeling of Computer Systems},
 doi = {10.1145/1993744.1993787},
 isbn = {978-1-4503-0814-4},
 keyword = {KL divergence, anomaly detection},
 link = {http://doi.acm.org/10.1145/1993744.1993787},
 location = {San Jose, California, USA},
 numpages = {2},
 pages = {123--124},
 publisher = {ACM},
 series = {SIGMETRICS '11},
 title = {The Role of KL Divergence in Anomaly Detection},
 year = {2011}
}


@inproceedings{Liu:2011:SIH:1993744.1993749,
 abstract = {Modern high performance microprocessors widely employ hardware prefetching technique to hide long memory access latency. While very useful, hardware prefetching tends to aggravate the bandwidth wall, a problem where system performance is increasingly limited by the availability of the off-chip pin bandwidth in Chip Multi-Processors (CMPs). In this paper, we propose an analytical model-based study to investigate how hardware prefetching and memory bandwidth partitioning impact CMP system performance and how they interact. The model includes a composite prefetching metric that can help determine under which conditions prefetching can improve system performance, a bandwidth partitioning model that takes into account prefetching effects, and a derivation of the weighted speedup optimum bandwidth partition sizes for different cores. Through model-driven case studies, we find several interesting observations that can be valuable for future CMP system design and optimization. We also explore simulation-based empirical evaluation to validate the observations and show that maximum system performance can be achieved by selective prefetching, guided by the composite prefetching metric, coupled with dynamic bandwidth partitioning.},
 acmid = {1993749},
 address = {New York, NY, USA},
 author = {Liu, Fang and Solihin, Yan},
 booktitle = {Proceedings of the ACM SIGMETRICS Joint International Conference on Measurement and Modeling of Computer Systems},
 doi = {10.1145/1993744.1993749},
 isbn = {978-1-4503-0814-4},
 keyword = {analytical model, chip multiprocessors, hardware prefetching, memory bandwidth partitioning},
 link = {http://doi.acm.org/10.1145/1993744.1993749},
 location = {San Jose, California, USA},
 numpages = {12},
 pages = {37--48},
 publisher = {ACM},
 series = {SIGMETRICS '11},
 title = {Studying the Impact of Hardware Prefetching and Bandwidth Partitioning in Chip-multiprocessors},
 year = {2011}
}


@inproceedings{Ciucu:2011:NCD:1993744.1993784,
 abstract = {The class of Gupta-Kumar results, which predict the throughput capacity in wireless networks, is restricted to asymptotic regimes. This tutorial presents a methodology to address a corresponding non-asymptotic analysis based on the framework of the stochastic network calculus, in a rigorous mathematical manner. In particular, we derive explicit closed-form results on the distribution of the end-to-end capacity and delay, for a fixed source-destination pair, in a network with broad assumptions on its topology and degree of spatial correlations. The results are non-asymptotic in that they hold for finite time scales and network sizes, as well as bursty arrivals. The generality of the results enables the research of several interesting problems, concerning for instance the effects of time scales or randomness in topology on the network capacity.},
 acmid = {1993784},
 address = {New York, NY, USA},
 author = {Ciucu, Florin},
 booktitle = {Proceedings of the ACM SIGMETRICS Joint International Conference on Measurement and Modeling of Computer Systems},
 doi = {10.1145/1993744.1993784},
 isbn = {978-1-4503-0814-4},
 keyword = {network capacity},
 link = {http://doi.acm.org/10.1145/1993744.1993784},
 location = {San Jose, California, USA},
 numpages = {2},
 pages = {359--360},
 publisher = {ACM},
 series = {SIGMETRICS '11},
 title = {Non-asymptotic Capacity and Delay Analysis of Mobile Wireless Networks},
 year = {2011}
}


@inproceedings{Xu:2011:CDN:1993744.1993777,
 abstract = {Despite the tremendous growth in the cellular data network usage due to the popularity of smartphones, so far there is rather limited understanding of the network infrastructure of various cellular carriers. Understanding the infrastructure characteristics such as the network topology, routing design, address allocation, and DNS service configuration is essential for predicting, diagnosing, and improving cellular network services, as well as for delivering content to the growing population of mobile wireless users. In this work, we propose a novel approach for discovering cellular infrastructure by intelligently combining several data sources, i.e., server logs from a popular location search application, active measurements results collected from smartphone users, DNS request logs from a DNS authoritative server, and publicly available routing updates. We perform the first comprehensive analysis to characterize the cellular data network infrastructure of four major cellular carriers within the U.S. in our study. We conclude among other previously little known results that the current routing of cellular data traffic is quite restricted, as it must traverse a rather limited number (i.e., 4-6) of infrastructure locations (i.e., GGSNs), which is in sharp contrast to wireline Internet traffic. We demonstrate how such findings have direct implications on important decisions such as mobile content placement and content server selection. We observe that although the local DNS server is a coarse-grained approximation on the user's network location, for some carriers, choosing content servers based on the local DNS server is accurate enough due to the restricted routing in cellular networks. Placing content servers close to GGSNs can potentially reduce the end-to-end latency by more than 50% excluding the variability from air interface.},
 acmid = {1993777},
 address = {New York, NY, USA},
 author = {Xu, Qiang and Huang, Junxian and Wang, Zhaoguang and Qian, Feng and Gerber, Alexandre and Mao, Zhuoqing Morley},
 booktitle = {Proceedings of the ACM SIGMETRICS Joint International Conference on Measurement and Modeling of Computer Systems},
 doi = {10.1145/1993744.1993777},
 isbn = {978-1-4503-0814-4},
 keyword = {GGSN placement, cellular network architecture, mobile content delivery},
 link = {http://doi.acm.org/10.1145/1993744.1993777},
 location = {San Jose, California, USA},
 numpages = {12},
 pages = {317--328},
 publisher = {ACM},
 series = {SIGMETRICS '11},
 title = {Cellular Data Network Infrastructure Characterization and Implication on Mobile Content Placement},
 year = {2011}
}


@proceedings{Harrison:2012:2254756,
 abstract = {It's a great pleasure to welcome you to the 12th joint ACM SIGMETRICS and IFIP PERFORMANCE International Conference, hosted by the Department of Computing, Imperial College London - one week after the Queen's Silver Jubilee celebrations and six weeks before the 2012 Olympic Games, just the other side of Town. In fact we chose these dates so as to avoid clashing with Her Majesty's special week, which might have been compromised by an event of such stature! This year's conference enhances the tradition of both of its constituents' being the premier fora for state-ofthe-art research in performance modeling and measurement techniques, tools and applications in the American and Europe continents, respectively. We have assembled a superb technical program with 31 full papers of the highest quality and 23 posters highlighting innovative research; further details are provided in the Program Chairs' Welcome that follows. Contributors come from 18 different countries in 3 continents; 49 papers have (co-)authors from academic institutions and 22 have industrial (co-)authors. This year we have increased the numbers of Tutorials and Workshops. On Monday, 11th June there are five tutorials and two workshops: the ever-popular GreenMetrics and, for the first time, W-PIN. My thanks to Cati Lladó for her organizing the tutorials and expanding this aspect of the conference. On Friday we have three more innovative Workshops: the long-established MAMA, a new one PADE and a Hands-on Tutorial-Workshop NetFPGA. We hope as many of you as possible will take advantage of these excellent satellite events.},
 address = {New York, NY, USA},
 isbn = {978-1-4503-1097-0},
 location = {London, England, UK},
 note = {81901201},
 publisher = {ACM},
 title = {SIGMETRICS '12: Proceedings of the 12th ACM SIGMETRICS/PERFORMANCE Joint International Conference on Measurement and Modeling of Computer Systems},
 year = {2012}
}


@inproceedings{Chen:2011:TBS:1993744.1993804,
 abstract = {Many network links in developing regions operate in the sub-packet regime, an environment where the typical per-flow throughput is less than 1 packet per round-trip time. TCP and other common congestion control protocols break down in the sub-packet regime, resulting in severe unfairness, high packet loss rates, and flow silences due to repetitive timeouts. To understand TCP's behavior in this regime, we propose a model particularly tailored to high packet loss-rates and relatively small congestion window sizes. We validate the model under a variety of network conditions.},
 acmid = {1993804},
 address = {New York, NY, USA},
 author = {Chen, Jay and Iyengar, Janardhan and Subramanian, Lakshminarayanan and Ford, Bryan},
 booktitle = {Proceedings of the ACM SIGMETRICS Joint International Conference on Measurement and Modeling of Computer Systems},
 doi = {10.1145/1993744.1993804},
 isbn = {978-1-4503-0814-4},
 keyword = {TCP, congestion control, low bandwidth networks},
 link = {http://doi.acm.org/10.1145/1993744.1993804},
 location = {San Jose, California, USA},
 numpages = {2},
 pages = {157--158},
 publisher = {ACM},
 series = {SIGMETRICS '11},
 title = {TCP Behavior in Sub Packet Regimes},
 year = {2011}
}


@inproceedings{Urgaonkar:2011:OPC:1993744.1993766,
 abstract = {Since the electricity bill of a data center constitutes a significant portion of its overall operational costs, reducing this has become important. We investigate cost reduction opportunities that arise by the use of uninterrupted power supply (UPS) units as energy storage devices. This represents a deviation from the usual use of these devices as mere transitional fail-over mechanisms between utility and captive sources such as diesel generators. We consider the problem of opportunistically using these devices to reduce the time average electric utility bill in a data center. Using the technique of Lyapunov optimization, we develop an online control algorithm that can optimally exploit these devices to minimize the time average cost. This algorithm operates without any knowledge of the statistics of the workload or electricity cost processes, making it attractive in the presence of workload and pricing uncertainties. An interesting feature of our algorithm is that its deviation from optimality reduces as the storage capacity is increased. Our work opens up a new area in data center power management.},
 acmid = {1993766},
 address = {New York, NY, USA},
 author = {Urgaonkar, Rahul and Urgaonkar, Bhuvan and Neely, Michael J. and Sivasubramaniam, Anand},
 booktitle = {Proceedings of the ACM SIGMETRICS Joint International Conference on Measurement and Modeling of Computer Systems},
 doi = {10.1145/1993744.1993766},
 isbn = {978-1-4503-0814-4},
 keyword = {data centers, optimal control, power management, stochastic optimization},
 link = {http://doi.acm.org/10.1145/1993744.1993766},
 location = {San Jose, California, USA},
 numpages = {12},
 pages = {221--232},
 publisher = {ACM},
 series = {SIGMETRICS '11},
 title = {Optimal Power Cost Management Using Stored Energy in Data Centers},
 year = {2011}
}


@proceedings{Merchant:2011:1993744,
 abstract = {Welcome to San Jose, to ACM's Federated Computing Research Conference (FCRC), and to SIGMETRICS 2011! We are proud to carry on the SIGMETRICS tradition of presenting highquality, innovative research on the measurement and modeling of computer systems. The program includes papers on a wide variety of topics, including resource allocation, multicore processing, network protocols, failure analysis, power management and network characterization, using a wide variety of techniques, including mathematical analysis, simulation, emulation, prototype experimentation, observation of real systems, and combinations thereof. SIGMETRICS 2011 received 177 submissions, from which 26 papers were accepted, for an acceptance rate of 15%. Additionally, 20 submissions were selected to appear as posters, leading to a combined paper and poster acceptance rate of 26%. Each paper received at least three reviews from PC members. Over 90% of papers received four or more reviews. Overall, program committee and external reviewers provided a total of 756 reviews. The review process was conducted online using the HotCRP conference management software over a period of two months. The program committee meeting, held at Columbia University in New York, NY, in January 2011, was attended in person by 36 of the 57 PC members, and "virtually" by another four members. During the review, deliberation, and decision process, we emphasized novelty and excitement: papers that took risks and were controversial were viewed more favorably than solid papers that provided limited new insights and took limited risks. As a result, we expect the program to generate active discussion.},
 address = {New York, NY, USA},
 isbn = {978-1-4503-0814-4},
 location = {San Jose, California, USA},
 note = {81901101},
 publisher = {ACM},
 title = {SIGMETRICS '11: Proceedings of the ACM SIGMETRICS Joint International Conference on Measurement and Modeling of Computer Systems},
 year = {2011}
}


@inproceedings{Li:2011:CAR:1993744.1993791,
 abstract = {An increasing number of data centers today start to incorporate renewable energy solutions to cap their carbon footprint. However, the impact of renewable energy on large-scale data center design is still not well understood. In this paper, we model and evaluate data centers driven by intermittent renewable energy. Using real-world data center and renewable energy source traces, we show that renewable power utilization and load tuning frequency are two critical metrics for designing sustainable high-performance data centers. Our characterization reveals that load power fluctuation together with the intermittent renewable power supply introduce unnecessary tuning activities, which can increase the management overhead and degrade the performance of renewable energy driven data centers.},
 acmid = {1993791},
 address = {New York, NY, USA},
 author = {Li, Chao and Qouneh, Amer and Li, Tao},
 booktitle = {Proceedings of the ACM SIGMETRICS Joint International Conference on Measurement and Modeling of Computer Systems},
 doi = {10.1145/1993744.1993791},
 isbn = {978-1-4503-0814-4},
 keyword = {data center, load tuning, power variation, renewable energy},
 link = {http://doi.acm.org/10.1145/1993744.1993791},
 location = {San Jose, California, USA},
 numpages = {2},
 pages = {131--132},
 publisher = {ACM},
 series = {SIGMETRICS '11},
 title = {Characterizing and Analyzing Renewable Energy Driven Data Centers},
 year = {2011}
}


@inproceedings{Hong:2011:DSP:1993744.1993799,
 abstract = {Cloud computing holds the exciting potential of elastically scaling computation to match time-varying demand, thus eliminating the need to provision for peak demand to satisfy response-time requirements. Moreover, cloud vendors often offer several commitment levels for their machine instances (e.g., users can choose to pay an upfront premium for the discounted hourly usage price). Because cost is a major concern that may limit the cloud adoption, two key challenges are to determine (a) the number of machines to provision and (b) the commitment level at which the machine instances should be acquired, to minimize cost while satisfying response-time targets. This paper address the above two challenges in an Infrastructure-as-a-Service (IaaS) cloud. Our simulations with real Web server load traces reveal that our techniques offer a cost reduction between 13% and 29% (21% on average) under Amazon EC2 pricing models.},
 acmid = {1993799},
 address = {New York, NY, USA},
 author = {Hong, Yu-Ju and Xue, Jiachen and Thottethodi, Mithuna},
 booktitle = {Proceedings of the ACM SIGMETRICS Joint International Conference on Measurement and Modeling of Computer Systems},
 doi = {10.1145/1993744.1993799},
 isbn = {978-1-4503-0814-4},
 keyword = {capacity planning, cloud computing, dynamic provisioning},
 link = {http://doi.acm.org/10.1145/1993744.1993799},
 location = {San Jose, California, USA},
 numpages = {2},
 pages = {147--148},
 publisher = {ACM},
 series = {SIGMETRICS '11},
 title = {Dynamic Server Provisioning to Minimize Cost in an IaaS Cloud},
 year = {2011}
}


@inproceedings{Alizadeh:2011:ADS:1993744.1993753,
 abstract = {Cloud computing, social networking and information networks (for search, news feeds, etc) are driving interest in the deployment of large data centers. TCP is the dominant Layer 3 transport protocol in these networks. However, the operating conditions---very high bandwidth links, low round-trip times, small-buffered switches---and traffic patterns cause TCP to perform very poorly. The Data Center TCP (DCTCP) algorithm has recently been proposed as a TCP variant for data centers and addresses these shortcomings. In this paper, we provide a mathematical analysis of DCTCP. We develop a fluid model of DCTCP and use it to analyze the throughput and delay performance of the algorithm, as a function of the design parameters and of network conditions like link speeds, round-trip times and the number of active flows. Unlike fluid model representations of standard congestion control loops, the DCTCP fluid model exhibits limit cycle behavior. Therefore, it is not amenable to analysis by linearization around a fixed point and we undertake a direct analysis of the limit cycles, proving their stability. Using a hybrid (continuous- and discrete-time) model, we analyze the convergence of DCTCP sources to their fair share, obtaining an explicit characterization of the convergence rate. Finally, we investigate the "RTT-fairness" of DCTCP; i.e., the rate obtained by DCTCP sources as a function of their RTTs. We find a very simple change to DCTCP which is suggested by the fluid model and which significantly improves DCTCP's RTT-fairness. We corroborate our results with ns2 simulations.},
 acmid = {1993753},
 address = {New York, NY, USA},
 author = {Alizadeh, Mohammad and Javanmard, Adel and Prabhakar, Balaji},
 booktitle = {Proceedings of the ACM SIGMETRICS Joint International Conference on Measurement and Modeling of Computer Systems},
 doi = {10.1145/1993744.1993753},
 isbn = {978-1-4503-0814-4},
 keyword = {TCP, analysis, congestion control, data center network},
 link = {http://doi.acm.org/10.1145/1993744.1993753},
 location = {San Jose, California, USA},
 numpages = {12},
 pages = {73--84},
 publisher = {ACM},
 series = {SIGMETRICS '11},
 title = {Analysis of DCTCP: Stability, Convergence, and Fairness},
 year = {2011}
}


@inproceedings{Korada:2011:GP:1993744.1993764,
 abstract = {Eigenvectors of data matrices play an important role in many computational problems, ranging from signal processing to machine learning and control. For instance, algorithms that compute positions of the nodes of a wireless network on the basis of pairwise distance measurements require a few leading eigenvectors of the distances matrix. While eigenvector calculation is a standard topic in numerical linear algebra, it becomes challenging under severe communication or computation constraints, or in absence of central scheduling. In this paper we investigate the possibility of computing the leading eigenvectors of a large data matrix through gossip algorithms. The proposed algorithm amounts to iteratively multiplying a vector by independent random sparsification of the original matrix and averaging the resulting normalized vectors. This can be viewed as a generalization of gossip algorithms for consensus, but the resulting dynamics is significantly more intricate. Our analysis is based on controlling the convergence to stationarity of the associated Kesten-Furstenberg Markov chain.},
 acmid = {1993764},
 address = {New York, NY, USA},
 author = {Korada, Satish Babu and Montanari, Andrea and Oh, Sewoong},
 booktitle = {Proceedings of the ACM SIGMETRICS Joint International Conference on Measurement and Modeling of Computer Systems},
 doi = {10.1145/1993744.1993764},
 isbn = {978-1-4503-0814-4},
 keyword = {PCA, gossip algorithm},
 link = {http://doi.acm.org/10.1145/1993744.1993764},
 location = {San Jose, California, USA},
 numpages = {12},
 pages = {209--220},
 publisher = {ACM},
 series = {SIGMETRICS '11},
 title = {Gossip PCA},
 year = {2011}
}


@inproceedings{Rozner:2011:MOO:1993744.1993771,
 abstract = {Opportunistic routing aims to improve wireless performance by exploiting communication opportunities arising by chance. A key challenge in opportunistic routing is how to achieve good, predictable performance despite the incidental nature of such communication opportunities and the complicated effects of wireless interference in IEEE 802.11 networks. To address the challenge, we develop a model-driven optimization framework to jointly optimize opportunistic routes and rate limits for both unicast and multicast traffic. A distinctive feature of our framework is that the performance derived from optimization can be achieved in a real IEEE 802.11 network. Our framework consists of three key components: (i) a model for capturing the interference among IEEE 802.11 broadcast transmissions, (ii) a novel algorithm for accurately optimizing different performance objectives, and (iii) effective techniques for mapping the resulting solutions to practical routing configurations. Extensive simulations and testbed experiments show that our approach significantly outperforms state-of-the-art shortest path routing and opportunistic routing protocols. Moreover, the difference between the achieved performance and our model estimation is typically within 20%. Evaluation in dynamic and uncontrolled environments further shows that our approach is robust against inaccuracy introduced by a dynamic network and it also consistently out-performs the existing schemes. These results clearly demonstrate the effectiveness and accuracy of our approach.},
 acmid = {1993771},
 address = {New York, NY, USA},
 author = {Rozner, Eric and Han, Mi Kyung and Qiu, Lili and Zhang, Yin},
 booktitle = {Proceedings of the ACM SIGMETRICS Joint International Conference on Measurement and Modeling of Computer Systems},
 doi = {10.1145/1993744.1993771},
 isbn = {978-1-4503-0814-4},
 keyword = {model-driven optimization, opportunistic routing, wireless interference, wireless mesh networks, wireless network model},
 link = {http://doi.acm.org/10.1145/1993744.1993771},
 location = {San Jose, California, USA},
 numpages = {12},
 pages = {269--280},
 publisher = {ACM},
 series = {SIGMETRICS '11},
 title = {Model-driven Optimization of Opportunistic Routing},
 year = {2011}
}


@inproceedings{Kurant:2011:WGM:1993744.1993773,
 abstract = {Our objective is to sample the node set of a large unknown graph via crawling, to accurately estimate a given metric of interest. We design a random walk on an appropriately defined weighted graph that achieves high efficiency by preferentially crawling those nodes and edges that convey greater information regarding the target metric. Our approach begins by employing the theory of stratification to find optimal node weights, for a given estimation problem, under an independence sampler. While optimal under independence sampling, these weights may be impractical under graph crawling due to constraints arising from the structure of the graph. Therefore, the edge weights for our random walk should be chosen so as to lead to an equilibrium distribution that strikes a balance between approximating the optimal weights under an independence sampler and achieving fast convergence. We propose a heuristic approach (stratified weighted random walk, or S-WRW) that achieves this goal, while using only limited information about the graph structure and the node properties. We evaluate our technique in simulation, and experimentally, by collecting a sample of Facebook college users. We show that S-WRW requires 13-15 times fewer samples than the simple re-weighted random walk (RW) to achieve the same estimation accuracy for a range of metrics.},
 acmid = {1993773},
 address = {New York, NY, USA},
 author = {Kurant, Maciej and Gjoka, Minas and Butts, Carter T. and Markopoulou, Athina},
 booktitle = {Proceedings of the ACM SIGMETRICS Joint International Conference on Measurement and Modeling of Computer Systems},
 doi = {10.1145/1993744.1993773},
 isbn = {978-1-4503-0814-4},
 keyword = {graph sampling, online social networks, random walks, stratified sampling, weighted graphs, weighted random walks},
 link = {http://doi.acm.org/10.1145/1993744.1993773},
 location = {San Jose, California, USA},
 numpages = {12},
 pages = {281--292},
 publisher = {ACM},
 series = {SIGMETRICS '11},
 title = {Walking on a Graph with a Magnifying Glass: Stratified Sampling via Weighted Random Walks},
 year = {2011}
}


@inproceedings{Adhikari:2011:YT:1993744.1993794,
 abstract = {In this paper we "reverse-engineer" the YouTube video delivery cloud by building a distributed measurement infrastructure. Through extensive data collection and analysis, we deduce the key design features underlying the YouTube video delivery cloud. The design of the YouTube video delivery cloud consists of three major components: a "flat" video id space, multiple DNS namespaces reflecting a multi-layered logical organization of video servers, and a 3-tier physical cache hierarchy. By mapping the video id space to the logical servers via consistent hashing and cleverly leveraging DNS and HTTP re-direction mechanisms, such a design leads to a scalable, robust and flexible content distribution system.},
 acmid = {1993794},
 address = {New York, NY, USA},
 author = {Adhikari, Vijay Kumar and Jain, Sourabh and Chen, Yingying and Zhang, Zhi-Li},
 booktitle = {Proceedings of the ACM SIGMETRICS Joint International Conference on Measurement and Modeling of Computer Systems},
 doi = {10.1145/1993744.1993794},
 isbn = {978-1-4503-0814-4},
 keyword = {3-tier physical cache hierarchy, YouTube, multi-layered logical organization},
 link = {http://doi.acm.org/10.1145/1993744.1993794},
 location = {San Jose, California, USA},
 numpages = {2},
 pages = {137--138},
 publisher = {ACM},
 series = {SIGMETRICS '11},
 title = {How Do You 'Tube'},
 year = {2011}
}


@inproceedings{Zhang:2011:ONS:1993744.1993796,
 abstract = {We study the problem of neighbor selection in BitTorrent-like peer-to-peer (P2P) systems, and propose a "soft-worst-neighbor-choking" algorithm that is provably optimal. In practical P2P systems, peers often keep a large set of potential neighbors, but only simultaneously upload/download to/from a small subset of them, which we call active neighbors, to avoid excessive connection overhead. A natural question to ask is: which active neighbor set should each peer choose to maximize the global system performance? The combinatorial nature of the problem makes it especially challenging. In this paper, we formulate an optimization problem and derive a distributed algorithm. We remark that our solution has a similar favor compared to the worst neighbor choking and optimistic unchoking neighbor selection algorithms that are implemented by BitTorrent. However, it encourages peers to stick to better performing neighbors for longer time and is provably globally optimal. Our proposed solution is easy to implement: each peer periodically waits for a constant period of time that depends on the size of the potential neighbor set and the aggregated utility of the active neighbors, chokes (drops) one of its current active neighbors with probability proportional to an exponential weight on the utility of the corresponding link, and randomly unchokes (adds) a new neighbor from its potential neighbor set. Our theoretical findings provide insightful guidelines to designing practical P2P systems. Simulation results corroborate our proposed solution.},
 acmid = {1993796},
 address = {New York, NY, USA},
 author = {Zhang, Hao and Shao, Ziyu and Chen, Minghua and Ramchandran, Kannan},
 booktitle = {Proceedings of the ACM SIGMETRICS Joint International Conference on Measurement and Modeling of Computer Systems},
 doi = {10.1145/1993744.1993796},
 isbn = {978-1-4503-0814-4},
 keyword = {BitTorrent, Markov chain, P2P, insensitivity},
 link = {http://doi.acm.org/10.1145/1993744.1993796},
 location = {San Jose, California, USA},
 numpages = {2},
 pages = {141--142},
 publisher = {ACM},
 series = {SIGMETRICS '11},
 title = {Optimal Neighbor Selection in BitTorrent-like Peer-to-peer Networks},
 year = {2011}
}


@inproceedings{Srinivasan:2011:HHA:1993744.1993800,
 abstract = {Designing heterogeneous chip multiprocessors (CMPs) with a mix of big cores (complex superscalar out-of-order pipelines) and small cores (simple in-order pipeline) is emerging as an attractive option for future architectures. Such architectures have the potential to deliver both high performance and power efficiency but this requires operating systems (OS) or virtual machine monitors (VMMs) to efficiently schedule each software thread on the type of core that is best suited for it. In this paper, we highlight the need for architectural support for OS scheduling in a heterogeneous CMP. We propose HeteroScouts, a hardware mechanism to assist the OS to efficiently predict the performance of a task on different cores in the platform.},
 acmid = {1993800},
 address = {New York, NY, USA},
 author = {Srinivasan, Sadagopan and Iyer, Ravishankar and Zhao, Li and Illikkal, Ramesh},
 booktitle = {Proceedings of the ACM SIGMETRICS Joint International Conference on Measurement and Modeling of Computer Systems},
 doi = {10.1145/1993744.1993800},
 isbn = {978-1-4503-0814-4},
 keyword = {CMPs, OS scheduling, heterogeneous architectures, performance},
 link = {http://doi.acm.org/10.1145/1993744.1993800},
 location = {San Jose, California, USA},
 numpages = {2},
 pages = {149--150},
 publisher = {ACM},
 series = {SIGMETRICS '11},
 title = {HeteroScouts: Hardware Assist for OS Scheduling in Heterogeneous CMPs},
 year = {2011}
}


@inproceedings{Gulati:2011:STM:1993744.1993781,
 abstract = {Storage management in virtualized environments is considered as one of the biggest cost factors. According to some estimates, majority of the cost and performance problems are related to storage devices. In this tutorial, we will discuss some of the key storage technologies deployed in virtual datacenters. We will discuss a set of unique challenges faced by administrators and users due to increasing number of layers of abstraction. Next we will discuss some tools and techniques to do workload characterization and monitor devices in order to understand and trouble-shoot IO problems. We will also present some of the recent solutions proposed by industry and academia to handle these problems followed by upcoming technological trends and directions for future research.},
 acmid = {1993781},
 address = {New York, NY, USA},
 author = {Gulati, Ajay and Ahmad, Irfan},
 booktitle = {Proceedings of the ACM SIGMETRICS Joint International Conference on Measurement and Modeling of Computer Systems},
 doi = {10.1145/1993744.1993781},
 isbn = {978-1-4503-0814-4},
 keyword = {peformance, resource management, storage virtualization, workload characterization},
 link = {http://doi.acm.org/10.1145/1993744.1993781},
 location = {San Jose, California, USA},
 numpages = {2},
 pages = {353--354},
 publisher = {ACM},
 series = {SIGMETRICS '11},
 title = {Storage Technologies, Management and Troubleshooting in Virtualized Datacenters},
 year = {2011}
}


@inproceedings{Bowden:2011:NLT:1993744.1993805,
 abstract = {
                  An abstract is not available.
              },
 acmid = {1993805},
 address = {New York, NY, USA},
 author = {Bowden, Rhys Alistair and Roughan, Matthew and Bean, Nigel},
 booktitle = {Proceedings of the ACM SIGMETRICS Joint International Conference on Measurement and Modeling of Computer Systems},
 doi = {10.1145/1993744.1993805},
 isbn = {978-1-4503-0814-4},
 keyword = {compressive sensing, link tomography, measurement, multiple destination, multiple source, network tomography, sparsity},
 link = {http://doi.acm.org/10.1145/1993744.1993805},
 location = {San Jose, California, USA},
 numpages = {2},
 pages = {159--160},
 publisher = {ACM},
 series = {SIGMETRICS '11},
 title = {Network Link Tomography and Compressive Sensing},
 year = {2011}
}


@inproceedings{Casale:2011:BAW:1993744.1993783,
 abstract = {
                  An abstract is not available.
              },
 acmid = {1993783},
 address = {New York, NY, USA},
 author = {Casale, Giuliano},
 booktitle = {Proceedings of the ACM SIGMETRICS Joint International Conference on Measurement and Modeling of Computer Systems},
 doi = {10.1145/1993744.1993783},
 isbn = {978-1-4503-0814-4},
 keyword = {Markovian arrival process, fitting techniques, workload models},
 link = {http://doi.acm.org/10.1145/1993744.1993783},
 location = {San Jose, California, USA},
 numpages = {2},
 pages = {357--358},
 publisher = {ACM},
 series = {SIGMETRICS '11},
 title = {Building Accurate Workload Models Using Markovian Arrival Processes},
 year = {2011}
}


@inproceedings{Shafiq:2011:CMI:1993744.1993776,
 abstract = {Understanding Internet traffic dynamics in large cellular networks is important for network design, troubleshooting, performance evaluation, and optimization. In this paper, we present the results from our study, which is based upon a week-long aggregated flow level mobile device traffic data collected from a major cellular operator's core network. In this study, we measure and characterize the spatial and temporal dynamics of mobile Internet traffic. We distinguish our study from other related work by conducting the measurement at a larger scale and exploring mobile data traffic patterns along two new dimensions -- device types and applications that generate such traffic patterns. Based on the findings of our measurement analysis, we propose a Zipf-like model to capture the volume distribution of application traffic and a Markov model to capture the volume dynamics of aggregate Internet traffic. We further customize our models for different device types using an unsupervised clustering algorithm to improve prediction accuracy.},
 acmid = {1993776},
 address = {New York, NY, USA},
 author = {Shafiq, M. Zubair and Ji, Lusheng and Liu, Alex X. and Wang, Jia},
 booktitle = {Proceedings of the ACM SIGMETRICS Joint International Conference on Measurement and Modeling of Computer Systems},
 doi = {10.1145/1993744.1993776},
 isbn = {978-1-4503-0814-4},
 keyword = {cellular network, internet traffic, mobile devices},
 link = {http://doi.acm.org/10.1145/1993744.1993776},
 location = {San Jose, California, USA},
 numpages = {12},
 pages = {305--316},
 publisher = {ACM},
 series = {SIGMETRICS '11},
 title = {Characterizing and Modeling Internet Traffic Dynamics of Cellular Devices},
 year = {2011}
}


