@inproceedings{Chalermsook:2015:SNM:2745844.2745853,
 abstract = {Viral marketing is a powerful tool for online advertising and sales because it exploits the influence people have on one another. While this marketing technique has been beneficial for advertisers, it has not been shown how the social network providers such as Facebook and Twitter can benefit from it. In this paper, we initiate the study of sponsored viral marketing where a social network provider that has complete knowledge of its network is hired by several advertisers to provide viral marketing. Each advertiser has its own advertising budget and a fixed amount they are willing to pay for each user that adopts their product or shares their ads. The goal of the social network provider is to gain the most revenue from the advertisers. Since the products or ads from different advertisers may compete with each other in getting users' attention, and advertisers pay differently per share and have different budgets, it is very important that the social network providers start the "seeds" of the viral marketing of each product at the right places in order to gain the most benefit. We study both when advertisers have limited and unlimited budgets. In the unlimited budget setting, we give a tight approximation algorithm for the above task: we present a polynomial-time O(log n)-approximation algorithm for maximizing the expected revenue, where n is the number of nodes (i.e., users) in the social network, and show that no polynomial-time O(log1-ε n)-approximation algorithm exists, unless NP ⊆ DTIME}(npoly log n). In the limited budget setting, we show that it is hopeless to solve the problem (even approximately): unless P = NP, there is no polynomial-time O(n1-ε)-approximation algorithm. We perform experiments on several data sets to compare our provable algorithms to several heuristic baselines.},
 acmid = {2745853},
 address = {New York, NY, USA},
 author = {Chalermsook, Parinya and Das Sarma, Atish and Lall, Ashwin and Nanongkai, Danupon},
 booktitle = {Proceedings of the 2015 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems},
 doi = {10.1145/2745844.2745853},
 isbn = {978-1-4503-3486-0},
 keyword = {approximation algorithms, social networks},
 link = {http://doi.acm.org/10.1145/2745844.2745853},
 location = {Portland, Oregon, USA},
 numpages = {12},
 pages = {259--270},
 publisher = {ACM},
 series = {SIGMETRICS '15},
 title = {Social Network Monetization via Sponsored Viral Marketing},
 year = {2015}
}


@inproceedings{Gast:2015:TSR:2745844.2745850,
 abstract = {In this paper we study the performance of a family of cache replacement algorithms. The cache is decomposed into lists. Items enter the cache via the first list. An item enters the cache via the first list and jumps to the next list whenever a hit on it occurs. The classical policies FIFO, RANDOM, CLIMB and its hybrids are obtained as special cases. We present explicit expressions for the cache content distribution and miss probability under the IRM model. We develop an algorithm with a time complexity that is polynomial in the cache size and linear in the number of items to compute the exact miss probability. We introduce lower and upper bounds on the latter that can be computed in a time that is linear in the cache size times the number of items. We further introduce a mean field model to approximate the transient behavior of the miss probability and prove that this model becomes exact as the cache size and number of items tends to infinity. We show that the set of ODEs associated to the mean field model has a unique fixed point that can be used to approximate the miss probability in case the exact computation becomes too time consuming. Using this approximation, we provide guidelines on how to select a replacement algorithm within the family considered such that a good trade-off is achieved between the cache reactivity and its steady-state hit probability. We simulate these cache replacement algorithms on traces of real data and show that they can outperform LRU. Finally, we also disprove the well-known conjecture that the CLIMB algorithm is the optimal finite-memory replacement algorithm under the IRM model.},
 acmid = {2745850},
 address = {New York, NY, USA},
 author = {Gast, Nicolas and Van Houdt, Benny},
 booktitle = {Proceedings of the 2015 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems},
 doi = {10.1145/2745844.2745850},
 isbn = {978-1-4503-3486-0},
 keyword = {cache replacement policies, independent reference model, lru, page replacement, self-organizing lists, storage management},
 link = {http://doi.acm.org/10.1145/2745844.2745850},
 location = {Portland, Oregon, USA},
 numpages = {14},
 pages = {123--136},
 publisher = {ACM},
 series = {SIGMETRICS '15},
 title = {Transient and Steady-state Regime of a Family of List-based Cache Replacement Algorithms},
 year = {2015}
}


@article{Combes:2015:LRR:2796314.2745852,
 abstract = {Algorithms for learning to rank Web documents, display ads, or other types of items constitute a fundamental component of search engines and more generally of online services. In such systems, when a user makes a request or visits a web page, an ordered list of items (e.g. documents or ads) is displayed; the user scans this list in order, and clicks on the first relevant item if any. When the user clicks on an item, the reward collected by the system typically decreases with the position of the item in the displayed list. The main challenge in the design of sequential list selection algorithms stems from the fact that the probabilities with which the user clicks on the various items are unknown and need to be learned. We formulate the design of such algorithms as a stochastic bandit optimization problem. This problem differs from the classical bandit framework: (1) the type of feedback received by the system depends on the actual relevance of the various items in the displayed list (if the user clicks on the last item, we know that none of the previous items in the list are relevant); (2) there are inherent correlations between the average relevance of the items (e.g. the user may be interested in a specific topic only). We assume that items are categorized according to their topic and that users are clustered, so that users of the same cluster are interested in the same topic. We investigate several scenarios depending on the available side-information on the user before selecting the displayed list: (a) we first treat the case where the topic the user is interested in is known when she places a request; (b) we then study the case where the user cluster is known but the mapping between user clusters and topics is unknown. For both scenarios, we derive regret lower bounds and devise algorithms that approach these fundamental limits.},
 acmid = {2745852},
 address = {New York, NY, USA},
 author = {Combes, Richard and Magureanu, Stefan and Proutiere, Alexandre and Laroche, Cyrille},
 doi = {10.1145/2796314.2745852},
 issn = {0163-5999},
 issue_date = {June 2015},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 keyword = {ad-display optimization, learning, multi-armed bandits, search engines},
 link = {http://doi.acm.org/10.1145/2796314.2745852},
 month = {jun},
 number = {1},
 numpages = {14},
 pages = {231--244},
 publisher = {ACM},
 title = {Learning to Rank: Regret Lower Bounds and Efficient Algorithms},
 volume = {43},
 year = {2015}
}


@inproceedings{Aalto:2015:WIA:2745844.2745851,
 abstract = {We consider the optimal opportunistic scheduling problem for downlink data traffic in a wireless cell with time-varying channels. The scheduler itself operates in a very fast timescale of milliseconds, but the objective function is related to minimizing the holding costs in a much longer timescale, at the so-called flow level. The Whittle index approach is a powerful tool in this context, since it renders the flow level optimization problem with heterogeneous users tractable. Until now, this approach has been applied to the opportunistic scheduling problem to generate non-anticipating index policies that may depend on the amount of attained service but do not utilize the exact size information. In this paper, we produce a size-aware (i.e., anticipating) index policy by applying the Whittle index approach in a novel way. By a numerical study based on simulations, we demonstrate that the resulting size-aware index policy systematically improves performance. As a side result, we show that the opportunistic scheduling problem is indexable when the file sizes follow the Pascal distribution, and we derive the corresponding Whittle index, which generalizes earlier results.},
 acmid = {2745851},
 address = {New York, NY, USA},
 author = {Aalto, Samuli and Lassila, Pasi and Osti, Prajwal},
 booktitle = {Proceedings of the 2015 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems},
 doi = {10.1145/2745844.2745851},
 isbn = {978-1-4503-3486-0},
 keyword = {opportunistic scheduling, size-based scheduling, stochastic optimization, whittle index},
 link = {http://doi.acm.org/10.1145/2745844.2745851},
 location = {Portland, Oregon, USA},
 numpages = {13},
 pages = {57--69},
 publisher = {ACM},
 series = {SIGMETRICS '15},
 title = {Whittle Index Approach to Size-aware Scheduling with Time-varying Channels},
 year = {2015}
}


@inproceedings{Bonald:2015:MFO:2745844.2745869,
 abstract = {Designing efficient and fair algorithms for sharing multiple resources between heterogeneous demands is becoming increasingly important. Applications include compute clusters shared by multi-task jobs and routers equipped with middleboxes shared by flows of different types. We show that the currently preferred objective of Dominant Resource Fairness (DRF) has a significantly less favorable efficiency-fairness tradeoff than alternatives like Proportional Fairness and our proposal, Bottleneck Max Fairness. We propose practical algorithms to realize these sharing objectives and evaluate their performance under a stochastic demand model. It is shown, in particular, that the strategyproofness property that motivated the choice of DRF for an assumed fixed set of jobs or flows, is largely irrelevant when demand is dynamic.},
 acmid = {2745869},
 address = {New York, NY, USA},
 author = {Bonald, Thomas and Roberts, James},
 booktitle = {Proceedings of the 2015 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems},
 doi = {10.1145/2745844.2745869},
 isbn = {978-1-4503-3486-0},
 keyword = {bottleneck max fairness, cluster computing, dominant resource fairness, multi-resource sharing, proportional fairness},
 link = {http://doi.acm.org/10.1145/2745844.2745869},
 location = {Portland, Oregon, USA},
 numpages = {12},
 pages = {31--42},
 publisher = {ACM},
 series = {SIGMETRICS '15},
 title = {Multi-Resource Fairness: Objectives, Algorithms and Performance},
 year = {2015}
}


@article{Meza:2015:LSF:2796314.2745848,
 abstract = {Servers use flash memory based solid state drives (SSDs) as a high-performance alternative to hard disk drives to store persistent data. Unfortunately, recent increases in flash density have also brought about decreases in chip-level reliability. In a data center environment, flash-based SSD failures can lead to downtime and, in the worst case, data loss. As a result, it is important to understand flash memory reliability characteristics over flash lifetime in a realistic production data center environment running modern applications and system software. This paper presents the first large-scale study of flash-based SSD reliability in the field. We analyze data collected across a majority of flash-based solid state drives at Facebook data centers over nearly four years and many millions of operational hours in order to understand failure properties and trends of flash-based SSDs. Our study considers a variety of SSD characteristics, including: the amount of data written to and read from flash chips; how data is mapped within the SSD address space; the amount of data copied, erased, and discarded by the flash controller; and flash board temperature and bus power. Based on our field analysis of how flash memory errors manifest when running modern workloads on modern SSDs, this paper is the first to make several major observations: (1) SSD failure rates do not increase monotonically with flash chip wear; instead they go through several distinct periods corresponding to how failures emerge and are subsequently detected, (2) the effects of read disturbance errors are not prevalent in the field, (3) sparse logical data layout across an SSD's physical address space (e.g., non-contiguous data), as measured by the amount of metadata required to track logical address translations stored in an SSD-internal DRAM buffer, can greatly affect SSD failure rate, (4) higher temperatures lead to higher failure rates, but techniques that throttle SSD operation appear to greatly reduce the negative reliability impact of higher temperatures, and (5) data written by the operating system to flash-based SSDs does not always accurately indicate the amount of wear induced on flash cells due to optimizations in the SSD controller and buffering employed in the system software. We hope that the findings of this first large-scale flash memory reliability study can inspire others to develop other publicly-available analyses and novel flash reliability solutions.},
 acmid = {2745848},
 address = {New York, NY, USA},
 author = {Meza, Justin and Wu, Qiang and Kumar, Sanjev and Mutlu, Onur},
 doi = {10.1145/2796314.2745848},
 issn = {0163-5999},
 issue_date = {June 2015},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 keyword = {flash memory, reliability, warehouse-scale data centers},
 link = {http://doi.acm.org/10.1145/2796314.2745848},
 month = {jun},
 number = {1},
 numpages = {14},
 pages = {177--190},
 publisher = {ACM},
 title = {A Large-Scale Study of Flash Memory Failures in the Field},
 volume = {43},
 year = {2015}
}


@article{Rizk:2015:CBF:2796314.2745859,
 abstract = {In a Fork-Join (FJ) queueing system an upstream fork station splits incoming jobs into N tasks to be further processed by N parallel servers, each with its own queue; the response time of one job is determined, at a downstream join station, by the maximum of the corresponding tasks' response times. This queueing system is useful to the modelling of multi-service systems subject to synchronization constraints, such as MapReduce clusters or multipath routing. Despite their apparent simplicity, FJ systems are hard to analyze. This paper provides the first computable stochastic bounds on the waiting and response time distributions in FJ systems. We consider four practical scenarios by combining 1a) renewal and 1b) non-renewal arrivals, and 2a) non-blocking and 2b) blocking servers. In the case of non blocking servers we prove that delays scale as O(logN), a law which is known for first moments under renewal input only. In the case of blocking servers, we prove that the same factor of log N dictates the stability region of the system. Simulation results indicate that our bounds are tight, especially at high utilizations, in all four scenarios. A remarkable insight gained from our results is that, at moderate to high utilizations, multipath routing 'makes sense' from a queueing perspective for two paths only, i.e., response times drop the most when N = 2; the technical explanation is that the resequencing (delay) price starts to quickly dominate the tempting gain due to multipath transmissions.},
 acmid = {2745859},
 address = {New York, NY, USA},
 author = {Rizk, Amr and Poloczek, Felix and Ciucu, Florin},
 doi = {10.1145/2796314.2745859},
 issn = {0163-5999},
 issue_date = {June 2015},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 keyword = {fork-join queue, mapreduce, multipath, parallel systems, performance evaluation},
 link = {http://doi.acm.org/10.1145/2796314.2745859},
 month = {jun},
 number = {1},
 numpages = {12},
 pages = {335--346},
 publisher = {ACM},
 title = {Computable Bounds in Fork-Join Queueing Systems},
 volume = {43},
 year = {2015}
}


@article{Xu:2015:PCH:2796314.2745901,
 abstract = {The storage subsystem has undergone tremendous innovation in order to keep up with the ever-increasing demand for throughput. NVMe based SSDs are the latest development in this domain, delivering unprecedented performance in terms of both latency and peak bandwidth. Given their superior performance, NVMe drives are expected to be particularly beneficial for I/O intensive applications in datacenter installations. In this paper we identify and analyze the different factors leading to the better performance of NVMe SSDs. Then, using databases as the prominent use-case, we show how these would translate into real-world benefits. We evaluate both a relational database (MySQL) and a NoSQL database (Cassandra) and demonstrate significant performance gains over best-in-class enterprise SATA SSDs: from 3.5x for TPC-C and up to 8.5x for Cassandra.},
 acmid = {2745901},
 address = {New York, NY, USA},
 author = {Xu, Qiumin and Siyamwala, Huzefa and Ghosh, Mrinmoy and Awasthi, Manu and Suri, Tameesh and Guz, Zvika and Shayesteh, Anahita and Balakrishnan, Vijay},
 doi = {10.1145/2796314.2745901},
 issn = {0163-5999},
 issue_date = {June 2015},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 keyword = {hyperscale applications, nosql databases, nvme, performance characterization, ssds},
 link = {http://doi.acm.org/10.1145/2796314.2745901},
 month = {jun},
 number = {1},
 numpages = {2},
 pages = {473--474},
 publisher = {ACM},
 title = {Performance Characterization of Hyperscale Applicationson on NVMe SSDs},
 volume = {43},
 year = {2015}
}


@article{Mirhoseini:2015:FTL:2796314.2745889,
 abstract = {This paper proposes a domain-specific solution for iterative learning of big and dense (non-sparse) datasets. A large host of learning algorithms, including linear and regularized regression techniques, rely on iterative updates on the data connectivity matrix in order to converge to a solution. The performance of such algorithms often severely degrade when it comes to large and dense data. Massive dense datasets not only induce obligatory large number of arithmetics, but they also incur unwanted message passing cost across the processing nodes. Our key observation is that despite the seemingly dense structures, in many applications, data can be transformed into a new space where sparse structures become revealed. We propose a scalable data transformation scheme that enables creating versatile sparse representations of the data. The transformation can be tuned to benefit the underlying platform's cost and constraints. Our evaluations demonstrate significant improvement in energy usage, runtime, and mem},
 acmid = {2745889},
 address = {New York, NY, USA},
 author = {Mirhoseini, Azalia and Songhori, Ebrahim M. and Darvish Rouhani, Bita and Koushanfar, Farinaz},
 doi = {10.1145/2796314.2745889},
 issn = {0163-5999},
 issue_date = {June 2015},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 keyword = {big and dense data, performance optimization, sparse factorization, subspace sampling},
 link = {http://doi.acm.org/10.1145/2796314.2745889},
 month = {jun},
 number = {1},
 numpages = {2},
 pages = {453--454},
 publisher = {ACM},
 title = {Flexible Transformations For Learning Big Data},
 volume = {43},
 year = {2015}
}


@article{Gupta:2015:TCI:2796314.2745898,
 abstract = {Internet-scale Distributed Networks (IDNs) are large distributed systems that comprise hundreds of thousands of servers located around the world. IDNs consume significant amounts of energy to power their deployed server infrastructure, and nearly as much energy to cool that infrastructure. We study the potential benefits of using renewable open air cooling (OAC) in an IDN. Our results show that by using OAC, a global IDN can extract 51% cooling energy reducing during summers and a 92% reduction in the winter.},
 acmid = {2745898},
 address = {New York, NY, USA},
 author = {Gupta, Vani and Lee, Stephen and Shenoy, Prashant and Sitaraman, Ramesh and Urgaonkar, Rahul},
 doi = {10.1145/2796314.2745898},
 issn = {0163-5999},
 issue_date = {June 2015},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 keyword = {energy, internet-scale distributed systems, load balancing},
 link = {http://doi.acm.org/10.1145/2796314.2745898},
 month = {jun},
 number = {1},
 numpages = {2},
 pages = {469--470},
 publisher = {ACM},
 title = {Towards Cooling Internet-Scale Distributed Networks on the Cheap},
 volume = {43},
 year = {2015}
}


@inproceedings{Li:2015:QRA:2745844.2745864,
 abstract = {The backpressure scheduling algorithm for multihop wireless networks is known to be throughput optimal, but it requires each node to maintain per-destination queues. Recently, a clever generalization of processor sharing has been proposed which is also throughput optimal, but which only uses per-link queues. Here we propose another algorithm called Queue Proportional Rate Allocation (QPRA) which also only uses per-link queues, and allocates service rates to links in proportion to their queue-lengths and employs the Serve-In-Random-Order (SIRO) discipline within each link. Through fluid limit techniques and using a novel Lyapunov function, we show that the QPRA achieves the maximum throughput. We demonstrate an advantage of QPRA by showing that, for the so-called primary interference model, it is able to develop a low-complexity scheduling scheme which approximates QPRA and achieves a constant fraction of the maximum throughput region, independent of network size.},
 acmid = {2745864},
 address = {New York, NY, USA},
 author = {Li, Bin and Srikant, Rayadurgam},
 booktitle = {Proceedings of the 2015 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems},
 doi = {10.1145/2745844.2745864},
 isbn = {978-1-4503-3486-0},
 keyword = {fluid limit analysis., low-complexity scheduler design, lyapunov functions, maximum throughput, multihop wireless networks, resource allocation},
 link = {http://doi.acm.org/10.1145/2745844.2745864},
 location = {Portland, Oregon, USA},
 numpages = {12},
 pages = {97--108},
 publisher = {ACM},
 series = {SIGMETRICS '15},
 title = {Queue-Proportional Rate Allocation with Per-Link Information in Multihop Networks},
 year = {2015}
}


@inproceedings{Marasevic:2015:RAR:2745844.2745872,
 abstract = {Full-duplex communication has the potential to substantially increase the throughput in wireless networks. However, the benefits of full-duplex are still not well understood. In this paper, we characterize the full-duplex rate gains in both single-channel and multi-channel use cases. For the single-channel case, we quantify the rate gain as a function of the remaining self-interference and SNR values. We also provide a sufficient condition under which the sum of uplink and downlink rates on a full-duplex channel is concave in the transmission power levels. Building on these results, we consider the multi-channel case. For that case, we introduce a new realistic model of a small form-factor (e.g., smartphone) full-duplex receiver and demonstrate its accuracy via measurements. We study the problem of jointly allocating power levels to different channels and selecting the frequency of maximum self-interference suppression, where the objective is maximizing the sum of the rates over uplink and downlink OFDM channels. We develop a polynomial time algorithm which is nearly optimal under very mild restrictions. To reduce the running time, we develop an efficient nearly-optimal algorithm under the high SINR approximation. Finally, we demonstrate via numerical evaluations the capacity gains in the different use cases and obtain insights into the impact of the remaining self-interference and wireless channel states on the performance.},
 acmid = {2745872},
 address = {New York, NY, USA},
 author = {Marasevic, Jelena and Zhou, Jin and Krishnaswamy, Harish and Zhong, Yuan and Zussman, Gil},
 booktitle = {Proceedings of the 2015 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems},
 doi = {10.1145/2745844.2745872},
 isbn = {978-1-4503-3486-0},
 keyword = {full-duplex, modeling, resource allocation},
 link = {http://doi.acm.org/10.1145/2745844.2745872},
 location = {Portland, Oregon, USA},
 numpages = {14},
 pages = {109--122},
 publisher = {ACM},
 series = {SIGMETRICS '15},
 title = {Resource Allocation and Rate Gains in Practical Full-Duplex Systems},
 year = {2015}
}


@inproceedings{Massoulie:2015:GTN:2745844.2745868,
 abstract = {This work addresses user targeting for news content delivery. Specifically, we wish to disseminate a fresh news content, whose topic is yet unknown, to all interested users, while "spamming" a minimum number of uninterested users. We formulate this as an online stochastic optimization problem that extends in several ways the classical multi-armed bandit problem. We introduce Greedy-Bayes, a policy with appealing robustness properties. We establish optimal scaling of a suitably defined regret measure in various scenarios of interest. To that end we develop an original proof technique based on martingale concentration inequalities. Numerical experiments show that Greedy-Bayes improves upon Thompson sampling, the state-of-the-art algorithm for bandit problems. Our analysis further implies that low regret can only be achieved if the assessment of content relevance for one user leverages feedback from users with widely distinct tastes. This impacts the design of efficient news dissemination platforms: existing systems typically do not leverage such negative feedback and could hence be improved upon with adequate extensions.},
 acmid = {2745868},
 address = {New York, NY, USA},
 author = {Massouli{\'e}, Laurent and Ohannessian, Mesrob I. and Prouti\`{e}re, Alexandre},
 booktitle = {Proceedings of the 2015 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems},
 doi = {10.1145/2745844.2745868},
 isbn = {978-1-4503-3486-0},
 keyword = {bayesian, multi-armed bandit, robust news delivery},
 link = {http://doi.acm.org/10.1145/2745844.2745868},
 location = {Portland, Oregon, USA},
 numpages = {12},
 pages = {285--296},
 publisher = {ACM},
 series = {SIGMETRICS '15},
 title = {Greedy-Bayes for Targeted News Dissemination},
 year = {2015}
}


@inproceedings{Xiao:2015:HVE:2745844.2745870,
 abstract = {Cardinality estimation over big network data consisting of numerous flows is a fundamental problem with many practical applications. Traditionally the research on this problem focused on using a small amount of memory to estimate each flow's cardinality from a large range (up to $10^9$). However, although the memory needed for each flow has been greatly compressed, when there is an extremely large number of flows, the overall memory demand can still be very high, exceeding the availability under some important scenarios, such as implementing online measurement modules in network processors using only on-chip cache memory. In this paper, instead of allocating a separated data structure (called estimator) for each flow, we take a different path by viewing all the flows together as a whole: Each flow is allocated with a virtual estimator, and these virtual estimators share a common memory space. We discover that sharing at the register (multi-bit) level is superior than sharing at the bit level. We propose a framework of virtual estimators that allows us to apply the idea of sharing to an array of cardinality estimation solutions, achieving far better memory efficiency than the best existing work. Our experiment shows that the new solution can work in a tight memory space of less than 1 bit per flow or even one tenth of a bit per flow --- a quest that has never been realized before.},
 acmid = {2745870},
 address = {New York, NY, USA},
 author = {Xiao, Qingjun and Chen, Shigang and Chen, Min and Ling, Yibei},
 booktitle = {Proceedings of the 2015 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems},
 doi = {10.1145/2745844.2745870},
 isbn = {978-1-4503-3486-0},
 keyword = {big network data, cardinality estimation, network stream monitoring},
 link = {http://doi.acm.org/10.1145/2745844.2745870},
 location = {Portland, Oregon, USA},
 numpages = {12},
 pages = {417--428},
 publisher = {ACM},
 series = {SIGMETRICS '15},
 title = {Hyper-Compact Virtual Estimators for Big Network Data Based on Register Sharing},
 year = {2015}
}


@article{Sur:2015:GIN:2796314.2745858,
 abstract = {60 GHz technology holds tremendous potential to upgrade wireless link throughput to Gbps level. To overcome inherent vulnerability to attenuation, 60 GHz radios communicate by forming highly-directional electronically-steerable beams. Standards like IEEE 802.11ad have tailored MAC/PHY protocols to such flexible-beam 60 GHz networks. However, lack of a reconfigurable platform has thwarted a realistic proof-of-concept evaluation. In this paper, we conduct an in-depth measurement of indoor 60 GHz networks using a first-of-its-kind software-radio platform. Our measurement focuses on the link-level behavior with three major perspectives: (i) coverage and bit-rate of a single link, and implications for 60 GHz MIMO; (ii) impact of beam-steering on network performance, particularly under human blockage and device mobility; (iii) spatial reuse between flexible beams. Our study dispels some common myths, and reveals key challenges in maintaining robust flexible-beam connection. We propose new principles that can tackle such challenges based on unique properties of 60 GHz channel and cognitive capability of 60 GHz links.},
 acmid = {2745858},
 address = {New York, NY, USA},
 author = {Sur, Sanjib and Venkateswaran, Vignesh and Zhang, Xinyu and Ramanathan, Parmesh},
 doi = {10.1145/2796314.2745858},
 issn = {0163-5999},
 issue_date = {June 2015},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 keyword = {60 ghz, ieee 802.11ad, millimeter-wave, software-radio},
 link = {http://doi.acm.org/10.1145/2796314.2745858},
 month = {jun},
 number = {1},
 numpages = {14},
 pages = {71--84},
 publisher = {ACM},
 title = {60 GHz Indoor Networking Through Flexible Beams: A Link-Level Profiling},
 volume = {43},
 year = {2015}
}


@article{Zhang:2015:OAI:2796314.2745855,
 abstract = {Auction design has recently been studied for dynamic resource bundling and VM provisioning in IaaS clouds, but is mostly restricted to the one-shot or offline setting. This work targets a more realistic case of online VM auction design, where: (i) cloud users bid for resources into the future to assemble customized VMs with desired occupation durations; (ii) the cloud provider dynamically packs multiple types of resources on heterogeneous physical machines (servers) into the requested VMs; (iii) the operational costs of servers are considered in resource allocation; (iv) both social welfare and the cloud provider's net profit are to be maximized over the system running span. We design truthful, polynomial time auctions to achieve social welfare maximization and/or the provider's profit maximization with good competitive ratios. Our mechanisms consist of two main modules: (1) an online primal-dual optimization framework for VM allocation to maximize the social welfare with server costs, and for revealing the payments through the dual variables to guarantee truthfulness; and (2) a randomized reduction algorithm to convert the social welfare maximizing auctions to ones that provide a maximal expected profit for the provider, with competitive ratios comparable to those for social welfare. We adopt a new application of Fenchel duality in our primal-dual framework, which provides richer structures for convex programs than the commonly used Lagrangian duality, and our optimization framework is general and expressive enough to handle various convex server cost functions. The efficacy of the online auctions is validated through careful theoretical analysis and trace-driven simulation studies.},
 acmid = {2745855},
 address = {New York, NY, USA},
 author = {Zhang, Xiaoxi and Huang, Zhiyi and Wu, Chuan and Li, Zongpeng and Lau, Francis C.M.},
 doi = {10.1145/2796314.2745855},
 issn = {0163-5999},
 issue_date = {June 2015},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 keyword = {auction, cloud computing, online algorithms, pricing, resource allocation, truthful mechanisms},
 link = {http://doi.acm.org/10.1145/2796314.2745855},
 month = {jun},
 number = {1},
 numpages = {13},
 pages = {3--15},
 publisher = {ACM},
 title = {Online Auctions in IaaS Clouds: Welfare and Profit Maximization with Server Costs},
 volume = {43},
 year = {2015}
}


@article{He:2015:LSD:2796314.2745880,
 abstract = {We conduct a comprehensive measurement study of switch control plane latencies using four types of production SDN switches. Our measurements show that control actions, such as rule installation, have surprisingly high latency, due to both software implementation inefficiencies and fundamental traits of switch hardware. We also propose three measurement-driven latency mitigation techniques---optimizing route selection, spreading rules across switches, and reordering rule installations---to effectively tame the flow setup latencies in SDN.},
 acmid = {2745880},
 address = {New York, NY, USA},
 author = {He, Keqiang and Khalid, Junaid and Das, Sourav and Gember-Jacobson, Aaron and Prakash, Chaithan and Akella, Aditya and Li, Li Erran and Thottan, Marina},
 doi = {10.1145/2796314.2745880},
 issn = {0163-5999},
 issue_date = {June 2015},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 keyword = {latency, measurement, mitigation, software-defined networks},
 link = {http://doi.acm.org/10.1145/2796314.2745880},
 month = {jun},
 number = {1},
 numpages = {2},
 pages = {435--436},
 publisher = {ACM},
 title = {Latency in Software Defined Networks: Measurements and Mitigation Techniques},
 volume = {43},
 year = {2015}
}


@inproceedings{Krishnasamy:2015:DSR:2745844.2745885,
 abstract = {Personalized recommender systems provide great opportunities for targeted advertisements, by displaying ads alongside genuine recommendations. We consider a biased recommendation system where such ads are displayed without any tags (disguised as genuine recommendations), rendering them indistinguishable to users. We consider the problem of detecting such a bias and propose an algorithm that uses statistical analysis based on binary feedback data from a subset of users. We prove that the proposed algorithm detects bias with high probability for a broad class of recommendation systems with sufficient number of feedback samples.},
 acmid = {2745885},
 address = {New York, NY, USA},
 author = {Krishnasamy, Subhashini and Sen, Rajat and Oh, Sewoong and Shakkottai, Sanjay},
 booktitle = {Proceedings of the 2015 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems},
 doi = {10.1145/2745844.2745885},
 isbn = {978-1-4503-3486-0},
 keyword = {anomaly detection, bias detection, recommender systems},
 link = {http://doi.acm.org/10.1145/2745844.2745885},
 location = {Portland, Oregon, USA},
 numpages = {2},
 pages = {445--446},
 publisher = {ACM},
 series = {SIGMETRICS '15},
 title = {Detecting Sponsored Recommendations},
 year = {2015}
}


@article{Ghaderi:2015:SSS:2796314.2745882,
 abstract = {Motivated by emerging big streaming data processing paradigms (e.g., Twitter Storm, Streaming MapReduce), we investigate the problem of scheduling graphs over a large cluster of servers. Each graph is a job, where nodes represent compute tasks and edges indicate data-flows between these compute tasks. Jobs (graphs) arrive randomly over time, and upon completion, leave the system. When a job arrives, the scheduler needs to partition the graph and distribute it over the servers to satisfy load balancing and cost considerations. Specifically, neighboring compute tasks in the graph that are mapped to different servers incur load on the network; thus a mapping of the jobs among the servers incurs a cost that is proportional to the number of "broken edges''. We propose a low complexity randomized scheduling algorithm that, without service preemptions, stabilizes the system with graph arrivals/departures; more importantly, it allows a smooth trade-off between minimizing average partitioning cost and average queue lengths. Interestingly, to avoid service preemptions, our approach does not rely on a Gibbs sampler; instead, we show that the corresponding limiting invariant measure has an interpretation stemming from a loss system.},
 acmid = {2745882},
 address = {New York, NY, USA},
 author = {Ghaderi, Javad and Shakkottai, Sanjay and Srikant, Rayadurgam},
 doi = {10.1145/2796314.2745882},
 issn = {0163-5999},
 issue_date = {June 2015},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 keyword = {dynamic resource allocation, graph partitioning},
 link = {http://doi.acm.org/10.1145/2796314.2745882},
 month = {jun},
 number = {1},
 numpages = {2},
 pages = {439--440},
 publisher = {ACM},
 title = {Scheduling Storms and Streams in the Cloud},
 volume = {43},
 year = {2015}
}


@article{Kotronis:2015:IPI:2796314.2745877,
 abstract = {In this work, we propose utilizing the rich connectivity between IXPs and ISPs for inter-domain path stitching, supervised by centralized QoS brokers. In this context, we highlight a novel abstraction of the Internet topology, i.e., the inter-IXP multigraph composed of IXPs and paths crossing the domains of their shared member ISPs. This can potentially serve as a dense Internet-wide substrate for provisioning guaranteed end-to-end (e2e) services with high path diversity and global IPv4 address space reach. We thus map the IXP multigraph, evaluate its potential, and introduce a rich algorithmic framework for path stitching on such graph structures.},
 acmid = {2745877},
 address = {New York, NY, USA},
 author = {Kotronis, Vasileios and Kl\"{o}ti, Rowan and Rost, Matthias and Georgopoulos, Panagiotis and Ager, Bernhard and Schmid, Stefan and Dimitropoulos, Xenofontas},
 doi = {10.1145/2796314.2745877},
 issn = {0163-5999},
 issue_date = {June 2015},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 keyword = {embedding, euroix, internet exchange point, qos},
 link = {http://doi.acm.org/10.1145/2796314.2745877},
 month = {jun},
 number = {1},
 numpages = {2},
 pages = {429--430},
 publisher = {ACM},
 title = {Investigating the Potential of the Inter-IXP Multigraph for the Provisioning of Guaranteed End-to-End Services},
 volume = {43},
 year = {2015}
}


@article{Li:2015:ECM:2796314.2745890,
 abstract = {
                  An abstract is not available.
              },
 acmid = {2745890},
 address = {New York, NY, USA},
 author = {Li, Jian and Xia, Bainan and Geng, Xinbo and Ming, Hao and Shakkottai, Srinivas and Subramanian, Vijay and Xie, Le},
 doi = {10.1145/2796314.2745890},
 issn = {0163-5999},
 issue_date = {June 2015},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 keyword = {demand response, game theory, incentives, lottery scheme, mean field games, smart grids},
 link = {http://doi.acm.org/10.1145/2796314.2745890},
 month = {jun},
 number = {1},
 numpages = {2},
 pages = {455--456},
 publisher = {ACM},
 title = {Energy Coupon: A Mean Field Game Perspective on Demand Response in Smart Grids},
 volume = {43},
 year = {2015}
}


@inproceedings{Tarihi:2015:DAD:2745844.2745856,
 abstract = {Disk traces are typically used to analyze real-life workloads and for replay-based evaluations. This approach benefits from capturing important details such as varying behavior patterns, bursty activity, and diurnal patterns of system activity, which are often missing from the behavior of workload synthesis tools. However, accurate capture of such details requires recording traces containing long durations of system activity, which are difficult to use for replay-based evaluation. One way of solving the problem of long storage trace duration is the use of disk simulators. While publicly available disk simulators can greatly accelerate experiments, they have not kept up with technological innovations in the field. The variety, complexity, and opaque nature of storage hardware make it very difficult to implement accurate simulators. The alternative, replaying the whole traces on real hardware, suffers from either long run-time or required manual reduction of experimental time, potentially at the cost of reduced accuracy. On the other hand, burstiness, auto-correlation, and complex spatio-temporal properties of storage workloads make the known methods of sampling workload traces less effective. In this paper, we present a methodology called DiskAccel to efficiently select key intervals of a trace as representatives and to replay them to estimate the response time of the whole workload. Our methodology extracts a variety of spatial and temporal features from each interval and uses efficient data mining techniques to select the representative intervals. To verify the proposed methodology, we have implemented a tool capable of running whole traces or selective intervals on real hardware, warming up hardware state in an accelerated manner, and emulating request causality while minimizing request inter-arrival time error. Based on our experiments, DiskAccel manages to speed up disk replay by more than two orders of magnitude, while keeping average estimation error at 7.6%.},
 acmid = {2745856},
 address = {New York, NY, USA},
 author = {Tarihi, Mojtaba and Asadi, Hossein and Sarbazi-Azad, Hamid},
 booktitle = {Proceedings of the 2015 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems},
 doi = {10.1145/2745844.2745856},
 isbn = {978-1-4503-3486-0},
 keyword = {data storage devices, i/o traces, performance evaluation, representative sampling, trace replay},
 link = {http://doi.acm.org/10.1145/2745844.2745856},
 location = {Portland, Oregon, USA},
 numpages = {12},
 pages = {297--308},
 publisher = {ACM},
 series = {SIGMETRICS '15},
 title = {DiskAccel: Accelerating Disk-Based Experiments by Representative Sampling},
 year = {2015}
}


@inproceedings{He:2015:FIE:2745844.2745862,
 abstract = {Network tomography aims to infer the individual performance of networked elements (e.g., links) using aggregate measurements on end-to-end paths. Previous work on network tomography focuses primarily on developing estimators using the given measurements, while the design of measurements is often neglected. We fill this gap by proposing a framework to design probing experiments with focus on probe allocation, and applying it to two concrete problems: packet loss tomography and packet delay variation (PDV) tomography. Based on the Fisher Information Matrix (FIM), we design the distribution of probes across paths to maximize the best accuracy of unbiased estimators, asymptotically achievable by the maximum likelihood estimator. We consider two widely-adopted objective functions: determinant of the inverse FIM (D-optimality) and trace of the inverse FIM (A-optimality). We also extend the A-optimal criterion to incorporate heterogeneity in link weights. Under certain conditions on the FIM, satisfied by both loss and PDV tomography, we derive explicit expressions for both objective functions. When the number of probing paths equals the number of links, these lead to closed-form solutions for the optimal design; when there are more paths, we develop a heuristic to select a subset of paths and optimally allocate probes within the subset. Observing the dependency of the optimal design on unknown parameters, we further propose an algorithm that iteratively updates the design based on parameter estimates, which converges to the design based on true parameters as the number of probes increases. Using packet-level simulations on real datasets, we verify that the proposed design effectively reduces estimation error compared with the common approach of uniformly distributing probes.},
 acmid = {2745862},
 address = {New York, NY, USA},
 author = {He, Ting and Liu, Chang and Swami, Ananthram and Towsley, Don and Salonidis, Theodoros and Bejan, Andrei Iu. and Yu, Paul},
 booktitle = {Proceedings of the 2015 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems},
 doi = {10.1145/2745844.2745862},
 isbn = {978-1-4503-3486-0},
 keyword = {experiment design, fisher information matrix, network tomography},
 link = {http://doi.acm.org/10.1145/2745844.2745862},
 location = {Portland, Oregon, USA},
 numpages = {14},
 pages = {389--402},
 publisher = {ACM},
 series = {SIGMETRICS '15},
 title = {Fisher Information-based Experiment Design for Network Tomography},
 year = {2015}
}


@inproceedings{Liu:2015:OLA:2745844.2745874,
 abstract = {We consider a crowd-sourcing problem where in the process of labeling massive datasets, multiple labelers with unknown annotation quality must be selected to perform the labeling task for each incoming data sample or task, with the results aggregated using for example simple or weighted majority voting rule. In this paper we approach this labeler selection problem in an online learning framework, whereby the quality of the labeling outcome by a specific set of labelers is estimated so that the learning algorithm over time learns to use the most effective combinations of labelers. This type of online learning in some sense falls under the family of multi-armed bandit (MAB) problems, but with a distinct feature not commonly seen: since the data is unlabeled to begin with and the labelers' quality is unknown, their labeling outcome (or reward in the MAB context) cannot be directly verified; it can only be estimated against the crowd and known probabilistically. We design an efficient online algorithm LS_OL using a simple majority voting rule that can differentiate high- and low-quality labelers over time, and is shown to have a regret (w.r.t. always using the optimal set of labelers) of O(log2 T) uniformly in time under mild assumptions on the collective quality of the crowd, thus regret free in the average sense. We discuss performance improvement by using a more sophisticated majority voting rule, and show how to detect and filter out "bad" (dishonest, malicious or very incompetent) labelers to further enhance the quality of crowd-sourcing. Extension to the case when a labeler's quality is task-type dependent is also discussed using techniques from the literature on continuous arms. We present numerical results using both simulation and a real dataset on a set of images labeled by Amazon Mechanic Turks (AMT).},
 acmid = {2745874},
 address = {New York, NY, USA},
 author = {Liu, Yang and Liu, Mingyan},
 booktitle = {Proceedings of the 2015 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems},
 doi = {10.1145/2745844.2745874},
 isbn = {978-1-4503-3486-0},
 keyword = {crowd-sourcing, online learning, quality control},
 link = {http://doi.acm.org/10.1145/2745844.2745874},
 location = {Portland, Oregon, USA},
 numpages = {14},
 pages = {217--230},
 publisher = {ACM},
 series = {SIGMETRICS '15},
 title = {An Online Learning Approach to Improving the Quality of Crowd-Sourcing},
 year = {2015}
}


@article{Varloot:2015:SUG:2796314.2745893,
 abstract = {The maximum independent set (MIS) problem is a well-studied combinatorial optimization problem that naturally arises in many applications, such as wireless communication, information theory and statistical mechanics. MIS problem is NP-hard, thus many results in the literature focus on fast generation of maximal independent sets of high cardinality. One possibility is to combine Gibbs sampling with coupling from the past arguments to detect convergence to the stationary regime. This results in a sampling procedure with time complexity that depends on the mixing time of the Glauber dynamics Markov chain. We propose an adaptive method for random event generation in the Glauber dynamics that considers only the events that are effective in the coupling from the past scheme, accelerating the convergence time of the Gibbs sampling algorithm. The full paper is available on arXiv.},
 acmid = {2745893},
 address = {New York, NY, USA},
 author = {Varloot, R{\'e}mi and Bu\v{s}\'{\i}\"{u}\'{c}, Ana and Bouillard, Anne},
 doi = {10.1145/2796314.2745893},
 issn = {0163-5999},
 issue_date = {June 2015},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 keyword = {exact sampling, glauber dynamics, independent sets},
 link = {http://doi.acm.org/10.1145/2796314.2745893},
 month = {jun},
 number = {1},
 numpages = {2},
 pages = {461--462},
 publisher = {ACM},
 title = {Speeding Up Glauber Dynamics for Random Generation of Independent Sets},
 volume = {43},
 year = {2015}
}


@inproceedings{Fanti:2015:SVS:2745844.2745866,
 abstract = {Anonymous messaging platforms, such as Secret, Yik Yak and Whisper, have emerged as important social media for sharing one's thoughts without the fear of being judged by friends, family, or the public. Further, such anonymous platforms are crucial in nations with authoritarian governments; the right to free expression and sometimes the personal safety of the author of the message depend on anonymity. Whether for fear of judgment or personal endangerment, it is crucial to keep anonymous the identity of the user who initially posted a sensitive message. In this paper, we consider an adversary who observes a snapshot of the spread of a message at a certain time. Recent advances in rumor source detection shows that the existing messaging protocols are vulnerable against such an adversary. We introduce a novel messaging protocol, which we call adaptive diffusion, and show that it spreads the messages fast and achieves a perfect obfuscation of the source when the underlying contact network is an infinite regular tree: all users with the message are nearly equally likely to have been the origin of the message. Experiments on a sampled Facebook network show that it effectively hides the location of the source even when the graph is finite, irregular and has cycles.},
 acmid = {2745866},
 address = {New York, NY, USA},
 author = {Fanti, Giulia and Kairouz, Peter and Oh, Sewoong and Viswanath, Pramod},
 booktitle = {Proceedings of the 2015 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems},
 doi = {10.1145/2745844.2745866},
 isbn = {978-1-4503-3486-0},
 keyword = {anonymous social media, privacy, rumor spreading},
 link = {http://doi.acm.org/10.1145/2745844.2745866},
 location = {Portland, Oregon, USA},
 numpages = {14},
 pages = {271--284},
 publisher = {ACM},
 series = {SIGMETRICS '15},
 title = {Spy vs. Spy: Rumor Source Obfuscation},
 year = {2015}
}


@article{Gupta:2015:LOS:2796314.2745897,
 abstract = {Motivated by the problem of packing Virtual Machines on physical servers in the cloud, we study the problem of online stochastic bin packing under two settings -- packing with permanent items, and packing under item departures. In the setting with permanent items, we present the first truly distribution-oblivious bin packing heuristic that achieves O(√n) regret compared to OPT for all distributions. Our algorithm is essentially gradient descent on suitably defined Lagrangian relaxation of the bin packing Linear Program. We also prove guarantees of our heuristic against non i.i.d. input using a randomly delayed Lyapunov function to smoothen the input. For the setting where items eventually depart, we are interested in minimizing the steady-state number of bins. Our algorithm extends as is to the case of item departures. Further, leveraging the Lagrangian approach, we generalize our algorithm to a setting where the processing time of an item is inflated by a certain known factor depending on the configuration it is packed in.},
 acmid = {2745897},
 address = {New York, NY, USA},
 author = {Gupta, Varun and Radovanovic, Ana},
 doi = {10.1145/2796314.2745897},
 issn = {0163-5999},
 issue_date = {June 2015},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 keyword = {best fit, primal-dual algorithm, stochastic bin packing},
 link = {http://doi.acm.org/10.1145/2796314.2745897},
 month = {jun},
 number = {1},
 numpages = {2},
 pages = {467--468},
 publisher = {ACM},
 title = {Lagrangian-based Online Stochastic Bin Packing},
 volume = {43},
 year = {2015}
}


@proceedings{Lin:2015:2745844,
 abstract = {Welcome to SIGMETRICS 2015. SIGMETRICS is the flagship conference of the ACM special interest group for the computer systems performance evaluation community. This year marks the forty-second anniversary since SIGMETRICS (under its prior name, SIGME) held the First National SIGME Symposium on Measurement and Evaluation in 1973. The past four decades have seen enormous changes in the field of computer science, but the importance of measurement, modeling, and performance evaluation remains as critical as ever. This year's conference includes papers on topics that have been a mainstay since the founding of our SIG, including queuing, scheduling, resource allocation, and performance measurement. Application areas that have emerged in recent years, such as data center and cloud, big data, solidstate storage, machine learning, crowdsourcing, energy optimization, continue to be represented in our program. We received 239 submissions to this year's conference, of which 32 appear in the program as full papers, which is a highly competitive acceptance ratio below 14%. An additional 24 submissions appear in the abbreviated form of poster presentations and extended abstracts. As in some prior years, we performed reviews in three rounds. In the first round, each paper was assigned to three reviewers. In the second round, papers were up for discussion among the reviewers, especially those with highly divergent review opinions. In some cases, additional reviews were necessary to ensure that every paper would receive at least three reviews. Based on this and immediately prior to the TPC meeting, papers were bucketed into "accept", "reject", and "discuss at TPC meeting" buckets. Finally, in the TPC meeting at MIT (Cambridge, MA), we entertained comments and objections on papers in the "accept" and "reject" buckets and discuss all papers in the "discuss at TPC meeting" bucket. It was no mean feat to winnow a set of nearly two hundred and fifty submissions down to a set of appropriate size for a three-day conference. We offer tremendous thanks to the 57 members of the program committee who collectively performed this daunting task. We are grateful for the support of the SIGMETRICS board during the lengthy process of selecting this year's conference program.},
 address = {New York, NY, USA},
 isbn = {978-1-4503-3486-0},
 location = {Portland, Oregon, USA},
 note = {488150},
 publisher = {ACM},
 title = {SIGMETRICS '15: Proceedings of the 2015 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems},
 year = {2015}
}


@inproceedings{Georgiadis:2015:ESN:2745844.2745860,
 abstract = {Exchange of services and resources in, or over, networks is attracting nowadays renewed interest. However, despite the broad applicability and the extensive study of such models, e.g., in the context of P2P networks, many fundamental questions regarding their properties and efficiency remain unanswered. We consider such a service exchange model and analyze the users' interactions under three different approaches. First, we study a centrally designed service allocation policy that yields the fair total service each user should receive based on the service it offers to the others. Accordingly, we consider a competitive market where each user determines selfishly its allocation policy so as to maximize the service it receives in return, and a coalitional game model where users are allowed to coordinate their policies. We prove that there is a unique equilibrium exchange allocation for both game theoretic formulations, which also coincides with the central fair service allocation. Furthermore, we characterize its properties in terms of the coalitions that emerge and the equilibrium allocations, and analyze its dependency on the underlying network graph. That servicing policy is the natural reference point to the various mechanisms that are currently proposed to incentivize user participation and improve the efficiency of such networked service (or, resource) exchange markets.},
 acmid = {2745860},
 address = {New York, NY, USA},
 author = {Georgiadis, Leonidas and Iosifidis, George and Tassiulas, Leandros},
 booktitle = {Proceedings of the 2015 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems},
 doi = {10.1145/2745844.2745860},
 isbn = {978-1-4503-3486-0},
 keyword = {competitive equilibriums, network economics},
 link = {http://doi.acm.org/10.1145/2745844.2745860},
 location = {Portland, Oregon, USA},
 numpages = {14},
 pages = {43--56},
 publisher = {ACM},
 series = {SIGMETRICS '15},
 title = {Exchange of Services in Networks: Competition, Cooperation, and Fairness},
 year = {2015}
}


@inproceedings{Chen:2015:OCO:2745844.2745854,
 abstract = {Making use of predictions is a crucial, but under-explored, area of online algorithms. This paper studies a class of online optimization problems where we have external noisy predictions available. We propose a stochastic prediction error model that generalizes prior models in the learning and stochastic control communities, incorporates correlation among prediction errors, and captures the fact that predictions improve as time passes. We prove that achieving sublinear regret and constant competitive ratio for online algorithms requires the use of an unbounded prediction window in adversarial settings, but that under more realistic stochastic prediction error models it is possible to use Averaging Fixed Horizon Control (AFHC) to simultaneously achieve sublinear regret and constant competitive ratio in expectation using only a constant-sized prediction window. Furthermore, we show that the performance of AFHC is tightly concentrated around its mean.},
 acmid = {2745854},
 address = {New York, NY, USA},
 author = {Chen, Niangjun and Agarwal, Anish and Wierman, Adam and Barman, Siddharth and Andrew, Lachlan L.H.},
 booktitle = {Proceedings of the 2015 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems},
 doi = {10.1145/2745844.2745854},
 isbn = {978-1-4503-3486-0},
 keyword = {competitive ratio, online convex optimization, prediction, regret},
 link = {http://doi.acm.org/10.1145/2745844.2745854},
 location = {Portland, Oregon, USA},
 numpages = {14},
 pages = {191--204},
 publisher = {ACM},
 series = {SIGMETRICS '15},
 title = {Online Convex Optimization Using Predictions},
 year = {2015}
}


@inproceedings{Birke:2015:VMP:2745844.2745865,
 abstract = {The wide deployment of virtualization in datacenters catalyzes the emergence of virtual traffic that delivers the network demands between the physical network and the virtual machines hosting clients' services. Virtual traffic presents new opportunities for reducing physical network demands, as well as challenges of increasing management complexity. Given the plethora of prior art on virtualization technologies in datacenters, surprisingly little is still known about such virtual traffic, and its dependence on the physical network and virtual machines. This paper provides a multi-faceted analysis of the patterns and impacts of multiplexing the virtual traffic onto the physical network, particularly from the perspective of the network edge. We use a large collection of field data from production datacenters hosting a large number of diversified services from multiple enterprise tenants. Our first focus is on uncovering the temporal and spatial characteristics of the virtual and physical traffic, i.e., network demand growth and communication patterns, with special attention paid to the traffic of migrating virtual machines. The second focus is on characterizing the effect of network multiplexing in terms of communication locality, traffic load heterogeneity, and the dependency on CPU processing power at the edges of the network. Last but not least, we conduct a mirroring analysis on service QoS, defined by the service unavailability induced by network related issues, e.g., loads. We qualitatively and quantitatively discuss the implications and opportunities that virtual traffic presents for network capacity planning of virtualized networks and datacenters.},
 acmid = {2745865},
 address = {New York, NY, USA},
 author = {Birke, Robert and Bj\"{o}rkqvist, Mathias and Minkenberg, Cyriel and Schmatz, Martin and Chen, Lydia Y.},
 booktitle = {Proceedings of the 2015 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems},
 doi = {10.1145/2745844.2745865},
 isbn = {978-1-4503-3486-0},
 keyword = {bandwidth demand, datacenter, growth, migration, virtual traffic},
 link = {http://doi.acm.org/10.1145/2745844.2745865},
 location = {Portland, Oregon, USA},
 numpages = {13},
 pages = {403--415},
 publisher = {ACM},
 series = {SIGMETRICS '15},
 title = {When Virtual Meets Physical at the Edge: A Field Study on Datacenters' Virtual Traffic},
 year = {2015}
}


@inproceedings{Soltan:2015:JCP:2745844.2745846,
 abstract = {Recent events demonstrated the vulnerability of power grids to cyber attacks and to physical attacks. Therefore, we focus on joint cyber and physical attacks and develop methods to retrieve the grid state information following such an attack. We consider a model in which an adversary attacks a zone by physically disconnecting some of its power lines and blocking the information flow from the zone to the grid's control center. We use tools from linear algebra and graph theory and leverage the properties of the power flow DC approximation to develop methods for information recovery. Using information observed outside the attacked zone, these methods recover information about the disconnected lines and the phase angles at the buses. We identify sufficient conditions on the zone structure and constraints on the attack characteristics such that these methods can recover the information. We also show that it is NP-hard to find an approximate solution to the problem of partitioning the power grid into the minimum number of attack-resilient zones. However, since power grids can often be represented by planar graphs, we develop a constant approximation partitioning algorithm for these graphs. Finally, we numerically study the relationships between the grid's resilience and its structural properties, and demonstrate the partitioning algorithm on real power grids. The results can provide insights into the design of a secure control network for the smart grid.},
 acmid = {2745846},
 address = {New York, NY, USA},
 author = {Soltan, Saleh and Yannakakis, Mihalis and Zussman, Gil},
 booktitle = {Proceedings of the 2015 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems},
 doi = {10.1145/2745844.2745846},
 isbn = {978-1-4503-3486-0},
 keyword = {algorithms, cyber attacks, graph theory, information recovery, physical attacks, power grids},
 link = {http://doi.acm.org/10.1145/2745844.2745846},
 location = {Portland, Oregon, USA},
 numpages = {14},
 pages = {361--374},
 publisher = {ACM},
 series = {SIGMETRICS '15},
 title = {Joint Cyber and Physical Attacks on Power Grids: Graph Theoretical Approaches for Information Recovery},
 year = {2015}
}


@inproceedings{Chen:2015:SED:2745844.2745875,
 abstract = {The limited battery life of modern smartphones remains a leading factor adversely affecting the mobile experience of millions of smartphone users. In order to extend battery life, it is critical to understand where and how is energy drain happening on users' phones under normal usage, for example, in a one-day cycle. In this paper, we conduct the first extensive measurement and modeling of energy drain of 1520 smartphone in the wild. We make two primary contributions. First, we develop a hybrid power model that integrates utilization-based models and FSM-based models for different phone components with a novel technique that estimates the triggers for the FSM-based network power model based on network utilization. Second, through analyzing traces collected on 1520 Galaxy S3 and S4 devices in the wild, we present detailed analysis of where the CPU time and energy are spent across the 1520 devices, inside the 800 apps, as well as along several evolution dimensions, including hardware, Android, cellular, and app updates. Our findings of smartphone energy drain in the wild have significant implications to the various key players of the Android phone eco-system, including phone vendors  Samsung, Android developers, app developers, and ultimately millions of smartphone users, towards the common goal of extending smartphone battery life and improving the user mobile experience.},
 acmid = {2745875},
 address = {New York, NY, USA},
 author = {Chen, Xiaomeng and Ding, Ning and Jindal, Abhilash and Hu, Y. Charlie and Gupta, Maruti and Vannithamby, Rath},
 booktitle = {Proceedings of the 2015 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems},
 doi = {10.1145/2745844.2745875},
 isbn = {978-1-4503-3486-0},
 keyword = {battery drain, power model, screen-off energy, smartphones},
 link = {http://doi.acm.org/10.1145/2745844.2745875},
 location = {Portland, Oregon, USA},
 numpages = {14},
 pages = {151--164},
 publisher = {ACM},
 series = {SIGMETRICS '15},
 title = {Smartphone Energy Drain in the Wild: Analysis and Implications},
 year = {2015}
}


@inproceedings{Lee:2015:RMI:2745844.2745871,
 abstract = {The appearance of web-based crowdsourcing systems gives a promising solution to exploiting the wisdom of crowds efficiently in a short time with a relatively low budget. Despite their efficiency, crowdsourcing systems have an inherent problem in that responses from workers can be unreliable since workers are low-paid and have low responsibility. Although simple majority voting can be a solution, various research studies have sought to aggregate noisy responses to obtain greater reliability in results through effective techniques such as Expectation-Maximization (EM) based algorithms. While EM-based algorithms get the limelight in crowdsourcing systems due to their useful inference techniques, Karger et al. made a significant breakthrough by proposing a novel iterative algorithm based on the idea of low-rank matrix approximations and the message passing technique. They showed that the performance of their iterative algorithm is order-optimal, which outperforms majority voting and EM-based algorithms. However, their algorithm is not always applicable in practice since it can only be applied to binary-choice questions. Recently, they devised an inference algorithm for multi-class labeling, which splits each task into a bunch of binary-choice questions and exploits their existing algorithm. However, it has difficulty in combining into real crowdsourcing systems since it overexploits redundancy in that each split question should be queried in multiple times to obtain reliable results. In this paper, we design an iterative algorithm to infer true answers for multiple-choice questions, which can be directly applied to real crowdsourcing systems. Our algorithm can also be applicable to short-answer questions as well. We analyze the performance of our algorithm, and prove that the error bound decays exponentially. Through extensive experiments, we verify that our algorithm outperforms majority voting and EM-based algorithm in accuracy.},
 acmid = {2745871},
 address = {New York, NY, USA},
 author = {Lee, Donghyeon and Kim, Joonyoung and Lee, Hyunmin and Jung, Kyomin},
 booktitle = {Proceedings of the 2015 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems},
 doi = {10.1145/2745844.2745871},
 isbn = {978-1-4503-3486-0},
 keyword = {crowdsourcing, iterative learning, multiple-choice, resource allocation},
 link = {http://doi.acm.org/10.1145/2745844.2745871},
 location = {Portland, Oregon, USA},
 numpages = {12},
 pages = {205--216},
 publisher = {ACM},
 series = {SIGMETRICS '15},
 title = {Reliable Multiple-choice Iterative Algorithm for Crowdsourcing Systems},
 year = {2015}
}


@inproceedings{Fu:2015:TBF:2745844.2745881,
 abstract = {Bloom filters are frequently used to perform set queries that test the existence of some items. However, Bloom filters face a dilemma: the transmission bandwidth and the accuracy cannot be optimized simultaneously. This dilemma is particularly severe for transmitting Bloom filters to remote nodes when the network bandwidth is limited. We propose a novel Bloom filter BloomTree that consists of a tree-structured organization of smaller Bloom filters, each one using a set of independent hash functions. BloomTree spreads items across levels that are compressed to reduce the transmission bandwidth need. We investigate in detail under which conditions BloomTree performs better than the compressed Bloom filter and the standard Bloom filter.},
 acmid = {2745881},
 address = {New York, NY, USA},
 author = {Fu, Yongquan and Biersack, Ernst},
 booktitle = {Proceedings of the 2015 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems},
 doi = {10.1145/2745844.2745881},
 isbn = {978-1-4503-3486-0},
 keyword = {bloom filter, tree},
 link = {http://doi.acm.org/10.1145/2745844.2745881},
 location = {Portland, Oregon, USA},
 numpages = {2},
 pages = {437--438},
 publisher = {ACM},
 series = {SIGMETRICS '15},
 title = {Tree-structured Bloom Filters for Joint Optimization of False Positive Probability and Transmission Bandwidth},
 year = {2015}
}


@article{Umar:2015:DLC:2796314.2745891,
 abstract = {Like other fundamental abstractions for high-performance computing, search trees need to support both high concurrency and data locality. However, existing locality-aware search trees based on the van Emde Boas layout (vEB-based trees), poorly support concurrent (update) operations. We present DeltaTree, a practical locality-aware concurrent search tree that integrates both locality-optimization techniques from vEB-based trees, and concurrency optimization techniques from highly-concurrent search trees. As a result, DeltaTree minimizes data transfer from memory to CPU and supports high concurrency. Our experimental evaluation shows that DeltaTree is up to 50% faster than highly concurrent B-trees on a commodity Intel high performance computing (HPC) platform and up to 65% faster on a commodity ARM embedded platform.},
 acmid = {2745891},
 address = {New York, NY, USA},
 author = {Umar, Ibrahim and Anshus, Otto Johan and Ha, Phuong Hoai},
 doi = {10.1145/2796314.2745891},
 issn = {0163-5999},
 issue_date = {June 2015},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 keyword = {concurrent algorithms, data locality, memory systems, multi-core processors, performance evaluation},
 link = {http://doi.acm.org/10.1145/2796314.2745891},
 month = {jun},
 number = {1},
 numpages = {2},
 pages = {457--458},
 publisher = {ACM},
 title = {DeltaTree: A Locality-aware Concurrent Search Tree},
 volume = {43},
 year = {2015}
}


@inproceedings{Singh:2015:MSA:2745844.2745878,
 abstract = {The model is a "generalized switch", serving multiple traffic flows in discrete time. The switch uses MaxWeight algorithm to make a service decision (scheduling choice) at each time step, which determines the probability distribution of the amount of service that will be provided. We are primarily motivated by the following question: in the heavy traffic regime, when the switch load approaches critical level, will the service processes provided to each flow remain "smooth" (i.e., without large gaps in service)? Addressing this question reduces to the analysis of the asymptotic behavior of the unscaled queue-differential process in heavy traffic. We prove that the stationary regime of this process converges to that of a positive recurrent Markov chain, whose structure we explicitly describe. This in turn implies asymptotic "smoothness" of the service processes.},
 acmid = {2745878},
 address = {New York, NY, USA},
 author = {Singh, Rahul and Stolyar, Alexander},
 booktitle = {Proceedings of the 2015 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems},
 doi = {10.1145/2745844.2745878},
 isbn = {978-1-4503-3486-0},
 keyword = {dynamic scheduling, heavy traffic asymptotic regime, markov chain, maxweight algorithm, queue length differentials, smooth service process},
 link = {http://doi.acm.org/10.1145/2745844.2745878},
 location = {Portland, Oregon, USA},
 numpages = {2},
 pages = {431--432},
 publisher = {ACM},
 series = {SIGMETRICS '15},
 title = {MaxWeight Scheduling: Asymptotic Behavior of Unscaled Queue-Differentials in Heavy Traffic},
 year = {2015}
}


@article{Fuerst:2015:KTE:2796314.2745879,
 abstract = {It is well-known that without strict network bandwidth guarantees, application performance in multi-tenant cloud environments is unpredictable. While recently proposed systems support explicit bandwidth reservation mechanisms, they require the resource schedules to be announced ahead of time. We argue that this is not practical in today's cloud environments, where application demands are inherently unpredictable, e.g., due to stragglers. We in this paper present KRAKEN, a system that allows tenants to dynamically request and update minimum resource guarantees for both network bandwidth and compute resources at runtime. Unlike previous work, Kraken does not require prior knowledge about the resource needs of the tenants' applications but allows tenants to modify their reservation at runtime. Kraken achieves this through an online resource reservation scheme, and by optimally embedding and reconfiguring virtual networks.},
 acmid = {2745879},
 address = {New York, NY, USA},
 author = {Fuerst, Carlo and Schmid, Stefan and Suresh, Lalith and Costa, Paolo},
 doi = {10.1145/2796314.2745879},
 issn = {0163-5999},
 issue_date = {June 2015},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 keyword = {allocation, bandwdith, datacenter, virtual network},
 link = {http://doi.acm.org/10.1145/2796314.2745879},
 month = {jun},
 number = {1},
 numpages = {2},
 pages = {433--434},
 publisher = {ACM},
 title = {Kraken: Towards Elastic Performance Guarantees in Multi-tenant Data Centers},
 volume = {43},
 year = {2015}
}


@proceedings{Sanghavi:2014:2591971,
 abstract = {It is our pleasure to welcome you to SIGMETRICS 2014. SIGMETRICS is the flagship conference of the ACM special interest group for the computer systems performance evaluation community. This year's conference continues the long-standing SIGMETRICS tradition to publish the highestquality research on the development and application of state-of-the-art, broadly applicable analytic, simulation, and measurement-based performance evaluation techniques. We are pleased to present a diverse set of papers in areas such as sensor, mobile and wireless networks, queuing and scheduling, msocial networks, memory technologies, large-scale measurement studies, system tracing and monitoring, data center resource provisioning and energy management. Our authors hail from 13 countries on 4 continents and represent both academia and industry. SIGMETRICS 2014 received 237 submissions, the second highest number since the founding of this SIG. Of these, we accepted 40 papers, the largest in the history of the conference, while still maintaining a highly competitive acceptance rate of 16.8%. During the review process, the Program Committee provided 4-6 reviews for each paper and made extensive use of HotCRP's Comment feature for online discussions. The Program Committee then met in person in a 1.5-day meeting on February 7-8, 2014, in Toronto, Canada, and selected 40 papers to be included as full papers in the technical program. In addition, 31 papers were invited as 2-page posters, and the authors of 30 of these papers accepted our invitation. As an experiment, we invited for the first time also all authors of full papers to present a poster version of their paper during one of the breaks at the conference to foster interaction between authors and attendees. We used Eddie Kohler's excellent HotCRP software to manage all stages of the review process, from submission to author notification.},
 address = {New York, NY, USA},
 isbn = {978-1-4503-2789-3},
 location = {Austin, Texas, USA},
 publisher = {ACM},
 title = {SIGMETRICS '14: The 2014 ACM International Conference on Measurement and Modeling of Computer Systems},
 year = {2014}
}


@inproceedings{Zhou:2015:PBE:2745844.2745884,
 abstract = {Datacenter demand response is envisioned as a promising approach for mitigating operational instability faced by smart grids. It enables significant potentials in peak load shedding and facilitates the incorporation of distributed generation and intermittent energy sources. This work considers two key aspects towards realtime electricity pricing for eliciting demand response: (i) Two-way electricity flow between smart grids and large datacenters with hybrid green generation capabilities. (ii) The geo-distributed nature of large cloud systems, and hence the potential competition among smart grids that serve different datacenters of the cloud. We propose a pricing scheme tailored for geo-distributed green datacenters, from a multi-leader single-follower game point of view. At the cloud side, in quest for performance, scalability and robustness, the energy cost is minimized in a distributed manner, based on the technique of alternating direction of multipliers (ADMM). At the smart grid side, a practical equilibrium of the pricing game is desired. To this end, we employ mathematical programming with equilibrium constraints (MPEC), equilibrium problem with equilibrium constraints (EPEC) and exact linearization, to transform the multi-leader single-follower pricing game into a mixed integer linear program (MILP) that can be readily solved. The effectiveness of the proposed solutions is evaluated based on trace-driven simulations.},
 acmid = {2745884},
 address = {New York, NY, USA},
 author = {Zhou, Zhi and Liu, Fangming and Li, Zongpeng},
 booktitle = {Proceedings of the 2015 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems},
 doi = {10.1145/2745844.2745884},
 isbn = {978-1-4503-3486-0},
 keyword = {demand response, geo-distributed cloud, hybrid green datacenters, multi-leader single-follower game, smart grid},
 link = {http://doi.acm.org/10.1145/2745844.2745884},
 location = {Portland, Oregon, USA},
 numpages = {2},
 pages = {443--444},
 publisher = {ACM},
 series = {SIGMETRICS '15},
 title = {Pricing Bilateral Electricity Trade Between Smart Grids and Hybrid Green Datacenters},
 year = {2015}
}


@article{Zhang:2015:SDP:2796314.2745863,
 abstract = {Data traffic demand over the Internet is increasing rapidly, and it is changing the pricing model between Internet service providers (ISPs), content providers (CPs) and end users. One recent pricing proposal is sponsored data plan, i.e., when accessing contents from a particular CP, end users do not need to pay for that volume of traffic consumed, but the CP will sponsor for this data consumption. In this paper, our goal is to understand the rationale behind this new pricing model, as well as its impacts to the wireless data market, in particular, who will benefit and who will be hurt from this scheme. We build a two-class service model to analyze the consumers' traffic demand under the sponsored data plan with consideration of QoS. We use a two-stage Stackelberg game to characterize the interaction between CPs and the ISP and reveal a number of important findings. Our conclusions include: 1) When the ISP's capacity is sufficient, the sponsored data plan benefits consumers and CPs in the short run, but the ISP does not have incentives to further improve its service in the long run. 2) When ISP's capacity is insufficient, the ISP and end users may achieve a win- win trade, while the ISP and CPs always compete for the revenue. 3) The sponsored data plan may enlarge the un- balance in revenue distribution between different CPs; CPs with higher unit income and poorer technology support are more likely to prefer the sponsored data plan.},
 acmid = {2745863},
 address = {New York, NY, USA},
 author = {Zhang, Liang and Wu, Weijie and Wang, Dan},
 doi = {10.1145/2796314.2745863},
 issn = {0163-5999},
 issue_date = {June 2015},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 keyword = {sponsored data, stackelberg game},
 link = {http://doi.acm.org/10.1145/2796314.2745863},
 month = {jun},
 number = {1},
 numpages = {12},
 pages = {85--96},
 publisher = {ACM},
 title = {Sponsored Data Plan: A Two-Class Service Model in Wireless Data Networks},
 volume = {43},
 year = {2015}
}


@inproceedings{Ducoffe:2015:WTC:2745844.2745896,
 abstract = {Big Data promises important societal progress but exacerbates the need for due process and accountability. Companies and institutions can now discriminate between users at an individual level using collected data or past behavior. Worse, today they can do so in near perfect opacity. The nascent field of web transparency aims to develop the tools and methods necessary to reveal how information is used, however today it lacks robust tools that let users and investigators identify targeting using multiple inputs. In this paper, we formalize for the first time the problem of detecting and identifying targeting on combinations of inputs and provide the first algorithm that is asymptotically exact. This algorithm is designed to serve as a theoretical foundational block to build future scalable and robust web transparency tools. It offers three key properties. First, our algorithm is service agnostic and applies to a variety of settings under a broad set of assumptions. Second, our algorithm's analysis delineates a theoretical detection limit that characterizes which forms of targeting can be distinguished from noise and which cannot. Third, our algorithm establishes fundamental tradeoffs that lead the way to new metrics for the science of web transparency.},
 acmid = {2745896},
 address = {New York, NY, USA},
 author = {Ducoffe, Guillaume and L{\'e}cuyer, Mathias and Chaintreau, Augustin and Geambasu, Roxana},
 booktitle = {Proceedings of the 2015 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems},
 doi = {10.1145/2745844.2745896},
 isbn = {978-1-4503-3486-0},
 keyword = {learning theory, targeting, web transparency},
 link = {http://doi.acm.org/10.1145/2745844.2745896},
 location = {Portland, Oregon, USA},
 numpages = {2},
 pages = {465--466},
 publisher = {ACM},
 series = {SIGMETRICS '15},
 title = {Web Transparency for Complex Targeting: Algorithms, Limits, and Tradeoffs},
 year = {2015}
}


@article{Clapp:2015:SMQ:2796314.2745900,
 abstract = {In recent years, DRAM technology improvements have scaled at a much slower pace than processors. While server processor core counts grow from 33% to 50% on a yearly cadence, DDR4 memory channel bandwidth has grown at a slower rate, and memory latency has remained relatively flat for some time. Meanwhile, new computing paradigms have emerged, which involve analyzing massive volumes of data in real time and place pressure on the memory subsystem. The combination of these trends makes it important for computer architects to understand the sensitivity of the workload performance to memory bandwidth and latency. In this paper, we outline and validate a methodology for quick and quantitative performance estimation using a real-world workload.},
 acmid = {2745900},
 address = {New York, NY, USA},
 author = {Clapp, Russell and Dimitrov, Martin and Kumar, Karthik and Viswanathan, Vish and Willhalm, Thomas},
 doi = {10.1145/2796314.2745900},
 issn = {0163-5999},
 issue_date = {June 2015},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 keyword = {performance modeling, workload characterization},
 link = {http://doi.acm.org/10.1145/2796314.2745900},
 month = {jun},
 number = {1},
 numpages = {2},
 pages = {471--472},
 publisher = {ACM},
 title = {A Simple Model to Quantify the Impact of Memory Latency and Bandwidth on Performance},
 volume = {43},
 year = {2015}
}


@inproceedings{Zhang:2015:OEC:2745844.2745894,
 abstract = {This work studies the online electricity cost minimization problem at a co-location data center. A co-location data center serves multiple tenants who rent the physical infrastructure within the data center to run their respective cloud computing services. Consequently, the co-location operator has no direct control over power consumption of its tenants, and an efficient mechanism is desired for eliciting desirable consumption patterns from the co-location tenants. Electricity billing faced by a data center is nowadays based on both the total volume consumed and the peak consumption rate. This leads to an interesting new combinatorial optimization structure on the electricity cost optimization problem, which also exhibits an online nature due to the definition of peak consumption. We model and solve the problem through two approaches: the pricing approach and the auction approach. For the former, we design an offline 2-approximation algorithm as well as an online algorithm with a small competitive ratio in most practical settings. For the latter, we design an efficient (2+c)-competitive online algorithm, where c is a system dependent parameter close to 1.49, and then convert it into an efficient mechanism that executes in an online fashion, runs in polynomial time, and guarantees truthful bidding and (2+2c)-competitive in social cost.},
 acmid = {2745894},
 address = {New York, NY, USA},
 author = {Zhang, Linquan and Li, Zongpeng and Wu, Chuan and Ren, Shaolei},
 booktitle = {Proceedings of the 2015 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems},
 doi = {10.1145/2745844.2745894},
 isbn = {978-1-4503-3486-0},
 keyword = {approximation algorithms, co-location data center, mechanism design, online algorithms},
 link = {http://doi.acm.org/10.1145/2745844.2745894},
 location = {Portland, Oregon, USA},
 numpages = {2},
 pages = {463--464},
 publisher = {ACM},
 series = {SIGMETRICS '15},
 title = {Online Electricity Cost Saving Algorithms for Co-Location Data Centers},
 year = {2015}
}


@article{Ahmed:2015:DLE:2796314.2745892,
 abstract = {Nowadays mobile device (e.g., smartphone) users not only have a high expectation on the availability of the cellular data service, but also increasingly depend on the high end-to-end (E2E) performance of their applications. Since the E2E performance of individual application sessions may vary greatly, depending on factors such as the cellular network condition, the content provider, the type/model of the mobile devices, and the application software, detecting and localizing service performance degradations in a timely manner at large scale is of great value to cellular service providers. In this paper, we build a holistic measurement system that tracks session-level E2E performance metrics along with the service attributes for these factors. Using data collected from a major cellular service provider, we first model the expected E2E service performance with a regression based approach, detect performance degradation conditions based on the time series of fine-grained measurement data, and finally localize the service degradation using association-rule-mining techniques. Our deployment experience reveals that in 80% of the detected problem instances, performance degradation can be attributed to non-network-location specific factors, such as a common content provider, or a set of applications running on certain models of devices.},
 acmid = {2745892},
 address = {New York, NY, USA},
 author = {Ahmed, Faraz and Erman, Jeffrey and Ge, Zihui and Liu, Alex X. and Wang, Jia and Yan, He},
 doi = {10.1145/2796314.2745892},
 issn = {0163-5999},
 issue_date = {June 2015},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 keyword = {cellular network, end-to-end, performance},
 link = {http://doi.acm.org/10.1145/2796314.2745892},
 month = {jun},
 number = {1},
 numpages = {2},
 pages = {459--460},
 publisher = {ACM},
 title = {Detecting and Localizing End-to-End Performance Degradation for Cellular Data Services},
 volume = {43},
 year = {2015}
}


@inproceedings{Chen:2015:NSB:2745844.2745845,
 abstract = {The popular Network File System (NFS) protocol is 30 years old. The latest version, NFSv4, is more than ten years old but has only recently gained stability and acceptance. NFSv4 is vastly different from its predecessors: it offers a stateful server, strong security, scalability/WAN features, and callbacks, among other things. Yet NFSv4's efficacy and ability to meet its stated design goals had not been thoroughly studied until now. This paper compares NFSv4.1's performance with NFSv3 using a wide range of micro- and macro-benchmarks on a testbed configured to exercise the core protocol features. We (1) tested NFSv4's unique features, such as delegations and statefulness; (2) evaluated performance comprehensively with different numbers of threads and clients, and different network latencies and TCP/IP features; (3) found, fixed, and reported several problems in Linux's NFSv4.1 implementation, which helped improve performance by up to 11X; and (4) discovered, analyzed, and explained several counter-intuitive results. Depending on the workload, NFSv4.1 was up to 67\% slower than NFSv3 in a low-latency network, but exceeded NFSv3's performance by up to 2.9X in a high-latency environment. Moreover, NFSv4.1 outperformed NFSv3 by up to 172X when delegations were used.},
 acmid = {2745845},
 address = {New York, NY, USA},
 author = {Chen, Ming and Hildebrand, Dean and Kuenning, Geoff and Shankaranarayana, Soujanya and Singh, Bharat and Zadok, Erez},
 booktitle = {Proceedings of the 2015 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems},
 doi = {10.1145/2745844.2745845},
 isbn = {978-1-4503-3486-0},
 keyword = {network file system, nfs, nfsv4.1},
 link = {http://doi.acm.org/10.1145/2745844.2745845},
 location = {Portland, Oregon, USA},
 numpages = {12},
 pages = {165--176},
 publisher = {ACM},
 series = {SIGMETRICS '15},
 title = {Newer Is Sometimes Better: An Evaluation of NFSv4.1},
 year = {2015}
}


@article{Hajek:2015:BID:2796314.2745902,
 abstract = {A recurring theme in the design of control schemes for computer communication networks has been to identify the drift of critical quantities such as queue lengths, and then devise control strategies that close the loop. A useful tool for the performance analysis of such strategies are bounds on deviations from the expected trajectory. This talk identifies an incomplete list of such tools that have been used in a broad class of applications, for both stochastic and deterministically constrained models of load.},
 acmid = {2745902},
 address = {New York, NY, USA},
 author = {Hajek, Bruce},
 doi = {10.1145/2796314.2745902},
 issn = {0163-5999},
 issue_date = {June 2015},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 keyword = {drift analysis, lyapunov function, stability},
 link = {http://doi.acm.org/10.1145/2796314.2745902},
 month = {jun},
 number = {1},
 numpages = {1},
 pages = {1--1},
 publisher = {ACM},
 title = {Bounds Implied by Drift with Applications},
 volume = {43},
 year = {2015}
}


@inproceedings{Jin:2015:CPI:2745844.2745876,
 abstract = {Parallel application benchmarks are indispensable for evaluating/optimizing HPC software and hardware. However, it is very challenging and costly to obtain high-fidelity benchmarks reflecting the scale and complexity of state-of-the-art parallel applications. Hand-extracted synthetic benchmarks are time- and labor-intensive to create. Real applications themselves, while offering most accurate performance evaluation, are expensive to compile, port, reconfigure, and often plainly inaccessible due to security or ownership concerns. This work contributes APPrime, a novel tool for trace-based automatic parallel benchmark generation. Taking as input standard communication-I/O traces of an application's execution, it couples accurate automatic phase identification with statistical regeneration of event parameters to create compact, portable, and to some degree reconfigurable parallel application benchmarks. Experiments with four NAS Parallel Benchmarks (NPB) and three real scientific simulation codes confirm the fidelity of APPrime benchmarks. They retain the original applications' performance characteristics, in particular their relative performance across platforms. Also, the result benchmarks, already released online, are much more compact and easy-to-port compared to the original applications.},
 acmid = {2745876},
 address = {New York, NY, USA},
 author = {Jin, Ye and Ma, Xiaosong and Liu, Mingliang and Liu, Qing and Logan, Jeremy and Podhorszki, Norbert and Choi, Jong Youl and Klasky, Scott},
 booktitle = {Proceedings of the 2015 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems},
 doi = {10.1145/2745844.2745876},
 isbn = {978-1-4503-3486-0},
 keyword = {asynchronous i/o, benchmark generation, hpc applications, markov chain model, phase identification, traces},
 link = {http://doi.acm.org/10.1145/2745844.2745876},
 location = {Portland, Oregon, USA},
 numpages = {12},
 pages = {309--320},
 publisher = {ACM},
 series = {SIGMETRICS '15},
 title = {Combining Phase Identification and Statistic Modeling for Automated Parallel Benchmark Generation},
 year = {2015}
}


@article{BojjaVenkatakrishnan:2015:DNP:2796314.2745888,
 abstract = {We consider live-streaming over a peer-to-peer network in which peers are allowed to enter or leave the system adversarially and arbitrarily. Previous approaches for streaming have either used randomized distribution graphs or structured trees with randomized maintenance algorithms. Randomized graphs handle peer churn well but have only probabilistic connectivity guarantees, while structured trees have good connectivity but have proven hard to maintain under peer churn. We improve upon both approaches by presenting a novel distribution structure with a deterministic and distributed algorithm for maintenance under peer churn. The algorithm has a constant repair time for connectivity, and near optimal delay. As opposed to order results, the guarantees provided by our algorithm are exact and hold for any network size.},
 acmid = {2745888},
 address = {New York, NY, USA},
 author = {Bojja Venkatakrishnan, Shaileshh and Viswanath, Pramod},
 doi = {10.1145/2796314.2745888},
 issn = {0163-5999},
 issue_date = {June 2015},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 keyword = {deterministic, one-to-many content distribution, peer-to-peer, streaming},
 link = {http://doi.acm.org/10.1145/2796314.2745888},
 month = {jun},
 number = {1},
 numpages = {2},
 pages = {451--452},
 publisher = {ACM},
 title = {Deterministic Near-Optimal P2P Streaming},
 volume = {43},
 year = {2015}
}


@inproceedings{Kandemir:2015:MRR:2745844.2745867,
 abstract = {Continuously increasing dataset sizes of large-scale applications overwhelm on-chip cache capacities and make the performance of last-level caches (LLC) increasingly important. That is, in addition to maximizing LLC hit rates, it is becoming equally important to reduce LLC miss latencies. One of the critical factors that influence LLC miss latencies is row-buffer locality (i.e., the fraction of LLC misses that hit in the large buffer attached to a memory bank). While there has been a plethora of recent works on optimizing row-buffer performance, to our knowledge, there is no study that quantifies the full potential of row-buffer locality and impact of maximizing it on application performance. Focusing on multithreaded applications, the first contribution of this paper is the definition of a new metric called (memory) row reuse distance (RRD). We show that, while intra-core RRDs are relatively small (increasing the chances for row-buffer hits), inter-core RRDs are quite large (increasing the chances for row-buffer misses). Motivated by this, we propose two schemes that measure the maximum potential benefits that could be obtained from minimizing RRDs, to the extent allowed by program dependencies. Specifically, one of our schemes (Scheme-I) targets only intra-core RRDs, whereas the other one (Scheme-II) aims at reducing both intra-core RRDs and inter-core RRDs. Our experimental evaluations demonstrate that (i) Scheme-I reduces intra-core RRDs but increases inter-core RRDs; (ii) Scheme-II reduces inter-core RRDs significantly while achieving a similar behavior to Scheme-I as far as intra-core RRDs are concerned; (iii) Scheme-I and Scheme-II improve execution times of our applications by 17% and 21%, respectively, on average; and (iv) both our schemes deliver consistently good results under different memory request scheduling policies.},
 acmid = {2745867},
 address = {New York, NY, USA},
 author = {Kandemir, Mahmut and Zhao, Hui and Tang, Xulong and Karakoy, Mustafa},
 booktitle = {Proceedings of the 2015 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems},
 doi = {10.1145/2745844.2745867},
 isbn = {978-1-4503-3486-0},
 keyword = {memory scheduling, multicores, row reuse distance, row-buffer locality},
 link = {http://doi.acm.org/10.1145/2745844.2745867},
 location = {Portland, Oregon, USA},
 numpages = {13},
 pages = {137--149},
 publisher = {ACM},
 series = {SIGMETRICS '15},
 title = {Memory Row Reuse Distance and Its Role in Optimizing Application Performance},
 year = {2015}
}


@article{Zhao:2015:UPP:2796314.2745886,
 abstract = {The performance of parallel programs is notoriously difficult to reason in virtualized environments. Although performance degradations caused by virtualization and interferences have been well studied, there is little understanding why different parallel programs have unpredictable slow- downs. We find that unpredictable performance is the result of complex interplays between the design of the program, the memory hierarchy of the hosting system, and the CPU scheduling in the hypervisor. We develop a profiling tool, vProfile, to decompose parallel runtime into three parts: compute, steal and synchronization. With the help of time breakdown, we devise two optimizations at the hypervisor to reduce slowdowns.},
 acmid = {2745886},
 address = {New York, NY, USA},
 author = {Zhao, Yong and Rao, Jia and Zhou, Xiaobo and Yi, Qing},
 doi = {10.1145/2796314.2745886},
 issn = {0163-5999},
 issue_date = {June 2015},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 keyword = {cloud computing, parallel computing, scheduling},
 link = {http://doi.acm.org/10.1145/2796314.2745886},
 month = {jun},
 number = {1},
 numpages = {2},
 pages = {447--448},
 publisher = {ACM},
 title = {Understanding Parallel Performance Under Interferences in Multi-tenant Clouds},
 volume = {43},
 year = {2015}
}


@inproceedings{Meirom:2015:LED:2745844.2745883,
 abstract = {We consider the problem of detecting an epidemic in a population where individual diagnoses are extremely noisy. We show that exclusively local, approximate knowledge of the contact network suffices to accurately detect the epidemic. The motivation for this problem is the plethora of examples (influenza strains in humans, or computer viruses in smartphones, etc.) where reliable diagnoses are scarce, but noisy data plentiful. In flu or phone-viruses, exceedingly few infected people/phones are professionally diagnosed (only a small fraction go to a doctor) but less reliable secondary signatures (e.g., people staying home, or greater-than-typical upload activity) are more readily available. Our algorithm requires only local-neighbor knowledge of this graph, and in a broad array of settings that we describe, succeeds even when false negatives and false positives make up an overwhelming majority of the data available. Our results show it succeeds in the presence of partial information about the contact network, and also when are many (hundreds, in our examples) of initial patients-zero.},
 acmid = {2745883},
 address = {New York, NY, USA},
 author = {Meirom, Eli A. and Milling, Chris and Caramanis, Constantine and Mannor, Shie and Shakkottai, Sanjay and Orda, Ariel},
 booktitle = {Proceedings of the 2015 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems},
 doi = {10.1145/2745844.2745883},
 isbn = {978-1-4503-3486-0},
 keyword = {diffusion, epidemics, first passage percolation, inference, machine learning, networks, random networks, security},
 link = {http://doi.acm.org/10.1145/2745844.2745883},
 location = {Portland, Oregon, USA},
 numpages = {2},
 pages = {441--442},
 publisher = {ACM},
 series = {SIGMETRICS '15},
 title = {Localized Epidemic Detection in Networks with Overwhelming Noise},
 year = {2015}
}


@article{Shah:2015:IFH:2796314.2745857,
 abstract = {We consider multi-class queueing systems where the per class service rates depend on the network state, fairness criterion, and is constrained to be in a symmetric polymatroid capacity region. We develop new comparison results leading to explicit bounds on the mean service time under various fairness criteria and possibly heterogeneous loads. We then study large-scale systems with growing numbers of service classes n (e.g., files), heterogenous servers m and polymatroid capacity resulting from a random bipartite graph modeling service availability (e.g., placement of files across servers). This models, for example, a large scale content delivery network (CDN) supporting parallel servicing of a download request. For an appropriate asymptotic regime, we show that the system's capacity region is uniformly close to a symmetric polymatroid -- i.e., heterogeneity in servers' capacity and file placement disappears. Combining our comparison results and the asymptotic 'symmetry' in large systems, we study performance robustness to heterogeneity in per class loads and fairness criteria. Roughly, if each class can be served by cn = ω(log n) servers, the load per class does not exceed θn = o (min(n/log n, cn)), and average server utilization is bounded by λ < 1, then mean delay satisfies the following bound: E[D(n)] ≤ K θn/cn 1/λ log (1/1--λ}\right), where K is a constant. Thus, large, randomly configured CDNs with a logarithmic number of file copies are robust to substantial load and server heterogeneities for a class of fairness criteria.},
 acmid = {2745857},
 address = {New York, NY, USA},
 author = {Shah, Virag and de Veciana, Gustavo},
 doi = {10.1145/2796314.2745857},
 issn = {0163-5999},
 issue_date = {June 2015},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 keyword = {asymptotics, bipartite graphs, content delivery networks, content placement, delays, fairness, parallel downloads, queuing system, robustness},
 link = {http://doi.acm.org/10.1145/2796314.2745857},
 month = {jun},
 number = {1},
 numpages = {13},
 pages = {375--387},
 publisher = {ACM},
 title = {Impact of Fairness and Heterogeneity on Delays in Large-scale Content Delivery Networks},
 volume = {43},
 year = {2015}
}


@inproceedings{Xie:2015:PDC:2745844.2745849,
 abstract = {We consider a system of $N$ parallel servers, where each server consists of B units of a resource. Jobs arrive at this system according to a Poisson process, and each job stays in the system for an exponentially distributed amount of time. Each job may request different units of the resource from the system. The goal is to understand how to route arriving jobs to the servers to minimize the probability that an arriving job does not find the required amount of resource at the server, i.e., the goal is to minimize blocking probability. The motivation for this problem arises from the design of cloud computing systems in which the jobs are virtual machines (VMs) that request resources such as memory from a large pool of servers. In this paper, we consider power-of-d-choices routing, where a job is routed to the server with the largest amount of available resource among d ≥ 2 randomly chosen servers. We consider a fluid model that corresponds to the limit as N goes to infinity and provide an explicit upper bound for the equilibrium blocking probability. We show that the upper bound exhibits different behavior as B goes to infinity depending on the relationship between the total traffic intensity λ and B. In particular, if (B -- λ)/√λ → α, the upper bound is doubly exponential in √λ and if (B -- λ)/logd λ → β, β > 1, the upper bound is exponential in λ. Simulation results show that the blocking probability, even for small B, exhibits qualitatively different behavior in the two traffic regimes. This is in contrast with the result for random routing, where the blocking probability scales as O(1/√λ) even if (B -- λ)/√λ → α.},
 acmid = {2745849},
 address = {New York, NY, USA},
 author = {Xie, Qiaomin and Dong, Xiaobo and Lu, Yi and Srikant, Rayadurgam},
 booktitle = {Proceedings of the 2015 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems},
 doi = {10.1145/2745844.2745849},
 isbn = {978-1-4503-3486-0},
 keyword = {fluid limit analysis, loss model, randomized algorithms, resource allocation, virtual machine assignment},
 link = {http://doi.acm.org/10.1145/2745844.2745849},
 location = {Portland, Oregon, USA},
 numpages = {14},
 pages = {321--334},
 publisher = {ACM},
 series = {SIGMETRICS '15},
 title = {Power of D Choices for Large-Scale Bin Packing: A Loss Model},
 year = {2015}
}


@inproceedings{Yun:2015:DPF:2745844.2745861,
 abstract = {We consider the problem of distributed load balancing in heterogenous parallel server systems, where the service rate achieved by a user at a server depends on both the user and the server. Such heterogeneity typically arises in wireless networks (e.g., servers may represent frequency bands, and the service rate of a user varies across bands). We assume that each server equally shares in time its capacity among users allocated to it. Users initially attach to an arbitrary server, but at random instants of time, they probe the load at a new server and migrate there if this improves their service rate. The dynamics under this distributed load balancing scheme, referred to as Random Local Search (RLS), may be interpreted as those generated by strategic players updating their strategy in a load balancing game. In closed systems, where the user population is fixed, we show that this game has pure Nash Equilibriums (NEs), and that these equilibriums get close to a Proportionally Fair (PF) allocation of users to servers when the user population grows large. We provide an anytime upper bound of the gap between the allocation under RLS and the PF allocation. In open systems, where users randomly enter the system and leave upon service completion, we establish that the RLS algorithm stabilizes the system whenever this it at all possible under centralized load balancing schemes, i.e., it is throughput-optimal. The proof of this result relies on a novel Lyapounov analysis that captures the dynamics due to both users' migration and their arrivals and departures. To our knowledge, the RLS algorithm constitutes the first fully distributed and throughput-optimal load balancing scheme in heterogenous parallel server systems. We extend our analysis to various scenarios, e.g. to cases where users can be simultaneously served by several servers. Finally we illustrate through numerical experiments the efficiency of the RLS algorithm.},
 acmid = {2745861},
 address = {New York, NY, USA},
 author = {Yun, Se-Young and Proutiere, Alexandre},
 booktitle = {Proceedings of the 2015 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems},
 doi = {10.1145/2745844.2745861},
 isbn = {978-1-4503-3486-0},
 keyword = {distributed scheduling, game theory, load balancing, stability},
 link = {http://doi.acm.org/10.1145/2745844.2745861},
 location = {Portland, Oregon, USA},
 numpages = {14},
 pages = {17--30},
 publisher = {ACM},
 series = {SIGMETRICS '15},
 title = {Distributed Proportional Fair Load Balancing in Heterogenous Systems},
 year = {2015}
}


@inproceedings{Gardner:2015:RLV:2745844.2745873,
 abstract = {Recent computer systems research has proposed using redundant requests to reduce latency. The idea is to run a request on multiple servers and wait for the first completion (discarding all remaining copies of the request). However there is no exact analysis of systems with redundancy. This paper presents the first exact analysis of systems with redundancy. We allow for any number of classes of redundant requests, any number of classes of non-redundant requests, any degree of redundancy, and any number of heterogeneous servers. In all cases we derive the limiting distribution on the state of the system. In small (two or three server) systems, we derive simple forms for the distribution of response time of both the redundant classes and non-redundant classes, and we quantify the "gain" to redundant classes and "pain" to non-redundant classes caused by redundancy. We find some surprising results. First, the response time of a fully redundant class follows a simple Exponential distribution and that of the non-redundant class follows a Generalized Hyperexponential. Second, fully redundant classes are "immune" to any pain caused by other classes becoming redundant. We also compare redundancy with other approaches for reducing latency, such as optimal probabilistic splitting of a class among servers (Opt-Split) and Join-the-Shortest-Queue (JSQ) routing of a class. We find that, in many cases, redundancy outperforms JSQ and Opt-Split with respect to overall response time, making it an attractive solution.},
 acmid = {2745873},
 address = {New York, NY, USA},
 author = {Gardner, Kristen and Zbarsky, Samuel and Doroudi, Sherwin and Harchol-Balter, Mor and Hyytia, Esa},
 booktitle = {Proceedings of the 2015 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems},
 doi = {10.1145/2745844.2745873},
 isbn = {978-1-4503-3486-0},
 keyword = {markov chain analysis, redundancy},
 link = {http://doi.acm.org/10.1145/2745844.2745873},
 location = {Portland, Oregon, USA},
 numpages = {14},
 pages = {347--360},
 publisher = {ACM},
 series = {SIGMETRICS '15},
 title = {Reducing Latency via Redundant Requests: Exact Analysis},
 year = {2015}
}


@proceedings{Alouf:2016:2896377,
 abstract = {It is our pleasure to welcome you to ACM SIGMETRICS / IFIP Performance 2016, the 13th Joint International Conference on Measurement and Modeling of Computer Systems. This joint gathering occurs every three years, bringing together the flagship conferences of ACM SIGMETRICS and the IFIP Working Group 7.3 into a forum that attracts top quality papers in performance evaluation. This year's program includes papers on topics that have been the foundation of our communities, including queueing, network resource allocation, and performance measurement. As we have seen over the last couple of years, some topics in particular have come to the forefront, such as graph theory and social networks, learning, energy optimization, memory systems, and network economics. We received 208 submissions, of which 28 were accepted into the program as full papers, a highly competitive acceptance rate of 13.5%. Additionally, 24 papers were accepted as poster presentations, and appear in abbreviated form in the proceedings. The paper review process was as in previous years, starting with a first round where each paper was assigned to three reviewers. An online discussion period followed where some papers received up to two additional reviews. The Program Committee then met in person on Feb 13, 2016 at Columbia University in New York. After an entire day of intense deliberation, the committee selected 28 papers to be included as full papers in the program.},
 address = {New York, NY, USA},
 isbn = {978-1-4503-4266-7},
 location = {Antibes Juan-les-Pins, France},
 publisher = {ACM},
 title = {SIGMETRICS '16: Proceedings of the 2016 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Science},
 year = {2016}
}


@article{Combes:2015:BBR:2796314.2745847,
 abstract = {We investigate multi-armed bandits with budgets, a natural model for ad-display optimization encountered in search engines. We provide asymptotic regret lower bounds satisfied by any algorithm, and propose algorithms which match those lower bounds. We consider different types of budgets: scenarios where the advertiser has a fixed budget over a time horizon, and scenarios where the amount of money that is available to spend is incremented in each time slot. Further, we consider two different pricing models, one in which an advertiser is charged for each time her ad is shown (i.e., for each impression) and one in which the advertiser is charged only if a user clicks on the ad. For all of these cases, we show that it is possible to achieve O(log(T)) regret. For both the cost-per-impression and cost-per-click models, with a fixed budget, we provide regret lower bounds that apply to any uniformly good algorithm. Further, we show that B-KL-UCB, a natural variant of KL-UCB, is asymptotically optimal for these cases. Numerical experiments (based on a real-world data set) further suggest that B-KL-UCB also has the same or better finite-time performance when compared to various previously proposed (UCB-like) algorithms, which is important when applying such algorithms to a real-world problem.},
 acmid = {2745847},
 address = {New York, NY, USA},
 author = {Combes, Richard and Jiang, Chong and Srikant, Rayadurgam},
 doi = {10.1145/2796314.2745847},
 issn = {0163-5999},
 issue_date = {June 2015},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 keyword = {ad-display optimization, budgets, kl-ucb, learning, multi-armed bandits, search engines, ucb},
 link = {http://doi.acm.org/10.1145/2796314.2745847},
 month = {jun},
 number = {1},
 numpages = {13},
 pages = {245--257},
 publisher = {ACM},
 title = {Bandits with Budgets: Regret Lower Bounds and Optimal Algorithms},
 volume = {43},
 year = {2015}
}


@inproceedings{Wu:2015:CIP:2745844.2745887,
 abstract = {Given a set of pairwise comparisons, the classical ranking problem computes a single ranking that best represents the preferences of all users. In this paper, we study the problem of inferring individual preferences, arising in the context of making personalized recommendations. In particular, we assume users form clusters; users of the same cluster provide similar pairwise comparisons for the items according to the Bradley-Terry model. We propose an efficient algorithm to estimate the preference for each user: first, compute the net-win vector for each user using the comparisons; second, cluster the users based on the net-win vectors; third, estimate a single preference for each cluster separately. We show that the net-win vectors are much less noisy than the high dimensional vectors of pairwise comparisons, therefore our algorithm can cluster the users reliably. Moreover, we show that, when a cluster is only approximately correct, the maximum likelihood estimation for the Bradley-Terry model is still close to the true preference.},
 acmid = {2745887},
 address = {New York, NY, USA},
 author = {Wu, Rui and Xu, Jiaming and Srikant, Rayadurgam and Massoulie, Laurent and Lelarge, Marc and Hajek, Bruce},
 booktitle = {Proceedings of the 2015 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems},
 doi = {10.1145/2745844.2745887},
 isbn = {978-1-4503-3486-0},
 keyword = {bradley-terry model, clustering, inference, ranking},
 link = {http://doi.acm.org/10.1145/2745844.2745887},
 location = {Portland, Oregon, USA},
 numpages = {2},
 pages = {449--450},
 publisher = {ACM},
 series = {SIGMETRICS '15},
 title = {Clustering and Inference From Pairwise Comparisons},
 year = {2015}
}


