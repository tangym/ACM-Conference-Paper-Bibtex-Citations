@inproceedings{Jog:2016:ECC:2896377.2901468,
 abstract = {Modern memory access schedulers employed in GPUs typically optimize for memory throughput. They implicitly assume that all requests from different cores are equally important. However, we show that during the execution of a subset of CUDA applications, different cores can have different amounts of tolerance to latency. In particular, cores with a larger fraction of warps waiting for data to come back from DRAM are less likely to tolerate the latency of an outstanding memory request. Requests from such cores are more critical than requests from others. Based on this observation, this paper introduces a new memory scheduler, called (C)ritica(L)ity (A)ware (M)emory (S)cheduler (CLAMS), which takes into account the latency-tolerance of the cores that generate memory requests. The key idea is to use the fraction of critical requests in the memory request buffer to switch between scheduling policies optimized for criticality and locality. If this fraction is below a threshold, CLAMS prioritizes critical requests to ensure cores that cannot tolerate latency are serviced faster. Otherwise, CLAMS optimizes for locality, anticipating that there are too many critical requests and prioritizing one over another would not significantly benefit performance. We first present a core-criticality estimation mechanism for determining critical cores and requests, and then discuss issues related to finding a balance between criticality and locality in the memory scheduler. We progressively devise three variants of CLAMS, and show that the Dynamic CLAMS provides significantly higher performance, across a variety of workloads, than the commonly-employed GPU memory schedulers optimized solely for locality. The results indicate that a GPU memory system that considers both core criticality and DRAM access locality can provide significant improvement in performance.},
 acmid = {2901468},
 address = {New York, NY, USA},
 author = {Jog, Adwait and Kayiran, Onur and Pattnaik, Ashutosh and Kandemir, Mahmut T. and Mutlu, Onur and Iyer, Ravishankar and Das, Chita R.},
 booktitle = {Proceedings of the 2016 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Science},
 doi = {10.1145/2896377.2901468},
 isbn = {978-1-4503-4266-7},
 keyword = {criticality, gpus, latency tolerance, memory system},
 link = {http://doi.acm.org/10.1145/2896377.2901468},
 location = {Antibes Juan-les-Pins, France},
 numpages = {13},
 pages = {351--363},
 publisher = {ACM},
 series = {SIGMETRICS '16},
 title = {Exploiting Core Criticality for Enhanced GPU Performance},
 year = {2016}
}


@inproceedings{Ying:2016:AEM:2896377.2901463,
 abstract = {Mean-field models have been used to study large-scale and complex stochastic systems, such as large-scale data centers and dense wireless networks, using simple deterministic models (dynamical systems). This paper analyzes the approximation error of mean-field models for continuous-time Markov chains (CTMC), and focuses on mean-field models that are represented as finite-dimensional dynamical systems with a unique equilibrium point. By applying Stein's method and the perturbation theory, the paper shows that under some mild conditions, if the mean-field model is globally asymptotically stable and locally exponentially stable, the mean square difference between the stationary distribution of the stochastic system with size M and the equilibrium point of the corresponding mean-field system is O(1/M). The result of this paper establishes a general theorem for establishing the convergence and the approximation error (i.e., the rate of convergence) of a large class of CTMCs to their mean-field limit by mainly looking into the stability of the mean-field model, which is a deterministic system and is often easier to analyze than the CTMCs. Two applications of mean-field models in data center networks are presented to demonstrate the novelty of our results.},
 acmid = {2901463},
 address = {New York, NY, USA},
 author = {Ying, Lei},
 booktitle = {Proceedings of the 2016 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Science},
 doi = {10.1145/2896377.2901463},
 isbn = {978-1-4503-4266-7},
 keyword = {approximation error, data center networks, cloud computing, mean-field models, perturbation theory, stein's method},
 link = {http://doi.acm.org/10.1145/2896377.2901463},
 location = {Antibes Juan-les-Pins, France},
 numpages = {13},
 pages = {285--297},
 publisher = {ACM},
 series = {SIGMETRICS '16},
 title = {On the Approximation Error of Mean-Field Models},
 year = {2016}
}


@inproceedings{Cao:2016:APC:2896377.2901491,
 abstract = {
                  An abstract is not available.
              },
 acmid = {2901491},
 address = {New York, NY, USA},
 author = {Cao, Yi and Nejati, Javad and Maguluri, Pavan and Balasubramanian, Aruna and Gandhi, Anshul},
 booktitle = {Proceedings of the 2016 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Science},
 doi = {10.1145/2896377.2901491},
 isbn = {978-1-4503-4266-7},
 keyword = {energy modeling, mobile web, web optimizations},
 link = {http://doi.acm.org/10.1145/2896377.2901491},
 location = {Antibes Juan-les-Pins, France},
 numpages = {2},
 pages = {369--370},
 publisher = {ACM},
 series = {SIGMETRICS '16},
 title = {Analyzing the Power Consumption of the Mobile Page Load},
 year = {2016}
}


@inproceedings{Yang:2016:SRL:2896377.2901480,
 abstract = {Load-balanced switch architectures are known to be scalable in both size and speed, which is of interest due to the continued exponential growth in Internet traffic. However, the main drawback of load-balanced switches is that packets can depart out of order from the switch. Randomized load-balancing of application flows by means of hashing on the packet header is a well-known simple solution to this packet reordering problem in which all packets belonging to the same application flow are routed through the same intermediate port and hence the same path through the switch. Unfortunately, this method of load-balancing can lead to instability, depending on the mix of flow sizes and durations in the group of flows that gets randomly assigned to route through the same intermediate port. In this paper, we show that the randomized load-balancing of application flows can be enhanced to provably guarantee both stability and packet ordering by extending the approach with safety mechanisms that can uniformly diffuse packets across the switch whenever there is a build-up of packets waiting to route through the some intermediate port. Although simple and intuitive, our experimental results show that our extended randomized load-balancing approach significantly outperforms existing load-balanced switch architectures.},
 acmid = {2901480},
 address = {New York, NY, USA},
 author = {Yang, Sen and Lin, Bill and Xu, Jun},
 booktitle = {Proceedings of the 2016 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Science},
 doi = {10.1145/2896377.2901480},
 isbn = {978-1-4503-4266-7},
 keyword = {algorithm, design, load-balanced switches, performance, throughput guarantees},
 link = {http://doi.acm.org/10.1145/2896377.2901480},
 location = {Antibes Juan-les-Pins, France},
 numpages = {2},
 pages = {397--398},
 publisher = {ACM},
 series = {SIGMETRICS '16},
 title = {Safe Randomized Load-Balanced Switching by Diffusing Extra Loads},
 year = {2016}
}


@article{Ferragut:2016:OTC:2964791.2901459,
 abstract = {In this paper we analyze the hit performance of cache systems that receive file requests with general arrival distributions and different popularities. We consider timer-based (TTL) policies, with differentiated timers over which we optimize. The optimal policy is shown to be related to the monotonicity of the hazard rate function of the inter-arrival distribution. In particular for decreasing hazard rates, timer policies outperform the static policy of caching the most popular contents. We provide explicit solutions for the optimal policy in the case of Pareto-distributed inter-request times and a Zipf distribution of file popularities, including a compact fluid characterization in the limit of a large number of files. We compare it through simulation with classical policies, such as least-recently-used and discuss its performance. Finally, we analyze extensions of the optimization framework to a line network of caches.},
 acmid = {2901459},
 address = {New York, NY, USA},
 author = {Ferragut, Andr{\'e}s and Rodriguez, Ismael and Paganini, Fernando},
 doi = {10.1145/2964791.2901459},
 issn = {0163-5999},
 issue_date = {June 2016},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 keyword = {caching algorithms, heavy-tails, performance evaluation},
 link = {http://doi.acm.org/10.1145/2964791.2901459},
 month = {jun},
 number = {1},
 numpages = {12},
 pages = {101--112},
 publisher = {ACM},
 title = {Optimizing TTL Caches Under Heavy-Tailed Demands},
 volume = {44},
 year = {2016}
}


@article{Chen:2016:UPO:2964791.2901464,
 abstract = {We consider online convex optimization (OCO) problems with switching costs and noisy predictions. While the design of online algorithms for OCO problems has received considerable attention, the design of algorithms in the context of noisy predictions is largely open. To this point, two promising algorithms have been proposed: Receding Horizon Control (RHC) and Averaging Fixed Horizon Control (AFHC). The comparison of these policies is largely open. AFHC has been shown to provide better worst-case performance, while RHC outperforms AFHC in many realistic settings. In this paper, we introduce a new class of policies, Committed Horizon Control (CHC), that generalizes both RHC and AFHC. We provide average-case analysis and concentration results for CHC policies, yielding the first analysis of RHC for OCO problems with noisy predictions. Further, we provide explicit results characterizing the optimal CHC policy as a function of properties of the prediction noise, e.g., variance and correlation structure. Our results provide a characterization of when AFHC outperforms RHC and vice versa, as well as when other CHC policies outperform both RHC and AFHC.},
 acmid = {2901464},
 address = {New York, NY, USA},
 author = {Chen, Niangjun and Comden, Joshua and Liu, Zhenhua and Gandhi, Anshul and Wierman, Adam},
 doi = {10.1145/2964791.2901464},
 issn = {0163-5999},
 issue_date = {June 2016},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 keyword = {model predictive control, online convex optimization, prediction},
 link = {http://doi.acm.org/10.1145/2964791.2901464},
 month = {jun},
 number = {1},
 numpages = {14},
 pages = {193--206},
 publisher = {ACM},
 title = {Using Predictions in Online Optimization: Looking Forward with an Eye on the Past},
 volume = {44},
 year = {2016}
}


@inproceedings{Gardner:2016:PDC:2896377.2901497,
 abstract = {An increasingly prevalent technique for improving response time in queueing systems is the use of redundancy. In a system with redundant requests, each job that arrives to the system is copied and dispatched to multiple servers. As soon as the first copy completes service, the job is considered complete, and all remaining copies are deleted. A great deal of empirical work has demonstrated that redundancy can significantly reduce response time in systems ranging from Google's BigTable service to kidney transplant waitlists. We propose a theoretical model of redundancy, the Redundancy-d system, in which each job sends redundant copies to d servers chosen uniformly at random. We derive the first exact expressions for mean response time in Redundancy-d systems with any finite number of servers. We also find asymptotically exact expressions for the distribution of response time as the number of servers approaches infinity.},
 acmid = {2901497},
 address = {New York, NY, USA},
 author = {Gardner, Kristen and Zbarsky, Samuel and Harchol-Balter, Mor and Scheller-Wolf, Alan},
 booktitle = {Proceedings of the 2016 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Science},
 doi = {10.1145/2896377.2901497},
 isbn = {978-1-4503-4266-7},
 keyword = {dispatching, power-of-d, redundancy, task assignment},
 link = {http://doi.acm.org/10.1145/2896377.2901497},
 location = {Antibes Juan-les-Pins, France},
 numpages = {2},
 pages = {409--410},
 publisher = {ACM},
 series = {SIGMETRICS '16},
 title = {The Power of D Choices for Redundancy},
 year = {2016}
}


@article{Ray:2016:SSC:2964791.2901494,
 abstract = {In standard graph clustering/community detection, one is interested in partitioning the graph into more densely connected subsets of nodes. In contrast, the search problem of this paper aims to only find the nodes in a single such community, the target, out of the many communities that may exist. To do so , we are given suitable side information about the target; for example, a very small number of nodes from the target are labeled as such. We consider a general yet simple notion of side information: all nodes are assumed to have random weights, with nodes in the target having higher weights on average. Given these weights and the graph, we develop a variant of the method of moments that identifies nodes in the target more reliably, and with lower computation, than generic community detection methods that do not use side information and partition the entire graph.},
 acmid = {2901494},
 address = {New York, NY, USA},
 author = {Ray, Avik and Sanghavi, Sujay and Shakkottai, Sanjay},
 doi = {10.1145/2964791.2901494},
 issn = {0163-5999},
 issue_date = {June 2016},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 keyword = {community detection, graph clustering, semi-supervised, side-information, stochastic block model},
 link = {http://doi.acm.org/10.1145/2964791.2901494},
 month = {jun},
 number = {1},
 numpages = {2},
 pages = {399--400},
 publisher = {ACM},
 title = {Searching For A Single Community in a Graph},
 volume = {44},
 year = {2016}
}


@inproceedings{Liu:2016:SMY:2896377.2901503,
 abstract = {
                  An abstract is not available.
              },
 acmid = {2901503},
 address = {New York, NY, USA},
 author = {Liu, Daiping and Gao, Xing and Zhang, Mingwei and Wang, Haining},
 booktitle = {Proceedings of the 2016 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Science},
 doi = {10.1145/2896377.2901503},
 isbn = {978-1-4503-4266-7},
 keyword = {behavioral detection, game cheating},
 link = {http://doi.acm.org/10.1145/2896377.2901503},
 location = {Antibes Juan-les-Pins, France},
 numpages = {2},
 pages = {401--402},
 publisher = {ACM},
 series = {SIGMETRICS '16},
 title = {Shoot for the Moon and You Will Never Miss: Characterizing and Detecting Aimbots in Online Games},
 year = {2016}
}


@article{Chang:2016:ULV:2964791.2901453,
 abstract = {Long DRAM latency is a critical performance bottleneck in current systems. DRAM access latency is defined by three fundamental operations that take place within the DRAM cell array: (i) activation of a memory row, which opens the row to perform accesses; (ii) precharge, which prepares the cell array for the next memory access; and (iii) restoration of the row, which restores the values of cells in the row that were destroyed due to activation. There is significant latency variation for each of these operations across the cells of a single DRAM chip due to irregularity in the manufacturing process. As a result, some cells are inherently faster to access, while others are inherently slower. Unfortunately, existing systems do not exploit this variation.   The goal of this work is to (i) experimentally characterize and understand the latency variation across cells within a DRAM chip for these three fundamental DRAM operations, and (ii) develop new mechanisms that exploit our understanding of the latency variation to reliably improve performance. To this end, we comprehensively characterize 240 DRAM chips from three major vendors, and make several new observations about latency variation within DRAM. We find that (i) there is large latency variation across the cells for each of the three operations; (ii) variation characteristics exhibit significant spatial locality: slower cells are clustered in certain regions of a DRAM chip; and (iii) the three fundamental operations exhibit different reliability characteristics when the latency of each operation is reduced.   Based on our observations, we propose Flexible-LatencY DRAM (FLY-DRAM), a mechanism that exploits latency variation across DRAM cells within a DRAM chip to improve system performance. The key idea of FLY-DRAM is to exploit the spatial locality of slower cells within DRAM, and access the faster DRAM regions with reduced latencies for the fundamental operations. Our evaluations show that FLY-DRAM improves the performance of a wide range of applications by 13.3%, 17.6%, and 19.5%, on average, for each of the three different vendors' real DRAM chips, in a simulated 8-core system. We conclude that the experimental characterization and analysis of latency variation within modern DRAM, provided by this work, can lead to new techniques that improve DRAM and system performance.},
 acmid = {2901453},
 address = {New York, NY, USA},
 author = {Chang, Kevin K. and Kashyap, Abhijith and Hassan, Hasan and Ghose, Saugata and Hsieh, Kevin and Lee, Donghyuk and Li, Tianshi and Pekhimenko, Gennady and Khan, Samira and Mutlu, Onur},
 doi = {10.1145/2964791.2901453},
 issn = {0163-5999},
 issue_date = {June 2016},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 keyword = {dram, dram errors, memory latency, process variation},
 link = {http://doi.acm.org/10.1145/2964791.2901453},
 month = {jun},
 number = {1},
 numpages = {14},
 pages = {323--336},
 publisher = {ACM},
 title = {Understanding Latency Variation in Modern DRAM Chips: Experimental Characterization, Analysis, and Optimization},
 volume = {44},
 year = {2016}
}


@inproceedings{PuzhavakathNarayanan:2016:RLT:2896377.2901472,
 abstract = {As popular web sites turn to content delivery networks (CDNs) for full-site delivery, there is an opportunity to improve the end-user experience by optimizing the delivery of entire web pages, rather than just individual objects. In particular, this paper explores page-structure-aware strategies for placing objects in CDN cache hierarchies. The key idea is that the objects in a web page that have the largest impact on page latency should be served out of the closest or fastest caches in the hierarchy. We present schemes for identifying these objects and develop mechanisms to ensure that they are served with higher priority by the CDN, while balancing traditional CDN concerns such as optimizing the delivery of popular objects and minimizing bandwidth costs. To establish a baseline for evaluating improvements in page latencies, we collect and analyze publicly visible HTTP headers that reveal the distribution of objects among the various levels of a major CDN's cache hierarchy. Through extensive experiments on 83 real-world web pages, we show that latency reductions of over 100 ms can be obtained for 30% of the popular pages, with even larger reductions for the less popular pages. Using anonymized server logs provided by the CDN, we show the feasibility of reducing capacity and staleness misses of critical objects by 60% with minimal increase in overall miss rates, and bandwidth overheads of under 0.02%.},
 acmid = {2901472},
 address = {New York, NY, USA},
 author = {Puzhavakath Narayanan, Shankaranarayanan and Nam, Yun Seong and Sivakumar, Ashiwan and Chandrasekaran, Balakrishnan and Maggs, Bruce and Rao, Sanjay},
 booktitle = {Proceedings of the 2016 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Science},
 doi = {10.1145/2896377.2901472},
 isbn = {978-1-4503-4266-7},
 keyword = {content delivery networks, content prioritization, http 2.0, web page latency},
 link = {http://doi.acm.org/10.1145/2896377.2901472},
 location = {Antibes Juan-les-Pins, France},
 numpages = {12},
 pages = {89--100},
 publisher = {ACM},
 series = {SIGMETRICS '16},
 title = {Reducing Latency Through Page-aware Management of Web Objects by Content Delivery Networks},
 year = {2016}
}


@article{Xie:2016:TDR:2964791.2901500,
 abstract = {We develop an optimization framework to trade short-term profits for reputation (i.e., reducing ramp-up time). We apply the stochastic bandits framework to design an online discounting mechanism which infers the optimal discount from a seller's historical transaction data. We conduct experiments on an eBay's dataset and show that our online discounting mechanism can trade 60% of the shortterm profits for reducing the ramp-up time by 40%.},
 acmid = {2901500},
 address = {New York, NY, USA},
 author = {Xie, Hong and Ma, Richard T.B. and Lui, John C.S.},
 doi = {10.1145/2964791.2901500},
 issn = {0163-5999},
 issue_date = {June 2016},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 keyword = {discount, e-commerce, reputation},
 link = {http://doi.acm.org/10.1145/2964791.2901500},
 month = {jun},
 number = {1},
 numpages = {2},
 pages = {417--418},
 publisher = {ACM},
 title = {Trading Discount for Reputation?: On the Design and Analysis of E-Commerce Discount Mechanisms},
 volume = {44},
 year = {2016}
}


@article{Harchol-Balter:2016:BMT:2964791.2901900,
 abstract = {An age-old problem in the design of server farms is the choice of the task assignment policy. This is the algorithm that determines how to assign incoming jobs to servers. Popular policies include Round-Robin assignment, Join-the-Shortest-Queue, Join-Queue-with-Least-Work, and so on. While much research has studied assignment policies, little has taken into account server-side variability -- the fact that the server we choose might be temporarily and unpredictably slow. We show that when server-side variability dominates runtime, replication of jobs can be very beneficial. We introduce the Replication-d algorithm that replicates each arrival to d servers chosen at random, where the job is considered "done" as soon as the first replica completes. We provide an exact closed-form analysis of Replication-d. We next introduce a much more general model, one which takes both the inherent job size distribution and the server-side variability into account. This is a departure from traditional queueing models which only allow for one "size" distribution. We propose and analyze a new task assignment policy, Replicate-Idle-Queue (RIQ), which is designed to perform well given these dual sources of variability.},
 acmid = {2901900},
 address = {New York, NY, USA},
 author = {Harchol-Balter, Mor},
 doi = {10.1145/2964791.2901900},
 issn = {0163-5999},
 issue_date = {June 2016},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 keyword = {load balancing, power-of-d, queueing, replication, resource allocation},
 link = {http://doi.acm.org/10.1145/2964791.2901900},
 month = {jun},
 number = {1},
 numpages = {1},
 pages = {73--73},
 publisher = {ACM},
 title = {A Better Model for Task Assignment in Server Farms: How Replication Can Help},
 volume = {44},
 year = {2016}
}


@inproceedings{Shamsi:2016:UCU:2896377.2901449,
 abstract = {Maintaining and updating signature databases is a tedious task that normally requires a large amount of user effort. The problem becomes harder when features can be distorted by observation noise, which we call volatility. To address this issue, we propose algorithms and models to automatically generate signatures in the presence of noise, with a focus on stack fingerprinting, which is a research area that aims to discover the operating system (OS) of remote hosts using TCP/IP packets. Armed with this framework, we construct a database with 420 network stacks, label the signatures, develop a robust classifier for this database, and fingerprint 66M visible webservers on the Internet.},
 acmid = {2901449},
 address = {New York, NY, USA},
 author = {Shamsi, Zain and Loguinov, Dmitri},
 booktitle = {Proceedings of the 2016 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Science},
 doi = {10.1145/2896377.2901449},
 isbn = {978-1-4503-4266-7},
 keyword = {internet measurement, os classification, os fingerprinting},
 link = {http://doi.acm.org/10.1145/2896377.2901449},
 location = {Antibes Juan-les-Pins, France},
 numpages = {12},
 pages = {127--138},
 publisher = {ACM},
 series = {SIGMETRICS '16},
 title = {Unsupervised Clustering Under Temporal Feature Volatility in Network Stack Fingerprinting},
 year = {2016}
}


@inproceedings{Jiang:2016:DIC:2896377.2901475,
 abstract = {Cumulative advantage (CA) refers to the notion that accumulated resources foster the accumulation of further resources in competitions, a phenomenon that has been empirically observed in various contexts. The oldest and arguably simplest mathematical model that embodies this general principle is the Pólya urn process, which finds applications in a myriad of problems. The original model captures the dynamics of competitions between two equally fit agents under linear CA effects, which can be readily generalized to incorporate different fitnesses and nonlinear CA effects. We study two statistics of competitions under the generalized model, namely duration (i.e., time of the last tie) and intensity (i.e., number of ties). We give rigorous mathematical characterizations of the tail distributions of both duration and intensity under the various regimes for fitness and nonlinearity, which reveal very interesting behaviors. For example, fitness superiority induces much shorter competitions in the sublinear regime while much longer competitions in the superlinear regime. Our findings can shed light on the application of Pólya urn processes in more general contexts where fitness and nonlinearity may be present.},
 acmid = {2901475},
 address = {New York, NY, USA},
 author = {Jiang, Bo and Figueiredo, Daniel R. and Ribeiro, Bruno and Towsley, Don},
 booktitle = {Proceedings of the 2016 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Science},
 doi = {10.1145/2896377.2901475},
 isbn = {978-1-4503-4266-7},
 keyword = {P\'{o}lya urn, competition, cumulative advantage, duration, fitness, intensity, nonlinearity},
 link = {http://doi.acm.org/10.1145/2896377.2901475},
 location = {Antibes Juan-les-Pins, France},
 numpages = {12},
 pages = {299--310},
 publisher = {ACM},
 series = {SIGMETRICS '16},
 title = {On the Duration and Intensity of Competitions in Nonlinear P\'{o}Lya Urn Processes with Fitness},
 year = {2016}
}


@inproceedings{Avrachenkov:2016:IOV:2896377.2901477,
 abstract = {Are Online Social Network (OSN) A users more likely to form friendships with those with similar attributes? Do users at an OSN B score content more favorably than OSN C users? Such questions frequently arise in the context of Social Network Analysis (SNA) but often crawling an OSN network via its Application Programming Interface (API) is the only way to gather data from a third party. To date, these partial API crawls are the majority of public datasets and the synonym of lack of statistical guarantees in incomplete-data comparisons, severely limiting SNA research progress. Using regenerative properties of the random walks, we propose estimation techniques based on short crawls that have proven statistical guarantees. Moreover, our short crawls can be implemented in massively distributed algorithms. We also provide an adaptive crawler that makes our method parameter-free, significantly improving our statistical guarantees. We then derive the Bayesian approximation of the posterior of the estimates, and in addition, obtain an estimator for the expected value of node and edge statistics in an equivalent configuration model or Chung-Lu random graph model of the given network (where nodes are connected randomly) and use it as a basis for testing null hypotheses. The theoretical results are supported with simulations on a variety of real-world networks.},
 acmid = {2901477},
 address = {New York, NY, USA},
 author = {Avrachenkov, Konstantin and Ribeiro, Bruno and Sreedharan, Jithin K.},
 booktitle = {Proceedings of the 2016 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Science},
 doi = {10.1145/2896377.2901477},
 isbn = {978-1-4503-4266-7},
 keyword = {bayesian inference, graph sampling, random walk on graphs, social network analysis},
 link = {http://doi.acm.org/10.1145/2896377.2901477},
 location = {Antibes Juan-les-Pins, France},
 numpages = {13},
 pages = {165--177},
 publisher = {ACM},
 series = {SIGMETRICS '16},
 title = {Inference in OSNs via Lightweight Partial Crawls},
 year = {2016}
}


@inproceedings{Ioannidis:2016:ACN:2896377.2901467,
 abstract = {We study the problem of optimal content placement over a network of caches, a problem naturally arising in several networking applications, including ICNs, CDNs, and P2P systems. Given a demand of content request rates and paths followed, we wish to determine the content placement that maximizes the expected caching gain, i.e., the reduction of routing costs due to intermediate caching. The offline version of this problem is NP-hard and, in general, the demand and topology may be a priori unknown. Hence, a distributed, adaptive, constant approximation content placement algorithm is desired. We show that path replication, a simple algorithm frequently encountered in literature, can be arbitrarily suboptimal when combined with traditional eviction policies, like LRU, LFU, or FIFO. We propose a distributed, adaptive algorithm that performs stochastic gradient ascent on a concave relaxation of the expected caching gain, and constructs a probabilistic content placement within 1-1/e factor from the optimal, in expectation. Motivated by our analysis, we also propose a novel greedy eviction policy to be used with path replication, and show through numerical evaluations that both algorithms significantly outperform path replication with traditional eviction policies over a broad array of network topologies.},
 acmid = {2901467},
 address = {New York, NY, USA},
 author = {Ioannidis, Stratis and Yeh, Edmund},
 booktitle = {Proceedings of the 2016 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Science},
 doi = {10.1145/2896377.2901467},
 isbn = {978-1-4503-4266-7},
 keyword = {adaptive caching, content-centric networks, distributed caching, path replication},
 link = {http://doi.acm.org/10.1145/2896377.2901467},
 location = {Antibes Juan-les-Pins, France},
 numpages = {12},
 pages = {113--124},
 publisher = {ACM},
 series = {SIGMETRICS '16},
 title = {Adaptive Caching Networks with Optimality Guarantees},
 year = {2016}
}


@inproceedings{Ren:2016:JDP:2896377.2901486,
 abstract = {This paper studies design challenges faced by a geo-distributed cloud data market: which data to purchase (data purchasing) and where to place/replicate the data (data placement). We show that the joint problem of data purchasing and data placement within a cloud data market is NP-hard in general. However, we give a provably optimal algorithm for the case of a data market made up of a single data center, and then generalize the structure from the single data center setting and propose Datum, a near-optimal, polynomial-time algorithm for a geo-distributed data market.},
 acmid = {2901486},
 address = {New York, NY, USA},
 author = {Ren, Xiaoqi and London, Palma and Ziani, Juba and Wierman, Adam},
 booktitle = {Proceedings of the 2016 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Science},
 doi = {10.1145/2896377.2901486},
 isbn = {978-1-4503-4266-7},
 keyword = {data markets, geo-distributed cloud, load balancing},
 link = {http://doi.acm.org/10.1145/2896377.2901486},
 location = {Antibes Juan-les-Pins, France},
 numpages = {2},
 pages = {383--384},
 publisher = {ACM},
 series = {SIGMETRICS '16},
 title = {Joint Data Purchasing and Data Placement in a Geo-Distributed Data Market},
 year = {2016}
}


@inproceedings{BojjaVenkatakrishnan:2016:CCS:2896377.2901479,
 abstract = {Hybrid switching -- in which a high bandwidth circuit switch (optical or wireless) is used in conjunction with a low bandwidth packet switch -- is a promising alternative to interconnect servers in today's large scale data centers. Circuit switches offer a very high link rate, but incur a non-trivial reconfiguration delay which makes their scheduling challenging. In this paper, we demonstrate a lightweight, simple and nearly-optimal scheduling algorithm that trades-off reconfiguration costs with the benefits of reconfiguration that match the traffic demands. Seen alternatively, the algorithm provides a fast and approximate solution towards a constructive version of Caratheodory's Theorem for the Birkhoff polytope. The algorithm also has strong connections to submodular optimization, achieves a performance at least half that of the optimal schedule and strictly outperforms state of the art in a variety of traffic demand settings. These ideas naturally generalize: we see that indirect routing leads to exponential connectivity; this is another phenomenon of the power of multi-hop routing, distinct from the well-known load balancing effects.},
 acmid = {2901479},
 address = {New York, NY, USA},
 author = {Bojja Venkatakrishnan, Shaileshh and Alizadeh, Mohammad and Viswanath, Pramod},
 booktitle = {Proceedings of the 2016 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Science},
 doi = {10.1145/2896377.2901479},
 isbn = {978-1-4503-4266-7},
 keyword = {caratheodory theorem, circuit switch, datacenters, hybrid networks, packet switch},
 link = {http://doi.acm.org/10.1145/2896377.2901479},
 location = {Antibes Juan-les-Pins, France},
 numpages = {14},
 pages = {75--88},
 publisher = {ACM},
 series = {SIGMETRICS '16},
 title = {Costly Circuits, Submodular Schedules and Approximate Carath{\'e}Odory Theorems},
 year = {2016}
}


@article{Jonckheere:2016:AIL:2964791.2901454,
 abstract = {Load balancing with various types of load information has become a key component of modern communication and information systems. In many systems, characterizing precisely the blocking probability allows to establish a performance trade-off between delay and losses. We address here the problem of giving robust performance bounds based on the study of the asymptotic behavior of the insensitive load balancing schemes when the number of servers and the load scales jointly. These schemes have the desirable property that the stationary distribution of the resulting stochastic network depends on the distribution of job sizes only through its mean. It was shown that they give good estimates of performance indicators for systems with finite buffers, generalizing henceforth Erlang's formula whereas optimal policies are already theoretically and computationally out of reach for networks of moderate size. We study a single class of traffic acting on a symmetric set of processor sharing queues with finite buffers and we consider the case where the load scales with the number of servers.We characterize the response of symmetric systems under those schemes at different scales and show that three amplitudes of deviations can be identified according to whether ρ < 1, ρ = 1, and ρ > 1. A central limit scaling takes place for a sub-critical load;for ρ=1, the number of free servers scales like n{θ/θ+1} (θ being the buffer depth and n being the number of servers) and is of order 1 for super-critical loads. This further implies the existence of different phases for the blocking probability. Before a (refined) critical load ρc(n)=1-a n- {θ/θ+1}, the blocking is exponentially small and becomes of order n- {θ/θ+1} at ρc(n). This generalizes the well-known Quality and Efficiency Driven (QED) regime or Halfin-Whitt regime for a one-dimensional queue, and leads to a generalized staffing rule for a given target blocking probability.},
 acmid = {2901454},
 address = {New York, NY, USA},
 author = {Jonckheere, Matthieu and Prabhu, Balakrishna J.},
 doi = {10.1145/2964791.2901454},
 issn = {0163-5999},
 issue_date = {June 2016},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 keyword = {Qed-Jagerman-Halfin-Whitt regime, blocking phases, insensitive load balancing, mean-field scalings},
 link = {http://doi.acm.org/10.1145/2964791.2901454},
 month = {jun},
 number = {1},
 numpages = {12},
 pages = {311--322},
 publisher = {ACM},
 title = {Asymptotics of Insensitive Load Balancing and Blocking Phases},
 volume = {44},
 year = {2016}
}


@article{Dai:2016:NBF:2964791.2901451,
 abstract = {This paper is on designing a compact data structure for multi-set membership testing allowing fast set querying. Multi-set membership testing is a fundamental operation for computing systems and networking applications. Most existing schemes for multi-set membership testing are built upon Bloom filter, and fall short in either storage space cost or query speed. To address this issue, in this paper we propose Noisy Bloom Filter (NBF) and Error Corrected Noisy Bloom Filter (NBF-E) for multi-set membership testing. For theoretical analysis, we optimize their classification failure rate and false positive rate, and present criteria for selection between NBF and NBF-E. The key novelty of NBF and NBF-E is to store set ID information in a compact but noisy way that allows fast recording and querying, and use denoising method for querying. Especially, NBF-E incorporates asymmetric error-correcting coding technique into NBF to enhance the resilience of query results to noise by revealing and leveraging the asymmetric error nature of query results. To evaluate NBF and NBF-E in comparison with prior art, we conducted experiments using real-world network traces. The results show that NBF and NBF-E significantly advance the state-of-the-art on multi-set membership testing.},
 acmid = {2901451},
 address = {New York, NY, USA},
 author = {Dai, Haipeng and Zhong, Yuankun and Liu, Alex X. and Wang, Wei and Li, Meng},
 doi = {10.1145/2964791.2901451},
 issn = {0163-5999},
 issue_date = {June 2016},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 keyword = {asymmetric error-correcting code, bloom filter, constant weight code., multi-set membership testing, noise},
 link = {http://doi.acm.org/10.1145/2964791.2901451},
 month = {jun},
 number = {1},
 numpages = {13},
 pages = {139--151},
 publisher = {ACM},
 title = {Noisy Bloom Filters for Multi-Set Membership Testing},
 volume = {44},
 year = {2016}
}


@article{Shekaramiz:2016:NCA:2964791.2901487,
 abstract = {Feedback mechanisms are integral components of network protocols and traffic control algorithms. Their performance evaluation is hard due to intricate time correlations introduced by feedback. Network calculus has been successfully applied for the analysis of feedback mechanisms in deterministic systems. However, an extension to random systems has remained an open problem for more than a decade. We present a stochastic network calculus analysis of a random system with feedback, specifically, a window flow control system with random service and fixed feedback delay. We quantify the service impediment due to the feedback mechanism by deriving statistical lower bounds on the available service, and obtain complementary upper bounds. We also discover special cases where an exact description of the service is feasible.},
 acmid = {2901487},
 address = {New York, NY, USA},
 author = {Shekaramiz, Alireza and Liebeherr, Jorg and Burchard, Almut},
 doi = {10.1145/2964791.2901487},
 issn = {0163-5999},
 issue_date = {June 2016},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 keyword = {feedback system, network calculus},
 link = {http://doi.acm.org/10.1145/2964791.2901487},
 month = {jun},
 number = {1},
 numpages = {2},
 pages = {393--394},
 publisher = {ACM},
 title = {Network Calculus Analysis of a Feedback System with Random Service},
 volume = {44},
 year = {2016}
}


@article{Narayanan:2016:SFD:2964791.2901489,
 abstract = {Despite the growing popularity of Solid State Disks (SSDs) in the datacenter, little is known about their reliability characteristics in the field. The little knowledge is mainly vendor supplied, which cannot really help understand how SSD failures can manifest and impact production systems, in order to take appropriate actions. Besides failure data, a detailed characterization requires wide spectrum of data about factors influencing SSD failures, right from provisioning (what models' where and when deployed' etc.) to the operational ones (workloads, read-write intensities, write amplification, etc.). We analyze over half a million SSDs that span multiple generations spread across several datacenters which host a wide range of workloads over nearly 3 years. By studying the diverse set of factors on SSD failures, and their symptoms, our work provides the first look at the what, when and why characteristics of SSD failures in production datacenters.},
 acmid = {2901489},
 address = {New York, NY, USA},
 author = {Narayanan, Iyswarya and Wang, Di and Jeon, Myeongjae and Sharma, Bikash and Caulfield, Laura and Sivasubramaniam, Anand and Cutler, Ben and Liu, Jie and Khessib, Badriddine and Vaid, Kushagra},
 doi = {10.1145/2964791.2901489},
 issn = {0163-5999},
 issue_date = {June 2016},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 keyword = {datacenters, reliability, solid state drives},
 link = {http://doi.acm.org/10.1145/2964791.2901489},
 month = {jun},
 number = {1},
 numpages = {2},
 pages = {407--408},
 publisher = {ACM},
 title = {SSD Failures in Datacenters: What, When and Why?},
 volume = {44},
 year = {2016}
}


@proceedings{Lin:2015:2745844,
 abstract = {Welcome to SIGMETRICS 2015. SIGMETRICS is the flagship conference of the ACM special interest group for the computer systems performance evaluation community. This year marks the forty-second anniversary since SIGMETRICS (under its prior name, SIGME) held the First National SIGME Symposium on Measurement and Evaluation in 1973. The past four decades have seen enormous changes in the field of computer science, but the importance of measurement, modeling, and performance evaluation remains as critical as ever. This year's conference includes papers on topics that have been a mainstay since the founding of our SIG, including queuing, scheduling, resource allocation, and performance measurement. Application areas that have emerged in recent years, such as data center and cloud, big data, solidstate storage, machine learning, crowdsourcing, energy optimization, continue to be represented in our program. We received 239 submissions to this year's conference, of which 32 appear in the program as full papers, which is a highly competitive acceptance ratio below 14%. An additional 24 submissions appear in the abbreviated form of poster presentations and extended abstracts. As in some prior years, we performed reviews in three rounds. In the first round, each paper was assigned to three reviewers. In the second round, papers were up for discussion among the reviewers, especially those with highly divergent review opinions. In some cases, additional reviews were necessary to ensure that every paper would receive at least three reviews. Based on this and immediately prior to the TPC meeting, papers were bucketed into "accept", "reject", and "discuss at TPC meeting" buckets. Finally, in the TPC meeting at MIT (Cambridge, MA), we entertained comments and objections on papers in the "accept" and "reject" buckets and discuss all papers in the "discuss at TPC meeting" bucket. It was no mean feat to winnow a set of nearly two hundred and fifty submissions down to a set of appropriate size for a three-day conference. We offer tremendous thanks to the 57 members of the program committee who collectively performed this daunting task. We are grateful for the support of the SIGMETRICS board during the lengthy process of selecting this year's conference program.},
 address = {New York, NY, USA},
 isbn = {978-1-4503-3486-0},
 location = {Portland, Oregon, USA},
 note = {488150},
 publisher = {ACM},
 title = {SIGMETRICS '15: Proceedings of the 2015 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems},
 year = {2015}
}


@inproceedings{Ahmed:2016:QAL:2896377.2901504,
 abstract = {Streaming video has received a lot of attention from industry and academia. In this work, we study the characteristics and challenges associated with large-scale live video delivery. Using logs from a commercial Content Delivery Network (CDN), we study live video delivery for a major entertainment event that was streamed by hundreds of thousands of viewers in North America. We analyze Quality-of-Experience (QoE) for the event and note that a significant number of users suffer QoE impairments. As a consequence of QoE impairments, these users exhibit lower engagement metrics.},
 acmid = {2901504},
 address = {New York, NY, USA},
 author = {Ahmed, Adnan and Shafiq, Zubair and Khakpour, Amir},
 booktitle = {Proceedings of the 2016 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Science},
 doi = {10.1145/2896377.2901504},
 isbn = {978-1-4503-4266-7},
 keyword = {content delivery, live video, quality of experience},
 link = {http://doi.acm.org/10.1145/2896377.2901504},
 location = {Antibes Juan-les-Pins, France},
 numpages = {2},
 pages = {395--396},
 publisher = {ACM},
 series = {SIGMETRICS '16},
 title = {QoE Analysis of a Large-Scale Live Video Streaming Event},
 year = {2016}
}


@inproceedings{Garetto:2016:GTE:2896377.2901455,
 abstract = {Bootstrap percolation is a well-known activation process in a graph, in which a node becomes active when it has at least r active neighbors. Such process, originally studied on regular structures, has been recently investigated also in the context of random graphs, where it can serve as a simple model for a wide variety of cascades, such as the spreading of ideas, trends, viral contents, etc. over large social networks. In particular, it has been shown that in G(n,p) the final active set can exhibit a phase transition for a sub-linear number of seeds. In this paper, we propose a unique framework to study similar sub-linear phase transitions for a much broader class of graph models and epidemic processes. Specifically, we consider i) a generalized version of bootstrap percolation in G(n,p) with random activation thresholds and random node-to-node influences; ii) different random graph models, including graphs with given degree sequence and graphs with community structure (block model). The common thread of our work is to show the surprising sensitivity of the critical seed set size to extreme values of distributions, which makes some systems dramatically vulnerable to large-scale outbreaks. We validate our results running simulation on both synthetic and real graphs.},
 acmid = {2901455},
 address = {New York, NY, USA},
 author = {Garetto, Michele and Leonardi, Emilio and Torrisi, Giovanni Luca},
 booktitle = {Proceedings of the 2016 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Science},
 doi = {10.1145/2896377.2901455},
 isbn = {978-1-4503-4266-7},
 keyword = {bootstrap percolation, epidemic, random graph},
 link = {http://doi.acm.org/10.1145/2896377.2901455},
 location = {Antibes Juan-les-Pins, France},
 numpages = {14},
 pages = {37--50},
 publisher = {ACM},
 series = {SIGMETRICS '16},
 title = {Generalized Threshold-Based Epidemics in Random Graphs: The Power of Extreme Values},
 year = {2016}
}


@article{Cullina:2016:IAC:2964791.2901460,
 abstract = {We consider the problem of perfectly recovering the vertex correspondence between two correlated Erdos-Renyi (ER) graphs. For a pair of correlated graphs on the same vertex set, the correspondence between the vertices can be obscured by randomly permuting the vertex labels of one of the graphs. In some cases, the structural information in the graphs allow this correspondence to be recovered. We investigate the information-theoretic threshold for exact recovery, i.e. the conditions under which the entire vertex correspondence can be correctly recovered given unbounded computational resources. Pedarsani and Grossglauser provided an achievability result of this type. Their result establishes the scaling dependence of the threshold on the number of vertices. We improve on their achievability bound. We also provide a converse bound, establishing conditions under which exact recovery is impossible. Together, these establish the scaling dependence of the threshold on the level of correlation between the two graphs. The converse and achievability bounds differ by a factor of two for sparse, significantly correlated graphs.},
 acmid = {2901460},
 address = {New York, NY, USA},
 author = {Cullina, Daniel and Kiyavash, Negar},
 doi = {10.1145/2964791.2901460},
 issn = {0163-5999},
 issue_date = {June 2016},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 keyword = {network alignment, random graphs, social networks},
 link = {http://doi.acm.org/10.1145/2964791.2901460},
 month = {jun},
 number = {1},
 numpages = {10},
 pages = {63--72},
 publisher = {ACM},
 title = {Improved Achievability and Converse Bounds for Erdos-Renyi Graph Matching},
 volume = {44},
 year = {2016}
}


@article{Fanti:2016:RSO:2964791.2901471,
 abstract = {Anonymous messaging applications have recently gained popularity as a means for sharing opinions without fear of judgment or repercussion. Messages in these applications propagate anonymously (without authorship metadata) over a network that is typically defined by social connections or physical proximity. However, recent advances in rumor source detection show that the source of such an anonymous message can be inferred by statistical inference attacks. Adaptive diffusion was recently proposed as a solution that achieves optimal source obfuscation over regular trees. However, in real social networks, node degrees differ from node to node, and adaptive diffusion can be significantly sub-optimal. This gap increases as the degrees become more irregular. In order to quantify this gap, we model the underlying network as coming from standard branching processes with i.i.d. degree distributions. Building upon the analysis techniques from branching processes, we give an analytical characterization of the dependence between the probability of detection achieved by adaptive diffusion and the degree distribution. Further, this analysis provides a key insight: passing a rumor to a friend who has many friends makes the source more ambiguous. This leads to a new family of protocols that we call Preferential Attachment Adaptive Diffusion (PAAD). When messages are propagated according to PAAD, we give both the MAP estimator for finding the source and also an analysis of the probability of detection achieved by this adversary. The analytical results are not directly comparable, since the adversary's observed information has a different distribution under adaptive diffusion than under PAAD. Instead, we present results from numerical experiments that suggest that PAAD achieves a lower probability of detection, at the cost of increased communication for coordination.},
 acmid = {2901471},
 address = {New York, NY, USA},
 author = {Fanti, Giulia and Kairouz, Peter and Oh, Sewoong and Ramchandran, Kannan and Viswanath, Pramod},
 doi = {10.1145/2964791.2901471},
 issn = {0163-5999},
 issue_date = {June 2016},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 keyword = {anonymous social media, privacy, rumor spreading},
 link = {http://doi.acm.org/10.1145/2964791.2901471},
 month = {jun},
 number = {1},
 numpages = {12},
 pages = {153--164},
 publisher = {ACM},
 title = {Rumor Source Obfuscation on Irregular Trees},
 volume = {44},
 year = {2016}
}


@article{Gabielkov:2016:SCG:2964791.2901462,
 abstract = {Online news domains increasingly rely on social media to drive traffic to their websites. Yet we know surprisingly little about how a social media conversation mentioning an online article actually generates clicks. Sharing behaviors, in contrast, have been fully or partially available and scrutinized over the years. While this has led to multiple assumptions on the diffusion of information, each assumption was designed or validated while ignoring actual clicks. We present a large scale, unbiased study of social clicks---that is also the first data of its kind---gathering a month of web visits to online resources that are located in 5 leading news domains and that are mentioned in the third largest social media by web referral (Twitter). Our dataset amounts to 2.8 million shares, together responsible for 75 billion potential views on this social media, and 9.6 million actual clicks to 59,088 unique resources. We design a reproducible methodology and carefully correct its biases. As we prove, properties of clicks impact multiple aspects of information diffusion, all previously unknown:(i) Secondary resources, that are not promoted through headlines and are responsible for the long tail of content popularity, generate more clicks both in absolute and relative terms; (ii) Social media attention is actually long-lived, in contrast with temporal evolution estimated from shares or receptions; (iii) The actual influence of an intermediary or a resource is poorly predicted by their share count, but we show how that prediction can be made more precise.},
 acmid = {2901462},
 address = {New York, NY, USA},
 author = {Gabielkov, Maksym and Ramachandran, Arthi and Chaintreau, Augustin and Legout, Arnaud},
 doi = {10.1145/2964791.2901462},
 issn = {0163-5999},
 issue_date = {June 2016},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 keyword = {news media, social clicks, social networks, twitter},
 link = {http://doi.acm.org/10.1145/2964791.2901462},
 month = {jun},
 number = {1},
 numpages = {14},
 pages = {179--192},
 publisher = {ACM},
 title = {Social Clicks: What and Who Gets Read on Twitter?},
 volume = {44},
 year = {2016}
}


@article{Liu:2016:ALF:2964791.2901474,
 abstract = {Due to the rapid growth of mobile data demands, there have been significant interests in stochastic resource control and optimization for wireless networks. Although significant advances have been made in stochastic network optimization theory, to date, most of the existing approaches are plagued by either slow convergence or unsatisfactory delay performances. To address these challenges, in this paper, we develop a new stochastic network optimization framework inspired by the Nesterov accelerated gradient method. We show that our proposed Nesterovian approach offers utility-optimality, fast-convergence, and significant delay reduction in stochastic network optimization. Our contributions in this paper are three-fold: i) we propose a Nesterovian joint congestion control and routing/scheduling framework for both single-hop and multi-hop wireless networks; ii) we establish the utility optimality and queueing stability of the proposed Nesterovian method, and analytically characterize its delay reduction and convergence speed; and iii) we show that the proposed Nesterovian approach offers a three-way performance control between utility-optimality, delay, and convergence.},
 acmid = {2901474},
 address = {New York, NY, USA},
 author = {Liu, Jia},
 doi = {10.1145/2964791.2901474},
 issn = {0163-5999},
 issue_date = {June 2016},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 keyword = {routing, scheduling, stochastic network optimization},
 link = {http://doi.acm.org/10.1145/2964791.2901474},
 month = {jun},
 number = {1},
 numpages = {14},
 pages = {221--234},
 publisher = {ACM},
 title = {Achieving Low-Delay and Fast-Convergence in Stochastic Network Optimization: A Nesterovian Approach},
 volume = {44},
 year = {2016}
}


@inproceedings{Buchnik:2016:RRG:2896377.2901458,
 abstract = {Distances in a network capture relations between nodes and are the basis of centrality, similarity, and influence measures. Often, however, the relevance of a node u to a node v is more precisely measured not by the magnitude of the distance, but by the number of nodes that are closer to v than u. That is, by the rank of u in an ordering of nodes by increasing distance from v. We identify and address fundamental challenges in rank-based graph mining. We first consider single-source computation of reverse-ranks and design a "Dijkstra-like" algorithm which computes nodes in order of increasing approximate reverse rank while only traversing edges adjacent to returned nodes. We then define reverse-rank influence, which naturally extends reverse nearest neighbors influence [Korn and Muthukrishnan 2000] and builds on a well studied distance-based influence. We present near-linear algorithms for greedy approximate reverse-rank influence maximization. The design relies on our single-source algorithm. Our algorithms utilize near-linear preprocessing of the network to compute all-distance sketches. As a contribution of independent interest, we present a novel algorithm for computing these sketches, which have many other applications, on multi-core architectures. We complement our algorithms by establishing the hardness of computing exact reverse-ranks for a single source and exact reverse-rank influence. This implies that when using near-linear algorithms, the small relative errors we obtain are the best we can currently hope for. Finally, we conduct an experimental evaluation on graphs with tens of millions of edges, demonstrating both scalability and accuracy.},
 acmid = {2901458},
 address = {New York, NY, USA},
 author = {Buchnik, Eliav and Cohen, Edith},
 booktitle = {Proceedings of the 2016 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Science},
 doi = {10.1145/2896377.2901458},
 isbn = {978-1-4503-4266-7},
 keyword = {influence maximization, reverse rank influence, reverse rank single source computation},
 link = {http://doi.acm.org/10.1145/2896377.2901458},
 location = {Antibes Juan-les-Pins, France},
 numpages = {12},
 pages = {51--62},
 publisher = {ACM},
 series = {SIGMETRICS '16},
 title = {Reverse Ranking by Graph Structure: Model and Scalable Algorithms},
 year = {2016}
}


@inproceedings{Combes:2016:MSF:2896377.2901485,
 abstract = {We investigate streaming over multiple links. We provide lower bounds on the starvation probability of any policy and simple, order-optimal policies with matching and tractable upper bounds.},
 acmid = {2901485},
 address = {New York, NY, USA},
 author = {Combes, Richard and Sidi, Habib and Elayoubi, Salah},
 booktitle = {Proceedings of the 2016 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Science},
 doi = {10.1145/2896377.2901485},
 isbn = {978-1-4503-4266-7},
 keyword = {networking, performance evaluation, streaming, video streaming, wireless networks},
 link = {http://doi.acm.org/10.1145/2896377.2901485},
 location = {Antibes Juan-les-Pins, France},
 numpages = {2},
 pages = {391--392},
 publisher = {ACM},
 series = {SIGMETRICS '16},
 title = {Multipath Streaming: Fundamental Limits and Efficient Algorithms},
 year = {2016}
}


@inproceedings{Yaniv:2016:HDC:2896377.2901456,
 abstract = {Radix page tables as implemented in the x86-64 architecture incur a penalty of four memory references for address translation upon each TLB miss. These 4 references become 24 in virtualized setups, accounting for 5%--90% of the runtime and thus motivating chip vendors to incorporate page walk caches (PWCs). Counterintuitively, an ISCA 2010 paper found that radix page tables with PWCs are superior to hashed page tables, yielding up to 5x fewer DRAM accesses per page walk. We challenge this finding and show that it is the result of comparing against a suboptimal hashed implementation---that of the Itanium architecture. We show that, when carefully optimized, hashed page tables in fact outperform existing PWC-aided x86-64 hardware, shortening benchmark runtimes by 1%--27% and 6%--32% in bare-metal and virtualized setups, without resorting to PWCs. We further show that hashed page tables are inherently more scalable than radix designs and are better suited to accommodate the ever increasing memory size; their downside is that they make it more challenging to support such features as superpages.},
 acmid = {2901456},
 address = {New York, NY, USA},
 author = {Yaniv, Idan and Tsafrir, Dan},
 booktitle = {Proceedings of the 2016 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Science},
 doi = {10.1145/2896377.2901456},
 isbn = {978-1-4503-4266-7},
 keyword = {hashed page tables, hypervisor, page table design, page walk caches, radix page tables, tlb, virtual memory},
 link = {http://doi.acm.org/10.1145/2896377.2901456},
 location = {Antibes Juan-les-Pins, France},
 numpages = {14},
 pages = {337--350},
 publisher = {ACM},
 series = {SIGMETRICS '16},
 title = {Hash, Don'T Cache (the Page Table)},
 year = {2016}
}


@article{Nguyen:2016:SSR:2964791.2901484,
 abstract = {We consider a service system where agents (or, servers) are invited on-demand. Customers arrive as a Poisson process and join a customer queue. Customer service times are i.i.d. exponential. Agents' behavior is random in two respects. First, they can be invited into the system exogenously, and join the agent queue after a random time. Second, with some probability they rejoin the agent queue after a service completion, and otherwise leave the system. The objective is to design a real-time adaptive agent invitation scheme that keeps both customer and agent queues/waiting-times small. We study an adaptive scheme, which controls the number of pending agent invitations, based on queue-state feedback. We study the system process fluid limits, in the asymptotic regime where the customer arrival rate goes to infinity. We use the machinery of switched linear systems and common quadratic Lyapunov functions to derive sufficient conditions for the local stability of fluid limits at the desired equilibrium point (with zero queues). We conjecture that, for our model, local stability is in fact sufficient for global stability of fluid limits; the validity of this conjecture is supported by numerical and simulation experiments. When the local stability conditions do hold, simulations show good overall performance of the scheme.},
 acmid = {2901484},
 address = {New York, NY, USA},
 author = {Nguyen, Lam M. and Stolyar, Alexander L.},
 doi = {10.1145/2964791.2901484},
 issn = {0163-5999},
 issue_date = {June 2016},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 keyword = {call centers, common quadratic lyapunov function, exponential stability, fluid limit, on-demand agent invitation, service systems, switched linear systems},
 link = {http://doi.acm.org/10.1145/2964791.2901484},
 month = {jun},
 number = {1},
 numpages = {2},
 pages = {365--366},
 publisher = {ACM},
 title = {A Service System with Randomly Behaving On-demand Agents},
 volume = {44},
 year = {2016}
}


@article{Li:2016:IDM:2964791.2901457,
 abstract = {Mobility support is critical to offering seamless data service to mobile devices in 3G/4G cellular networks. To accommodate policy requests by users and carriers, micro-mobility management scheme among cells (i.e., handoff) is designated to be configurable. Each cell and mobile device can configure or even customize its own handoff procedure. In this paper, we examine the handoff misconfiguration issues in 3G/4G networks. We show that they may incur handoff instability in the form of persistent loops, where the device oscillates between cells even without radio-link and location changes. Such instability is mainly triggered by uncoordinated parameter configurations and inconsistent decision logic in the hand- off procedure. It can degrade user data performance, incur excessive signaling overhead, and violate network's expected handoff goals. We derive the instability conditions, and validate them on two major US mobile carrier networks. We further design a soft- ware tool for automatic loop detection, and run it over operational networks. We discuss possible fixes to such uncoordinated configurations among devices and cells.},
 acmid = {2901457},
 address = {New York, NY, USA},
 author = {Li, Yuanjie and Deng, Haotian and Li, Jiayao and Peng, Chunyi and Lu, Songwu},
 doi = {10.1145/2964791.2901457},
 issn = {0163-5999},
 issue_date = {June 2016},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 keyword = {3G/4G mobile network, configuration, instability, mobility management},
 link = {http://doi.acm.org/10.1145/2964791.2901457},
 month = {jun},
 number = {1},
 numpages = {12},
 pages = {261--272},
 publisher = {ACM},
 title = {Instability in Distributed Mobility Management: Revisiting Configuration Management in 3G/4G Mobile Networks},
 volume = {44},
 year = {2016}
}


@article{Wang:2016:VPS:2964791.2901461,
 abstract = {We study the value of data privacy in a game-theoretic model of trading private data, where a data collector purchases private data from strategic data subjects (individuals) through an incentive mechanism. The private data of each individual represents her knowledge about an underlying state, which is the information that the data collector desires to learn. Different from most of the existing work on privacy-aware surveys, our model does not assume the data collector to be trustworthy. Then, an individual takes full control of its own data privacy and reports only a privacy-preserving version of her data. In this paper, the value of ε units of privacy is measured by the minimum payment of all nonnegative payment mechanisms, under which an individual's best response at a Nash equilibrium is to report the data with a privacy level of ε. The higher ε is, the less private the reported data is. We derive lower and upper bounds on the value of privacy which are asymptotically tight as the number of data subjects becomes large. Specifically, the lower bound assures that it is impossible to use less amount of payment to buy ε units of privacy, and the upper bound is given by an achievable payment mechanism that we designed. Based on these fundamental limits, we further derive lower and upper bounds on the minimum total payment for the data collector to achieve a given learning accuracy target, and show that the total payment of the designed mechanism is at most one individual's payment away from the minimum.},
 acmid = {2901461},
 address = {New York, NY, USA},
 author = {Wang, Weina and Ying, Lei and Zhang, Junshan},
 doi = {10.1145/2964791.2901461},
 issn = {0163-5999},
 issue_date = {June 2016},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 keyword = {differential privacy, game theory, incentive mechanism, mechanism design, strategic data subjects},
 link = {http://doi.acm.org/10.1145/2964791.2901461},
 month = {jun},
 number = {1},
 numpages = {12},
 pages = {249--260},
 publisher = {ACM},
 title = {The Value of Privacy: Strategic Data Subjects, Incentive Mechanisms and Fundamental Limits},
 volume = {44},
 year = {2016}
}


@article{Ludwig:2016:TSN:2964791.2901476,
 abstract = {Computer networks have become a critical infrastructure. Especially in shared environments such as datacenters it is important that a correct, consistent and secure network operation is guaranteed at any time, even during routing policy updates. In particular, at no point in time should it be possible for packets to bypass security critical waypoints~(such as a firewall or IDS) or to be forwarded along loops. This paper studies the problem of how to change routing policies in a transiently consistent manner. Transiently consistent network updates have been proposed as a fast and resource efficient alternative to per-packet consistent updates. Our main result is a negative one: we show that there are settings where the two basic properties waypoint enforcement and loop-freedom cannot be satisfied simultaneously. Even worse, we rigorously prove that deciding whether a waypoint enforcing, loop-free network update schedule exists is NP-hard. These results hold for both kinds of loop-freedom used in the literature: strong and relaxed loop-freedom. This paper also presents optimized, exact mixed integer programs to compute optimal update schedules. We report on extensive simulation results and initiate the discussion of scenarios where multiple waypoints need to be ensured (also known as service chains).},
 acmid = {2901476},
 address = {New York, NY, USA},
 author = {Ludwig, Arne and Dudycz, Szymon and Rost, Matthias and Schmid, Stefan},
 doi = {10.1145/2964791.2901476},
 issn = {0163-5999},
 issue_date = {June 2016},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 keyword = {NP-hardness, graph algorithms, middleboxes, scheduling, service chains, software-defined networking},
 link = {http://doi.acm.org/10.1145/2964791.2901476},
 month = {jun},
 number = {1},
 numpages = {12},
 pages = {273--284},
 publisher = {ACM},
 title = {Transiently Secure Network Updates},
 volume = {44},
 year = {2016}
}


@inproceedings{Giovanidis:2016:SMC:2896377.2901483,
 abstract = {This article introduces a novel family of decentralised caching policies for wireless networks, referred to as spatial multi-LRU. Based on these, cache inventories are updated in a way that provides content diversity to users that are covered by, and thus have access to, more than one station. Two variations are proposed, the multi-LRU-One and -All, which differ in the number of replicas inserted in the involved edge caches. Che-like approximations are proposed to accurately predict their hit probability under the Independent Reference Model (IRM). For IRM traffic multi-LRU-One outperforms multi-LRU-All, whereas when the traffic exhibits temporal locality the -All variation can perform better.},
 acmid = {2901483},
 address = {New York, NY, USA},
 author = {Giovanidis, Anastasios and Avranas, Apostolos},
 booktitle = {Proceedings of the 2016 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Science},
 doi = {10.1145/2896377.2901483},
 isbn = {978-1-4503-4266-7},
 keyword = {caching, che, lru, multi-coverage areas, point processes},
 link = {http://doi.acm.org/10.1145/2896377.2901483},
 location = {Antibes Juan-les-Pins, France},
 numpages = {3},
 pages = {403--405},
 publisher = {ACM},
 series = {SIGMETRICS '16},
 title = {Spatial Multi-LRU Caching for Wireless Networks with Coverage Overlaps},
 year = {2016}
}


@inproceedings{Poloczek:2016:CER:2896377.2901499,
 abstract = {Task replication has recently been advocated as a practical solution to reduce latencies in parallel systems. In addition to several convincing empirical studies, analytical results have been provided, yet under some strong assumptions such as independent service times of the replicas, which may lend themselves to some contrasting and perhaps contriving behavior. For instance, under the independence assumption, an overloaded system can be stabilized by a replication factor, but can be sent back in overload through further replication. Motivated by the need to dispense with such common and restricting assumptions, which may cause unexpected behavior, we develop a unified and general theoretical framework to compute tight bounds on the distribution of response times in general replication systems. These results immediately lend themselves to the optimal number of replicas minimizing response time quantiles, depending on the parameters of the system (e.g., the degree of correlation amongst replicas).},
 acmid = {2901499},
 address = {New York, NY, USA},
 author = {Poloczek, Felix and Ciucu, Florin},
 booktitle = {Proceedings of the 2016 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Science},
 doi = {10.1145/2896377.2901499},
 isbn = {978-1-4503-4266-7},
 keyword = {martingales, queueing theory, replication},
 link = {http://doi.acm.org/10.1145/2896377.2901499},
 location = {Antibes Juan-les-Pins, France},
 numpages = {2},
 pages = {375--376},
 publisher = {ACM},
 series = {SIGMETRICS '16},
 title = {Contrasting Effects of Replication in Parallel Systems: From Overload to Underload and Back},
 year = {2016}
}


@article{Qureshi:2016:ATL:2964791.2901505,
 abstract = {Cellular networks are constantly evolving due to frequent changes in radio access and end user equipment technologies, applications, and traffic. Network upgrades should be performed with extreme caution since millions of users heavily depend on the cellular networks. Before upgrading the entire network, it is important to conduct field evaluation of upgrades.The choice and number of field test locations have significant impact on the time-to-market and confidence in how well various network upgrades will work out in the rest of the network. We propose a novel approach -- Reflection to automatically determine where to conduct the upgrade field tests to accurately identify important features that affect the upgrade and predict for the performance of untested locations. We demonstrate its effectiveness using real traces collected from a major US cellular network as well as synthetic traces.},
 acmid = {2901505},
 address = {New York, NY, USA},
 author = {Qureshi, Mubashir Adnan and Mahimkar, Ajay and Qiu, Lili and Ge, Zihui and Puthenpura, Sarat and Mir, Nabeel and Ahuja, Sanjeev},
 doi = {10.1145/2964791.2901505},
 issn = {0163-5999},
 issue_date = {June 2016},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 keyword = {cellular network, diagnosis, test planning},
 link = {http://doi.acm.org/10.1145/2964791.2901505},
 month = {jun},
 number = {1},
 numpages = {2},
 pages = {371--372},
 publisher = {ACM},
 title = {Automated Test Location Selection For Cellular Network Upgrades},
 volume = {44},
 year = {2016}
}


@article{Liu:2016:CCR:2964791.2901498,
 abstract = {DRAM memory is suffering increasingly aggravating refresh penalty, which no longer causes trivial performance degradation and power consumption. As memory capacity increases, refresh penalty has become increasingly worse as more rows have to be refreshed. In this work, we propose a simple, practical, and effective refresh approach called CAR (Compression-Aware Refresh) to efficiently mitigate refresh overheads. We apply data compression technique to store data in compressed format so that data blocks which are originally distributed across all the constituent chips of a rank only need to be stored in a subset of those chips, leaving banks in the remaining chips not fully occupied. As a result, the memory controller can safely skip refreshing memory rows which contain no useful data without compromising data integrity. Such a compression-aware refresh scheme can result in significant refresh savings and thus improve overall memory performance and energy efficiency. Moreover, to further take advantage of data compression, we adopt the rank subsetting technique to enable accesses to only those occupied chips for memory requests accessing compressed data blocks. Evaluations using benchmarks from SPEC CPU 2006 and the PARSEC 3.0 on the recent DDR4 memory systems have shown that CAR can achieve up to 1.66x performance improvement (11.7% on average).},
 acmid = {2901498},
 address = {New York, NY, USA},
 author = {Liu, Wenjie and Huang, Ping and Tang, Kun and Zhou, Ke and He, Xubin},
 doi = {10.1145/2964791.2901498},
 issn = {0163-5999},
 issue_date = {June 2016},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 keyword = {data compression, memory architecture},
 link = {http://doi.acm.org/10.1145/2964791.2901498},
 month = {jun},
 number = {1},
 numpages = {2},
 pages = {373--374},
 publisher = {ACM},
 title = {CAR: A Compression-Aware Refresh Approach to Improve Memory Performance and Energy Efficiency},
 volume = {44},
 year = {2016}
}


@inproceedings{Shafaei:2016:MSD:2896377.2901496,
 abstract = {
                  An abstract is not available.
              },
 acmid = {2901496},
 address = {New York, NY, USA},
 author = {Shafaei, Mansour and Hajkazemi, Mohammad Hossein and Desnoyers, Peter and Aghayev, Abutalib},
 booktitle = {Proceedings of the 2016 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Science},
 doi = {10.1145/2896377.2901496},
 isbn = {978-1-4503-4266-7},
 keyword = {hard disk drive, performance modeling, shingled magnetic recording},
 link = {http://doi.acm.org/10.1145/2896377.2901496},
 location = {Antibes Juan-les-Pins, France},
 numpages = {2},
 pages = {389--390},
 publisher = {ACM},
 series = {SIGMETRICS '16},
 title = {Modeling SMR Drive Performance},
 year = {2016}
}


@article{Jacquet:2016:BMT:2964791.2901899,
 abstract = {Mankind has never been connected as it is now and as it will be tomorrow. Nowadays thanks to the rise of social networks such as Tweeter and Facebook, we can follow in real time the thought of millions of people. In fact we can almost feel the thoughts of a whole humanity and maybe project ourselves in a position where we could predict the major trends in the collective behavior of this humanity. However such an ambitious aim would require considerable resources in processing and networking which may be far from affordable. Indeed trends and topics are carried in a multiple of small texts written in various language and vocabularies like an hologram carries information in a dispersed way. Their capture and classification pose serious problems of data mining and analytics. Processes based on pure semantic analysis would require too much processing power and memory. We will present alternative methods based on string complexity also inspired on geolocalization in wireless networks which saves processing power by several order of magnitude. The ultimate goal is to detect when people are thinking about the very same topics before they become aware. Beyond the problem of topic detection and classification one must also estimate the potential of an isolated topic to become a lasting trend. In other word one must probe the topic foundations, for example by challenging how trustworthy are its sources. Designing an efficient source finder algorithm is indissociable with building realistic models about topic propagation. If we suppose that topics propagate inside communities via the followers-followees links, the propagation is highly amplified by the unbalances in the graph topology. It is established that dominating and semi dominating nodes such as the CNN Tweeter site are the main accelerator of topic propagation. The difficulty is to find the actual source of a topic beyond those screening nodes and the search is prone to false positive and true negative effects. In fact we will show that finding a source of topic is similar to finding a common ancestor in a Darwin channel where spurious mutations complicate the task.},
 acmid = {2901899},
 address = {New York, NY, USA},
 author = {Jacquet, Philippe},
 doi = {10.1145/2964791.2901899},
 issn = {0163-5999},
 issue_date = {June 2016},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 keyword = {social networks, string combinatorics, topic detection, topic propagation},
 link = {http://doi.acm.org/10.1145/2964791.2901899},
 month = {jun},
 number = {1},
 numpages = {1},
 pages = {125--125},
 publisher = {ACM},
 title = {Breathing Mankind Thoughts},
 volume = {44},
 year = {2016}
}


@inproceedings{Gamarnik:2016:DMM:2896377.2901478,
 abstract = {We consider the following distributed service model: jobs with unit mean, exponentially distributed, and independent processing times arrive as a Poisson process of rate λ N, with 0<λ<1, and are immediately dispatched to one of several queues associated with N identical servers with unit processing rate. We assume that the dispatching decisions are made by a central dispatcher endowed with a finite memory, and with the ability to exchange messages with the servers. We study the fundamental resource requirements (memory bits and message exchange rate), in order to drive the expected steady-state queueing delay of a typical job to zero, as N increases. We propose a certain policy and establish (using a fluid limit approach) that it drives the delay to zero when either (i) the message rate grows superlinearly with N, or (ii) the memory grows superlogarithmically with N. Moreover, we show that any policy that has a certain symmetry property, and for which neither condition (i) or (ii) holds, results in an expected queueing delay which is bounded away from zero. Finally, using the fluid limit approach once more, we show that for any given α>0 (no matter how small), if the policy only uses a linear message rate α N, the resulting asymptotic (as N->∞) expected queueing delay is positive but upper bounded, uniformly over all λ>1. This is a significant improvement over the popular "power-of-d-choices" policy, which has a limiting expected delay that grows as log ←(1/(1-λ)→) when λ↑ 1.},
 acmid = {2901478},
 address = {New York, NY, USA},
 author = {Gamarnik, David and Tsitsiklis, John N. and Zubeldia, Martin},
 booktitle = {Proceedings of the 2016 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Science},
 doi = {10.1145/2896377.2901478},
 isbn = {978-1-4503-4266-7},
 keyword = {dispatching policies, distributed service, performance tradeoffs},
 link = {http://doi.acm.org/10.1145/2896377.2901478},
 location = {Antibes Juan-les-Pins, France},
 numpages = {12},
 pages = {1--12},
 publisher = {ACM},
 series = {SIGMETRICS '16},
 title = {Delay, Memory, and Messaging Tradeoffs in Distributed Service Systems},
 year = {2016}
}


@article{Maguluri:2016:OHQ:2964791.2901466,
 abstract = {We consider an input queued switch operating under the MaxWeight scheduling algorithm. This system is interesting to study because it is a model for Internet routers and data center networks. Recently, it was shown that the MaxWeight algorithm has optimal heavy-traffic queue length scaling when all ports are uniformly saturated. Here we consider the case where a fraction of the ports are saturated and others are not (which we call the incompletely saturated case), and also the case where the rates at which the ports are saturated can be different. We use a recently developed drift technique to show that the heavy-traffic queue length under the MaxWeight scheduling algorithm has optimal scaling with respect to the switch size even in these cases.},
 acmid = {2901466},
 address = {New York, NY, USA},
 author = {Maguluri, Siva Theja and Burle, Sai Kiran and Srikant, R.},
 doi = {10.1145/2964791.2901466},
 issn = {0163-5999},
 issue_date = {June 2016},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 keyword = {drift method, heavy traffic optimality, nxn switch, performance analysis},
 link = {http://doi.acm.org/10.1145/2964791.2901466},
 month = {jun},
 number = {1},
 numpages = {12},
 pages = {13--24},
 publisher = {ACM},
 title = {Optimal Heavy-Traffic Queue Length Scaling in an Incompletely Saturated Switch},
 volume = {44},
 year = {2016}
}


@inproceedings{Zheng:2016:VCV:2896377.2901452,
 abstract = {Cloud service providers (CSPs) often face highly dynamic user demands for their resources, which can make it difficult for them to maintain consistent quality-of-service. Some CSPs try to stabilize user demands by offering sustained-use discounts to jobs that consume more instance-hours per month. These discounts present an opportunity for users to pool their usage together into a single ``job.'' In this paper, we examine the viability of a middleman, the cloud virtual service provider (CVSP), that rents cloud resources from a CSP and then resells them to users. We show that the CVSP's business model is only viable if the average job runtimes and thresholds for sustained-use discounts are sufficiently small; otherwise, the CVSP cannot simultaneously maintain low job waiting times while qualifying for a sustained-use discount. We quantify these viability conditions by modeling the CVSP's job scheduling and then use this model to derive users' utility-maximizing demands and the CVSP's profit-maximizing price, as well as the optimal number of instances that the CVSP should rent from the CSP. We verify our results on a one-month trace from Google's production compute cluster, through which we first validate our assumptions on the job arrival and runtime distributions, and then show that the CVSP is viable under these workload traces. Indeed, the CVSP can earn a positive profit without significantly impacting the CSP's revenue, indicating that the CSP and CVSP can coexist in the cloud market.},
 acmid = {2901452},
 address = {New York, NY, USA},
 author = {Zheng, Liang and Joe-Wong, Carlee and Brinton, Christopher G. and Tan, Chee Wei and Ha, Sangtae and Chiang, Mung},
 booktitle = {Proceedings of the 2016 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Science},
 doi = {10.1145/2896377.2901452},
 isbn = {978-1-4503-4266-7},
 keyword = {cloud pricing, economic viability, virtual service provider},
 link = {http://doi.acm.org/10.1145/2896377.2901452},
 location = {Antibes Juan-les-Pins, France},
 numpages = {14},
 pages = {235--248},
 publisher = {ACM},
 series = {SIGMETRICS '16},
 title = {On the Viability of a Cloud Virtual Service Provider},
 year = {2016}
}


@article{Mukhopadhyay:2016:MRB:2964791.2901488,
 abstract = {In this paper, we investigate the impact of majority-rule based random interactions among agents in a large social network on the diffusion of opinions in the network. Opinion of each agent is assumed to be a binary variable taking values in the set {0, 1}. Interactions among agents are modeled using the majority rule, where each agent updates its opinion at random instants by adopting the 'majority' opinion among a group of randomly sampled agents. We investigate two scenarios that respectively incorporate `bias' of the agents towards a specific opinion and stubbornness of some of the agents in the majority rule dynamics. For the first scenario, where all the agents are assumed to be 'biased' towards one of the opinions, it is shown that the agents reach a consensus on the preferred opinion (with high probability) only if the initial fraction of agents having the preferred opinion is above a certain threshold. Furthermore, the mean time taken to reach the consensus is shown to be logarithmic in the network size. In the second scenario, where the presence of 'stubborn' agents, who never update their opinions, is assumed, we characterize the equilibrium distribution of opinions of the non-stubborn agents using mean field techniques. The mean field limit is shown to have multiple stable equilibrium points which leads to a phenomenon known as metastability.},
 acmid = {2901488},
 address = {New York, NY, USA},
 author = {Mukhopadhyay, Arpan and Mazumdar, Ravi R. and Roy, Rahul},
 doi = {10.1145/2964791.2901488},
 issn = {0163-5999},
 issue_date = {June 2016},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 keyword = {majority rule, mean field, metastability, opinion dynamics},
 link = {http://doi.acm.org/10.1145/2964791.2901488},
 month = {jun},
 number = {1},
 numpages = {2},
 pages = {385--386},
 publisher = {ACM},
 title = {Majority Rule Based Opinion Dynamics with Biased and Stubborn Agents},
 volume = {44},
 year = {2016}
}


@article{Wang:2016:TBA:2964791.2901492,
 abstract = {Providing fairness and system efficiency are important, often conflicting, requirements when allocating shared resources. In a hybrid storage system the problem is complicated by the high variability in request service times, caused by speed differences between heterogeneous devices and workload-specific variations in access time within a device. This paper describes a model for fair time-based resource allocation in a hybrid storage system that has better fairness and efficiency than traditional IOPS-based schemes. An analytical model is developed and an optimal algorithm for fairly allocating device times to applications while maximizing the system throughput or utilization is presented. The results are validated using Linux implementations.},
 acmid = {2901492},
 address = {New York, NY, USA},
 author = {Wang, Hui and Varman, Peter},
 doi = {10.1145/2964791.2901492},
 issn = {0163-5999},
 issue_date = {June 2016},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 keyword = {fairness, heterogeneous storage, quality of services (QoS), resource allocation, system utilization},
 link = {http://doi.acm.org/10.1145/2964791.2901492},
 month = {jun},
 number = {1},
 numpages = {3},
 pages = {411--413},
 publisher = {ACM},
 title = {Time-Based Bandwidth Allocation for Heterogeneous Storage},
 volume = {44},
 year = {2016}
}


@proceedings{Alouf:2016:2896377,
 abstract = {It is our pleasure to welcome you to ACM SIGMETRICS / IFIP Performance 2016, the 13th Joint International Conference on Measurement and Modeling of Computer Systems. This joint gathering occurs every three years, bringing together the flagship conferences of ACM SIGMETRICS and the IFIP Working Group 7.3 into a forum that attracts top quality papers in performance evaluation. This year's program includes papers on topics that have been the foundation of our communities, including queueing, network resource allocation, and performance measurement. As we have seen over the last couple of years, some topics in particular have come to the forefront, such as graph theory and social networks, learning, energy optimization, memory systems, and network economics. We received 208 submissions, of which 28 were accepted into the program as full papers, a highly competitive acceptance rate of 13.5%. Additionally, 24 papers were accepted as poster presentations, and appear in abbreviated form in the proceedings. The paper review process was as in previous years, starting with a first round where each paper was assigned to three reviewers. An online discussion period followed where some papers received up to two additional reviews. The Program Committee then met in person on Feb 13, 2016 at Columbia University in New York. After an entire day of intense deliberation, the committee selected 28 papers to be included as full papers in the program.},
 address = {New York, NY, USA},
 isbn = {978-1-4503-4266-7},
 location = {Antibes Juan-les-Pins, France},
 publisher = {ACM},
 title = {SIGMETRICS '16: Proceedings of the 2016 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Science},
 year = {2016}
}


@article{ReddyvariRaja:2016:MFE:2964791.2901495,
 abstract = {We model an Internet marketplace using a set of servers that choose prices for performing jobs. Each server has a queue of unfinished jobs, and is penalized for delay by the market maker via a holding cost. A server completes jobs with a low or high "quality", and jobs truthfully report the quality with which they were completed. The best estimate of quality based on these reports is the "reputation" of the server. A server bases its pricing decision on the distribution of its competitors offered prices and reputations. An entering job is given a random sample of servers, and chooses the best one based on a linear combination of price and reputation. We seek to understand how prices would be determined in such a marketplace using the theory of Mean Field Games. We show the existence of a Mean Field Equilibrium and show how reputation plays a role in allowing servers to declare larger prices than their competitors. We illustrate our results by a numerical study of the system via simulation with parameters chosen from data gathered from existing Internet marketplaces.},
 acmid = {2901495},
 address = {New York, NY, USA},
 author = {Reddyvari Raja, Vamseedhar and Ramaswamy, Vinod and Shakkottai, Srinivas and Subramanian, Vijay},
 doi = {10.1145/2964791.2901495},
 issn = {0163-5999},
 issue_date = {June 2016},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 keyword = {mean field games, pricing, super market},
 link = {http://doi.acm.org/10.1145/2964791.2901495},
 month = {jun},
 number = {1},
 numpages = {2},
 pages = {387--388},
 publisher = {ACM},
 title = {Mean Field Equilibria of Pricing Games in Internet Marketplaces},
 volume = {44},
 year = {2016}
}


@article{Novakovic:2016:ALI:2964791.2901501,
 abstract = {Despite the natural parallelism across lookups, performance of distributed key-value stores is often limited due to load imbalance induced by heavy skew in the popularity distribution of the dataset. To avoid violating service level objectives expressed in terms of tail latency, systems tend to keep server utilization low and organize the data in micro-shards, which in turn provides units of migration and replication for the purpose of load balancing. These techniques reduce the skew, but incur additional monitoring, data replication and consistency maintenance overheads. This work shows that the trend towards extreme scale-out will further exacerbate the skew-induced load imbalance, and hence the overhead of migration and replication.},
 acmid = {2901501},
 address = {New York, NY, USA},
 author = {Novakovic, Stanko and Daglis, Alexandros and Bugnion, Edouard and Falsafi, Babak and Grot, Boris},
 doi = {10.1145/2964791.2901501},
 issn = {0163-5999},
 issue_date = {June 2016},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 keyword = {load imbalance, replication},
 link = {http://doi.acm.org/10.1145/2964791.2901501},
 month = {jun},
 number = {1},
 numpages = {2},
 pages = {367--368},
 publisher = {ACM},
 title = {An Analysis of Load Imbalance in Scale-out Data Serving},
 volume = {44},
 year = {2016}
}


@article{VanHoudt:2016:EBR:2964791.2901482,
 abstract = {CSMA/CA networks have often been analyzed using a stylized model that is fully characterized by a vector of back-off rates and a conflict graph. We present an explicit formula for the unique vector of back-off rate needed to achieve any achievable throughput vector provided that the network has a chordal conflict graph. These back-off rates are such that the back-off rate of a node only depends on its own target throughput and the target throughput of its neighbors and can be determined in a distributed manner. We also introduce a distributed chordal approximation algorithm for general conflict graphs which is shown (using numerical examples) to be more accurate than the Bethe approximation.},
 acmid = {2901482},
 address = {New York, NY, USA},
 author = {Van Houdt, Benny},
 doi = {10.1145/2964791.2901482},
 issn = {0163-5999},
 issue_date = {June 2016},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 keyword = {chordal, conflict graph, csma/ca},
 link = {http://doi.acm.org/10.1145/2964791.2901482},
 month = {jun},
 number = {1},
 numpages = {3},
 pages = {377--379},
 publisher = {ACM},
 title = {Explicit Back-off Rates for Achieving Target Throughputs in CSMA/CA Networks},
 volume = {44},
 year = {2016}
}


@article{Bresler:2016:CFL:2964791.2901469,
 abstract = {There is much empirical evidence that item-item collaborative filtering works well in practice. Motivated to understand this, we provide a framework to design and analyze various recommendation algorithms. The setup amounts to online binary matrix completion, where at each time a random user requests a recommendation and the algorithm chooses an entry to reveal in the user's row. The goal is to minimize regret, or equivalently to maximize the number of +1 entries revealed at any time. We analyze an item-item collaborative filtering algorithm that can achieve fundamentally better performance compared to user-user collaborative filtering. The algorithm achieves good "cold-start" performance (appropriately defined) by quickly making good recommendations to new users about whom there is little information.},
 acmid = {2901469},
 address = {New York, NY, USA},
 author = {Bresler, Guy and Shah, Devavrat and Voloch, Luis Filipe},
 doi = {10.1145/2964791.2901469},
 issn = {0163-5999},
 issue_date = {June 2016},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 keyword = {clustering, collaborative filtering, doubling dimension, online decision making},
 link = {http://doi.acm.org/10.1145/2964791.2901469},
 month = {jun},
 number = {1},
 numpages = {14},
 pages = {207--220},
 publisher = {ACM},
 title = {Collaborative Filtering with Low Regret},
 volume = {44},
 year = {2016}
}


@article{Zeng:2016:NSC:2964791.2901470,
 abstract = {Due to emerging applications such as cloud computing and big data analytics, modern information processing systems are growing increasingly large and complex. A critical issue concerns the throughput performance as the system grows in size. This paper models distributed information processing systems as fork and join queueing networks with blocking. We identify necessary and sufficient conditions for throughput scalability of such fork and join networks as they grow in size. Previous studies have either focused on special structured networks such as tandem or tree networks, or provided only necessary conditions for throughput scalability. In this paper, we show that such necessary conditions are not sufficient. We present a key topological concept called ``minimum level" of the underlying graph, and develop lower and upper bounds for the throughput of arbitrary FJQN/Bs. The bounds depend on network degree, minimum level, deterministic cycle time, buffer sizes, and service time distributions, but not on network size. We show that level-boundedness and degree-boundedness are necessary and sufficient conditions to guarantee that the throughput of an FJQN/B is bounded away from zero as network size goes to infinity.},
 acmid = {2901470},
 address = {New York, NY, USA},
 author = {Zeng, Yun and Chaintreau, Augustin and Towsley, Don and Xia, Cathy H.},
 doi = {10.1145/2964791.2901470},
 issn = {0163-5999},
 issue_date = {June 2016},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 keyword = {fork/join, performance bound, queueing network, scalability, throughput},
 link = {http://doi.acm.org/10.1145/2964791.2901470},
 month = {jun},
 number = {1},
 numpages = {12},
 pages = {25--36},
 publisher = {ACM},
 title = {A Necessary and Sufficient Condition for Throughput Scalability of Fork and Join Networks with Blocking},
 volume = {44},
 year = {2016}
}


@inproceedings{Liu:2016:FDR:2896377.2901481,
 abstract = {In this work, we study a challenging research problem that arises in minimizing the cost of storing customer data online for reliable accesses in a cloud. It is how to near-perfectly balance the remaining capacities of all disks across the cloud system while adding new file blocks so that the inevitable event of capacity expansion can be postponed as much as possible. The challenges of solving this problem are twofold. First, new file blocks are added to the cloud concurrently by many dispatchers (computing servers) that have no communication or coordination among themselves. Though each dispatcher is updated with information on disk occupancies, the update is infrequent and not synchronized. Second, for fault-tolerance purposes, a combinatorial constraint has to be satisfied in distributing the blocks of each new file across the cloud system. We propose a randomized algorithm, in which each dispatcher independently samples a blocks-to-disks assignment according to a probability distribution on a set of assignments conforming to the aforementioned combinatorial requirement. We show that this algorithm allows a cloud system to near-perfectly balance the remaining disk capacities as rapidly as theoretically possible, when starting from any unbalanced state that is correctable mathematically.},
 acmid = {2901481},
 address = {New York, NY, USA},
 author = {Liu, Liang and Wang, Yating and Fortnow, Lance and Li, Jin and Xu, Jun},
 booktitle = {Proceedings of the 2016 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Science},
 doi = {10.1145/2896377.2901481},
 isbn = {978-1-4503-4266-7},
 keyword = {birkhoff decomposition, capacity distribution, load balance, load distribution},
 link = {http://doi.acm.org/10.1145/2896377.2901481},
 location = {Antibes Juan-les-Pins, France},
 numpages = {2},
 pages = {381--382},
 publisher = {ACM},
 series = {SIGMETRICS '16},
 title = {Freestyle Dancing: Randomized Algorithms for Dynamic Storage Load-Balancing},
 year = {2016}
}


@inproceedings{Wang:2016:TMF:2896377.2901493,
 abstract = {Multi-resource fair schedulers have been widely implemented in compute clusters to provide service isolation guarantees. Existing multi-resource sharing policies, notably Dominant Resource Fairness (DRF) and its variants, are designed for unconstrained jobs that can run on all machines in a cluster. However, an increasing number of datacenter jobs specify placement constraints and can only run on a particular class of machines meeting specific hardware/software requirements (e.g., GPUs or a particular kernel version). We show that directly extending existing policies to constrained jobs either compromises isolation guarantees or allows users to gain more resources by deceiving the scheduler. It remains unclear how multi-resource fair sharing is defined and achieved in the presence of placement constraints. We address this open problem by a new sharing policy, called Task Share Fairness (TSF), that provides provable isolation guarantees and is strategy-proof against gaming the allocation policy. TSF is shown to be envy-free and Pareto optimal as well.},
 acmid = {2901493},
 address = {New York, NY, USA},
 author = {Wang, Wei and Li, Baochun and Liang, Ben and Li, Jun},
 booktitle = {Proceedings of the 2016 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Science},
 doi = {10.1145/2896377.2901493},
 isbn = {978-1-4503-4266-7},
 keyword = {datacenter, job scheduling, multi-resource fairness, placement constraint},
 link = {http://doi.acm.org/10.1145/2896377.2901493},
 location = {Antibes Juan-les-Pins, France},
 numpages = {2},
 pages = {415--416},
 publisher = {ACM},
 series = {SIGMETRICS '16},
 title = {Towards Multi-Resource Fair Allocation with Placement Constraints},
 year = {2016}
}


