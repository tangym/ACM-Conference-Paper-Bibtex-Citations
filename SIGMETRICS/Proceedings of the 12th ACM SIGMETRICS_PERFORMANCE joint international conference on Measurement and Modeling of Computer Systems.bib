@inproceedings{Han:2012:BPB:2254756.2254768,
 abstract = {We conduct comprehensive measurements on the current practice of content bundling to understand the structural patterns of torrents and the participant behaviors of swarms on one of the largest BitTorrent portals: The Pirate Bay. From the datasets of the 120K torrents and 14.8M peers, we investigate what constitutes torrents and how users participate in swarms from the perspective of bundling, across different content categories: Movie, TV, Porn, Music, Application, Game and E-book. In particular, we focus on: (1) how prevalent content bundling is, (2) how and what files are bundled into torrents, (3) what motivates publishers to bundle files, and (4) how peers access the bundled files. We find that over 72% of BitTorrent torrents contain multiple files, which indicates that bundling is widely used for file sharing. We reveal that profit-driven BitTorrent publishers who promote their own web sites for financial gains like advertising tend to prefer to use the bundling. We also observe that most files (94%) in a bundle torrent are selected by users and the bundle torrents are more popular than the single (or non-bundle) ones on average. Overall, there are notable differences in the structural patterns of torrents and swarm characteristics (i) across different content categories and (ii) between single and bundle torrents.},
 acmid = {2254768},
 address = {New York, NY, USA},
 author = {Han, Jinyoung and Kim, Seungbae and Chung, Taejoong and Kwon, Ted Taekyoung and Kim, Hyun-chul and Choi, Yanghee},
 booktitle = {Proceedings of the 12th ACM SIGMETRICS/PERFORMANCE Joint International Conference on Measurement and Modeling of Computer Systems},
 doi = {10.1145/2254756.2254768},
 isbn = {978-1-4503-1097-0},
 keyword = {BitTorrent, content bundling, peer-to-peer},
 link = {http://doi.acm.org/10.1145/2254756.2254768},
 location = {London, England, UK},
 numpages = {12},
 pages = {77--88},
 publisher = {ACM},
 series = {SIGMETRICS '12},
 title = {Bundling Practice in BitTorrent: What, How, and Why},
 year = {2012}
}


@article{Sommers:2012:CMC:2318857.2254805,
 abstract = {Cellular and 802.11 WiFi offer two compelling connectivity options for mobile users. The goal of our work is to better understand performance characteristics of these technologies in diverse environments and conditions. To that end, we compare and contrast cellular and Wifi performance using crowd-sourced data from speedtest.net. We consider spatio-temporal performance aspects (e.g., upload and download throughput and latency) using over 3 million user-initiated tests initiated in 15 different metro areas, collected over 15 weeks. In these preliminary results, we find that WiFi performance generally exceeds cellular performance, and that observed characteristics are highly variable across different locations and times of day. We also observe diverse performance characteristics resulting from the rollout of new cell access technologies and service differences among local providers.},
 acmid = {2254805},
 address = {New York, NY, USA},
 author = {Sommers, Joel and Barford, Paul},
 doi = {10.1145/2318857.2254805},
 issn = {0163-5999},
 issue_date = {June 2012},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 keyword = {cellular, latency, throughput, wifi},
 link = {http://doi.acm.org/10.1145/2318857.2254805},
 month = {jun},
 number = {1},
 numpages = {2},
 pages = {385--386},
 publisher = {ACM},
 title = {Comparing Metro-area Cellular and WiFi Performance: Extended Abstract},
 volume = {40},
 year = {2012}
}


@article{Lim:2012:DQM:2318857.2254790,
 abstract = {Scheduling multiple jobs onto a platform enhances system utilization by sharing resources. The benefits from higher resource utilization include reduced cost to construct, operate, and maintain a system, which often include energy consumption. Maximizing these benefits, while satisfying performance limits, comes at a price -- resource contention among jobs increases job completion time. In this paper, we analyze slow-downs of jobs due to contention for multiple resources in a system; referred to as dilation factor. We observe that multiple-resource contention creates non-linear dilation factors of jobs. From this observation, we establish a general quantitative model for dilation factors of jobs in multi-resource systems. A job is characterized by a vector-valued loading statistics and dilation factors of a job set are given by a quadratic function of their loading vectors. We demonstrate how to systematically characterize a job, maintain the data structure to calculate the dilation factor (loading matrix), and calculate the dilation factor of each job. We validated the accuracy of the model with multiple processes running on a native Linux server, virtualized servers, and with multiple MapReduce workloads co-scheduled in a cluster. Evaluation with measured data shows that the D-factor model has an error margin of less than 16%. We also show that the model can be integrated with an existing on-line scheduler to minimize the makespan of workloads.},
 acmid = {2254790},
 address = {New York, NY, USA},
 author = {Lim, Seung-Hwan and Huh, Jae-Seok and Kim, Youngjae and Shipman, Galen M. and Das, Chita R.},
 doi = {10.1145/2318857.2254790},
 issn = {0163-5999},
 issue_date = {June 2012},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 keyword = {application running time, cloud computing, performance modeling, shared resource management},
 link = {http://doi.acm.org/10.1145/2318857.2254790},
 month = {jun},
 number = {1},
 numpages = {12},
 pages = {271--282},
 publisher = {ACM},
 title = {D-factor: A Quantitative Model of Application Slow-down in Multi-resource Shared Systems},
 volume = {40},
 year = {2012}
}


@article{Lee:2012:SAM:2318857.2254808,
 abstract = {Latency has become an important metric for network monitoring since the emergence of new latency-sensitive applications (e.g., algorithmic trading and high-performance computing). In this paper, to provide latency measurements at both finer (e.g., packet) as well as flexible (e.g., flow subsets) levels of granularity, we propose an architecture called MAPLE that essentially stores packet-level latencies in routers and allows network operators to query the latency of arbitrary traffic sub-populations. MAPLE is built using a scalable data structure called SVBF with small storage needs.},
 acmid = {2254808},
 address = {New York, NY, USA},
 author = {Lee, Myungjin and Duffield, Nick and Kompella, Ramana Rao},
 doi = {10.1145/2318857.2254808},
 issn = {0163-5999},
 issue_date = {June 2012},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 keyword = {bloom filter, data structures, latency},
 link = {http://doi.acm.org/10.1145/2318857.2254808},
 month = {jun},
 number = {1},
 numpages = {2},
 pages = {391--392},
 publisher = {ACM},
 title = {A Scalable Architecture for Maintaining Packet Latency Measurements},
 volume = {40},
 year = {2012}
}


@article{Narayana:2012:DWT:2318857.2254817,
 abstract = {The performance of interactive cloud services depends heavily on which data centers handle client requests, and which wide-area paths carry traffic. While making these decisions, cloud service providers also need to weigh operational considerations like electricity and bandwidth costs, and balancing server loads across replicas. We argue that selecting data centers and network routes independently, as is common in today's services, can lead to much lower performance or higher costs than a coordinated decision. However, fine-grained joint control of two large distributed systems---e.g., DNS-based replica-mapping and data center multi-homed routing---can be administratively challenging. In this paper, we introduce the design of a system that jointly optimizes replica-mapping and multi-homed routing, while retaining the functional separation that exists between them today. We show how to construct a provably optimal distributed solution implemented through local computations and message exchanges between the mapping and routing systems.},
 acmid = {2254817},
 address = {New York, NY, USA},
 author = {Narayana, Srinivas and Jiang, Joe Wenjie and Rexford, Jennifer and Chiang, Mung},
 doi = {10.1145/2318857.2254817},
 issn = {0163-5999},
 issue_date = {June 2012},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 keyword = {joint optimization, multi-homed routing, request mapping},
 link = {http://doi.acm.org/10.1145/2318857.2254817},
 month = {jun},
 number = {1},
 numpages = {2},
 pages = {409--410},
 publisher = {ACM},
 title = {Distributed Wide-area Traffic Management for Cloud Services},
 volume = {40},
 year = {2012}
}


@inproceedings{Laner:2012:MRN:2254756.2254809,
 abstract = {A continuous challenge in the field of network traffic modeling is to map recorded traffic onto parameters of random processes, in order to enable simulations of the respective traffic. A key element thereof is a convenient model which is simple, yet, captures the most relevant statistics. This work aims to find such a model which, more precisely, enables the generation of multiple random processes with arbitrary but jointly characterized distributions, auto-correlation functions and cross-correlations. Hence, we present the definition of a novel class of models, the derivation of a respective closed-form analytical representation and its application on real network traffic. Our modeling approach comprises: (i) generating statistical dependent Gaussian random processes, (ii) introducing auto-correlation to each process with a linear filter and, (iii) transforming them sample-wise by real-valued polynomial functions in order to shape their distributions. This particular structure allows to split the parameter fitting problem into three independent parts, each of which solvable by standard methods. Therefore, it is simple and straightforward to fit the model to measurement data.},
 acmid = {2254809},
 address = {New York, NY, USA},
 author = {Laner, Markus and Svoboda, Philipp and Rupp, Markus},
 booktitle = {Proceedings of the 12th ACM SIGMETRICS/PERFORMANCE Joint International Conference on Measurement and Modeling of Computer Systems},
 doi = {10.1145/2254756.2254809},
 isbn = {978-1-4503-1097-0},
 keyword = {gaussian process, polynomial transformation},
 link = {http://doi.acm.org/10.1145/2254756.2254809},
 location = {London, England, UK},
 numpages = {2},
 pages = {393--394},
 publisher = {ACM},
 series = {SIGMETRICS '12},
 title = {Modeling Randomness in Network Traffic},
 year = {2012}
}


@article{Duffield:2012:FSA:2318857.2254800,
 abstract = {Sampling is crucial for controlling resource consumption by internet traffic flow measurements. Routers use Packet Sampled NetFlow, and completed flow records are sampled in the measurement infrastructure. Recent research, motivated by the need of service providers to accurately measure both small and large traffic subpopulations, has focused on distributing a packet sampling budget amongst subpopulations. But long timescales of hardware development and lower bandwidth costs motivate post-measurement analysis of complete flow records at collectors instead. Sampling in collector databases then manages data volumes, yielding general purpose summaries that are rapidly queried to trigger drill-down analysis on a time limited window of full data. These are sufficiently small to be archived. This paper addresses the problem of distributing a sampling budget over subpopulations of flow records. Estimation accuracy goals are met by fairly sharing the budget. We establish a correspondence between the type of accuracy goal, and the flavor of fair sharing used. A streaming Max-Min Fair Sampling algorithm fairly shares the sampling budget across subpopulations, with sampling as a mechanism to deallocate budget. This provides timely samples and is robust against uncertainties in configuration and demand. We illustrate using flow records from an access router of a large ISP, where rates over interface traffic subpopulations vary over several orders of magnitude. We detail an implementation whose computational cost is no worse than subpopulation-oblivious sampling.},
 acmid = {2254800},
 address = {New York, NY, USA},
 author = {Duffield, Nick},
 doi = {10.1145/2318857.2254800},
 issn = {0163-5999},
 issue_date = {June 2012},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 keyword = {IP flows, estimation, max-min fairness, sampling, streaming},
 link = {http://doi.acm.org/10.1145/2318857.2254800},
 month = {jun},
 number = {1},
 numpages = {12},
 pages = {367--378},
 publisher = {ACM},
 title = {Fair Sampling Across Network Flow Measurements},
 volume = {40},
 year = {2012}
}


@inproceedings{Wang:2012:CIW:2254756.2254815,
 abstract = {Energy consumption imposes a significant cost for data centers; yet much of that energy is used to maintain excess service capacity during periods of predictably low load. Resultantly, there has recently been interest in developing designs that allow the service capacity to be dynamically resized to match the current workload. However, there is still much debate about the value of such approaches in real settings. In this paper, we show that the value of dynamic resizing is highly dependent on statistics of the workload process. In particular, both slow time-scale non-stationarities of the workload (e.g., the peak-to-mean ratio) and the fast time-scale stochasticity (e.g., the burstiness of arrivals) play key roles. To illustrate the impact of these factors, we combine optimization-based modeling of the slow time-scale with stochastic modeling of the fast time scale. Within this framework, we provide both analytic and numerical results characterizing when dynamic resizing does (and does not) provide benefits.},
 acmid = {2254815},
 address = {New York, NY, USA},
 author = {Wang, Kai and Lin, Minghong and Ciucu, Florin and Wierman, Adam and Lin, Chuang},
 booktitle = {Proceedings of the 12th ACM SIGMETRICS/PERFORMANCE Joint International Conference on Measurement and Modeling of Computer Systems},
 doi = {10.1145/2254756.2254815},
 isbn = {978-1-4503-1097-0},
 keyword = {data centers, dynamic resizing, energy efficient IT},
 link = {http://doi.acm.org/10.1145/2254756.2254815},
 location = {London, England, UK},
 numpages = {2},
 pages = {405--406},
 publisher = {ACM},
 series = {SIGMETRICS '12},
 title = {Characterizing the Impact of the Workload on the Value of Dynamic Resizing in Data Centers},
 year = {2012}
}


@article{Papapanagiotou:2012:SVL:2318857.2254824,
 abstract = {In this work we present the differences and similarities of the web browsing behavior in most common mobile platforms. We devise a novel Operating System (OS) fingerprinting methodology to distinguish different types of wireless devices (smartphone vs laptops) as well as operating system instances (iOS, Android, BlackBerry etc.). We showcase that most of the multimedia content in smartphone devices is delivered via Range-Requests, and a large portion of the video transfers are aborted. We also show that laptop devices have more intelligent browser caching capabilities. We investigate the impact of an additional browser cache, and demonstrate that a 10MB browser cache that is able to handle partial downloads in smartphones would be enough to handle the majority of the savings. Finally, we showcase that caching policies need to be amended to attain the maximum possible savings in proxy caches. Based on those optimizations the emulated proxy cache provides 10%-20% in bandwidth savings.},
 acmid = {2254824},
 address = {New York, NY, USA},
 author = {Papapanagiotou, Ioannis and Nahum, Erich M. and Pappas, Vasileios},
 doi = {10.1145/2318857.2254824},
 issn = {0163-5999},
 issue_date = {June 2012},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 keyword = {caching, mobile traffic, network usage, smartphones},
 link = {http://doi.acm.org/10.1145/2318857.2254824},
 month = {jun},
 number = {1},
 numpages = {2},
 pages = {423--424},
 publisher = {ACM},
 title = {Smartphones vs. Laptops: Comparing Web Browsing Behavior and the Implications for Caching},
 volume = {40},
 year = {2012}
}


@article{Tantawi:2012:OCP:2318857.2254813,
 abstract = {We introduce an algorithm for the placement of constrained, networked virtual clusters in the cloud, that is based on importance sampling (also known as cross-entropy). Rather than using a straightforward implementation of such a technique, which proved inefficient, we considerably enhance the method by biasing the sampling process to incorporate communication needs and other constraints of placement requests to yield an efficient algorithm that is linear in the size of the cloud. We investigate the quality of the results of using our algorithm on a simulated cloud.},
 acmid = {2254813},
 address = {New York, NY, USA},
 author = {Tantawi, Asser N.},
 doi = {10.1145/2318857.2254813},
 issn = {0163-5999},
 issue_date = {June 2012},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 link = {http://doi.acm.org/10.1145/2318857.2254813},
 month = {jun},
 number = {1},
 numpages = {2},
 pages = {401--402},
 publisher = {ACM},
 title = {Optimized Cloud Placement of Virtual Clusters Using Biased Importance Sampling},
 volume = {40},
 year = {2012}
}


@article{Wang:2012:ESD:2318857.2254780,
 abstract = {Energy storage - in the form of UPS units - in a datacenter has been primarily used to fail-over to diesel generators upon power outages. There has been recent interest in using these Energy Storage Devices (ESDs) for demand-response (DR) to either shift peak demand away from high tariff periods, or to shave demand allowing aggressive under-provisioning of the power infrastructure. All such prior work has only considered a single/specific type of ESD (typically re-chargeable lead-acid batteries), and has only employed them at a single level of the power delivery network. Continuing technological advances have provided us a plethora of competitive ESD options ranging from ultra-capacitors, to different kinds of batteries, flywheels and even compressed air-based storage. These ESDs offer very different trade-offs between their power and energy costs, densities, lifetimes, and energy efficiency, among other factors, suggesting that employing hybrid combinations of these may allow more effective DR than with a single technology. Furthermore, ESDs can be placed at different, and possibly multiple, levels of the power delivery hierarchy with different associated trade-offs. To our knowledge, no prior work has studied the extensive design space involving multiple ESD technology provisioning and placement options. This paper intends to fill this critical void, by presenting a theoretical framework for capturing important characteristics of different ESD technologies, the trade-offs of placing them at different levels of the power hierarchy, and quantifying the resulting cost-benefit trade-offs as a function of workload properties.},
 acmid = {2254780},
 address = {New York, NY, USA},
 author = {Wang, Di and Ren, Chuangang and Sivasubramaniam, Anand and Urgaonkar, Bhuvan and Fathy, Hosam},
 doi = {10.1145/2318857.2254780},
 issn = {0163-5999},
 issue_date = {June 2012},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 keyword = {cost reduction, datacenters, energy storage, power provisioning},
 link = {http://doi.acm.org/10.1145/2318857.2254780},
 month = {jun},
 number = {1},
 numpages = {12},
 pages = {187--198},
 publisher = {ACM},
 title = {Energy Storage in Datacenters: What, Where, and How Much?},
 volume = {40},
 year = {2012}
}


@proceedings{Harchol-Balter:2013:2465529,
 abstract = {Welcome to SIGMETRICS 2013. SIGMETRICS is the flagship conference of the ACM special interest group for the computer systems performance evaluation community. This year marks the fortieth anniversary since SIGMETRICS (under its prior name, SIGME) held the First National SIGME Symposium on Measurement and Evaluation in 1973. The past four decades have seen enormous changes in the field of computer science, but the importance of measurement, modeling, and performance evaluation remains as critical as ever. This year's conference includes papers on topics that have been a mainstay since the founding of our SIG, including queuing, scheduling, resource allocation, and performance measurement. Application areas that have emerged in recent years, such as multicore systems, cellular networks, and energy optimization, continue to be represented in our program. Papers on solid-state storage have seen a significant uptick this year, and we have papers on some topics that are new to SIGMETRICS, including crowdsourcing and RFID systems. Interestingly, the program also shows a drop-off in topics that were hot just a brief while ago, such as social networks, BitTorrent, swarms, peer-to-peer, and MapReduce. We received 196 submissions to this year's conference, of which 26 appear in the program as full papers, which is a highly competitive acceptance ratio below 14%. An additional 28 submissions appear in the abbreviated form of poster presentations with brief summaries in the proceedings. As in some prior years, we performed reviews in two rounds. In the first round, each paper was assigned to four reviewers. In the second round, additional reviews were assigned to papers with fewer than three completed reviews and papers with highly divergent review opinions and fewer than two high-confidence reviews. We experimented with two changes to the review process this year. The first change to the review process was the addition of a rebuttal phase between the first and second review rounds, to give authors an opportunity to respond to questions raised in first-round reviews. To impede the addition of new substantive material in the rebuttals, and instead reserve rebuttals for merely highlighting information already contained in the submission, we strictly limited each rebuttal to 500 characters. It is not easy to gauge the effectiveness of the rebuttal process: There were many occasions during the PC meeting when reviewers commented on items in authors' rebuttals, which suggests that the rebuttals provided additional information; however, reviewers mostly found that their opinions were unchanged by what they read in the rebuttals. The second change to the review process was the use of rankings rather than ratings. Instead of rating their assigned papers with accept/reject recommendations, each PC member was asked to produce a list of assigned papers ordered by the reviewer's assessment of each paper's overall quality. Our intent was to eliminate the bias that is inherent in accept/reject recommendations because each reviewer has only a narrow view of the conference's submissions. Reviewers' individual rankings were combined into a global ranking using an algorithm similar to PageRank, and the top 60 papers were discussed at the PC meeting. During the PC meeting, whenever a paper was accepted, we identified any paper with global rank below 60 that at least one reviewer had ranked substantially higher than the accepted paper. The reviewer was given the option to add this other paper to the discussion list. About a dozen such additional papers were discussed, although none were accepted as full papers. We are pleased to present three awards to two of this year's papers. The SIGMETRICS Best Paper Award honors the overall best paper in each year's conference, and the Kenneth C. Sevcik Outstanding Student Paper Award honors an outstanding paper whose primary author is a student. This year, both awards are presented to an outstanding student paper that is also the overall best paper in the conference: "Queueing System Topologies with Limited Flexibility," by John N. Tsitsiklis and Kuang Xu. We are inaugurating a new award this year, the SIGMETRICS Best Practical Paper Award, to honor the best paper from among those whose research has the most direct practical applicability. This award is presented to "Practical Conflict Graphs for Dynamic Spectrum Distribution," by Xia Zhou, Zengbin Zhang, Gang Wang, Xiaoxiao Yu, Ben Y. Zhao, and Haitao Zheng.},
 address = {New York, NY, USA},
 isbn = {978-1-4503-1900-3},
 location = {Pittsburgh, PA, USA},
 publisher = {ACM},
 title = {SIGMETRICS '13: Proceedings of the ACM SIGMETRICS/International Conference on Measurement and Modeling of Computer Systems},
 year = {2013}
}


@article{Leconte:2012:BGS:2318857.2254764,
 abstract = {This paper considers large scale distributed content service platforms, such as peer-to-peer video-on-demand systems. Such systems feature two basic resources, namely storage and bandwidth. Their efficiency critically depends on two factors: (i) content replication within servers, and (ii) how incoming service requests are matched to servers holding requested content. To inform the corresponding design choices, we make the following contributions. We first show that, for underloaded systems, so-called proportional content placement with a simple greedy strategy for matching requests to servers ensures full system efficiency provided storage size grows logarithmically with the system size. However, for constant storage size, this strategy undergoes a phase transition with severe loss of efficiency as system load approaches criticality. To better understand the role of the matching strategy in this performance degradation, we characterize the asymptotic system efficiency under an optimal matching policy. Our analysis shows that -in contrast to greedy matching- optimal matching incurs an inefficiency that is exponentially small in the server storage size, even at critical system loads. It further allows a characterization of content replication policies that minimize the inefficiency. These optimal policies, which differ markedly from proportional placement, have a simple structure which makes them implementable in practice. On the methodological side, our analysis of matching performance uses the theory of local weak limits of random graphs, and highlights a novel characterization of matching numbers in bipartite graphs, which may both be of independent interest.},
 acmid = {2254764},
 address = {New York, NY, USA},
 author = {Leconte, Mathieu and Lelarge, Marc and Massouli{\'e}, Laurent},
 doi = {10.1145/2318857.2254764},
 issn = {0163-5999},
 issue_date = {June 2012},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 keyword = {content delivery networks, content placement policies, matchings, random graphs},
 link = {http://doi.acm.org/10.1145/2318857.2254764},
 month = {jun},
 number = {1},
 numpages = {12},
 pages = {41--52},
 publisher = {ACM},
 title = {Bipartite Graph Structures for Efficient Balancing of Heterogeneous Loads},
 volume = {40},
 year = {2012}
}


@inproceedings{Hyytia:2012:MSH:2254756.2254763,
 abstract = {We consider a system of parallel queues where tasks are assigned (dispatched) to one of the available servers upon arrival. The dispatching decision is based on the full state information, i.e., on the sizes of the new and existing jobs. We are interested in minimizing the so-called mean slowdown criterion corresponding to the mean of the sojourn time divided by the processing time. Assuming no new jobs arrive, the shortest-processing-time-product (SPTP) schedule is known to minimize the slowdown of the existing jobs. The main contribution of this paper is three-fold: 1) To show the optimality of SPTP with respect to slowdown in a single server queue under Poisson arrivals; 2) to derive the so-called size-aware value functions for M/G/1-FIFO/LIFO/SPTP with general holding costs of which the slowdown criterion is a special case; and 3) to utilize the value functions to derive efficient dispatching policies so as to minimize the mean slowdown in a heterogeneous server system. The derived policies offer a significantly better performance than e.g., the size-aware-task-assignment with equal load (SITA-E) and least-work-left (LWL) policies.},
 acmid = {2254763},
 address = {New York, NY, USA},
 author = {Hyyti\"{a}, Esa and Aalto, Samuli and Penttinen, Aleksi},
 booktitle = {Proceedings of the 12th ACM SIGMETRICS/PERFORMANCE Joint International Conference on Measurement and Modeling of Computer Systems},
 doi = {10.1145/2254756.2254763},
 isbn = {978-1-4503-1097-0},
 keyword = {M/G/1, MDP, policy improvement, task assignment},
 link = {http://doi.acm.org/10.1145/2254756.2254763},
 location = {London, England, UK},
 numpages = {12},
 pages = {29--40},
 publisher = {ACM},
 series = {SIGMETRICS '12},
 title = {Minimizing Slowdown in Heterogeneous Size-aware Dispatching Systems},
 year = {2012}
}


@inproceedings{Yoo:2012:AAD:2254756.2254791,
 abstract = {Performance characterization of applications' hardware behavior is essential for making the best use of available hardware resources. Modern architectures offer access to many hardware events that are capable of providing information to reveal architectural performance bottlenecks throughout the core and memory hierarchy. These events can provide programmers with unique and powerful insights into the causes of the resource bottlenecks in their applications. However, interpreting these events has been a significant challenge. We present an automated system that uses machine learning to identify an application's performance problems. Our system provides programmers with insights about the performance of their applications while shielding them from the onerous task of digesting hardware events. It uses a decision tree algorithm, random forests on our micro-benchmarks to fingerprint the performance problems. Our system divides a profiled application into functions and automatically classifies each function by the dominant hardware resource bottlenecks. Using the classifications from the hotspot functions, we were able to achieve an average speedup of 1.73 from three applications in the PARSEC benchmark suite. Our system provides programmers with a guideline of where, what, and how to fix the detected performance problems in applications, which would have otherwise required considerable architectural knowledge.},
 acmid = {2254791},
 address = {New York, NY, USA},
 author = {Yoo, Wucherl and Larson, Kevin and Baugh, Lee and Kim, Sangkyum and Campbell, Roy H.},
 booktitle = {Proceedings of the 12th ACM SIGMETRICS/PERFORMANCE Joint International Conference on Measurement and Modeling of Computer Systems},
 doi = {10.1145/2254756.2254791},
 isbn = {978-1-4503-1097-0},
 keyword = {fingerprint, hardware event, machine learning, micro-benchmark, performance analysis, resource bottleneck},
 link = {http://doi.acm.org/10.1145/2254756.2254791},
 location = {London, England, UK},
 numpages = {12},
 pages = {283--294},
 publisher = {ACM},
 series = {SIGMETRICS '12},
 title = {ADP: Automated Diagnosis of Performance Pathologies Using Hardware Events},
 year = {2012}
}


@article{Peng:2012:TNI:2318857.2254802,
 abstract = {Regular expression matching as the core packet inspection engine of network systems has long been striving to be both fast in matching speed (like DFA) and scalable in storage space (like NFA). Recently, ternary content addressable memory (TCAM) has been investigated as a promising way out, by implementing DFA using TCAM for regular express matching. In this paper, we present the first method for implementing NFA using TCAM. Through proper TCAM encoding, our method matches each input byte with one single TCAM lookup --- operating at precisely the same speed as DFA, while using a number of TCAM entries that can be close to NFA size. These properties make our method an important step along a new path --- TCAM-based NFA implementation --- towards the long-standing goal of fast and scalable regular expression matching.},
 acmid = {2254802},
 address = {New York, NY, USA},
 author = {Peng, Kunyang and Dong, Qunfeng},
 doi = {10.1145/2318857.2254802},
 issn = {0163-5999},
 issue_date = {June 2012},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 keyword = {NFA, TCAM, regular expression matching},
 link = {http://doi.acm.org/10.1145/2318857.2254802},
 month = {jun},
 number = {1},
 numpages = {2},
 pages = {379--380},
 publisher = {ACM},
 title = {TCAM-based NFA Implementation},
 volume = {40},
 year = {2012}
}


@inproceedings{Tan:2012:PLS:2254756.2254816,
 abstract = {Resource provisioning, the task of planning sufficient amounts of resources to meet service level agreements, has become an important management task in emerging cloud computing services. In this paper, we present a stochastic modeling approach to guide the resource provisioning task for future service clouds as the demand grows large. We focus on on-demand services and consider service availability as the key quality of service constraint. A specific scenario under consideration is when resources can be measured in base instances. We develop an asymptotic provisioning methodology that utilizes tight performance bounds for the Erlang loss system to determine the minimum capacity levels that meet the service availability requirements. We show that our provisioning solutions are not only asymptotically exact but also provide better QoS guarantees at all load conditions.},
 acmid = {2254816},
 address = {New York, NY, USA},
 author = {Tan, Yue and Lu, Yingdong and Xia, Cathy H.},
 booktitle = {Proceedings of the 12th ACM SIGMETRICS/PERFORMANCE Joint International Conference on Measurement and Modeling of Computer Systems},
 doi = {10.1145/2254756.2254816},
 isbn = {978-1-4503-1097-0},
 keyword = {asymptotic optimization, cloud computing, large-scale, loss network models, resource provisioning},
 link = {http://doi.acm.org/10.1145/2254756.2254816},
 location = {London, England, UK},
 numpages = {2},
 pages = {407--408},
 publisher = {ACM},
 series = {SIGMETRICS '12},
 title = {Provisioning for Large Scale Cloud Computing Services},
 year = {2012}
}


@article{Milling:2012:NFR:2318857.2254784,
 abstract = {Computer (and human) networks have long had to contend with spreading viruses. Effectively controlling or curbing an outbreak requires understanding the dynamics of the spread. A virus that spreads by taking advantage of physical links or user-acquaintance links on a social network can grow explosively if it spreads beyond a critical radius. On the other hand, random infections (that do not take advantage of network structure) have very different propagation characteristics. If too many machines (or humans) are infected, network structure becomes essentially irrelevant, and the different spreading modes appear identical. When can we distinguish between mechanics of infection? Further, how can this be done efficiently? This paper studies these two questions. We provide sufficient conditions for different graph topologies, for when it is possible to distinguish between a random model of infection and a spreading epidemic model, with probability of misclassification going to zero. We further provide efficient algorithms that are guaranteed to work in different regimes.},
 acmid = {2254784},
 address = {New York, NY, USA},
 author = {Milling, Chris and Caramanis, Constantine and Mannor, Shie and Shakkottai, Sanjay},
 doi = {10.1145/2318857.2254784},
 issn = {0163-5999},
 issue_date = {June 2012},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 keyword = {epidemic process, network inference},
 link = {http://doi.acm.org/10.1145/2318857.2254784},
 month = {jun},
 number = {1},
 numpages = {12},
 pages = {223--234},
 publisher = {ACM},
 title = {Network Forensics: Random Infection vs Spreading Epidemic},
 volume = {40},
 year = {2012}
}


@article{Mukherjee:2012:SCT:2318857.2254811,
 abstract = {
                  An abstract is not available.
              },
 acmid = {2254811},
 address = {New York, NY, USA},
 author = {Mukherjee, Koyel and Khuller, Samir and Deshpande, Amol},
 doi = {10.1145/2318857.2254811},
 issn = {0163-5999},
 issue_date = {June 2012},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 keyword = {cooling energy, data centers, multi-core, parallel, scheduling},
 link = {http://doi.acm.org/10.1145/2318857.2254811},
 month = {jun},
 number = {1},
 numpages = {2},
 pages = {397--398},
 publisher = {ACM},
 title = {Saving on Cooling: The Thermal Scheduling Problem},
 volume = {40},
 year = {2012}
}


@inproceedings{Frank:2012:CTE:2254756.2254819,
 abstract = {Recent studies show that a large fraction of Internet traffic is originated by Content Providers (CPs) such as content distribution networks and hyper-giants. To cope with the increasing demand for content, CPs deploy massively distributed server infrastructures. Thus, content is available in many network locations and can be downloaded by traversing different paths in a network. Despite the prominent server location and path diversity, the decisions on how to map users to servers by CPs and how to perform traffic engineering by ISPs, are independent. This leads to a lose-lose situation as CPs are not aware about the network bottlenecks nor the location of end-users, and the ISPs struggle to cope with rapid traffic shifts caused by the dynamic CP server selection process. In this paper we propose and evaluate Content-aware Traffic Engineering (CaTE), which dynamically adapts the traffic demand for content hosted on CPs by utilizing ISP network information and end-user location during the server selection process. This leads to a win-win situation because CPs are able to enhance their end-user to server mapping and ISPs gain the ability to partially influence the traffic demands in their networks. Indeed, our results using traces from a Tier-1 ISP show that a number of network metrics can be improved when utilizing CaTE.},
 acmid = {2254819},
 address = {New York, NY, USA},
 author = {Frank, Benjamin and Poese, Ingmar and Smaragdakis, Georgios and Uhlig, Steve and Feldmann, Anja},
 booktitle = {Proceedings of the 12th ACM SIGMETRICS/PERFORMANCE Joint International Conference on Measurement and Modeling of Computer Systems},
 doi = {10.1145/2254756.2254819},
 isbn = {978-1-4503-1097-0},
 keyword = {ISP-CDN collaboration, content delivery, load balancing, network optimization, traffic engineering},
 link = {http://doi.acm.org/10.1145/2254756.2254819},
 location = {London, England, UK},
 numpages = {2},
 pages = {413--414},
 publisher = {ACM},
 series = {SIGMETRICS '12},
 title = {Content-aware Traffic Engineering},
 year = {2012}
}


@article{Atikoglu:2012:WAL:2318857.2254766,
 abstract = {Key-value stores are a vital component in many scale-out enterprises, including social networks, online retail, and risk analysis. Accordingly, they are receiving increased attention from the research community in an effort to improve their performance, scalability, reliability, cost, and power consumption. To be effective, such efforts require a detailed understanding of realistic key-value workloads. And yet little is known about these workloads outside of the companies that operate them. This paper aims to address this gap. To this end, we have collected detailed traces from Facebook's Memcached deployment, arguably the world's largest. The traces capture over 284 billion requests from five different Memcached use cases over several days. We analyze the workloads from multiple angles, including: request composition, size, and rate; cache efficacy; temporal patterns; and application use cases. We also propose a simple model of the most representative trace to enable the generation of more realistic synthetic workloads by the community. Our analysis details many characteristics of the caching workload. It also reveals a number of surprises: a GET/SET ratio of 30:1 that is higher than assumed in the literature; some applications of Memcached behave more like persistent storage than a cache; strong locality metrics, such as keys accessed many millions of times a day, do not always suffice for a high hit rate; and there is still room for efficiency and hit rate improvements in Memcached's implementation. Toward the last point, we make several suggestions that address the exposed deficiencies.},
 acmid = {2254766},
 address = {New York, NY, USA},
 author = {Atikoglu, Berk and Xu, Yuehai and Frachtenberg, Eitan and Jiang, Song and Paleczny, Mike},
 doi = {10.1145/2318857.2254766},
 issn = {0163-5999},
 issue_date = {June 2012},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 keyword = {key-value store, memcached, workload analysis, workload modeling},
 link = {http://doi.acm.org/10.1145/2318857.2254766},
 month = {jun},
 number = {1},
 numpages = {12},
 pages = {53--64},
 publisher = {ACM},
 title = {Workload Analysis of a Large-scale Key-value Store},
 volume = {40},
 year = {2012}
}


@inproceedings{El-Sayed:2012:TMD:2254756.2254778,
 abstract = {The energy consumed by data centers is starting to make up a significant fraction of the world's energy consumption and carbon emissions. A large fraction of the consumed energy is spent on data center cooling, which has motivated a large body of work on temperature management in data centers. Interestingly, a key aspect of temperature management has not been well understood: controlling the setpoint temperature at which to run a data center's cooling system. Most data centers set their thermostat based on (conservative) suggestions by manufacturers, as there is limited understanding of how higher temperatures will affect the system. At the same time, studies suggest that increasing the temperature setpoint by just one degree could save 2-5% of the energy consumption. This paper provides a multi-faceted study of temperature management in data centers. We use a large collection of field data from different production environments to study the impact of temperature on hardware reliability, including the reliability of the storage subsystem, the memory subsystem and server reliability as a whole. We also use an experimental testbed based on a thermal chamber and a large array of benchmarks to study two other potential issues with higher data center temperatures: the effect on server performance and power. Based on our findings, we make recommendations for temperature management in data centers, that create the potential for saving energy, while limiting negative effects on system reliability and performance.},
 acmid = {2254778},
 address = {New York, NY, USA},
 author = {El-Sayed, Nosayba and Stefanovici, Ioan A. and Amvrosiadis, George and Hwang, Andy A. and Schroeder, Bianca},
 booktitle = {Proceedings of the 12th ACM SIGMETRICS/PERFORMANCE Joint International Conference on Measurement and Modeling of Computer Systems},
 doi = {10.1145/2254756.2254778},
 isbn = {978-1-4503-1097-0},
 keyword = {CPU, DRAM, LSE, data center, energy, fans, hard drive, memory, performance, reliability, temperature},
 link = {http://doi.acm.org/10.1145/2254756.2254778},
 location = {London, England, UK},
 numpages = {12},
 pages = {163--174},
 publisher = {ACM},
 series = {SIGMETRICS '12},
 title = {Temperature Management in Data Centers: Why Some (Might) Like It Hot},
 year = {2012}
}


@article{Alizadeh:2012:VRL:2318857.2254787,
 abstract = {Multi-banked embedded DRAM (eDRAM) has become increasingly popular in high-performance systems. However, the data retention problem of eDRAM is exacerbated by the larger number of banks and the high-performance environment in which it is deployed: The data retention time of each memory cell decreases while the number of cells to be refreshed increases. For this, multi-bank designs offer a concurrent refresh mode, where idle banks can be refreshed concurrently during read and write operations. However, conventional techniques such as periodically scheduling refreshes---with priority given to refreshes in case of conflicts with reads or writes---have variable performance, increase read latency, and can perform poorly in worst case memory access patterns. We propose a novel refresh scheduling algorithm that is low-complexity, produces near-optimal throughput with universal guarantees, and is tolerant to bursty memory access patterns. The central idea is to decouple the scheduler into two simple-to-implement modules: one determines which cell to refresh next and the other determines when to force an idle cycle in all banks. We derive necessary and sufficient conditions to guarantee data integrity for all access patterns, with any given number of banks, rows per bank, read/write ports and data retention time. Our analysis shows that there is a tradeoff between refresh overhead and burst tolerance and characterizes this tradeoff precisely. The algorithm is shown to be near-optimal and achieves, for instance, 76.6% reduction in worst-case refresh overhead from the periodic refresh algorithm for a 250MHz eDRAM with 10us retention time and 16 banks each with 128 rows. Simulations with Apex-Map synthetic benchmarks and switch lookup table traffic show that VR can almost completely hide the refresh overhead for memory accesses with moderate-to-high multiplexing across memory banks.},
 acmid = {2254787},
 address = {New York, NY, USA},
 author = {Alizadeh, Mohammad and Javanmard, Adel and Chuang, Shang-Tse and Iyer, Sundar and Lu, Yi},
 doi = {10.1145/2318857.2254787},
 issn = {0163-5999},
 issue_date = {June 2012},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 keyword = {embedded DRAM, memory refresh scheduling, multi-banked},
 link = {http://doi.acm.org/10.1145/2318857.2254787},
 month = {jun},
 number = {1},
 numpages = {12},
 pages = {247--258},
 publisher = {ACM},
 title = {Versatile Refresh: Low Complexity Refresh Scheduling for High-throughput Multi-banked eDRAM},
 volume = {40},
 year = {2012}
}


@article{VanHoudt:2012:FLA:2318857.2254772,
 abstract = {We consider an asynchronous all optical packet switch (OPS) where each link consists of N wavelength channels and a pool of C ≤ N full range tunable wavelength converters. Under the assumption of Poisson arrivals with rate λ (per wavelength channel) and exponential packet lengths, we determine a simple closed-form expression for the limit of the loss probabilities Ploss(N) as N tends to infinity (while the load and conversion ratio σ=C/N remains fixed). More specifically, for σ ≤ λ2 the loss probability tends to (λ2-σ)/λ(1+λ), while for σ > λ2 the loss tends to zero. We also prove an insensitivity result when the exponential packet lengths are replaced by certain classes of phase-type distributions. A key feature of the dynamical system (i.e., set of ODEs) that describes the limit behavior of this OPS switch, is that its right-hand side is discontinuous. To prove the convergence, we therefore had to generalize some existing result to the setting of piece-wise smooth dynamical systems.},
 acmid = {2254772},
 address = {New York, NY, USA},
 author = {Van Houdt, Benny and Bortolussi, Luca},
 doi = {10.1145/2318857.2254772},
 issn = {0163-5999},
 issue_date = {June 2012},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 keyword = {fluid limit, optical packet switch, wavelength conversion},
 link = {http://doi.acm.org/10.1145/2318857.2254772},
 month = {jun},
 number = {1},
 numpages = {12},
 pages = {113--124},
 publisher = {ACM},
 title = {Fluid Limit of an Asynchronous Optical Packet Switch with Shared Per Link Full Range Wavelength Conversion},
 volume = {40},
 year = {2012}
}


@article{Hua:2012:TOE:2318857.2254773,
 abstract = {Error estimating coding (EEC) has recently been established as an important tool to estimate bit error rates in the transmission of packets over wireless links, with a number of potential applications in wireless networks. In this paper, we present an in-depth study of error estimating codes through the lens of Fisher information analysis and find that the original EEC estimator fails to exploit the information contained in its code to the fullest extent. Motivated by this discovery, we design a new estimator for the original EEC algorithm, which significantly improves the estimation accuracy, and is empirically very close to the Cramer-Rao bound. Following this path, we generalize the EEC algorithm to a new family of algorithms called gEEC generalized EEC. These algorithms can be tuned to hold 25-35% more information with the same overhead, and hence deliver even better estimation accuracy---close to optimal, as evidenced by the Cramer-Rao bound. Our theoretical analysis and assertions are supported by extensive experimental evaluation.},
 acmid = {2254773},
 address = {New York, NY, USA},
 author = {Hua, Nan and Lall, Ashwin and Li, Baochun and Xu, Jun},
 doi = {10.1145/2318857.2254773},
 issn = {0163-5999},
 issue_date = {June 2012},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 keyword = {Fisher information, error estimating coding},
 link = {http://doi.acm.org/10.1145/2318857.2254773},
 month = {jun},
 number = {1},
 numpages = {12},
 pages = {125--136},
 publisher = {ACM},
 title = {Towards Optimal Error-estimating Codes Through the Lens of Fisher Information Analysis},
 volume = {40},
 year = {2012}
}


@inproceedings{Ammar:2012:ERA:2254756.2254799,
 abstract = {The need to rank items based on user input arises in many practical applications such as elections, group decision making and recommendation systems. The primary challenge in such scenarios is to decide on a global ranking based on partial preferences provided by users. The standard approach to address this challenge is to ask users to provide explicit numerical ratings (cardinal information) of a subset of the items. The main appeal of such an approach is the ease of aggregation. However, the rating scale as well as the individual ratings are often arbitrary and may not be consistent from one user to another. A more natural alternative to numerical ratings requires users to compare pairs of items (ordinal information). On the one hand, such comparisons provide an "absolute" indicator of the user's preference. On the other hand, it is often hard to combine or aggregate these comparisons to obtain a consistent global ranking. In this work, we provide a tractable framework for utilizing comparison data as well as first-order marginal information (see Section 2) for the purpose of ranking. We treat the available information as partial samples from an unknown distribution over permutations. We then reduce ranking problems of interest to performing inference on this distribution. Specifically, we consider the problems of (a) finding an aggregate ranking of n items, (b) learning the mode of the distribution, and (c) identifying the top k items. For many of these problems, we provide efficient algorithms to infer the ranking directly from the data without the need to estimate the underlying distribution. In other cases, we use the Principle of Maximum Entropy to devise a concise parameterization of a distribution consistent with observations using only O(n2) parameters, where n is the number of items in question. We propose a distributed, iterative algorithm for estimating the parameters of the distribution. We establish the correctness of the algorithm and identify its rate of convergence explicitly.},
 acmid = {2254799},
 address = {New York, NY, USA},
 author = {Ammar, Ammar and Shah, Devavrat},
 booktitle = {Proceedings of the 12th ACM SIGMETRICS/PERFORMANCE Joint International Conference on Measurement and Modeling of Computer Systems},
 doi = {10.1145/2254756.2254799},
 isbn = {978-1-4503-1097-0},
 keyword = {aggregation, maximum entropy, ranking},
 link = {http://doi.acm.org/10.1145/2254756.2254799},
 location = {London, England, UK},
 numpages = {12},
 pages = {355--366},
 publisher = {ACM},
 series = {SIGMETRICS '12},
 title = {Efficient Rank Aggregation Using Partial Data},
 year = {2012}
}


@article{Dixit:2012:EFT:2318857.2254818,
 abstract = {Current multipath routing techniques split traffic at a per-flow level because, according to conventional wisdom, forwarding packets of a TCP flow along different paths leads to packet reordering which is detrimental to TCP. In this paper, we revisit this "myth" in the context of cloud data center networks which have regular topologies such as multi-rooted trees. We argue that due to the symmetry in the multiple equal-cost paths in such networks, simply spraying packets of a given flow among all equal-cost paths, leads to balanced queues across multiple paths, and consequently little packet reordering. Using a testbed comprising of NetFPGA switches, we show how cloud applications benefit from better network utilization in data centers.},
 acmid = {2254818},
 address = {New York, NY, USA},
 author = {Dixit, Advait Abhay and Prakash, Pawan and Kompella, Ramana Rao and Hu, Charlie},
 doi = {10.1145/2318857.2254818},
 issn = {0163-5999},
 issue_date = {June 2012},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 keyword = {data centers, traffic splitting},
 link = {http://doi.acm.org/10.1145/2318857.2254818},
 month = {jun},
 number = {1},
 numpages = {2},
 pages = {411--412},
 publisher = {ACM},
 title = {On the Efficacy of Fine-grained Traffic Splitting Protocols in Data Center Networks},
 volume = {40},
 year = {2012}
}


@article{Anshelevich:2012:SEP:2318857.2254803,
 abstract = {We address the question of strategic pricing of inter-domain traffic forwarding services provided by ISPs, which is also closely coupled with the question of how ISPs route their traffic towards their neighboring ISPs. Posing this question as a non-cooperative game between neighboring ISPs, we study the properties of this pricing game in terms of the existence and efficiency of the equilibrium. We observe that for "well-provisioned" ISPs, Nash equilibrium prices exist and they result in flows that maximize the overall network utility (generalized end-to-end throughput). For general ISP topologies, equilibrium prices may not exist; however, simulations on a large number of realistic topologies show that best-response based simple price update solutions converge to stable and efficient prices and flows for most topologies.},
 acmid = {2254803},
 address = {New York, NY, USA},
 author = {Anshelevich, Elliot and Hate, Ameya and Kar, Koushik and Usher, Michael},
 doi = {10.1145/2318857.2254803},
 issn = {0163-5999},
 issue_date = {June 2012},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 keyword = {economics, internet, next-hop routing, pricing},
 link = {http://doi.acm.org/10.1145/2318857.2254803},
 month = {jun},
 number = {1},
 numpages = {2},
 pages = {381--382},
 publisher = {ACM},
 title = {Stable and Efficient Pricing for Inter-domain Traffic Forwarding},
 volume = {40},
 year = {2012}
}


@inproceedings{deSouzaeSilva:2012:AML:2254756.2254829,
 abstract = {
                  An abstract is not available.
              },
 acmid = {2254829},
 address = {New York, NY, USA},
 author = {de Souza e Silva, Edmundo and Menasche, Daniel Sadoc},
 booktitle = {Proceedings of the 12th ACM SIGMETRICS/PERFORMANCE Joint International Conference on Measurement and Modeling of Computer Systems},
 doi = {10.1145/2254756.2254829},
 isbn = {978-1-4503-1097-0},
 keyword = {machine learning, performance evaluation},
 link = {http://doi.acm.org/10.1145/2254756.2254829},
 location = {London, England, UK},
 numpages = {2},
 pages = {431--432},
 publisher = {ACM},
 series = {SIGMETRICS '12},
 title = {Applications of Machine Learning to Performance Evaluation},
 year = {2012}
}


@article{Niu:2012:PCB:2318857.2254776,
 abstract = {In a public cloud, bandwidth is traditionally priced in a pay-as-you-go model. Reflecting the recent trend of augmenting cloud computing with bandwidth guarantees, we consider a novel model of cloud bandwidth allocation and pricing when explicit bandwidth reservation is enabled. We argue that a tenant's utility depends not only on its bandwidth usage, but more importantly on the portion of its demand that is satisfied with a performance guarantee. Our objective is to determine the optimal policy for pricing cloud bandwidth reservations, in order to maximize social welfare, i.e., the sum of the expected profits that can be made by all tenants and the cloud provider, even with the presence of demand uncertainty. The problem turns out to be a large-scale network optimization problem with a coupled objective function. We propose two new distributed solutions --- based on chaotic equation updates and cutting-plane methods --- that prove to be more efficient than existing solutions based on consistency pricing and subgradient methods. In addition, we address the practical challenge of forecasting demand statistics, required by our optimization problem as input. We propose a factor model for near-future demand prediction, and test it on a real-world video workload dataset. All included, we have designed a fully computerized trading environment for cloud bandwidth reservations, which operates effectively at a fine granularity of as small as ten minutes in our trace-driven simulations.},
 acmid = {2254776},
 address = {New York, NY, USA},
 author = {Niu, Di and Feng, Chen and Li, Baochun},
 doi = {10.1145/2318857.2254776},
 issn = {0163-5999},
 issue_date = {June 2012},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 keyword = {bandwidth pricing, cloud computing, distributed optimization, prediction, time series},
 link = {http://doi.acm.org/10.1145/2318857.2254776},
 month = {jun},
 number = {1},
 numpages = {12},
 pages = {151--162},
 publisher = {ACM},
 title = {Pricing Cloud Bandwidth Reservations Under Demand Uncertainty},
 volume = {40},
 year = {2012}
}


@inproceedings{Shafiq:2012:FLC:2254756.2254767,
 abstract = {Cellular network based Machine-to-Machine (M2M) communication is fast becoming a market-changing force for a wide spectrum of businesses and applications such as telematics, smart metering, point-of-sale terminals, and home security and automation systems. In this paper, we aim to answer the following important question: Does traffic generated by M2M devices impose new requirements and challenges for cellular network design and management? To answer this question, we take a first look at the characteristics of M2M traffic and compare it with traditional smartphone traffic. We have conducted our measurement analysis using a week-long traffic trace collected from a tier-1 cellular network in the United States. We characterize M2M traffic from a wide range of perspectives, including temporal dynamics, device mobility, application usage, and network performance. Our experimental results show that M2M traffic exhibits significantly different patterns than smartphone traffic in multiple aspects. For instance, M2M devices have a much larger ratio of uplink to downlink traffic volume, their traffic typically exhibits different diurnal patterns, they are more likely to generate synchronized traffic resulting in bursty aggregate traffic volumes, and are less mobile compared to smartphones. On the other hand, we also find that M2M devices are generally competing with smartphones for network resources in co-located geographical regions. These and other findings suggest that better protocol design, more careful spectrum allocation, and modified pricing schemes may be needed to accommodate the rise of M2M devices.},
 acmid = {2254767},
 address = {New York, NY, USA},
 author = {Shafiq, Muhammad Zubair and Ji, Lusheng and Liu, Alex X. and Pang, Jeffrey and Wang, Jia},
 booktitle = {Proceedings of the 12th ACM SIGMETRICS/PERFORMANCE Joint International Conference on Measurement and Modeling of Computer Systems},
 doi = {10.1145/2254756.2254767},
 isbn = {978-1-4503-1097-0},
 keyword = {cellular network, machine-to-machine traffic},
 link = {http://doi.acm.org/10.1145/2254756.2254767},
 location = {London, England, UK},
 numpages = {12},
 pages = {65--76},
 publisher = {ACM},
 series = {SIGMETRICS '12},
 title = {A First Look at Cellular Machine-to-machine Traffic: Large Scale Measurement and Characterization},
 year = {2012}
}


@article{Nemeth:2012:TSC:2318857.2254806,
 abstract = {Oblivious routing asks for a static routing that serves arbitrary user demands with minimal performance penalty. Performance is measured in terms of the competitive ratio, the proportion of the maximum congestion to the best possible congestion. In this paper, we take the first steps towards extending this worst-case characterization to a more revealing statistical one. We define new performance metrics and we present numerical evaluations showing that, in statistical terms, oblivious routing is not as competitive as the worst-case performance characterizations would suggest.},
 acmid = {2254806},
 address = {New York, NY, USA},
 author = {N{\'e}meth, G\'{a}bor and R{\'e}tv\'{a}ri, G\'{a}bor},
 doi = {10.1145/2318857.2254806},
 issn = {0163-5999},
 issue_date = {June 2012},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 keyword = {numerical analysis, oblivious ratio, performance evaluation},
 link = {http://doi.acm.org/10.1145/2318857.2254806},
 month = {jun},
 number = {1},
 numpages = {2},
 pages = {387--388},
 publisher = {ACM},
 title = {Towards a Statistical Characterization of the Competitiveness of Oblivious Routing},
 volume = {40},
 year = {2012}
}


@article{Liu:2012:HCM:2318857.2254759,
 abstract = {With the ever increasing popularity of smart phones, mobile services have been evolving rapidly to allow users to enjoy localized and personalized experiences. Users can discover local information and keep connected with family and friends on the go, and ultimately to experience the convergence of cyber space and physical world where digital technologies are interwoven into the day-to-day life. A pivotal component of such a cyber-physical convergence is the contextual intelligence. The extraction and dissemination of contextual information around users is the key for the cyber capabilities to be applied to physical activities and for the cyber world to better reflect the physical reality. In this talk, we shall address some issues arising from context-based mobile services. In particular, we discuss how mobility impacts contextual relevancy and personalization in mobile services. The relevancy and timeliness of contextual information not only are essential for these services to deliver great user experiences, but also put significant computation pressure on service infrastructure that processes continuous data streams in real time and disseminate relevant data to a large amount of mobile users. This talk will explore the challenges and opportunities for high-performance computing in mobile services. Based on key findings from large-scale mobile measurement data, the talk will analyze the tradeoff of different computing architectures, present case studies of scalable system design and implementation for personalized mobile services, and conclude with open challenges for the broad research community in performance measurement and modeling.},
 acmid = {2254759},
 address = {New York, NY, USA},
 author = {Liu, Zhen},
 doi = {10.1145/2318857.2254759},
 issn = {0163-5999},
 issue_date = {June 2012},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 keyword = {keynote},
 link = {http://doi.acm.org/10.1145/2318857.2254759},
 month = {jun},
 number = {1},
 numpages = {2},
 pages = {3--4},
 publisher = {ACM},
 title = {High-performance Computing in Mobile Services},
 volume = {40},
 year = {2012}
}


@inproceedings{Shah:2012:RCU:2254756.2254782,
 abstract = {We consider the problem of detecting the source of a rumor (information diffusion) in a network based on observations about which set of nodes possess the rumor. In a recent work [10], this question was introduced and studied. The authors proposed rumor centrality as an estimator for detecting the source. They establish it to be the maximum likelihood estimator with respect to the popular Susceptible Infected (SI) model with exponential spreading time for regular trees. They showed that as the size of infected graph increases, for a line (2-regular tree) graph, the probability of source detection goes to 0 while for d-regular trees with d ≥ 3 the probability of detection, say αd, remains bounded away from 0 and is less than 1/2. Their results, however stop short of providing insights for the heterogeneous setting such as irregular trees or the SI model with non-exponential spreading times. This paper overcomes this limitation and establishes the effectiveness of rumor centrality for source detection for generic random trees and the SI model with a generic spreading time distribution. The key result is an interesting connection between a multi-type continuous time branching process (an equivalent representation of a generalized Polya's urn, cf. [1]) and the effectiveness of rumor centrality. Through this, it is possible to quantify the detection probability precisely. As a consequence, we recover all the results of [10] as a special case and more importantly, we obtain a variety of results establishing the universality of rumor centrality in the context of tree-like graphs and the SI model with a generic spreading time distribution.},
 acmid = {2254782},
 address = {New York, NY, USA},
 author = {Shah, Devavrat and Zaman, Tauhid},
 booktitle = {Proceedings of the 12th ACM SIGMETRICS/PERFORMANCE Joint International Conference on Measurement and Modeling of Computer Systems},
 doi = {10.1145/2254756.2254782},
 isbn = {978-1-4503-1097-0},
 keyword = {epidemics, estimation},
 link = {http://doi.acm.org/10.1145/2254756.2254782},
 location = {London, England, UK},
 numpages = {12},
 pages = {199--210},
 publisher = {ACM},
 series = {SIGMETRICS '12},
 title = {Rumor Centrality: A Universal Source Detector},
 year = {2012}
}


@inproceedings{Cohen:2012:DLN:2254756.2254798,
 abstract = {Random sampling has been proven time and time again to be a powerful tool for working with large data. Queries over the full dataset are replaced by approximate queries over the smaller (and hence easier to store and manipulate) sample. The sample constitutes a flexible summary that supports a wide class of queries. But in many applications, datasets are modified with time, and it is desirable to update samples without requiring access to the full underlying datasets. In this paper, we introduce and analyze novel techniques for sampling over dynamic data, modeled as a stream of modifications to weights associated with each key. While sampling schemes designed for stream applications can often readily accommodate positive updates to the dataset, much less is known for the case of negative updates, where weights are reduced or items deleted altogether. We primarily consider the turnstile model of streams, and extend classic schemes to incorporate negative updates. Perhaps surprisingly, the modifications to handle negative updates turn out to be natural and seamless extensions of the well-known positive update-only algorithms. We show that they produce unbiased estimators, and we relate their performance to the behavior of corresponding algorithms on insert-only streams with different parameters. A careful analysis is necessitated, in order to account for the fact that sampling choices for one key now depend on the choices made for other keys. In practice, our solutions turn out to be efficient and accurate. Compared to recent algorithms for Lp sampling which can be applied to this problem, they are significantly more reliable, and dramatically faster.},
 acmid = {2254798},
 address = {New York, NY, USA},
 author = {Cohen, Edith and Cormode, Graham and Duffield, Nick},
 booktitle = {Proceedings of the 12th ACM SIGMETRICS/PERFORMANCE Joint International Conference on Measurement and Modeling of Computer Systems},
 doi = {10.1145/2254756.2254798},
 isbn = {978-1-4503-1097-0},
 keyword = {data streams, deletions, sampling, updates},
 link = {http://doi.acm.org/10.1145/2254756.2254798},
 location = {London, England, UK},
 numpages = {12},
 pages = {343--354},
 publisher = {ACM},
 series = {SIGMETRICS '12},
 title = {Don'T Let the Negatives Bring You Down: Sampling from Streams of Signed Updates},
 year = {2012}
}


@inproceedings{Hu:2012:UPA:2254756.2254820,
 abstract = {SSD is known to have the erase-before-write and out-of-place update properties. When the number of invalidated pages is more than a given threshold, a process referred to as garbage collection (GC) is triggered to erase blocks after valid pages in these blocks are copied somewhere else. GC degrades both the performance and lifetime of SSD significantly because of the read-write-erase operation sequence. In this paper, we conduct intensive experiments on a 120GB Intel 320 SATA SSD and a 320GB Fusion IO ioDrive PCI-E SSD to show and analyze the following important performance issues and anomalies. The commonly accepted knowledge that the performance drops sharply as more data is being written is not always true. This is because GC efficiency, a more important factor affecting SSD performance, has not been carefully considered. It is defined as the percentage of invalid pages of a GC erased block. It is possible to avoid the performance degradation by managing the addressable LBA range. Estimating the residual lifetime of an SSD is a very challenging problem because it involves several interdependent and mutually interacting factors such as FTL, GC, wear leveling, workload characteristics, etc. We develop an analytical model to estimate the residual lifetime of a given SSD. The high random-read performance is widely accepted as one of the advantages of SSD. We will show that this is not true if the GC efficiency is low.},
 acmid = {2254820},
 address = {New York, NY, USA},
 author = {Hu, Jian and Jiang, Hong and Manden, Prakash},
 booktitle = {Proceedings of the 12th ACM SIGMETRICS/PERFORMANCE Joint International Conference on Measurement and Modeling of Computer Systems},
 doi = {10.1145/2254756.2254820},
 isbn = {978-1-4503-1097-0},
 keyword = {SSD, anomaly},
 link = {http://doi.acm.org/10.1145/2254756.2254820},
 location = {London, England, UK},
 numpages = {2},
 pages = {415--416},
 publisher = {ACM},
 series = {SIGMETRICS '12},
 title = {Understanding Performance Anomalies of SSDs and Their Impact in Enterprise Application Environment},
 year = {2012}
}


@inproceedings{Liu:2012:RCA:2254756.2254779,
 abstract = {Recently, the demand for data center computing has surged, increasing the total energy footprint of data centers worldwide. Data centers typically comprise three subsystems: IT equipment provides services to customers; power infrastructure supports the IT and cooling equipment; and the cooling infrastructure removes heat generated by these subsystems. This work presents a novel approach to model the energy flows in a data center and optimize its operation. Traditionally, supply-side constraints such as energy or cooling availability were treated independently from IT workload management. This work reduces electricity cost and environmental impact using a holistic approach that integrates renewable supply, dynamic pricing, and cooling supply including chiller and outside air cooling, with IT workload planning to improve the overall sustainability of data center operations. Specifically, we first predict renewable energy as well as IT demand. Then we use these predictions to generate an IT workload management plan that schedules IT workload and allocates IT resources within a data center according to time varying power supply and cooling efficiency. We have implemented and evaluated our approach using traces from real data centers and production systems. The results demonstrate that our approach can reduce both the recurring power costs and the use of non-renewable energy by as much as 60% compared to existing techniques, while still meeting the Service Level Agreements.},
 acmid = {2254779},
 address = {New York, NY, USA},
 author = {Liu, Zhenhua and Chen, Yuan and Bash, Cullen and Wierman, Adam and Gmach, Daniel and Wang, Zhikui and Marwah, Manish and Hyser, Chris},
 booktitle = {Proceedings of the 12th ACM SIGMETRICS/PERFORMANCE Joint International Conference on Measurement and Modeling of Computer Systems},
 doi = {10.1145/2254756.2254779},
 isbn = {978-1-4503-1097-0},
 keyword = {cooling optimization, demand shaping, renewable energy, scheduling, sustainable data center},
 link = {http://doi.acm.org/10.1145/2254756.2254779},
 location = {London, England, UK},
 numpages = {12},
 pages = {175--186},
 publisher = {ACM},
 series = {SIGMETRICS '12},
 title = {Renewable and Cooling Aware Workload Management for Sustainable Data Centers},
 year = {2012}
}


@inproceedings{Zarifzadeh:2012:RT:2254756.2254807,
 abstract = {
                  An abstract is not available.
              },
 acmid = {2254807},
 address = {New York, NY, USA},
 author = {Zarifzadeh, Sajjad and G K, Madhwaraj and Dovrolis, Constantine},
 booktitle = {Proceedings of the 12th ACM SIGMETRICS/PERFORMANCE Joint International Conference on Measurement and Modeling of Computer Systems},
 doi = {10.1145/2254756.2254807},
 isbn = {978-1-4503-1097-0},
 keyword = {localization, network tomography, performance metric},
 link = {http://doi.acm.org/10.1145/2254756.2254807},
 location = {London, England, UK},
 numpages = {2},
 pages = {389--390},
 publisher = {ACM},
 series = {SIGMETRICS '12},
 title = {Range Tomography},
 year = {2012}
}


@article{Keller:2012:MNT:2318857.2254823,
 abstract = {In the context of low-power wireless sensor networks, this paper presents multi-hop network tomography (MNT), a novel, non-intrusive algorithm for reconstructing the path, the per-hop arrival order, and the per-hop arrival time of individual packets at runtime. While explicitly transmitting this information over the radio would negatively impact the performance of the system under investigation, information is instead reconstructed after packets have been received at the sink.},
 acmid = {2254823},
 address = {New York, NY, USA},
 author = {Keller, Matthias and Beutel, Jan and Thiele, Lothar},
 doi = {10.1145/2318857.2254823},
 issn = {0163-5999},
 issue_date = {June 2012},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 keyword = {data analysis, tomography, wireless sensor networks},
 link = {http://doi.acm.org/10.1145/2318857.2254823},
 month = {jun},
 number = {1},
 numpages = {2},
 pages = {421--422},
 publisher = {ACM},
 title = {Multi-hop Network Tomography: Path Reconstruction and Per-hop Arrival Time Estimation from Partial Information},
 volume = {40},
 year = {2012}
}


@inproceedings{Jelenkovic:2012:UAD:2254756.2254771,
 abstract = {Retransmission-based failure recovery represents a primary approach in existing communication networks, on all protocol layers, that guarantees data delivery in the presence of channel failures. Contrary to the traditional belief that the number of retransmissions is geometrically distributed, a new phenomenon was discovered recently, which shows that retransmissions can cause long (-tailed) delays and instabilities even if all traffic and network characteristics are light-tailed, e.g., exponential or Gaussian. Since the preceding finding holds under the assumption that data sizes have infinite support, in this paper we investigate the practically important case of bounded data units 0≤ Lb≤ b. To this end, we provide an explicit and uniform characterization of the entire body of the retransmission distribution Pr[Nb > n] in both n and b. This rigorous approximation clearly demonstrates the previously observed transition from power law distributions in the main body to exponential tails. The accuracy of our approximation is validated with a number of simulation experiments. Furthermore, the results highlight the importance of wisely determining the size of data units in order to accommodate the performance needs in retransmission-based systems. From a broader perspective, this study applies to any other system, e.g., computing, where restart mechanisms are employed after a job processing failure.},
 acmid = {2254771},
 address = {New York, NY, USA},
 author = {Jelenkovic, Predrag R. and Skiani, Evangelia D.},
 booktitle = {Proceedings of the 12th ACM SIGMETRICS/PERFORMANCE Joint International Conference on Measurement and Modeling of Computer Systems},
 doi = {10.1145/2254756.2254771},
 isbn = {978-1-4503-1097-0},
 keyword = {channel with failures, gamma distributions, heavy-tailed distributions, light-tailed distributions., power laws, restarts, retransmissions, truncated distributions},
 link = {http://doi.acm.org/10.1145/2254756.2254771},
 location = {London, England, UK},
 numpages = {12},
 pages = {101--112},
 publisher = {ACM},
 series = {SIGMETRICS '12},
 title = {Uniform Approximation of the Distribution for the Number of Retransmissions of Bounded Documents},
 year = {2012}
}


@article{Kim:2012:GBC:2318857.2254786,
 abstract = {Smartphones are becoming ubiquitous and powerful. The Achilles' heel in such devices that limits performance is the storage. Low-end flash memory is the storage technology of choice in such devices due to energy, size, and cost considerations. In this paper, we take a critical look at the performance of flash on smartphones for mobile applications. Specifically, we ask the question whether the state-of-the-art buffer cache replacement schemes proposed thus far (both flash-agnostic and flash-aware ones) are the right ones for mobile flash storage. To answer this question, we first expose the limitations of current buffer cache performance evaluation methods, and propose a novel evaluation framework that is a hybrid between trace-driven simulation and real implementation of such schemes inside an operating system. Such an evaluation reveals some unexpected and surprising insights on the performance of buffer management schemes that contradicts conventional wisdom. Armed with this knowledge, we propose a new buffer cache replacement scheme called SpatialClock. Using our evaluation framework, we show the superior performance of SpatialClock relative to the state-of-the-art for mobile flash storage.},
 acmid = {2254786},
 address = {New York, NY, USA},
 author = {Kim, Hyojun and Ryu, Moonkyung and Ramachandran, Umakishore},
 doi = {10.1145/2318857.2254786},
 issn = {0163-5999},
 issue_date = {June 2012},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 keyword = {flash storage, mobile platform, page cache},
 link = {http://doi.acm.org/10.1145/2318857.2254786},
 month = {jun},
 number = {1},
 numpages = {12},
 pages = {235--246},
 publisher = {ACM},
 title = {What is a Good Buffer Cache Replacement Scheme for Mobile Flash Storage?},
 volume = {40},
 year = {2012}
}


@inproceedings{Reinecke:2012:MMV:2254756.2254826,
 abstract = {
                  An abstract is not available.
              },
 acmid = {2254826},
 address = {New York, NY, USA},
 author = {Reinecke, Philipp and Telek, Mikl\'{o}s and Wolter, Katinka},
 booktitle = {Proceedings of the 12th ACM SIGMETRICS/PERFORMANCE Joint International Conference on Measurement and Modeling of Computer Systems},
 doi = {10.1145/2254756.2254826},
 isbn = {978-1-4503-1097-0},
 keyword = {efficient evaluation, micro/macro views, phase-type distributions},
 link = {http://doi.acm.org/10.1145/2254756.2254826},
 location = {London, England, UK},
 numpages = {2},
 pages = {425--426},
 publisher = {ACM},
 series = {SIGMETRICS '12},
 title = {Micro and Macro Views of Discrete-state Markov Models and Their Application to Efficient Simulation with Phase-type Distributions},
 year = {2012}
}


@article{Bertran:2012:PFB:2318857.2254827,
 abstract = {
                  An abstract is not available.
              },
 acmid = {2254827},
 address = {New York, NY, USA},
 author = {Bertran, Ramon and Gonz\`{a}lez, Marc and Martorell, Xavier and Navarro, Nacho and Ayguad{\'e}, Eduard},
 doi = {10.1145/2318857.2254827},
 issn = {0163-5999},
 issue_date = {June 2012},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 keyword = {DVFs, performance counters, power estimation},
 link = {http://doi.acm.org/10.1145/2318857.2254827},
 month = {jun},
 number = {1},
 numpages = {2},
 pages = {427--428},
 publisher = {ACM},
 title = {POTRA: A Framework for Building Power Models for Next Generation Multicore Architectures},
 volume = {40},
 year = {2012}
}


@article{Shen:2012:PEC:2318857.2254814,
 abstract = {Power capping and energy efficiency are critical concerns in server systems, particularly when serving dynamic workloads on resource-sharing multicores. We present a new operating system facility (power and energy containers) that accounts for and controls the power/energy usage of individual fine-grained server requests. This facility is enabled by novel techniques for multicore power attribution to concurrent tasks, measurement/modeling alignment to enhance predictability, and request power accounting and control.},
 acmid = {2254814},
 address = {New York, NY, USA},
 author = {Shen, Kai and Shriraman, Arrvindh and Dwarkadas, Sandhya and Zhang, Xiao},
 doi = {10.1145/2318857.2254814},
 issn = {0163-5999},
 issue_date = {June 2012},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 keyword = {multicore, power and energy management, server system},
 link = {http://doi.acm.org/10.1145/2318857.2254814},
 month = {jun},
 number = {1},
 numpages = {2},
 pages = {403--404},
 publisher = {ACM},
 title = {Power and Energy Containers for Multicore Servers},
 volume = {40},
 year = {2012}
}


@article{Netrapalli:2012:LGE:2318857.2254783,
 abstract = {We consider the problem of finding the graph on which an epidemic spreads, given only the times when each node gets infected. While this is a problem of central importance in several contexts -- offline and online social networks, e-commerce, epidemiology -- there has been very little work, analytical or empirical, on finding the graph. Clearly, it is impossible to do so from just one epidemic; our interest is in learning the graph from a small number of independent epidemics. For the classic and popular "independent cascade" epidemics, we analytically establish sufficient conditions on the number of epidemics for both the global maximum-likelihood (ML) estimator, and a natural greedy algorithm to succeed with high probability. Both results are based on a key observation: the global graph learning problem decouples into n local problems -- one for each node. For a node of degree d, we show that its neighborhood can be reliably found once it has been infected O(d2 log n) times (for ML on general graphs) or O(d log n) times (for greedy on trees). We also provide a corresponding information-theoretic lower bound of Ω(d log n); thus our bounds are essentially tight. Furthermore, if we are given side-information in the form of a super-graph of the actual graph (as is often the case), then the number of epidemic samples required -- in all cases -- becomes independent of the network size n.},
 acmid = {2254783},
 address = {New York, NY, USA},
 author = {Netrapalli, Praneeth and Sanghavi, Sujay},
 doi = {10.1145/2318857.2254783},
 issn = {0163-5999},
 issue_date = {June 2012},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 keyword = {cascades, epidemics, graph structure learning},
 link = {http://doi.acm.org/10.1145/2318857.2254783},
 month = {jun},
 number = {1},
 numpages = {12},
 pages = {211--222},
 publisher = {ACM},
 title = {Learning the Graph of Epidemic Cascades},
 volume = {40},
 year = {2012}
}


@proceedings{Harrison:2012:2254756,
 abstract = {It's a great pleasure to welcome you to the 12th joint ACM SIGMETRICS and IFIP PERFORMANCE International Conference, hosted by the Department of Computing, Imperial College London - one week after the Queen's Silver Jubilee celebrations and six weeks before the 2012 Olympic Games, just the other side of Town. In fact we chose these dates so as to avoid clashing with Her Majesty's special week, which might have been compromised by an event of such stature! This year's conference enhances the tradition of both of its constituents' being the premier fora for state-ofthe-art research in performance modeling and measurement techniques, tools and applications in the American and Europe continents, respectively. We have assembled a superb technical program with 31 full papers of the highest quality and 23 posters highlighting innovative research; further details are provided in the Program Chairs' Welcome that follows. Contributors come from 18 different countries in 3 continents; 49 papers have (co-)authors from academic institutions and 22 have industrial (co-)authors. This year we have increased the numbers of Tutorials and Workshops. On Monday, 11th June there are five tutorials and two workshops: the ever-popular GreenMetrics and, for the first time, W-PIN. My thanks to Cati Lladó for her organizing the tutorials and expanding this aspect of the conference. On Friday we have three more innovative Workshops: the long-established MAMA, a new one PADE and a Hands-on Tutorial-Workshop NetFPGA. We hope as many of you as possible will take advantage of these excellent satellite events.},
 address = {New York, NY, USA},
 isbn = {978-1-4503-1097-0},
 location = {London, England, UK},
 note = {81901201},
 publisher = {ACM},
 title = {SIGMETRICS '12: Proceedings of the 12th ACM SIGMETRICS/PERFORMANCE Joint International Conference on Measurement and Modeling of Computer Systems},
 year = {2012}
}


@article{Lee:2012:BRW:2318857.2254795,
 abstract = {Graph sampling via crawling has been actively considered as a generic and important tool for collecting uniform node samples so as to consistently estimate and uncover various characteristics of complex networks. The so-called simple random walk with re-weighting (SRW-rw) and Metropolis-Hastings (MH) algorithm have been popular in the literature for such unbiased graph sampling. However, an unavoidable downside of their core random walks -- slow diffusion over the space, can cause poor estimation accuracy. In this paper, we propose non-backtracking random walk with re-weighting (NBRW-rw) and MH algorithm with delayed acceptance (MHDA) which are theoretically guaranteed to achieve, at almost no additional cost, not only unbiased graph sampling but also higher efficiency (smaller asymptotic variance of the resulting unbiased estimators) than the SRW-rw and the MH algorithm, respectively. In particular, a remarkable feature of the MHDA is its applicability for any non-uniform node sampling like the MH algorithm, but ensuring better sampling efficiency than the MH algorithm. We also provide simulation results to confirm our theoretical findings.},
 acmid = {2254795},
 address = {New York, NY, USA},
 author = {Lee, Chul-Ho and Xu, Xin and Eun, Do Young},
 doi = {10.1145/2318857.2254795},
 issn = {0163-5999},
 issue_date = {June 2012},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 keyword = {asymptotic variance, non-reversible markov chains, random walks, unbiased graph sampling},
 link = {http://doi.acm.org/10.1145/2318857.2254795},
 month = {jun},
 number = {1},
 numpages = {12},
 pages = {319--330},
 publisher = {ACM},
 title = {Beyond Random Walk and Metropolis-hastings Samplers: Why You Should Not Backtrack for Unbiased Graph Sampling},
 volume = {40},
 year = {2012}
}


@article{Patel:2012:PIF:2318857.2254758,
 abstract = {The storage industry has seen incredible growth in data storage needs by both consumers and enterprises. Long-term technology trends mean that the data deluge will continue well into the future. These trends include the big-data trend (driven by data mining analytics, high-bandwidth needs, and large content repositories), server virtualization, cloud storage, and Flash. We will cover how Flash and storage class memories (SCM) interact with some of these major trends from a performance perspective.},
 acmid = {2254758},
 address = {New York, NY, USA},
 author = {Patel, Naresh M.},
 doi = {10.1145/2318857.2254758},
 issn = {0163-5999},
 issue_date = {June 2012},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 keyword = {analytical modeling, flash-memory, performance, storage systems},
 link = {http://doi.acm.org/10.1145/2318857.2254758},
 month = {jun},
 number = {1},
 numpages = {2},
 pages = {1--2},
 publisher = {ACM},
 title = {Performance Implications of Flash and Storage Class Memories},
 volume = {40},
 year = {2012}
}


@article{Bodas:2012:CCM:2318857.2254812,
 abstract = {This paper looks at the problem of designing medium access algorithm for wireless networks with the objective of providing high throughput and low delay performance to the users, while requiring only a modest computational effort at the transmitters and receivers. Additive inter-user interference at the receivers is an important physical layer characteristic of wireless networks. Today's Wi-Fi networks are based upon the abstraction of physical layer where inter-user interference is considered as noise leading to the 'collision' model in which users are required to co-ordinate their transmissions through Carrier Sensing Multiple Access (CSMA)-based schemes to avoid interference. This, in turn, leads to an inherent performance trade-off [1]: it is impossible to obtain high throughput and low delay by means of low complexity medium access algorithm (unless P=NP). As the main result, we establish that this trade-off is primarily due to treating interference as noise in the current wireless architecture. Concretely, we develop a simple medium access algorithm that allows for simultaneous transmissions of users to the same receiver by performing joint decoding at receivers, over time. For a receiver to be able to decode multiple transmissions quickly enough, we develop appropriate congestion control where each transmitter maintains a "window" of undecoded transmitted data that is adjusted based upon the "feedback" from the receiver. In summary, this provides an efficient, low complexity "online" code operating at varying rate, and the system as a whole experiences only small amount of delay (including decoding time) while operating at high throughput.},
 acmid = {2254812},
 address = {New York, NY, USA},
 author = {Bodas, Shreeshankar and Shah, Devavrat and Wischik, Damon},
 doi = {10.1145/2318857.2254812},
 issn = {0163-5999},
 issue_date = {June 2012},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 keyword = {complexity, congestion control, delay, medium access, throughput},
 link = {http://doi.acm.org/10.1145/2318857.2254812},
 month = {jun},
 number = {1},
 numpages = {2},
 pages = {399--400},
 publisher = {ACM},
 title = {Congestion Control Meets Medium Access: Throughput, Delay, and Complexity},
 volume = {40},
 year = {2012}
}


@inproceedings{Arora:2012:FCE:2254756.2254822,
 abstract = {CPU processor design involves a large set of increasingly complex design decisions, and simulating all possible designs is typically not feasible. Sensitivity analysis, a commonly used technique, can be dependent on the starting point of the design and does not necessarily account for the cost of each parameter. This work proposes a method to simultaneously analyzes multiple parameters with a small number of experiments by leveraging the Plackett and Burman (P&B) analysis method. It builds upon the technique in two specific ways. It allows a parameter to take multiple values and replaces the unit-less impact factor with cost-proportional values.},
 acmid = {2254822},
 address = {New York, NY, USA},
 author = {Arora, Manish and Wang, Feng and Rychlik, Bob and Tullsen, Dean},
 booktitle = {Proceedings of the 12th ACM SIGMETRICS/PERFORMANCE Joint International Conference on Measurement and Modeling of Computer Systems},
 doi = {10.1145/2254756.2254822},
 isbn = {978-1-4503-1097-0},
 keyword = {Plackett and Burman, bottleneck, cost optimized},
 link = {http://doi.acm.org/10.1145/2254756.2254822},
 location = {London, England, UK},
 numpages = {2},
 pages = {419--420},
 publisher = {ACM},
 series = {SIGMETRICS '12},
 title = {Fast Cost Efficient Designs by Building Upon the Plackett and Burman Method},
 year = {2012}
}


@proceedings{Merchant:2011:1993744,
 abstract = {Welcome to San Jose, to ACM's Federated Computing Research Conference (FCRC), and to SIGMETRICS 2011! We are proud to carry on the SIGMETRICS tradition of presenting highquality, innovative research on the measurement and modeling of computer systems. The program includes papers on a wide variety of topics, including resource allocation, multicore processing, network protocols, failure analysis, power management and network characterization, using a wide variety of techniques, including mathematical analysis, simulation, emulation, prototype experimentation, observation of real systems, and combinations thereof. SIGMETRICS 2011 received 177 submissions, from which 26 papers were accepted, for an acceptance rate of 15%. Additionally, 20 submissions were selected to appear as posters, leading to a combined paper and poster acceptance rate of 26%. Each paper received at least three reviews from PC members. Over 90% of papers received four or more reviews. Overall, program committee and external reviewers provided a total of 756 reviews. The review process was conducted online using the HotCRP conference management software over a period of two months. The program committee meeting, held at Columbia University in New York, NY, in January 2011, was attended in person by 36 of the 57 PC members, and "virtually" by another four members. During the review, deliberation, and decision process, we emphasized novelty and excitement: papers that took risks and were controversial were viewed more favorably than solid papers that provided limited new insights and took limited risks. As a result, we expect the program to generate active discussion.},
 address = {New York, NY, USA},
 isbn = {978-1-4503-0814-4},
 location = {San Jose, California, USA},
 note = {81901101},
 publisher = {ACM},
 title = {SIGMETRICS '11: Proceedings of the ACM SIGMETRICS Joint International Conference on Measurement and Modeling of Computer Systems},
 year = {2011}
}


@inproceedings{Song:2012:CEM:2254756.2254796,
 abstract = {The explosive growth of social networks has created numerous exciting research opportunities. A central concept in the analysis of social networks is a proximity measure, which captures the closeness or similarity between nodes in the network. Despite much research on proximity measures, there is a lack of techniques to efficiently and accurately compute proximity measures for large-scale social networks. In this paper, we embed the original massive social graph into a much smaller graph, using a novel dimensionality reduction technique termed Clustered Spectral Graph Embedding. We show that the embedded graph captures the essential clustering and spectral structure of the original graph and allow a wide range of analysis to be performed on massive social graphs. Applying the clustered embedding to proximity measurement of social networks, we develop accurate, scalable, and flexible solutions to three important social network analysis tasks: proximity estimation, missing link inference, and link prediction. We demonstrate the effectiveness of our solutions to the tasks in the context of large real-world social network datasets: Flickr, LiveJournal, and MySpace with up to 2 million nodes and 90 million links.},
 acmid = {2254796},
 address = {New York, NY, USA},
 author = {Song, Han Hee and Savas, Berkant and Cho, Tae Won and Dave, Vacha and Lu, Zhengdong and Dhillon, Inderjit S. and Zhang, Yin and Qiu, Lili},
 booktitle = {Proceedings of the 12th ACM SIGMETRICS/PERFORMANCE Joint International Conference on Measurement and Modeling of Computer Systems},
 doi = {10.1145/2254756.2254796},
 isbn = {978-1-4503-1097-0},
 keyword = {graph clustering, graph embedding, link prediction, missing link inference, proximity estimation, social network},
 link = {http://doi.acm.org/10.1145/2254756.2254796},
 location = {London, England, UK},
 numpages = {12},
 pages = {331--342},
 publisher = {ACM},
 series = {SIGMETRICS '12},
 title = {Clustered Embedding of Massive Social Networks},
 year = {2012}
}


@inproceedings{Xu:2012:PFS:2254756.2254792,
 abstract = {Competition for shared memory resources on multiprocessors is the most dominant cause for slowing down applications and makes their performance varies unpredictably. It exacerbates the need for Quality of Service (QoS) on such systems. In this paper, we propose a fair-progress process scheduling (FPS) policy to improve system fairness. Its strategy is to force the equally-weighted applications to have the same amount of slowdown when they run concurrently. The basic approach is to monitor the progress of all applications at runtime. When we find an application suffered more slowdown and accumulated less effective work than others, we allocate more CPU time to give it a better parity. Our policy also allows different weights to different threads, and provides an effective and robust tuner that allows the OS to freely make tradeoffs between system fairness and higher throughput. Evaluation results show that FPS can significantly improve system fairness by an average of 53.5% and 65.0% on a 4-core processor with a private cache and a 4-core processor with a shared cache, respectively. The penalty is about 1.1% and 1.6% of the system throughput. For memory-intensive workloads, FPS also improves system fairness by an average of 45.2% and 21.1% on 4-core and 8-core system respectively at the expense of a throughput loss of about 2%.},
 acmid = {2254792},
 address = {New York, NY, USA},
 author = {Xu, Di and Wu, Chenggang and Yew, Pen-Chung and Li, Jianjun and Wang, Zhenjiang},
 booktitle = {Proceedings of the 12th ACM SIGMETRICS/PERFORMANCE Joint International Conference on Measurement and Modeling of Computer Systems},
 doi = {10.1145/2254756.2254792},
 isbn = {978-1-4503-1097-0},
 keyword = {memory bandwidth, performance fairness, process scheduling},
 link = {http://doi.acm.org/10.1145/2254756.2254792},
 location = {London, England, UK},
 numpages = {12},
 pages = {295--306},
 publisher = {ACM},
 series = {SIGMETRICS '12},
 title = {Providing Fairness on Shared-memory Multiprocessors via Process Scheduling},
 year = {2012}
}


@inproceedings{DiCioccio:2012:MCH:2254756.2254804,
 abstract = {This paper presents the design and evaluation of HomeNet Profiler, a tool that runs on an end-system in the home to collect data from home networks. HomeNet Profiler collects a wide range of measurements including: the set of devices, the set of services (with UPnP and Zeroconf), and the characteristics of the WiFi environment. Since the release of HomeNet Profiler in April 2011, we have collected data from over 2,400 distinct homes in 46 different countries.},
 acmid = {2254804},
 address = {New York, NY, USA},
 author = {DiCioccio, Lucas and Teixeira, Renata and Rosenberg, Catherine},
 booktitle = {Proceedings of the 12th ACM SIGMETRICS/PERFORMANCE Joint International Conference on Measurement and Modeling of Computer Systems},
 doi = {10.1145/2254756.2254804},
 isbn = {978-1-4503-1097-0},
 keyword = {end-host monitoring, home network},
 link = {http://doi.acm.org/10.1145/2254756.2254804},
 location = {London, England, UK},
 numpages = {2},
 pages = {383--384},
 publisher = {ACM},
 series = {SIGMETRICS '12},
 title = {Measuring and Characterizing Home Networks},
 year = {2012}
}


@inproceedings{Vulimiri:2012:WCP:2254756.2254775,
 abstract = {Denial of service protection mechanisms usually require classifying malicious traffic, which can be difficult. Another approach is to price scarce resources. However, while congestion pricing has been suggested as a way to combat DoS attacks, it has not been shown quantitatively how much damage a malicious player could cause to the utility of benign participants. In this paper, we quantify the protection that congestion pricing affords against DoS attacks, even for powerful attackers that can control their packets' routes. Specifically, we model the limits on the resources available to the attackers in three different ways and, in each case, quantify the maximum amount of damage they can cause as a function of their resource bounds. In addition, we show that congestion pricing is provably superior to fair queueing in attack resilience.},
 acmid = {2254775},
 address = {New York, NY, USA},
 author = {Vulimiri, Ashish and Agha, Gul A. and Godfrey, Philip Brighten and Lakshminarayanan, Karthik},
 booktitle = {Proceedings of the 12th ACM SIGMETRICS/PERFORMANCE Joint International Conference on Measurement and Modeling of Computer Systems},
 doi = {10.1145/2254756.2254775},
 isbn = {978-1-4503-1097-0},
 keyword = {DoS, congestion pricing, denial of service, security},
 link = {http://doi.acm.org/10.1145/2254756.2254775},
 location = {London, England, UK},
 numpages = {14},
 pages = {137--150},
 publisher = {ACM},
 series = {SIGMETRICS '12},
 title = {How Well Can Congestion Pricing Neutralize Denial of Service Attacks?},
 year = {2012}
}


@article{Aikat:2012:INE:2318857.2254830,
 abstract = {In this tutorial, we will introduce the SIGMETRICS/Performance community to the vast testbeds, tools and resources openly available through the GENI (Global Environment for Network Innovations) project. We will present details about the distributed computing resources available on GENI for researchers interested in simulation as well as measurement-based performance evaluation experiments. We will demonstrate simple experiments on GENI, and leave them with information on how to run experiments for research and education using GENI resources.},
 acmid = {2254830},
 address = {New York, NY, USA},
 author = {Aikat, Jay and Jeffay, Kevin},
 doi = {10.1145/2318857.2254830},
 issn = {0163-5999},
 issue_date = {June 2012},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 keyword = {measurement, modeling, network experiments, traffic generation},
 link = {http://doi.acm.org/10.1145/2318857.2254830},
 month = {jun},
 number = {1},
 numpages = {2},
 pages = {433--434},
 publisher = {ACM},
 title = {Introduction to Network Experiments Using the GENI Cyberinfrastructure},
 volume = {40},
 year = {2012}
}


@article{Glatz:2012:CIO:2318857.2254821,
 abstract = {In this work we analyze a massive data-set that captures 5.23 petabytes of traffic to shed light into the composition of one-way traffic towards a large network based on a novel one-way traffic classifier. We find that one-way traffic makes a very large fraction of all traffic in terms of flows, it can be primarily attributed to malicious causes, and it has declined since 2004 because of relative decrease of scan traffic. In addition, we show how our classifier is useful for detecting network outages.},
 acmid = {2254821},
 address = {New York, NY, USA},
 author = {Glatz, Eduard and Dimitropoulos, Xenofontas},
 doi = {10.1145/2318857.2254821},
 issn = {0163-5999},
 issue_date = {June 2012},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 keyword = {internet background radiation, traffic classification},
 link = {http://doi.acm.org/10.1145/2318857.2254821},
 month = {jun},
 number = {1},
 numpages = {2},
 pages = {417--418},
 publisher = {ACM},
 title = {Classifying Internet One-way Traffic},
 volume = {40},
 year = {2012}
}


@article{Shah:2012:OQS:2318857.2254762,
 abstract = {We consider a switched (queueing) network in which there are constraints on which queues may be served simultaneously; such networks have been used to effectively model input-queued switches and wireless networks. The scheduling policy for such a network specifies which queues to serve at any point in time, based on the current state or past history of the system. In the main result of this paper, we provide a new class of online scheduling policies that achieve optimal average queue-size scaling for a class of switched networks including input-queued switches. In particular, it establishes the validity of a conjecture about optimal queue-size scaling for input-queued switches.},
 acmid = {2254762},
 address = {New York, NY, USA},
 author = {Shah, Devavrat and Walton, Neil and Zhong, Yuan},
 doi = {10.1145/2318857.2254762},
 issn = {0163-5999},
 issue_date = {June 2012},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 keyword = {Markov chain, emulation, heavy traffic, large deviations, store-and-forward, switched network},
 link = {http://doi.acm.org/10.1145/2318857.2254762},
 month = {jun},
 number = {1},
 numpages = {12},
 pages = {17--28},
 publisher = {ACM},
 title = {Optimal Queue-size Scaling in Switched Networks},
 volume = {40},
 year = {2012}
}


@article{Gallo:2012:PER:2318857.2254810,
 abstract = {Caching is a key component for Content Distribution Networks and new Information-Centric Network architectures. In this paper, we address performance issues of caching networks running the RND replacement policy. We first prove that when the popularity distribution follows a general power-law with decay exponent α > 1, the miss probability is asymptotic to O( C1-α) for large cache size C. We further evaluate network of caches under RND policy for homogeneous tree networks and extend the analysis to tandem cache networks where caches employ either LRU or RND policies.},
 acmid = {2254810},
 address = {New York, NY, USA},
 author = {Gallo, Massimo and Kauffmann, Bruno and Muscariello, Luca and Simonian, Alain and Tanguy, Christian},
 doi = {10.1145/2318857.2254810},
 issn = {0163-5999},
 issue_date = {June 2012},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 keyword = {asymptotic analysis, cache replacement policies, content distribution networks, information-centric networking},
 link = {http://doi.acm.org/10.1145/2318857.2254810},
 month = {jun},
 number = {1},
 numpages = {2},
 pages = {395--396},
 publisher = {ACM},
 title = {Performance Evaluation of the Random Replacement Policy for Networks of Caches},
 volume = {40},
 year = {2012}
}


@inproceedings{Hayden:2012:BTA:2254756.2254828,
 abstract = {This tutorial surveys the fundamental results of the theory of martingales from the perspective of the performance engineer. We will present the fundamental results and illustrate their power through simple and elegant proofs of important and well-known results in performance analysis. The remainder of the tutorial will introduce the martingale functional central limit theorem and semi-martingale decomposition methodology for the characterization and proof of heavy-traffic limit results for Markovian queueing systems.},
 acmid = {2254828},
 address = {New York, NY, USA},
 author = {Hayden, Richard A.},
 booktitle = {Proceedings of the 12th ACM SIGMETRICS/PERFORMANCE Joint International Conference on Measurement and Modeling of Computer Systems},
 doi = {10.1145/2254756.2254828},
 isbn = {978-1-4503-1097-0},
 keyword = {heavy-traffic, martingales, queueing theory, stochastic processes},
 link = {http://doi.acm.org/10.1145/2254756.2254828},
 location = {London, England, UK},
 numpages = {2},
 pages = {429--430},
 publisher = {ACM},
 series = {SIGMETRICS '12},
 title = {Basic Theory and Some Applications of Martingales},
 year = {2012}
}


@inproceedings{Tan:2012:DTM:2254756.2254761,
 abstract = {MapReduce/Hadoop production clusters exhibit heavy-tailed characteristics for job processing times. These phenomena are resultant of the workload features and the adopted scheduling algorithms. Analytically understanding the delays under different schedulers for MapReduce can facilitate the design and deployment of large Hadoop clusters. The map and reduce tasks of a MapReduce job have fundamental difference and tight dependence between them, complicating the analysis. This also leads to an interesting starvation problem with the widely used Fair Scheduler due to its greedy approach to launching reduce tasks. To address this issue, we design and implement Coupling Scheduler, which gradually launches reduce tasks depending on map task progresses. Real experiments demonstrate improvements to job response times by up to an order of magnitude. Based on extensive measurements and source code investigations, we propose analytical models for the default FIFO and Fair Scheduler as well as our implemented Coupling Scheduler. For a class of heavy-tailed map service time distributions, i.e., regularly varying of index -a, we derive the distribution tail of the job processing delay under the three schedulers, respectively. The default FIFO Scheduler causes the delay to be regularly varying of index -a+1. Interestingly, we discover a criticality phenomenon for Fair Scheduler, the delay under which can change from regularly varying of index -a to -a+1, depending on the maximum number of reduce tasks of a job. Other more subtle behaviors also exist. In contrast, the delay distribution tail under Coupling Scheduler can be one order lower than Fair Scheduler under some conditions, implying a better performance.},
 acmid = {2254761},
 address = {New York, NY, USA},
 author = {Tan, Jian and Meng, Xiaoqiao and Zhang, Li},
 booktitle = {Proceedings of the 12th ACM SIGMETRICS/PERFORMANCE Joint International Conference on Measurement and Modeling of Computer Systems},
 doi = {10.1145/2254756.2254761},
 isbn = {978-1-4503-1097-0},
 keyword = {MapReduce, coupling scheduler, fair scheduler, first in first out, hadoop, heavy-tails, processor sharing},
 link = {http://doi.acm.org/10.1145/2254756.2254761},
 location = {London, England, UK},
 numpages = {12},
 pages = {5--16},
 publisher = {ACM},
 series = {SIGMETRICS '12},
 title = {Delay Tails in MapReduce Scheduling},
 year = {2012}
}


@article{Bhattacharya:2012:LIG:2318857.2254789,
 abstract = {The presence of software bloat in large flexible software systems can hurt energy efficiency. However, identifying and mitigating bloat is fairly effort intensive. To enable such efforts to be directed where there is a substantial potential for energy savings, we investigate the impact of bloat on power consumption under different situations. We conduct the first systematic experimental study of the joint power-performance implications of bloat across a range of hardware and software configurations on modern server platforms. The study employs controlled experiments to expose different effects of a common type of Java runtime bloat, excess temporary objects, in the context of the SPECPower_ssj2008 workload. We introduce the notion of equi-performance power reduction to characterize the impact, in addition to peak power comparisons. The results show a wide variation in energy savings from bloat reduction across these configurations. Energy efficiency benefits at peak performance tend to be most pronounced when bloat affects a performance bottleneck and non-bloated resources have low energy-proportionality. Equi-performance power savings are highest when bloated resources have a high degree of energy proportionality. We develop an analytical model that establishes a general relation between resource pressure caused by bloat and its energy efficiency impact under different conditions of resource bottlenecks and energy proportionality. Applying the model to different "what-if" scenarios, we predict the impact of bloat reduction and corroborate these predictions with empirical observations. Our work shows that the prevalent software-only view of bloat is inadequate for assessing its power-performance impact and instead provides a full systems approach for reasoning about its implications.},
 acmid = {2254789},
 address = {New York, NY, USA},
 author = {Bhattacharya, Suparna and Rajamani, Karthick and Gopinath, K. and Gupta, Manish},
 doi = {10.1145/2318857.2254789},
 issn = {0163-5999},
 issue_date = {June 2012},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 keyword = {energy proportional, energy-efficiency, power, software bloat},
 link = {http://doi.acm.org/10.1145/2318857.2254789},
 month = {jun},
 number = {1},
 numpages = {12},
 pages = {259--270},
 publisher = {ACM},
 title = {Does Lean Imply Green?: A Study of the Power Performance Implications of Java Runtime Bloat},
 volume = {40},
 year = {2012}
}


@article{Gan:2012:ECC:2318857.2254770,
 abstract = {Various link bandwidth adjustment mechanisms are being developed to save network energy. However, their interaction with congestion control can significantly reduce network throughput, and is not well understood. We firstly put forward a framework to study this interaction, and then propose an easily implementable dynamic bandwidth adjustment (DBA) mechanism for the links. In DBA, each link updates its bandwidth according to an integral control law to match its average buffer size with a target buffer size. We prove that DBA reduces link bandwidth without sacrificing throughput---DBA only turns off excess bandwidth---in the presence of congestion control. Preliminary ns2 simulations confirm this result.},
 acmid = {2254770},
 address = {New York, NY, USA},
 author = {Gan, Lingwen and Walid, Anwar and Low, Steven},
 doi = {10.1145/2318857.2254770},
 issn = {0163-5999},
 issue_date = {June 2012},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 keyword = {bandwidth adjustment, congestion control, stability},
 link = {http://doi.acm.org/10.1145/2318857.2254770},
 month = {jun},
 number = {1},
 numpages = {12},
 pages = {89--100},
 publisher = {ACM},
 title = {Energy-efficient Congestion Control},
 volume = {40},
 year = {2012}
}


@inproceedings{Figueiredo:2012:CCT:2254756.2254794,
 abstract = {In this paper we study the behavior of a continuous time random walk (CTRW) on a stationary and ergodic time varying dynamic graph. We establish conditions under which the CTRW is a stationary and ergodic process. In general, the stationary distribution of the walker depends on the walker rate and is difficult to characterize. However, we characterize the stationary distribution in the following cases: i) the walker rate is significantly larger or smaller than the rate in which the graph changes (time-scale separation), ii) the walker rate is proportional to the degree of the node that it resides on (coupled dynamics), and iii) the degrees of node belonging to the same connected component are identical (structural constraints). We provide examples that illustrate our theoretical findings.},
 acmid = {2254794},
 address = {New York, NY, USA},
 author = {Figueiredo, Daniel and Nain, Philippe and Ribeiro, Bruno and de Souza e Silva, Edmundo and Towsley, Don},
 booktitle = {Proceedings of the 12th ACM SIGMETRICS/PERFORMANCE Joint International Conference on Measurement and Modeling of Computer Systems},
 doi = {10.1145/2254756.2254794},
 isbn = {978-1-4503-1097-0},
 keyword = {continuous time random walks, dynamic graphs, random walks, stationary distribution, time scale separation, time varying graphs},
 link = {http://doi.acm.org/10.1145/2254756.2254794},
 location = {London, England, UK},
 numpages = {12},
 pages = {307--318},
 publisher = {ACM},
 series = {SIGMETRICS '12},
 title = {Characterizing Continuous Time Random Walks on Time Varying Graphs},
 year = {2012}
}


