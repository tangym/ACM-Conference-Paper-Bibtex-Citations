@INPROCEEDINGS{8056941, 
author={}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Index}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-1}, 
abstract={Start of the above-titled section of the conference proceedings record.}, 
keywords={}, 
doi={10.1109/INFOCOM.2017.8056941}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8056942, 
author={}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Program}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-25}, 
abstract={Provides a schedule of conference events and a listing of which papers were presented in each session.}, 
keywords={}, 
doi={10.1109/INFOCOM.2017.8056942}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8056943, 
author={}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Committees}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-2}, 
abstract={Provides a listing of current committee members and society officers.}, 
keywords={}, 
doi={10.1109/INFOCOM.2017.8056943}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8056944, 
author={C. Tian and A. Munir and A. X. Liu and Y. Liu and Y. Li and J. Sun and F. Zhang and G. Zhang}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Multi-tenant multi-objective bandwidth allocation in datacenters using stacked congestion control}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={In datacenter networks, flows can have different performance objectives. We use a tenant-objective division to denote all flows of a tenant that share the same objective. Bandwidth allocation in datacenters should support not only performance isolation among divisions but also objective-oriented scheduling among flows within the same division. This paper studies the Multi-Tenant Multi-Objective (MT-MO) bandwidth allocation problem. To our best knowledge, no existing practical work support performance isolation and objective scheduling simultaneously. We propose Stacked Congestion Control (SCC), a distributed host-based bandwidth allocation design, where an underlay congestion control (UCC) layer handles contention among divisions, and a private congestion control (PCC) layer for each division optimizes its performance objective. Via the tenant-objective tunnel abstraction, SCC achieves weighted bandwidth sharing for each division in a distributed and transparent way. By adding a rate-limiting send queue in the ingress of each tunnel, mechanisms between performance isolation and objective scheduling are completely decoupled. We evaluate SCC both on a small-scale testbed and with large-scale NS-2 simulations. Compared to the direct coexistence cases, SCC reduces latency by up to 40% for Latency-Sensitive flows, deadline miss ratio by up to 3.2× for Deadline-Sensitive flows, and average flow-completion-time by up to 53% for Completion-Sensitive flows.}, 
keywords={bandwidth allocation;computer centres;computer networks;control engineering computing;queueing theory;telecommunication congestion control;telecommunication scheduling;telecommunication traffic;Completion-Sensitive flows;Deadline-Sensitive flows;Latency-Sensitive flows;MT-MO bandwidth allocation problem;Multitenant multiobjective bandwidth allocation;PCC;SCC;UCC;datacenter networks;distributed host-based bandwidth allocation design;large-scale NS-2 simulation;objective-oriented scheduling;private congestion control;private congestion control layer;stacked congestion control;tenant-objective tunnel abstraction;underlay congestion control;Bandwidth;Channel allocation;Conferences;Interference;Optimal scheduling;Servers;Switches}, 
doi={10.1109/INFOCOM.2017.8056944}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8056945, 
author={R. Yu and G. Xue and X. Zhang and D. Li}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Survivable and bandwidth-guaranteed embedding of virtual clusters in cloud data centers}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Cloud computing has emerged as a powerful and elastic platform for internet service hosting, yet it also draws concerns of the unpredictable performance of cloud-based services due to network congestion. To offer predictable performance, the virtual cluster abstraction of cloud services has been proposed, which enables allocation and performance isolation regarding both computing resources and network bandwidth in a simplified virtual network model. One issue arisen in virtual cluster allocation is the survivability of tenant services against physical failures. Existing works have studied virtual cluster backup provisioning with fixed primary embeddings, but have not considered the impact of primary embeddings on backup resource consumption. To address this issue, in this paper we study how to embed virtual clusters survivably in the cloud data center, by jointly optimizing primary and backup embeddings of the virtual clusters. We formally define the survivable virtual cluster embedding problem. We then propose a novel algorithm, which computes the most resource-efficient embedding given a tenant request. Since the optimal algorithm has high time complexity, we further propose a faster heuristic algorithm, which is several orders faster than the optimal solution, yet able to achieve similar performance. Besides theoretical analysis, we evaluate our algorithms via extensive simulations.}, 
keywords={Internet;cloud computing;computer centres;embedded systems;optimisation;resource allocation;telecommunication network reliability;telecommunication traffic;virtualisation;backup resource consumption;cloud computing;cloud data center;cloud-based services;computing resources;internet service hosting;network bandwidth;network congestion;survivable virtual cluster embedding problem;tenant services;virtual cluster abstraction;virtual cluster allocation;virtual cluster backup;virtual network model;Algorithm design and analysis;Bandwidth;Cloud computing;Clustering algorithms;Heuristic algorithms;Resource management;Virtual cluster;bandwidth guarantee;survivability}, 
doi={10.1109/INFOCOM.2017.8056945}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8056946, 
author={Y. Lu and G. Chen and L. Luo and K. Tan and Y. Xiong and X. Wang and E. Chen}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={One more queue is enough: Minimizing flow completion time with explicit priority notification}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Ideally, minimizing the flow completion time (FCT) requires millions of priorities supported by the underlying network so that each flow has its unique priority. However, in production datacenters, the available switch priority queues for flow scheduling are very limited (merely 2 or 3). This practical constraint seriously degrades the performance of previous approaches. In this paper, we introduce Explicit Priority Notification (EPN), a novel scheduling mechanism which emulates fine-grained priorities (i.e., desired priorities or DP) using only two switch priority queues. EPN can support various flow scheduling disciplines with or without flow size information. We have implemented EPN on commodity switches and evaluated its performance with both testbed experiments and extensive simulations. Our results show that, with flow size information, EPN achieves comparable FCT as pFabric that requires clean-slate switch hardware. And EPN also outperforms TCP by up to 60.5% if it bins the traffic into two priority queues according to flow size. In information-agnostic setting, EPN outperforms PIAS with two priority queues by up to 37.7%. To the best of our knowledge, EPN is the first system that provides millions of priorities for flow scheduling with commodity switches.}, 
keywords={computer centres;packet switching;queueing theory;scheduling;telecommunication traffic;transport protocols;EPN;TCP;clean-slate switch hardware;commodity switches;fine-grained priorities;flow completion time;flow scheduling;flow size information;queue;scheduling mechanism;switch priority queues;Bandwidth;Conferences;Dynamic scheduling;Hardware;Processor scheduling;Switches}, 
doi={10.1109/INFOCOM.2017.8056946}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8056947, 
author={J. E and Y. Cui and P. Wang and Z. Li and C. Zhang}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={CoCloud: Enabling efficient cross-cloud file collaboration based on inefficient web APIs}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Cloud storage services such as Dropbox have been widely used for file collaboration among multiple users. However, this desirable functionality is yet restricted to the “walled-garden” of each service. At present, the only effective approach to cross-cloud file collaboration seems to be using web APIs, whose performance is known to be highly unstable and unpredictable. Now that using inefficient web APIs is inevitable, in this paper we attempt to achieve sound user-perceived performance for cross-cloud file collaboration. This attempt is enabled by two key observations from real-world measurements. First, for each cloud, we are always able to deploy one or several nearby (client) proxies which can efficiently access the web APIs. Second, during file collaboration, significant similarity exists among different versions of a file. This can be exploited to substantially reduce inter-proxy traffic and thus shorten the data sync time. Guided by the observations, we design and implement an open-source prototype system called CoCloud. Currently, it supports file collaboration among four popular cloud storage services in the US and China. Its performance is well acceptable to users under representative workloads, even approaching or exceeding intra-cloud performance in many cases.}, 
keywords={application program interfaces;cloud computing;public domain software;storage management;China;CoCloud;US;Web API;cloud storage services;cross-cloud file collaboration;data sync time reduction;interproxy traffic reduction;intracloud performance;open-source prototype system;user-perceived performance;Cloud computing;Collaboration;Google;Metadata;Protocols;Servers;Synchronization}, 
doi={10.1109/INFOCOM.2017.8056947}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8056948, 
author={C. Chen and W. Wang and S. Zhang and B. Li}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Cluster fair queueing: Speeding up data-parallel jobs with delay guarantees}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Cluster scheduler serves as a critical component to data-parallel systems in datacenters. Ideally, a scheduler should provide predictable performance with guarantees on the maximal job completion delay, while at the same time ensuring the minimal mean response time. Practically however, performance predictability and optimality are often conflicting with each other. The results often are a plethora of scheduling policies that either achieve predictable performance at the expense of long response times (e.g., max-min fairness), or run the risk of starving some jobs to obtain the minimal mean response time (e.g., Shortest Remaining Processing Time First). To address these problems, we develop a new scheduler, Cluster Fair Queueing (CFQ), which preferentially offers resources to jobs that complete the earliest under a fair sharing policy. We show that CFQ is able to minimize the mean response time while at the same time ensuring jobs to finish within a constant time after their completion under fair sharing. Our Spark deployment on a 100-node EC2 cluster demonstrates that compared to the built-in fair scheduler, CFQ can decrease the mean response time by 40%, which speeds up more than 40% of jobs by over 75% on average.}, 
keywords={queueing theory;scheduling;CFQ;Cluster Fair Queueing;Cluster fair queueing;Cluster scheduler;EC2 cluster;Shortest Remaining Processing Time First;Spark deployment;constant time;data-parallel jobs;data-parallel systems;delay guarantees;fair scheduler;fair sharing policy;job completion delay;max-min fairness;mean response time;performance predictability;scheduling policies;Delays;Global Positioning System;Prediction algorithms;Resource management;Sparks;Time factors}, 
doi={10.1109/INFOCOM.2017.8056948}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8056949, 
author={L. Chen and S. Liu and B. Li and B. Li}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Scheduling jobs across geo-distributed datacenters with max-min fairness}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={It has become routine for large volumes of data to be generated, stored, and processed across geographically distributed datacenters. To run a single data analytic job on such geo-distributed data, recent research proposed to distribute its tasks across datacenters, considering both data locality and network bandwidth across datacenters. Yet, it remains an open problem in the more general case, where multiple analytic jobs need to fairly share the resources at these geo-distributed data-centers. In this paper, we focus on the problem of assigning tasks belonging to multiple jobs across datacenters, with the specific objective of achieving max-min fairness across jobs sharing these datacenters, in terms of their job completion times. We formulate this problem as a lexicographical minimization problem, which is challenging to solve in practice due to its inherent multi-objective and discrete nature. To address these challenges, we iteratively solve its single-objective subproblems, which can be transformed to equivalent linear programming (LP) problems to be efficiently solved, thanks to their favorable properties. As a highlight of this paper, we have designed and implemented our proposed solution as a fair job scheduler based on Apache Spark, a modern data processing framework. With extensive evaluations of our real-world implementation on Amazon EC2, we have shown convincing evidence that max-min fairness has been achieved using our new job scheduler.}, 
keywords={cloud computing;computer centres;data analysis;linear programming;minimax techniques;scheduling;Amazon EC2;Apache Spark;data analytic job scheduling;data centers;data locality;data processing framework;fair job scheduler;geo-distributed datacenters;geographically distributed datacenters;job completion times;lexicographical minimization problem;linear programming problems;max-min fairness;network bandwidth;single-objective subproblems;Bandwidth;Conferences;Data analysis;Distributed databases;Minimization;Sparks}, 
doi={10.1109/INFOCOM.2017.8056949}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8056950, 
author={C. Li and D. Feng and Y. Hua and W. Xia and L. Qin and Y. Huang and Y. Zhou}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={BAC: Bandwidth-aware compression for efficient live migration of virtual machines}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Live migration of virtual machines (VM) is one of the key characteristics of virtualization for load balancing, system maintenance, power management, etc., in data centers or clusters. In order to reduce the data transferred and shorten the migration time, the compression techniques have been widely used to accelerate VM migration. However, different compression approaches have different compression ratios and speeds. Because there is a trade-off between compression and transmission, the migration performance improvements obtained from different compression approaches are differentiated, and the improvements vary with the network bandwidth. Besides, the compression window sizes used in most compression algorithms are typically much larger than a single page size, so the traditional single page compression loses some potential compression benefits. In this paper, we design and implement a Bandwidth-Aware Compression (BAC) scheme for VM migration. BAC chooses suitable compression approach according to the network bandwidth available for the migration process, and employs multi-page compression. These features make BAC obtain more migration performance improvements from compression. Experiments under various network scenarios demonstrate that, compared with conventional compression approaches, BAC shortens the total migration time while achieving comparable performance for the total data transferred and the downtime.}, 
keywords={computer centres;data compression;resource allocation;virtual machines;workstation clusters;BAC;Bandwidth-Aware Compression scheme;VM migration;clusters;compression ratios;compression window;data centers;efficient live migration;migration performance improvements;migration process;multipage compression;network bandwidth;single page size;total migration time;traditional single page compression;virtual machines;Acceleration;Bandwidth;Kernel;Load management;Maintenance engineering;Virtualization}, 
doi={10.1109/INFOCOM.2017.8056950}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8056951, 
author={A. O. F. Atya and Z. Qian and S. V. Krishnamurthy and T. L. Porta and P. McDaniel and L. Marvel}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Malicious co-residency on the cloud: Attacks and defense}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Attacker VMs try to co-reside with victim VMs on the same physical infrastructure as a precursor to launching attacks that target information leakage. VM migration is an effective countermeasure against attempts at malicious co-residency. In this paper, we first undertake an experimental study on Amazon EC2 to obtain an in-depth understanding of the side-channels an attacker can use to ascertain co-residency with a victim. Here, we identify a new set of stealthy side-channel attacks which, we show to be more effective than currently available attacks towards verifying co-residency. Based on the study, we develop a set of guidelines to determine under what conditions victim VM migrations should be triggered given performance costs in terms of bandwidth and downtime, that a user is willing to bear. Via extensive experiments on our private in-house cloud, we show that migrations using our guidelines can limit the fraction of the time that an attacker VM co-resides with a victim VM to about 1 % of the time with bandwidth costs of a few MB and downtimes of a few seconds, per day per VM migrated.}, 
keywords={cloud computing;security of data;virtual machines;Amazon EC2;VM migration;attacker VM;bandwidth costs;in-house cloud;information leakage;launching attacks;malicious co-residency;physical infrastructure;stealthy side-channel attacks;victim VM;Bandwidth;Cloud computing;Conferences;Guidelines;Side-channel attacks;Timing;Virtual machine monitors}, 
doi={10.1109/INFOCOM.2017.8056951}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8056952, 
author={W. Sun and N. Zhang and W. Lou and Y. T. Hou}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={When gene meets cloud: Enabling scalable and efficient range query on encrypted genomic data}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={As the cost of human full genome sequencing continues to fall, we will soon witness a prodigious amount of human genomic data in the public cloud. To protect the confidentiality of the genetic information of individuals, the data has to be encrypted at rest. On the other hand, encryption severely hinders the use of this valuable information, such as Genome-wide Range Query (GRQ), in medical/genomic research. While the problem of secure range query on outsourced encrypted data has been extensively studied, the current schemes are far from practical deployment in terms of efficiency and scalability due to the data volume in human genome sequencing. In this paper, we investigate the problem of secure GRQ over human raw aligned genomic data in a third-party outsourcing model. Our solution contains a novel secure range query scheme based on multi-keyword symmetric searchable encryption (MSSE). The proposed scheme incurs minimal ciphertext expansion and computation overhead. We also present a hierarchical GRQ-oriented secure index structure tailored for efficient and large-scale genomic data lookup in the cloud while preserving the query privacy. Our experiment on real human genomic data shows that a secure GRQ request with range size 100,000 over more than 300 million encrypted short reads takes less than 3 minutes, which is orders of magnitude faster than existing solutions.}, 
keywords={biology computing;cloud computing;cryptography;data privacy;genetics;genomics;outsourcing;query processing;GRQ;GRQ request;Genome-wide Range Query;MSSE;ciphertext expansion;data volume;encrypted genomic data;hierarchical GRQ;human full genome sequencing;human genome sequencing;human genomic data;human raw aligned genomic data;large-scale genomic data lookup;medical/genomic research;multikeyword symmetric searchable encryption;outsourced encrypted data;public cloud;query privacy;scalable range query;secure index structure;secure range query scheme;third-party outsourcing model;Bioinformatics;Cloud computing;Encryption;Genomics;Sequential analysis}, 
doi={10.1109/INFOCOM.2017.8056952}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8056953, 
author={X. Li and Q. Xue and M. C. Chuah}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={CASHEIRS: Cloud assisted scalable hierarchical encrypted based image retrieval system}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Image retrieval has become an important function in many emerging computer vision applications e.g. online shopping via images, medical health care systems. More and more images are being generated and stored in public clouds. However, recent photo leakage events raise concerns about privacy leaks for images stored in public clouds. In this paper, we present an efficient scalable hierarchical image retrieval system (CASHEIRS) which provides privacy-aware image retrieval feature. CASHEIRS employs transformed Convolutional Neural Network features to improve image retrieval accuracy and an encrypted hierarchical index tree to speed up the query process. Extensive evaluations using Caltech256 and INRIA Holiday datasets show that CASHEIRS is more effective than three existing schemes. We also demonstrate its practicality on a mobile device.}, 
keywords={cloud computing;computer vision;content-based retrieval;data privacy;health care;image retrieval;neural nets;CASHEIRS system;Caltech256 dataset;Convolutional Neural Network features;INRIA Holiday dataset;computer vision applications;encrypted hierarchical index tree;hierarchical index tree;image retrieval accuracy;medical health care systems;photo leakage events;privacy leaks;privacy-aware image retrieval feature;public clouds;scalable hierarchical image retrieval system;Binary codes;Cloud computing;Cryptography;Feature extraction;Image retrieval;Indexes;Servers;Convolutional Neural Network;Hierarchical image retrieval;image privacy}, 
doi={10.1109/INFOCOM.2017.8056953}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8056954, 
author={L. Yang and Q. Zheng and X. Fan}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={RSPP: A reliable, searchable and privacy-preserving e-healthcare system for cloud-assisted body area networks}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={The integration of cloud computing and Internet of Things (loT) is quickly becoming the key enabler for the digital transformation of the healthcare industry by offering comprehensive improvements in patient engagements, productivity and risk mitigation. This paradigm shift, while bringing numerous benefits and new opportunities to healthcare organizations, has raised a lot of security and privacy concerns. In this paper, we present a reliable, searchable and privacy-preserving e-healthcare system, which takes advantage of emerging cloud storage and IoT infrastructure and enables healthcare service providers (HSPs) to realize remote patient monitoring in a secure and regulatory compliant manner. Our system is built upon a novel dynamic searchable symmetric encryption scheme with forward privacy and delegated verifiability for periodically generated healthcare data. While the forward privacy is achieved by maintaining an increasing counter for each keyword at an IoT gateway, the data owner delegated verifiability comes from the combination of the Bloom filter and aggregate message authentication code. Moreover, our system is able to support multiple HSPs through either data owner assistance or delegation. The detailed security analysis as well as the extensive simulations on a large data set with millions of records demonstrate the practical efficiency of the proposed system for real world healthcare applications.}, 
keywords={body area networks;cloud computing;cryptography;data privacy;health care;medical computing;message authentication;patient monitoring;Bloom filter;HSP;cloud computing;cloud storage;cloud-assisted body area networks;data owner assistance;dynamic searchable symmetric encryption scheme;forward privacy;healthcare industry;healthcare service providers;message authentication code;periodically generated healthcare data;privacy concerns;privacy-preserving e-healthcare system;security analysis;security concerns;world healthcare applications;Cloud computing;Encryption;Medical services;Reliability;Servers}, 
doi={10.1109/INFOCOM.2017.8056954}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8056955, 
author={Q. Wang and S. Hu and M. Du and J. Wang and K. Ren}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Learning privately: Privacy-preserving canonical correlation analysis for cross-media retrieval}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={A massive explosion of various types of data has been triggered in the “Big Data” era. In big data systems, machine learning plays an important role due to its effectiveness in discovering hidden information and valuable knowledge. Data privacy, however, becomes an unavoidable concern since big data usually involve multiple organizations, e.g., different healthcare systems and hospitals, who are not in the same trust domain and may be reluctant to share their data publicly. Applying traditional cryptographic tools is a straightforward approach to protect sensitive information, but it often renders learning algorithms useless inevitably. In this work, we, for the first time, propose a novel privacy-preserving scheme for canonical correlation analysis (CCA), which is a well-known learning technique and has been widely used in cross-media retrieval system. We first develop a library of building blocks to support various arithmetics over encrypted real numbers by leveraging additively homomorphic encryption and garbled circuits. Then we encrypt private data by randomly splitting the numerical data, formalize CCA problem and reduce it to a symmetric eigenvalue problem by designing new protocols for privacy-preserving QR decomposition. Finally, we solve all the eigenvalues and the corresponding eigenvectors by running Newton-Raphson method and inverse power method over the ciphertext domain. We carefully analyze the security and extensively evaluate the effectiveness of our design. The results show that our scheme is practically secure, incurs negligible errors compared with performing CCA in the clear and performs comparably in cross-media retrieval systems.}, 
keywords={Big Data;Newton-Raphson method;cryptography;data privacy;eigenvalues and eigenfunctions;information retrieval;learning (artificial intelligence);CCA problem;Data privacy;Privacy-preserving canonical correlation analysis;big data systems;cross-media retrieval system;cryptographic tools;encrypted real numbers;garbled circuits;health care systems;hidden information;homomorphic encryption;hospitals;inverse power method;machine learning;numerical data;privacy-preserving QR decomposition;privacy-preserving scheme;private data;symmetric eigenvalue problem;trust domain;Big Data;Correlation;Cryptography;Eigenvalues and eigenfunctions;Protocols;Servers;Wires}, 
doi={10.1109/INFOCOM.2017.8056955}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8056956, 
author={P. Li and H. Dau and G. Puleo and O. Milenkovic}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Motif clustering and overlapping clustering for social network analysis}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Motivated by applications in social network community analysis, we introduce a new clustering paradigm termed motif clustering. Unlike classical clustering, motif clustering aims to minimize the number of clustering errors associated with both edges and certain higher order graph structures (motifs) that represent “atomic units” of social organizations. Our contributions are two-fold: We first introduce motif correlation clustering, in which the goal is to agnostically partition the vertices of a weighted complete graph so that certain predetermined “important” social subgraphs mostly lie within the same cluster, while “less relevant” social subgraphs are allowed to lie across clusters. We then proceed to introduce the notion of motif covers, in which the goal is to cover the vertices of motifs via the smallest number of (near) cliques in the graph. Motif cover algorithms provide a natural solution for overlapping clustering and they also play an important role in latent feature inference of networks. For both motif correlation clustering and its extension introduced via the covering problem, we provide hardness results, algorithmic solutions and community detection results for two well-studied social networks.}, 
keywords={graph theory;network theory (graphs);pattern clustering;clustering errors;clustering paradigm;higher order graph structures;motif clustering;motif correlation clustering;motif cover algorithms;motif covers;overlapping clustering;social network analysis;social network community analysis;social networks;social organizations;social subgraphs;weighted complete graph;Approximation algorithms;Clustering algorithms;Clustering methods;Conferences;Correlation;Linear programming;Social network services}, 
doi={10.1109/INFOCOM.2017.8056956}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8056957, 
author={G. A. Tong and W. Wu and L. Guo and D. Li and C. Liu and B. Liu and D. Z. Du}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={An efficient randomized algorithm for rumor blocking in online social networks}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Social networks allow rapid spread of ideas and innovations while the negative information can also propagate widely. When the cascades with different opinions reaching the same user, the cascade arriving first is the most likely to be taken by the user. Therefore, once misinformation or rumor is detected, a natural containment method is to introduce a positive cascade competing against the rumor. Given a budget k, the rumor blocking problem asks for k seed users to trigger the spread of the positive cascade such that the number of the users who are not influenced by rumor can be maximized. The prior works have shown that the rumor blocking problem can be approximated within a factor of (1 - 1/e- δ) by a classic greedy algorithm combined with Monte Carlo simulation with the running time of O(k3 mn ln n/δ2), where n and m are the number of users and edges, respectively. Unfortunately, the Monte-Carlo-simulation-based methods are extremely time consuming and the existing algorithms either trade performance guarantees for practical efficiency or vice versa. In this paper, we present a randomized algorithm which runs in O(km ln n/δ2) expected time and provides a (1 - 1/e - δ)-approximation with a high probability. The experimentally results on both the real-world and synthetic social networks have shown that the proposed randomized rumor blocking algorithm is much more efficient than the state-of-the-art method and it is able to find the seed nodes which are effective in limiting the spread of rumor.}, 
keywords={Monte Carlo methods;greedy algorithms;randomised algorithms;social networking (online);Monte Carlo simulation;classic greedy algorithm;natural containment method;online social networks;positive cascade;randomized algorithm;randomized rumor blocking algorithm;rumor blocking problem;synthetic social networks;Approximation algorithms;Diffusion processes;Integrated circuit modeling;Monte Carlo methods;Sampling methods;Social network services}, 
doi={10.1109/INFOCOM.2017.8056957}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8056958, 
author={J. Ok and J. Shin and Y. Yi}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Incentivizing strategic users for social diffusion: Quantity or quality?}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={We consider a problem of how to effectively diffuse a new product over social networks by incentivizing selfish users. Traditionally, this problem has been studied in the form of influence maximization via seeding, where most prior work assumes that seeded users unconditionally and immediately start by adopting the new product and they stay at the new product throughout their lifetime. However, in practice, seeded users often adjust the degree of their willingness to diffuse, depending on how much incentive is given. To address such diffusion willingness, we propose a new incentive model and characterize the speed of diffusion as the value of a combinatorial optimization. Then, we apply the characterization to popular network graph topologies (Erdos-Renyi, planted partition and power law graphs) as well as general ones, for asymptotically computing the diffusion time for those graphs. Our analysis shows that the diffusion time undergoes two levels of order-wise reduction, where the first and second one are solely contributed by the number of seeded users, i.e., quantity, and the amount of incentives, i.e., quality, respectively. In other words, it implies that the best strategy given budget is (a) first identify the minimum seed set depending on the underlying graph topology, and (b) then assign largest possible incentives to users in the set. We believe that our theoretical results provide useful implications and guidelines for designing successful advertising strategies in various practical applications.}, 
keywords={complex networks;graph theory;network theory (graphs);social sciences;Erdos-Renyi graph;diffusion time;graph topology;incentive model;minimum seed set;network graph topologies;planted partition graph;power law graphs;seeded users;selfish users;social diffusion;social networks;strategic users;Algorithm design and analysis;Conferences;Games;Network topology;Noise measurement;Social network services;Topology}, 
doi={10.1109/INFOCOM.2017.8056958}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8056959, 
author={M. T. A. Amin and C. Aggarwal and S. Yao and T. Abdelzaher and L. Kaplan}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Unveiling polarization in social networks: A matrix factorization approach}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={This paper presents unsupervised algorithms to uncover polarization in social networks (namely, Twitter) and identify polarized groups. The approach is language-agnostic and thus broadly applicable to global and multilingual media. In cases of conflict, dispute, or situations involving multiple parties with contrasting interests, opinions get divided into different camps. Previous manual inspection of tweets has shown that such situations produce distinguishable signatures on Twitter, as people take sides leading to clusters that preferentially propagate information confirming their individual cluster-specific bias. We propose a model for polarized social networks, and show that approaches based on factorizing the matrix of sources and their claims can automate the discovery of polarized clusters with no need for prior training or natural language processing. In turn, identifying such clusters offers insights into prevalent social conflicts and helps automate the generation of less biased descriptions of ongoing events. We evaluate our factorization algorithms and their results on multiple Twitter datasets involving polarization of opinions, demonstrating the efficacy of our approach. Experiments show that our method is almost always correct in identifying the polarized information from real-world twitter traces, and outperforms the baseline mechanisms by a large margin.}, 
keywords={matrix decomposition;social networking (online);biased descriptions;distinguishable signatures;factorization algorithms;global media;individual cluster-specific bias;language-agnostic;matrix factorization approach;multilingual media;multiple Twitter datasets;polarized clusters;polarized groups;polarized information;polarized social networks;prevalent social conflicts;real-world twitter traces;social networks;unsupervised algorithms;unveiling polarization;Algorithm design and analysis;Clustering algorithms;Conferences;Media;Natural language processing;Twitter}, 
doi={10.1109/INFOCOM.2017.8056959}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8056960, 
author={Q. Chen and H. Gao and S. Cheng and J. Li and Z. Cai}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Distributed non-structure based data aggregation for duty-cycle wireless sensor networks}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Data aggregation is an essential operation for the sink to obtain summary information in a Wireless Sensor Network (WSN). The problem of Minimum Latency Aggregation Schedule (MLAS) which seeks a fastest and collision-free aggregation schedule has been well studied when nodes are always awake. However, in duty-cycle WSNs, nodes can only receive data in active state. In such networks, it is of great importance to exploit the limited active time slots to reduce aggregation latency. Unfortunately, few studies have addressed this issue and most previous aggregation methods rely on fixed structures which greatly limit the exploitation of the active time slots from other neighbors. In this paper, we investigate the MLAS problem in duty-cycle WSNs without considering structures. We propose the first distributed aggregation algorithm for duty-cycle WSNs, in which the aggregation tree and a conflict-free schedule are generated simultaneously. Compared with the previous centralized and distributed methods, the aggregation latency and the utilization ratio of available time slots are greatly improved. The theoretical analysis and simulation results verify that the proposed algorithm has high performance in terms of latency and communication cost.}, 
keywords={data aggregation;telecommunication scheduling;trees (mathematics);wireless sensor networks;MLAS problem;Minimum Latency Aggregation Schedule;Wireless Sensor Network;active state;active time slots;aggregation methods;aggregation tree;centralized distributed methods;collision-free aggregation schedule;conflict-free schedule;distributed aggregation algorithm;distributed nonstructure based data aggregation;duty-cycle WSNs;duty-cycle wireless sensor networks;fixed structures;time slots;Approximation algorithms;Conferences;Data aggregation;Interference;Schedules;Scheduling algorithms;Wireless sensor networks}, 
doi={10.1109/INFOCOM.2017.8056960}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8056961, 
author={K. S. Liu and T. Mayer and H. T. Yang and E. Arkin and J. Gao and M. Goswami and M. P. Johnson and N. Kumar and S. Lin}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Joint sensing duty cycle scheduling for heterogeneous coverage guarantee}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={In this paper we study the following problem: given a set of m sensors that collectively cover a set of n target points with heterogeneous coverage requirements (target j needs to be covered every fj slots), how to schedule the sensor duty cycles such that all coverage requirements are satisfied and the maximum number of sensors turned on at any time slot is minimized. The problem models varied real-world applications in which sensing tasks exhibit high discrepancy in coverage requirements - critical locations often need to be covered much more frequently. We provide multiple algorithms with best approximation ratio of O (log n + log m) for the maximum number of sensors to turn on, and bi-criteria algorithm with (α, β)-approximation factors with high probability, where the number of sensors turned on is an α = O(δ(log (n) + log(m))/β)-approximation of the optimal (satisfying all requirements) and the coverage requirement is a β-approximation; δ is the approximation ratio achievable in an appropriate instance of set multi-cover. When the sensor coverage exhibits extra geometric properties, the approximation ratios can be further improved. We also evaluated our algorithms via simulations and experiments on a camera testbed. The performance improvement (energy saving) is substantial compared to turning on all sensors all the time, or a random scheduling baseline.}, 
keywords={approximation theory;probability;sensor placement;telecommunication power management;telecommunication scheduling;bicriteria algorithm;heterogeneous coverage requirements;joint sensing duty cycle scheduling;sensor duty cycles;Approximation algorithms;Cameras;Computer science;Monitoring;Optimal scheduling;Schedules;Sensors}, 
doi={10.1109/INFOCOM.2017.8056961}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8056962, 
author={K. Fu and W. Ren and W. Dong}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Multihop calibration for mobile sensing: K-hop Calibratability and reference sensor deployment}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Mobile vehicles, equipped with low-cost sensors, can provide unprecedented opportunities for urban monitoring with a wide coverage. The quality of collected data is very important for many applications. The low-cost mobile sensors can, however, suffer from limited accuracy, high instability, and sensor drift. Therefore, they need to be frequently calibrated to preserve a good data quality. Frequent sensor calibration can be achieved by deploying static high-precision sensors (called reference sensors) and exploiting meeting points (i.e., rendezvous) between the reference sensors and mobile sensors. A calibrated mobile sensor can also be used to calibrate an uncalibrated mobile sensor when they meet, referred to as multihop calibration. In this paper, we introduce a novel concept, k-hop calibratability, in the context of multihop calibration: a sensor is k-hop calibratable if the calibration path is no larger than k. We consider the problem of how to deploy the reference sensors to ensure that all sensors in the network are k-hop calibratable. To address this problem, we formally define the rendezvous connection graph to precisely describe the relationship between mobile sensors and (virtual) reference sensors. Based on this definition, we formulate the reference sensor deployment problem as a set cover problem. We also extend our problem to consider practical deployment requirements. We propose efficient algorithms and conduct an extensive evaluation in the context of mobile air quality monitoring. We present a detailed prototype implementation of the mobile sensors and reference sensors. Evaluations using real-world datasets show the effectiveness of the algorithms.}, 
keywords={calibration;mobile radio;sensor placement;wireless sensor networks;frequent sensor calibration;high-precision sensors;k-hop calibratability;low-cost mobile sensors;mobile air quality monitoring;mobile sensing;mobile vehicles;multihop calibration;reference sensor deployment problem;rendezvous connection graph;sensor drift;urban monitoring;Air quality;Calibration;Mobile communication;Monitoring;Robot sensing systems;Urban areas}, 
doi={10.1109/INFOCOM.2017.8056962}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8056963, 
author={Y. Wu and Y. Wang and G. Cao}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Photo crowdsourcing for area coverage in resource constrained environments}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Photos crowdsourced from mobile devices can be used in many applications such as disaster recovery to obtain information about a target area. However, such applications often have resource constraints in terms of bandwidth, storage, and processing capability, which limit the number of photos that can be crowdsourced. Thus, it is a challenge to use the limited resources to crowdsource photos that best cover the target area. In this paper, we leverage various geographical and geometrical information about photos, called metadata, to address this challenge. Metadata includes the location, orientation, field of view, and range of a camera. Based on metadata, we define photo utility to measure how well a target area is covered by a set of photos. We propose various techniques to analyze such coverage and calculate photo utility accurately and efficiently. We also study the problem of selecting photos with the largest utility under a resource budget, and propose an efficient algorithm that achieves constant approximation ratio. With our design, the crowdsourcing server can select photos based on metadata instead of real images, and thus use the limited resources to crowdsource the most useful photos. Both simulation and experimental results demonstrate the effectiveness of our design.}, 
keywords={approximation theory;computational complexity;emergency management;geophysical image processing;meta data;mobile computing;area coverage;constant approximation ratio;disaster recovery;geographical information;geometrical information;metadata;mobile devices;photo crowdsourcing;resource budget;resource constrained environments;Bandwidth;Cameras;Conferences;Crowdsourcing;Metadata;Mobile handsets;Servers}, 
doi={10.1109/INFOCOM.2017.8056963}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8056964, 
author={Y. Zhuo and H. Zhu and H. Xue and S. Chang}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Perceiving accurate CSI phases with commodity WiFi devices}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={WiFi technology has gained a wide prevalence for not only wireless communication but also pervasive sensing. A wide variety of emerging applications leverage accurate measurements of the Channel State Information (CSI) information obtained from commodity WiFi devices. Due to hardware imperfection of commodity WiFi devices, the frequency response of internal signal processing circuit is mixed with the real channel frequency response in passband, which makes deriving accurate channel frequency response from CSI measurements a challenging task. In this paper, we identify non-negligible non-linear CSI phase errors and report that IQ imbalance is the root source of non-linear CSI phase errors. We conduct intensive analysis on the characteristics of such non-linear errors and find that such errors are prevalent among various WiFi devices. Furthermore, they are rather stable along time and the received signal strength indication (RSSI) but sensitive to frequency bands used between a transmission pair. Based on these key observations, we propose new calibration methods to compensate both non-linear and linear CSI phase errors. We demonstrate the efficacy of the proposed methods by applying them in CSI splicing. Results of extensive real-world experiments indicate that accurate CSI phase measurements can significantly improve the performance of splicing and the stability of the derived power delay profiles (PDPs).}, 
keywords={RSSI;calibration;frequency response;phase measurement;signal processing;wireless LAN;wireless channels;CSI phase measurements;CSI splicing;Channel State Information;IQ imbalance;RSSI;WiFi technology;calibration methods;channel frequency response;commodity WiFi devices;internal signal processing circuit;nonlinear CSI phase errors;perceiving accurate CSI phases;power delay profiles;received signal strength indication;Delays;Frequency measurement;Frequency response;Measurement uncertainty;Phase measurement;Receivers;Wireless fidelity;CSI splicing;Channel State Information (CSI);empirical study;non-linear phase errors;rotation phase error}, 
doi={10.1109/INFOCOM.2017.8056964}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8056965, 
author={S. Byeon and K. Yoon and C. Yang and S. Choi}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={STRALE: Mobility-aware PHY rate and frame aggregation length adaptation in WLANs}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={IEEE 802.11n/ac wireless local area network (WLAN) supports frame aggregation, called aggregate medium access control (MAC) protocol data unit (A-MPDU), to enhance MAC efficiency by reducing protocol overhead. However, the current channel estimation process conducted only once during the preamble reception is known to be insufficient to ensure robust delivery of long A-MPDU frames in mobile environments. To cope with this problem, we first build a model which represents the impact of mobility with a noise vector in the I-Q plane, and then analyze how the mobility affects the A-MPDU reception performance. Based on our analysis, we develop STRALE, a standard-compliant and mobility-aware PHY rate and A-MPDU length adaptation scheme with ease of implementation. Through extensive simulations with 802.11ac using ns-3 and prototype implementation with commercial 802.11n devices, we demonstrate that STRALE achieves up to 2.9x higher throughput, compared to a fixed duration setting according to IEEE 802.11 standard.}, 
keywords={access protocols;channel estimation;mobility management (mobile radio);wireless LAN;wireless channels;A-MPDU length adaptation scheme;IEEE 802.11n/ac wireless local area network;STRALE;aggregate medium access control protocol data unit;channel estimation;mobility-aware PHY frame aggregation length adaptation;mobility-aware PHY rate aggregation length adaptation;Channel estimation;MIMO;OFDM;Protocols;Receivers;Signal to noise ratio;Wireless LAN}, 
doi={10.1109/INFOCOM.2017.8056965}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8056966, 
author={S. Han and K. G. Shin}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Enhancing wireless performance using reflectors}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Signal decay is the fundamental problem of wireless communications, especially in an indoor environment where line-of-sight (LOS) paths for signal propagation are often blocked and various indoor objects exacerbate signal fading. There are three reasons for signal decay: long transmission distance, signal penetration, and reflection. In this paper, we propose OptRe which optimally places metallic reflectors - providing a highly reflective surface that can reflect impinging signals almost 100% - in indoor environments to reduce the reflection loss and enhance wireless transmissions. It enhances both WiFi signal and low-power IoT devices without changing their configurations or network protocols. To enable OptRe, we first develop an empirical signal propagation model that can accurately estimate the signal strength and adapt itself to the reflectors' location. Using micro-benchmarks, our empirical signal propagation model is shown to be more accurate than the other existing path loss models. We also optimally place reflectors to maximize the worst-case signal coverage within the target indoor areas. Our extensive experimental evaluation results have shown OptRe to enhance signal strength for different types of wireless signals by almost 2x.}, 
keywords={Internet of Things;indoor radio;protocols;wireless LAN;OptRe;WiFi signal;empirical signal propagation model;highly reflective surface;impinging signals;indoor environment;indoor objects exacerbate signal fading;line-of-sight paths;long transmission distance;low-power IoT devices;metallic reflectors;path loss models;reflection loss;signal decay;signal penetration;signal strength estimation;target indoor areas;wireless communications;wireless performance enhancement;wireless signals;wireless transmissions;worst-case signal coverage;Adaptation models;Conferences;Fading channels;Indoor environments;Propagation losses;Wireless communication;Wireless fidelity}, 
doi={10.1109/INFOCOM.2017.8056966}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8056967, 
author={A. A. Renani and J. Huang and G. Xing and A. H. Esfahanian}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Harnessing hardware defects for improving wireless link performance: Measurements and applications}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={The design trade-offs of transceiver hardware are crucial to the performance of wireless systems. In this paper, we present an in-depth study to characterize the surprisingly notable systemic impacts of low-pass filter (LPF) design, which is a small yet indispensable component used for shaping spectrum and rejecting interference. Using a bottom-up approach, we examine how signal-level distortions caused by the trade-off of LPF design propagate to the upper-layers of wireless communication, reshaping bit error patterns and degrading link performance of today's 802.11 systems. Moreover, we propose a novel algorithm that harnesses LPF defects for improving video streaming, which substantially enhances video quality in mobile environments.}, 
keywords={low-pass filters;radio links;radio transceivers;video streaming;wireless LAN;802.11 systems;LPF defects;LPF design propagate;bit error patterns;design trade-offs;hardware defects;in-depth study;low-pass filter design;rejecting interference;shaping spectrum;signal-level distortions;transceiver hardware;video streaming;wireless communication;wireless link performance;wireless systems;Distortion;Distortion measurement;Frequency measurement;Hardware;IEEE 802.11 Standard;Semiconductor device measurement;Wireless communication}, 
doi={10.1109/INFOCOM.2017.8056967}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8056968, 
author={G. C. Sankaran and K. M. Sivalingam}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Combinatorial approach for network switch design in data center networks}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={This paper deals with the efficient design of network switch/routers for an optical data center network. Each switch has multiple components such as ingress/egress interfaces, optical and/or electronic buffers, interconnection switching fabric and so on. There are several possible choices available for each of these components. This paper presents a systematic approach to designing the switch architecture using a combination of these component choices, while meeting specified design criteria. It requires formally defining the structure of a switch and enforcing semantics across components. This is formulated as a constraint optimization problem with formal language grammar guiding its search process. This problem formulation is used to identify the best-possible architecture for a hierarchical DCN. Two of the three solutions identified were new and were not reported in literature. These solutions were also validated experimentally.}, 
keywords={computer centres;formal languages;optical communication equipment;optical switches;telecommunication network routing;telecommunication traffic;best-possible architecture;combinatorial approach;constraint optimization problem;design criteria;formal language grammar;hierarchical DCN;multiple components;network routers;network switch design;optical data center network;switch architecture;systematic approach;Computer architecture;Formal languages;Grammar;Optical switches;Servers}, 
doi={10.1109/INFOCOM.2017.8056968}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8056969, 
author={S. Jia and X. Jin and G. Ghasemiesfeh and J. Ding and J. Gao}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Competitive analysis for online scheduling in software-defined optical WAN}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Modern planetary-scale online services have massive data to transfer over the wide area network (WAN). Due to the tremendous cost of building WANs and the stringent timing requirement of distributed applications, it is critical for network operators to make efficient use of network resources to optimize data transfers. By leveraging software-defined networking (SDN) and reconfigurable optical devices, recent solutions design centralized systems to jointly control the network layer and the optical layer. While these solutions show it is promising to significantly reduce data transfer times by centralized cross-layer control, they do not have any theoretical guarantees on the proposed algorithms. This paper presents approximation algorithms and theoretical analysis for the online transfer scheduling problem over optical WANs. The goal of the scheduling problem is to minimize the makespan (the time to finish all transfers) or the total sum of completion times. We design and analyze various greedy, online scheduling algorithms that can achieve 3-competitive ratio for makespan, 2-competitive ratio for minimum sum completion time for jobs of unit size, and 3α-competitive ratio for jobs of arbitrary transfer size and each node having degree constraint d, where α = 1 when d = 1 and α = 1.86 when d ≥ 2. We also evaluated the performance of these algorithms and compared the performance with prior heuristics.}, 
keywords={approximation theory;computational complexity;greedy algorithms;optical fibre networks;software defined networking;telecommunication scheduling;wide area networks;2-competitive ratio;3α-competitive ratio;3-competitive ratio;approximation algorithms;arbitrary transfer size;centralized cross-layer control;centralized system design;data transfers;distributed applications;greedy scheduling algorithms;minimum sum completion time;network layer;network operators;network resources;online scheduling algorithms;online transfer scheduling problem;optical layer;planetary-scale online services;reconfigurable optical devices;software-defined networking;software-defined optical WAN;stringent timing requirement;wide area network;Algorithm design and analysis;Data transfer;Optical devices;Optical fiber networks;Schedules;Wide area networks}, 
doi={10.1109/INFOCOM.2017.8056969}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8056970, 
author={Y. Nakayama and T. Tsutsumi and K. Maruta and K. Sezaki}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={ABSORB: Autonomous base station with optical reflex backhaul to adapt to fluctuating demand}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Metropolitan areas witness significant fluctuations in mobile traffic due to patterns of human mobility. This fluctuation drastically deteriorates the efficiency and financial viability of conventional maximum-based network design. If networks are deployed to deal with the peak traffic rate at each site, their capacities are underutilized for most of time. To improve the efficiency of deploying base stations (BSs), this paper proposes a concept of an Autonomous Base Station with Optical Reflex Backhaul (ABSORB) architecture that can adapt to fluctuations in mobile traffic. In the ABSORB architecture, traffic at demand nodes is forwarded to and from an ABS with an arbitrary radio access technology (RAT). An ABS is connected to a gateway node through ORB, which consists of fiber optic networks. ABSs move to new locations following the demand movement, according to a relocation schedule that is periodically rearranged by an ABSORB controller. The network is flexibly reconstructed according to the demand distribution. The ABSORB architecture can be employed in various networks, and can coexist with traditional static architectures. It will drastically reduce the number of BSs, total deployment cost, and power consumption in comparison with the traditional design.}, 
keywords={metropolitan area networks;mobile computing;mobility management (mobile radio);optical fibre networks;radio access networks;telecommunication network planning;telecommunication traffic;ABSORB controller;Autonomous Base Station;Autonomous base station;Optical Reflex Backhaul architecture;arbitrary radio access technology;demand distribution;demand nodes;deployment cost;fiber optic networks;gateway node;human mobility;maximum-based network design;metropolitan areas;mobile traffic;traditional static architectures;Ad hoc networks;Adaptation models;Computer architecture;Mobile communication;Network topology;Optical fiber communication;Topology}, 
doi={10.1109/INFOCOM.2017.8056970}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8056971, 
author={S. Gay and R. Hartert and S. Vissicchio}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Expect the unexpected: Sub-second optimization for segment routing}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={In this paper, we study how to perform traffic engineering at an extremely-small time scale with segment routing, addressing a critical need for modern wide area networks. Prior work has shown that segment routing enables to better engineer traffic, thanks to its ability to program detours in forwarding paths, at scale. Two main approaches have been explored for traffic engineering with segment routing, respectively based on integer linear programming and constraint programming. However, no previous work deeply investigated how quickly those approaches can react to unexpected traffic changes and failures. We highlight limitations of existing algorithms, both in terms of required execution time and amount of path changes to be applied. Thus, we propose a new approach, based on local search and focused on the quick re-arrangement of (few) forwarding paths. We describe heuristics for sub-second recomputation of segment-routing paths that comply with requirements on the maximum link load (e.g., for congestion avoidance). Our heuristics enable a prompt answer to sudden criticalities affecting network services and business agreements. Through extensive simulations, we indeed experimentally show that our proposal significantly outperforms previous algorithms in the context of time-constrained optimization, supporting radical traffic changes in few tens of milliseconds for realistic networks.}, 
keywords={constraint handling;integer programming;linear programming;telecommunication network routing;telecommunication traffic;wide area networks;constraint programming;engineer traffic;forwarding paths;integer linear programming;radical traffic changes;segment routing;segment-routing paths;traffic engineering;wide area networks;Computational modeling;Conferences;Heuristic algorithms;Optimization;Proposals;Routing;Routing protocols}, 
doi={10.1109/INFOCOM.2017.8056971}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8056972, 
author={H. Xu and G. de Veciana and W. C. Lau}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Addressing job processing variability through redundant execution and opportunistic checkpointing: A competitive analysis}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={The completion times of jobs in a computing cluster may be influenced by a variety of factors including job size and machine processing variability. In this paper, we explore online resource allocation policies which combine size-dependent scheduling with redundant execution and opportunistic checkpointing to minimize the overall job flowtime. We introduce a simplified model for the job service capacity of a computing cluster while leveraging redundant execution/checkpointing. In this setting, we propose two resource allocation algorithms, SRPT+R and LAPS+R(β) subject to checkpointing overhead not exceeding the number of jobs which are processed. We provide new theoretical performance bounds for these algorithms: SRPT+R is shown to be O(1/ϵ) competitive under (1 + ϵ)-speed resource augmentation, while LAPS+R(β) is shown to be O(1/βϵ) competitive under (2+ 2β + 2ϵ)-speed resource augmentation.}, 
keywords={checkpointing;job shop scheduling;resource allocation;scheduling;LAPS+R(β) algorithm;SRPT+R algorithm;checkpointing overhead;combine size-dependent scheduling;computing cluster;job flowtime;job processing variability;job service capacity;job size;machine processing variability;online resource allocation policies;opportunistic checkpointing;redundant execution;resource allocation algorithms;resource augmentation;Algorithm design and analysis;Checkpointing;Multitasking;Redundancy;Scheduling algorithms;Servers;Competitive Analysis;Dual-Fitting;Job Scheduling;Optimization;Redundancy}, 
doi={10.1109/INFOCOM.2017.8056972}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8056973, 
author={Y. Niu and F. Liu and X. Fei and B. Li}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Handling flash deals with soft guarantee in hybrid cloud}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Flash deal applications, which offer significant benefits (e.g., discount) to subscribers within a short period of time, are becoming increasingly prevalent. Motivated by such transient profit, flash crowds of subscribers request services simultaneously. Considering the unique business logic, a hybrid cloud with soft guarantee, i.e., bounding the response time of delay-tolerant requests, has great potential to handle flash crowds. In this paper, to cost-effectively withstand flash crowds with soft guarantee, we propose a solution that makes smart decisions on scheduling requests in the hybrid cloud and adjusting the capacity of the public cloud. In respect of scheduling requests, we apply Sequential Quadratic Programming (SQP) to achieve soft guarantee. Furthermore, for adjusting capacity, we design an online algorithm to tune the scale of the public cloud towards jointly minimizing cost and response time, yet without a priori knowledge of request arrival rate. We prove that the online algorithm can obtain a competitive ratio of 1-6ε against the optimal solution, where ε can be tuned close to 0. By conducting extensive trace-driven experiments in a website prototype deployed on OpenStack Mitaka and Amazon Web Service, our solution reduces response time by 15% compared with previous work under given budget.}, 
keywords={Web services;bandwidth allocation;cloud computing;peer-to-peer computing;quadratic programming;resource allocation;scheduling;Amazon Web service;OpenStack Mitaka;SQP;business logic;competitive ratio;cost minimization;delay-tolerant requests;flash crowds;flash deals;hybrid cloud;online algorithm;public cloud;request arrival rate;response time;response time minimization;scheduling requests;sequential quadratic programming;soft guarantee;Algorithm design and analysis;Cloud computing;Computational modeling;Degradation;Outsourcing;Time factors;Web servers}, 
doi={10.1109/INFOCOM.2017.8056973}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8056974, 
author={R. Birke and J. F. Pérez and Z. Qiu and M. Björkqvist and L. Y. Chen}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Power of redundancy: Designing partial replication for multi-tier applications}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Replicating redundant requests has been shown to be an effective mechanism to defend application performance from high capacity variability - the common pitfall in the cloud. While the prior art centers on single-tier systems, it still remains an open question how to design replication strategies for distributed multi-tier systems, where interference from neighboring workloads is entangled with complex tier interdependency. In this paper, we design a first of its kind PArtial REplication system, sPARE, that replicates and dispatches read-only workloads for multi-tier web applications, determining replication factors per tier. The two key components of sPARE are (i) the variability-aware replicator that coordinates the replication levels on all tiers via an iterative searching algorithm, and (ii) the replication-aware arbiter that uses a novel token-based arbitration algorithm (TAD) to dispatch requests in each tier. We evaluate sPARE on web serving and web searching applications, i.e., MediaWiki and Solr, deployed on our private cloud testbed. Our results based on various interference patterns and traffic loads show that sPARE is able to improve the tail latency of MediaWiki and Solr by a factor of almost 2.7x and 2.9x, respectively.}, 
keywords={Internet;MediaWiki;Solr;Web serving applications;complex tier interdependency;distributed multitier systems;iterative searching algorithm;multitier applications;multitier web applications;partial replication system;replication factors;replication levels;replication strategies;replication-aware arbiter;sPARE system;single-tier systems;token-based arbitration algorithm;variability-aware replicator;web searching applications;Cloud computing;Dispatching;Interference;Measurement;Protocols;Redundancy;Servers}, 
doi={10.1109/INFOCOM.2017.8056974}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8056975, 
author={Z. Huang and S. M. Weinberg and L. Zheng and C. Joe-Wong and M. Chiang}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Discovering valuations and enforcing truthfulness in a deadline-aware scheduler}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={A cloud computing cluster equipped with a deadline-aware job scheduler faces fairness and efficiency challenges when greedy users falsely advertise the urgency of their jobs. Penalizing such untruthfulness without demotivating users from using the cloud service calls for advanced mechanism design techniques that work together with deadline-aware job scheduling. We propose a Bayesian incentive compatible pricing mechanism based on matching by replica-surrogate valuation functions. User valuations can be discovered by the mechanism, even when the users themselves do not fully understand their own valuations. Furthermore, users who are charged a Bayesian incentive compatible price have no reason to lie about the urgency of their jobs. The proposed mechanism achieves multiple desired truthful properties such as Bayesian incentive compatibility and ex-post individual rationality. We implement the proposed pricing mechanism. Through experiments in a Hadoop cluster with real-world datasets, we show that our prototype is capable of suppressing untruthful behavior from users.}, 
keywords={Bayes methods;cloud computing;incentive schemes;pricing;scheduling;Bayesian incentive compatible pricing mechanism;Hadoop cluster;advanced mechanism design techniques;cloud computing;cloud service;deadline-aware job scheduler;ex-post individual rationality;replica-surrogate valuation functions;truthfulness enforcement;user valuations;valuation discovery;Bayes methods;Cloud computing;Cost accounting;Pricing;Processor scheduling;Quality of service;Resource management}, 
doi={10.1109/INFOCOM.2017.8056975}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8056976, 
author={X. Zheng and Z. Cai and J. Li and H. Gao}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Location-privacy-aware review publication mechanism for local business service systems}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Local business service systems (LBSS), such as Yelp and Dianping, play an essential role in making decisions like choosing a restaurant for our daily life. These systems heavily rely on individuals' voluntarily submitted reviews to build the reputation for nearby businesses. Unfortunately, the reviews expose users' private information such as visited places to the public and adversaries. Even worse, such location information is always public as it is the basic information of businesses, and adversaries could be anyone ranging from advertisement spammer to physical stalker. This paper formalizes the privacy preserving problem in local business service systems and propose a novel location privacy preserving framework. The framework can preserve users' location privacy in arbitrary local area and can maintain a good utility for both the system and every user. We evaluate our framework thoroughly towards real-world data traces. The results validate that the framework can achieve a good performance.}, 
keywords={catering industry;data protection;electronic commerce;mobile computing;Dianping;LBSS;Yelp;arbitrary local area;local business service systems;location privacy preserving framework;location-privacy-aware review publication mechanism;user private information;Business;Computer science;Conferences;Electronic mail;Mobile radio mobility management;Privacy;Publishing}, 
doi={10.1109/INFOCOM.2017.8056976}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8056977, 
author={S. Wang and Y. Nie and P. Wang and H. Xu and W. Yang and L. Huang}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Local private ordinal data distribution estimation}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={The categorical data that have natural ordering between categories are termed ordinal data, which are pervasive in numerous areas, including discrete sensor readings, metering data or preference options. Though aggregating such ordinal data from the population is facilitating plenty of crowdsourcing applications, contributing such data is privacy risky and may reveal sensitive information (e.g. locations, identities) about individuals. This work studies ordinal data aggregation for distribution estimation meanwhile locally preserving individuals' data privacy (such as on their mobile devices). Under ε-geo-indistinguishable constraints, which capture intrinsic dissimilarity between ordinal categories in the framework of differential privacy, we provide an efficient and effective locally private mechanism: Subset Exponential Mechanism (SEM) for ordinal data distribution estimation. The mechanism randomly responds with a fixed-size subset of the categories with calibrated probability assignment. Specially for uniform ordinal data, we propose a circling technique to symmetrically randomizing categories and estimating frequencies of categories, hence the computational/space costs and estimation performance of SEM are further optimized. Besides contributing theoretical error bounds of SEM, we also evaluate the mechanism on extensive scenarios, the evaluation results show that SEM reduces distribution estimation error on average by exp(ϵ/2) factor over existing private mechanisms.}, 
keywords={data privacy;probability;security of data;ε-geo-indistinguishable constraints;SEM;Subset Exponential Mechanism;calibrated probability assignment;categorical data;circling technique;discrete sensor readings;distribution estimation error;effective locally private mechanism;frequency estimation;local private ordinal data distribution estimation;metering data;natural ordering;ordinal categories;ordinal data aggregation;preference options;privacy risky;private mechanisms;symmetrical categories randomization;uniform ordinal data;Crowdsourcing;Data aggregation;Data models;Data privacy;Databases;Estimation;Privacy}, 
doi={10.1109/INFOCOM.2017.8056977}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8056978, 
author={H. Liu and X. Li and H. Li and J. Ma and X. Ma}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Spatiotemporal correlation-aware dummy-based privacy protection scheme for location-based services}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Since the dummy-based method can provide precise query results without any requirement for a third party or key sharing, it has been widely used to protect the user's location privacy in location-based services. However, the neighboring location sets submitted in consecutive requests always include a close spatiotemporal correlation, which enables the adversary to identify some dummies. Therefore, the existing dummy-based schemes cannot protect the user's location privacy completely. To solve this problem, based on the dummies generated by the existing schemes, this paper filters out the dummies that can be identified by taking into account of the spatiotemporal correlation from three aspects, namely time reachability, direction similarity and in-degree/out-degree. In this way, the rest dummies can satisfy the user's personalized privacy protection requirement. Security analysis shows that the proposed scheme successfully perturbs the spatiotemporal correlation between neighboring location sets, therefore, it is infeasible for the adversary to distinguish the user's real location from the dummies. Furthermore, extensive experiments indicate that the proposal is able to protect the user's location privacy effectively and efficiently.}, 
keywords={data protection;mobile computing;query processing;direction similarity;dummy-based method;in-degree/out-degree;location-based services;neighboring location sets;security analysis;spatiotemporal correlation;time reachability;user location privacy protection;Computer security;Conferences;Correlation;Mobile radio mobility management;Privacy;Spatiotemporal phenomena}, 
doi={10.1109/INFOCOM.2017.8056978}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8056979, 
author={M. Gramaglia and M. Fiore and A. Tarable and A. Banchs}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Preserving mobile subscriber privacy in open datasets of spatiotemporal trajectories}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Mobile network operators can track subscribers via passive or active monitoring of device locations. The recorded trajectories offer an unprecedented outlook on the activities of large user populations, which enables developing new networking solutions and services, and scaling up studies across research disciplines. Yet, the disclosure of individual trajectories raises significant privacy concerns: thus, these data are often protected by restrictive non-disclosure agreements that limit their availability and impede potential usages. In this paper, we contribute to the development of technical solutions to the problem of privacy-preserving publishing of spatiotemporal trajectories of mobile subscribers. We propose an algorithm that generalizes the data so that they satisfy-anonymity, an original privacy criterion that thwarts attacks on trajectories. Evaluations with real-world datasets demonstrate that our algorithm attains its objective while retaining a substantial level of accuracy in the data. Our work is a step forward in the direction of open, privacy-preserving datasets of spatiotemporal trajectories.}, 
keywords={data protection;mobile computing;active monitoring;data protection;device locations;mobile network operators;mobile subscriber privacy preservation;open privacy-preserving datasets;passive monitoring;privacy-preserving publishing;real-world datasets;restrictive nondisclosure agreements;spatiotemporal trajectories;Couplings;Data privacy;Databases;Mobile communication;Privacy;Spatiotemporal phenomena;Trajectory}, 
doi={10.1109/INFOCOM.2017.8056979}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8056980, 
author={A. Sinha and E. Modiano}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Optimal control for generalized network-flow problems}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={We consider the problem of throughput-optimal packet dissemination, in the presence of an arbitrary mix of unicast, broadcast, multicast and anycast traffic, in a general wireless network. We propose an online dynamic policy, called Universal Max-Weight (UMW), which solves the above problem efficiently. To the best of our knowledge, UMW is the first throughput-optimal algorithm of such versatility in the context of generalized network flow problems. Conceptually, the UMW policy is derived by relaxing the precedence constraints associated with multi-hop routing, and then solving a min-cost routing and max-weight scheduling problem on a virtual network of queues. When specialized to the unicast setting, the UMW policy yields a throughput-optimal cycle-free routing and link scheduling policy. This is in contrast to the well-known throughput-optimal BackPressure (BP) policy which allows for packet cycling, resulting in excessive delay. Extensive simulation results show that the proposed policy incurs a substantially lower delay as compared to the BP policy. The proof of throughput-optimality of the UMW policy combines techniques from stochastic Lyapunov theory with a sample path argument from adversarial queueing theory and may be of independent theoretical interest.}, 
keywords={Lyapunov methods;multicast communication;optimal control;optimisation;queueing theory;radio networks;telecommunication control;telecommunication network routing;telecommunication scheduling;telecommunication traffic;BP policy;UMW policy;Universal Max-Weight;anycast traffic;generalized network-flow problems;link scheduling policy;max-weight scheduling problem;min-cost routing;multicast traffic;multihop routing;online dynamic policy;optimal control;throughput-optimal BackPressure policy;throughput-optimal algorithm;throughput-optimal packet dissemination;throughput-optimality;wireless network;Delays;Heuristic algorithms;Routing;Spread spectrum communication;Throughput;Unicast;Wireless networks}, 
doi={10.1109/INFOCOM.2017.8056980}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8056981, 
author={K. Lampka and S. Bondorf and J. B. Schmitt and N. Guan and W. Yi}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Generalized finitary real-time calculus}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Real-time Calculus (RTC) is a non-stochastic queuing theory to the worst-case performance analysis of distributed real-time systems. Workload as well as resources are modelled as piece-wise linear, pseudo-periodic curves and the system under investigation is modelled as a sequence of algebraic operations over these curves. The memory footprint of computed curves increases exponentially with the sequence of operations and RTC may become computationally infeasible fast. Recently, Finitary RTC has been proposed to counteract this problem. Finitary RTC restricts curves to finite input domains and thereby counteracts the memory demand explosion seen with pseudo periodic curves of common RTC implementations. However, the proof to the correctness of Finitary RTC specifically exploits the operational semantic of the greed processing component (GPC) model and is tied to the maximum busy window size. This is an inherent limitation, which prevents a straight-forward generalization. In this paper, we provide a generalized Finitary RTC that abstracts from the operational semantic of a specific component model and reduces the finite input domains of curves even further. The novel approach allows for faster computations and the extension of the Finitary RTC idea to a much wider range of RTC models.}, 
keywords={calculus;queueing theory;RTC implementations;algebraic operations;distributed real-time systems;generalized Finitary RTC;generalized finitary realtime calculus;greed processing component model;memory demand explosion;nonstochastic queuing theory;operational semantic;piecewise linear curves;pseudoperiodic curves;worst-case performance analysis;Analytical models;Calculus;Computational modeling;Delays;Mathematical model;Real-time systems;Semantics}, 
doi={10.1109/INFOCOM.2017.8056981}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8056982, 
author={P. J. Wan and H. Yuan and X. Jia and J. Wang and Z. Wang}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Maximum-weighted subset of communication requests schedulable without spectral splitting}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Consider a set of point-to-point communication requests in a multi-channel multihop wireless network, each of which is associated with a traffic demand of at most one unit of transmission time, and a weight representing the utility if its demand is fully met. A subset of requests is said to be schedulable without spectral splitting if they can be scheduled within one unit of time subject to the constraint each request is assigned with a unique channel throughout its transmission. This paper develops efficient and provably good approximation algorithms for finding a maximum-weighted subset of communication requests schedulable without spectral splitting.}, 
keywords={approximation theory;radio networks;telecommunication scheduling;telecommunication traffic;wireless channels;approximation algorithms;constraint each request;maximum-weighted subset;multichannel multihop wireless network;point-to-point communication;spectral splitting;time subject;traffic demand;transmission time;Approximation algorithms;Channel allocation;Computer science;Conferences;Interference;Receivers;Schedules}, 
doi={10.1109/INFOCOM.2017.8056982}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8056983, 
author={S. Krishnasamy and P. T. Akhil and A. Arapostathis and S. Shakkottai and R. Sundaresan}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Augmenting max-weight with explicit learning for wireless scheduling with switching costs}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={In small-cell wireless networks where users are connected to multiple base stations (BSs), it is often advantageous to opportunistically switch off a subset of BSs to minimize energy costs. We consider two types of energy cost: (i) the cost of maintaining a BS in the active state, and (ii) the cost of switching a BS from the active state to inactive state. The problem is to operate the network at the lowest possible energy cost (sum of activation and switching costs) subject to queue stability. In this setting, the traditional approach - a Max-Weight algorithm along with a Lyapunov-based stability argument - does not suffice to show queue stability, essentially due to the temporal co-evolution between channel scheduling and the BS activation decisions induced by the switching cost. Instead, we develop a learning and BS activation algorithm with slow temporal dynamics, and a Max-Weight based channel scheduler that has fast temporal dynamics. We show using convergence of time-inhomogeneous Markov chains, that the co-evolving dynamics of learning, BS activation and queue lengths lead to near optimal average energy costs along with queue stability.}, 
keywords={Markov processes;cellular radio;cost reduction;learning (artificial intelligence);minimisation;queueing theory;radio networks;telecommunication computing;telecommunication scheduling;BS activation decisions;Max-Weight algorithm;Max-Weight based channel scheduler;active state;energy costs minimization;inactive state;max-weight augmentation;multiple base stations;optimal average energy costs;queue lengths;queue stability;small-cell wireless networks;switching costs;time-inhomogeneous Markov chains;wireless scheduling;Convergence;Energy consumption;Heuristic algorithms;Markov processes;Resource management;Stability criteria;Switches;base-station activation;energy minimization;wireless scheduling}, 
doi={10.1109/INFOCOM.2017.8056983}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8056984, 
author={Y. Hou and Y. Zheng}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={PHY assisted tree-based RFID identification}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Tree-based RFID identification adopts a binary-tree structure to collect IDs of an unknown set. Tag IDs locate at the leaf nodes and the reader queries through intermediate tree nodes and converges to these IDs using feedbacks from tag responses. Existing works cannot function well under random ID distribution as they ignore the distribution information hidden in the physical-layer signal of colliding tags. Different from them, we introduce PHY-Tree, a novel tree-based scheme that collects two types of distribution information from every encountered colliding signal. First, we detect if all colliding tags send the same bit content at each bit index by looking into inherent temporal features of the tag modulation schemes. If such resonant states are detected, either left or right branch of a certain subtree can be trimmed horizontally. Second, we estimate the number of colliding tags in a slot by computing a related metric defined over the signal's constellation map, based on which nodes in the same layers of a certain subtree can be skipped vertically. Evaluations from both experiments and simulations demonstrate that PHY-Tree outperforms state-of-the-art schemes by at least 1.79×.}, 
keywords={radiofrequency identification;trees (mathematics);PHY-Tree;RFID identification;binary-tree structure;colliding signal;colliding tags;distribution information;intermediate tree nodes;leaf nodes;physical-layer signal;random ID distribution;tag IDs;tag modulation schemes;tag responses;Conferences;Encoding;Feature extraction;Indexes;Modulation;Physical layer;Radiofrequency identification}, 
doi={10.1109/INFOCOM.2017.8056984}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8056985, 
author={J. Liu and F. Zhu and Y. Wang and X. Wang and Q. Pan and L. Chen}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={RF-scanner: Shelf scanning with robot-assisted RFID systems}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Shelf scanning is one of the most important processes for inventory management in a library. It helps the librarians and library users discover the miss-shelved books and pinpoint where they are, improving the quality of service. By traditional means, however, manually checking each bookshelf suffers from extremely intensive labor and long scanning delay. Although some existing RFID-enabled approaches have been proposed, they suffer from either high-cost infrastructure or complicated system deployment, forming a great barrier to commercial adoption. In light of this, we in this paper propose a smart system called RF-Scanner that can perform the shelf scanning automatically by combining the robot technology and the RFID technology. The former is used for replacing the librarians and liberating them from intensively manual labor. The later is installed on the robot and moves with the robot to scan the on-the-shelf books. We formulate two important issues concerned by librarians, the book localization and the lying-down book detection, and give the sophisticated solutions to them. Besides, we implement RF-Scanner and put it into practical use in our school library. Long-term experiments and studies show that RF-Scanner provides fine-grained book localization with a mean error of just 1.3 cm and accurate detection accuracy of lying-down books with a mean error of 6%.}, 
keywords={inventory management;library automation;radiofrequency identification;service robots;RF-Scanner;RFID technology;book localization;complicated system deployment;high-cost infrastructure;intensively manual labor;inventory management;lying-down book detection;lying-down books;miss-shelved books;on-the-shelf books;quality of service;robot technology;robot-assisted RFID systems;school library;shelf scanning;size 1.3 cm;smart system;Libraries;Manuals;Radio frequency;Radiofrequency identification;Robot kinematics;Systems architecture}, 
doi={10.1109/INFOCOM.2017.8056985}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8056986, 
author={M. Chen and J. Liu and S. Chen and Y. Qiao and Y. Zheng}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={DBF: A general framework for anomaly detection in RFID systems}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={RFID technologies are making their way into numerous applications, including inventory management, supply chain, product tracking, transportation, logistics, etc. One important application is to automatically detect anomalies in RFID systems, such as missing tags, unknown tags, or cloned tags due to theft, management error, or targeted attacks. Existing solutions are all designed to detect a certain type of RFID anomalies, but lack a general functionality for detecting different types of anomalies. This paper attempts to propose a general framework for anomaly detection in RFID systems, thereby reducing the complexity for readers and tags to implement different anomaly-detection protocols. We introduce a new concept of differential Bloom filter (DBF), which turns physical-layer signal data into a segmented Bloom filter that encodes the IDs of abnormal tags. As a case study, we propose a protocol that builds DBF for identifying all missing tags in an efficient way. We implement a prototype for missing-tag identification using USRP and WISP tags to verify the effectiveness our protocol, and use large-scale simulations for performance evaluation. The results show that our solution can significantly improve time efficiency, when comparing with the best existing work.}, 
keywords={data structures;protocols;radiofrequency identification;telecommunication security;DBF;RFID anomalies;RFID systems;RFID technologies;WISP tags;abnormal tags;anomaly detection;anomaly-detection protocols;differential Bloom filter;missing-tag identification;segmented Bloom filter;Aggregates;Anomaly detection;Databases;Nickel;Protocols;Radiofrequency identification;Wireless communication}, 
doi={10.1109/INFOCOM.2017.8056986}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8056987, 
author={H. Aantjes and A. Y. Majid and P. Pawełczak and J. Tan and A. Parks and J. R. Smith}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Fast downstream to many (computational) RFIDs}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={We present Stork - an extension of the EPC C1G2 protocol allowing streaming of data to multiple Computational Radio Frequency IDentification tags (CRFIDs) simultaneously at up to 20 times faster than the prior state of the art. Stork introduces downstream attributes never before seen in (C)RFIDs: (i) fast feedback for CRFID downstream verification based on the internal EPC C1G2 memory check command - which we analytically and experimentally show to be the best possible downstream verification process based on EPC C1G2; (ii) ability to perform multi-CRFID transfer - which in our experiments speeds up downstream by more than two times compared to sequential transmission; and (iii) the use of compressed data streams - which improves firmware reprogramming times by up to 10% at large reader-to-CRFID distances.}, 
keywords={cryptographic protocols;data compression;firmware;program verification;radiofrequency identification;CRFID downstream verification;EPC C1G2 protocol allowing;Stork;compressed data streams;fast downstream;fast feedback;firmware reprogramming times;internal EPC C1G2 memory check command;multiCRFID transfer;multiple Computational Radio Frequency IDentification tags;Payloads;Protocols;Radiofrequency identification;Robustness;Software}, 
doi={10.1109/INFOCOM.2017.8056987}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8056988, 
author={Z. Chen and X. Zhang and S. Wang and Y. Xu and J. Xiong and X. Wang}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={BUSH: Empowering large-scale MU-MIMO in WLANs with hybrid beamforming}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Large-scale MU-MIMO is a promising technology to scale network capacity and the capacity gain grows linearly with the numbers of antennas and users in theory. However, its practical deployment faces three critical challenges in the state-of-the-art WLANs: i) the demand of a large number of expensive RF chains; ii) the linear growth of feedback overheads with the number of antennas; iii) the lack of scalable user selection scheme for a large user population. In this paper, we design BUSH, a large-scale MU-MIMO prototype that performs scalable beam user selection with hybrid beamforming for phased-array antennas in legacy WLANs. The architecture of BUSH consists of three components. Firstly, a low complexity algorithm assigns each pair of RF chain and analog beam to the users to effectively reduce channel correlation and cross-talk interference without instantaneous CSI feedbacks. Secondly, as a prerequisite of user selection, BUSH presents a low-overhead probing scheme in multi-carrier WLANs, and designs a highly accurate blind Power Azimuth Spectrum (PAS) estimation algorithm using a single RF chain. Thirdly, the phased-array antennas use analog beamforming to steer spatial beams toward each selected downlink user, and the finite number of RF chains use beamforming to further mitigate the interference among users. We implement BUSH on the WARPv3 boards and evaluate its performance in more than 30 indoor scenarios. The experimental results show that in terms of total throughput BUSH outperforms the legacy 802.11ac by 2.08×, and an alternative benchmark system by 1.22× on average.}, 
keywords={MIMO communication;antenna phased arrays;array signal processing;wireless LAN;wireless channels;analog beam;analog beamforming;capacity gain;downlink user;expensive RF chains;feedback overheads;highly accurate blind power azimuth spectrum estimation algorithm;hybrid beamforming;instantaneous CSI feedbacks;large-scale MU-MIMO prototype;legacy WLAN;linear growth;low complexity algorithm;low-overhead probing scheme;multicarrier WLANs;network capacity;phased-array antennas;scalable beam user selection;scalable user selection scheme;single RF chain;state-of-the-art WLAN;total throughput BUSH;user population;Antennas;Array signal processing;Estimation;Interference;MIMO;Radio frequency;Throughput}, 
doi={10.1109/INFOCOM.2017.8056988}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8056989, 
author={M. Swaminathan and A. Vizziello and D. Duong and P. Savazzi and K. R. Chowdhury}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Beamforming in the body: Energy-efficient and collision-free communication for implants}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Implants are poised to revolutionize personalized healthcare by monitoring and actuating physiological functions. Such implants operate under challenging constraints of limited battery energy, heterogeneous tissue-dependent channel conditions and human-safety regulations. To address these issues, we propose a new cross-layer protocol for galvanic coupled implants wherein weak electrical currents are used in place of classical radio frequency (RF) links. As the first step, we devise a method that allows multiple implants to communicate individual sensed data to each other through CDMA code assignments, but delegates the computational burden of decoding only to the on-body surface relays. Then, we devise a distributed beamforming approach that allows coordinated transmissions from the implants to the relays by considering the specific tissue path chosen and tissue heating-related safety constraints. Our contributions are two fold: First, we devise a collision-free protocol that prevents undue interference at neighboring implants, especially for multiple deployments. Second, this is the first application of near-field distributed beamforming in human tissue. Results reveal significant improvement in the network lifetime for implants of up to 79% compared to the galvanic coupled links without beamforming.}, 
keywords={array signal processing;biological tissues;code division multiple access;medical signal processing;patient monitoring;prosthetics;telecommunication power management;CDMA code assignments;battery energy;classical radio frequency links;collision-free communication;collision-free protocol;cross-layer protocol;distributed beamforming approach;galvanic coupled implants;heterogeneous tissue-dependent channel conditions;human tissue;human-safety regulations;neighboring implants;on-body surface relays;physiological functions;specific tissue path;tissue heating-related safety constraints;Array signal processing;Implants;Multiaccess communication;Muscles;Relays;Sensors;Transmitters}, 
doi={10.1109/INFOCOM.2017.8056989}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8056990, 
author={A. Salam and M. C. Vuran}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Smart underground antenna arrays: A soil moisture adaptive beamforming approach}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Current wireless underground (UG) communication techniques are limited by their achievable distance. In this paper, a novel framework for underground beamforming using adaptive antenna arrays is presented to extend communication distances for practical applications. Based on the analysis of propagation in wireless underground channel, a theoretical model is developed which uses soil moisture information to improve wireless underground communications performance. Array element in soil is analyzed empirically and impacts of soil type and soil moisture on return loss (RL) and resonant frequency are investigated. Accordingly, beam patterns are analyzed to communicate with underground and above ground devices. Depending on the incident angle, refraction from soil-air interface has adverse effects in the UG communications. It is shown that beam steering improves UG communications by providing a high-gain lateral wave. To this end, the angle, which enhances lateral wave, is shown to be a function of dielectric properties of the soil, soil moisture, and soil texture. Evaluations show that this critical angle varies from 0° to 16° and decreases with soil moisture. Accordingly, a soil moisture adaptive beamforming (SMABF) algorithm is developed for planar array structures and evaluated with different optimization approaches to improve UG communication performance.}, 
keywords={adaptive antenna arrays;array signal processing;beam steering;moisture;soil;underground communication;UG communication performance;adaptive antenna arrays;planar array structures;smart underground antenna arrays;soil moisture adaptive beamforming algorithm;soil moisture adaptive beamforming approach;soil moisture information;soil-air interface;underground beamforming;underground ground devices;wireless underground channel;Antenna arrays;Array signal processing;Resonant frequency;Soil moisture;Wireless communication}, 
doi={10.1109/INFOCOM.2017.8056990}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8056991, 
author={J. Palacios and D. De Donno and J. Widmer}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Tracking mm-Wave channel dynamics: Fast beam training strategies under mobility}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={In order to cope with the severe path loss, millimeter-wave (mm-wave) systems exploit highly directional communication. As a consequence, even a slight beam mis-alignment between two communicating devices (for example, due to mobility) can generate a significant signal drop. This leads to frequent invocations of time-consuming mechanisms for beam re-alignment, which deteriorate system performance. In this paper, we propose smart beam training and tracking strategies for fast mm-wave link establishment and maintenance under node mobility. We leverage the ability of hybrid analog-digital transceivers to collect channel information from multiple spatial directions simultaneously and formulate a probabilistic optimization problem to model the temporal evolution of the mm-wave channel under mobility. In addition, we present for the first time a beam tracking algorithm that extracts information needed to update the steering directions directly from data packets, without the need for spatial scanning during the ongoing data transmission. Simulation results, obtained by a custom simulator based on ray tracing, demonstrate the ability of our beam training/tracking strategies to keep the communication rate only 10% below the optimal bound. Compared to the state of the art, our approach provides a 40% to 150% rate increase, yet requires lower complexity hardware.}, 
keywords={array signal processing;optimisation;probability;radio links;ray tracing;wireless channels;beam re-alignment;beam training-tracking strategies;channel information;communicating devices;fast beam training strategies;highly directional communication;hybrid analog-digital transceivers;maintenance;millimeter-wave;mm-Wave channel dynamics;mm-wave link establishment;multiple spatial directions;node mobility;ongoing data transmission;probabilistic optimization problem;severe path loss;signal drop;slight beam mis-alignment;smart beam training;spatial scanning;steering directions;system performance;time-consuming mechanisms;Array signal processing;Heuristic algorithms;Maintenance engineering;Probabilistic logic;Radio frequency;Training;Transceivers}, 
doi={10.1109/INFOCOM.2017.8056991}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8056992, 
author={H. Xu and Z. Yu and C. Qian and X. Y. Li and Z. Liu}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Minimizing flow statistics collection cost of SDN using wildcard requests}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={In a software defined network (SDN), the control plane needs to frequently collect flow statistics measured at the data plane switches for different applications, such as traffic engineering, flow re-routing, and attack detection. However, existing solutions for flow statistics collection may result in large bandwidth cost in the control channel and long processing delay on switches, which significantly interfere with the basic functions such as packet forwarding and route update. To address this challenge, we propose a Cost-Optimized Flow Statistics Collection (CO-FSC) scheme using wildcard-based requests. We prove that the CO-FSC problem is NP-Hard and present a rounding-based algorithm with an approximation factor f, where f is the maximum number of switches visited by each flow. Moreover, our CO-FSC problem is extended to the general case, in which only a part of flows in a network need to be collected. The extensive simulation results show that the proposed algorithms can reduce the bandwidth overhead by over 41% and switch processing delay by over 45% compared with the existing solutions.}, 
keywords={computational complexity;software defined networking;statistical analysis;telecommunication network routing;telecommunication traffic;CO-FSC problem;Cost-Optimized Flow Statistics Collection scheme;SDN;attack detection;control channel;control plane;data plane switches;flow statistics collection cost minimization;long processing delay;packet forwarding;rounding-based algorithm;route update;software defined network;switch processing delay;traffic engineering;wildcard requests;Algorithm design and analysis;Approximation algorithms;Bandwidth;Conferences;Control systems;Delays;Power capacitors;Approximation;Cost;Flow Statistics Collection;Software Defined Networks;Wildcard}, 
doi={10.1109/INFOCOM.2017.8056992}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8056993, 
author={W. Ma and O. Sandoval and J. Beltran and D. Pan and N. Pissinou}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Traffic aware placement of interdependent NFV middleboxes}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Network function virtualization enables flexible implementation of network functions, or middleboxes, as virtual machines running on standard servers. However, the flexibility also creates a challenge for efficiently placing such middleboxes, due to the availability of multiple hosting servers, capability of middleboxes to change traffic volumes, and dependency between middleboxes. In this paper, we address the optimal placement challenge of NFV middleboxes, and propose solutions for middleboxes of different traffic changing effects and with different dependency relations. First, we formulate the Traffic Aware Placement of Interdependent Middleboxes problem as a graph optimization problem. When the flow path is predetermined, we design optimal algorithms to place a non-ordered or totally-ordered middlebox set, and propose an efficient heuristic for the general scenario of a partially-ordered middlebox set after proving its NP-hardness. When the flow path is not predetermined, we show that the problem is NP-hard even for a non-ordered middlebox set, and propose a traffic and space aware routing heuristic. We have evaluated the proposed algorithms using large scale simulations and prototype experiments, and present extensive evaluation results to demonstrate the effectiveness of our design.}, 
keywords={computational complexity;graph theory;optimisation;telecommunication network routing;telecommunication traffic;virtual machines;virtualisation;NP-hardness;Traffic Aware Placement;Traffic aware placement;graph optimization problem;interdependent NFV middleboxes;interdependent middlebox problem;large scale simulations;network function virtualization;nonordered middlebox set;prototype experiments;space aware routing heuristic;totally-ordered middlebox set;traffic changing effects;virtual machines;Algorithm design and analysis;Computer architecture;Hardware;Middleboxes;Optimization;Prototypes;Servers;NFV;SDN;middlebox}, 
doi={10.1109/INFOCOM.2017.8056993}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8056994, 
author={P. Zhang}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Towards rule enforcement verification for software defined networks}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Software defined networks (SDNs) reshape the ossified network architectures, by introducing centralized and programmable network control. Despite the huge benefits, SDNs also open doors to what we call rule modification attack, an attack largely overlooked by the community. In such an attack, the adversary can modify rules by exploiting implementation vulnerabilities of switch OSes and control channels. As a result, packets may deviate from their original paths, thereby violating network policies. To defend against rule modification attack, this paper introduces a new security primitive named rule enforcement verification (REV). REV allows a controller to check whether switches have enforced the rules installed by it, using message authentication code (MAC). Since using standard MACs will incur heavy switch-to-controller traffic, this paper proposes a new compressive MAC, which allows switches to compress MACs before reporting to the controller. Experiments show that REV based on compressive MAC can achieve a 97% reduction in switch-to-controller traffic, and a Sx increase in verification throughput.}, 
keywords={access protocols;computer network security;cryptography;message authentication;software defined networking;REV;SDN;centralized network control;compressive MAC;control channels;message authentication code;network policies;open doors;ossified network architectures;programmable network control;rule enforcement verification;rule modification attack;software defined networks;switch OSes;switch-to-controller traffic;Message authentication;Ports (Computers);Silicon;Standards;Switches;Compressive MAC;Rule modification attack;Software defined networks;verification}, 
doi={10.1109/INFOCOM.2017.8056994}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8056995, 
author={R. Jang and D. Cho and Y. Noh and D. Nyang}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={RFlow #x002B;: An SDN-based WLAN monitoring and management framework}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={In this work, we propose an SDN-based WLAN monitoring and management framework called RFlow+ to address WiFi service dissatisfaction caused by the limited view (lack of scalability) of network traffic monitoring and absence of intelligent and timely network treatments. Existing solutions (e.g., OpenFlow and sFlow) have limited view, no generic flow description, and poor trade-off between measurement accuracy and network overhead depending on the selection of the sampling rate. To resolve these issues, we devise a two-level counting mechanism, namely a distributed local counter (on-site and real-time) and central collector (a summation of local counters). With this, we proposed a highly scalable monitoring and management framework to handle immediate actions based on short-term (e.g., 50 ms) monitoring and eventual actions based on long-term (e.g., 1 month) monitoring. The former uses the local view of each access point (AP), and the latter uses the global view of the collector. Experimental results verify that RFlow+ can achieve high accuracy (less than 5% standard error for short-term and less than 1% for long-term) and fast detection of flows of interest (within 23 ms) with manageable network overhead. We prove the practicality of RFlow+ by showing the effectiveness of a MAC flooding attacker quarantine in a real-world testbed.}, 
keywords={IP networks;computer network management;computerised monitoring;software defined networking;telecommunication security;telecommunication traffic;wireless LAN;MAC flooding attacker;RFlow+;SDN;WLAN monitoring;WiFi service dissatisfaction;access point;central collector;distributed local counter;eventual actions;intelligent network treatments;long-term monitoring;manageable network overhead;management framework;measurement accuracy;network traffic monitoring;sampling rate selection;short-term monitoring;time 1.0 month;time 23.0 ms;time 50.0 ms;two-level counting mechanism}, 
doi={10.1109/INFOCOM.2017.8056995}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8056996, 
author={P. Parag and A. Bura and J. F. Chamberland}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Latency analysis for distributed storage}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Modern communication and computation systems consist of large networks of unreliable nodes. Yet, it is well known that such systems can provide aggregate reliability via information redundancy, duplicating paths, or replicating computations. While redundancy may increase the load on a system, it can also lead to major performance improvements through the judicious management of additional system resources. Two important examples of this abstract paradigm are content access from multiple caches in content delivery networks and master/slave computations on compute clusters. Many recent articles in the area have proposed bounds on the latency performance of redundant systems, characterizing the latency-redundancy tradeoff under specific load profiles. Following a similar line of research, this article introduces new analytical bounds and approximation techniques for the latency-redundancy tradeoff for a range of system loads and two popular redundancy schemes. The proposed framework allows for approximating the equilibrium latency distribution, from which various metrics can be derived including mean, variance, and the tail decay of stationary distributions.}, 
keywords={Internet;cache storage;redundancy;telecommunication traffic;aggregate reliability;approximation techniques;computation systems;compute clusters;content access;content delivery networks;distributed storage;equilibrium latency distribution;information redundancy;judicious management;latency analysis;latency performance;latency-redundancy tradeoff;master-slave computations;multiple caches;redundancy schemes;redundant systems;specific load profiles;stationary distributions;system loads;unreliable nodes;Encoding;Markov processes;Parity check codes;Queueing analysis;Redundancy;Servers;Data storage;Markov processes;content delivery networks;distributed storage systems;equilibrium distribution;forward error correction;queueing analysis;waiting time}, 
doi={10.1109/INFOCOM.2017.8056996}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8056997, 
author={V. Aggarwal and J. Fan and T. Lan}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Taming tail latency for erasure-coded, distributee storage systems}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Distributed storage systems are known to be susceptible to long tails in response time. It has been shown that in modern online applications such as Bing, Facebook, and Amazon, the long tail of latency is of particular concern, with 99.9th percentile response times being orders of magnitude worse than the mean. As erasure codes emerge as a popular technique in distributed storage to achieve high data reliability while attaining space efficiency, taming tail latency remains an open problem due to the lack of mathematical models for analyzing such erasure-coded storage systems. In this paper, we quantify tail latency in distributed storage systems that employ erasure coding. In particular, we derive upper bounds on tail latency in closed-form for arbitrary service time distribution and heterogeneous files. Based on the model, we formulate an optimization problem to jointly minimize weighted latency tail probability of all files. The non-convex problem is solved using an efficient, alternating optimization algorithm. Simulation results show significant reduction of tail latency for erasure-coded storage systems with realistic workload.}, 
keywords={optimisation;probability;storage management;alternating optimization algorithm;distributed storage systems;erasure codes;erasure coding;erasure-coded distributed storage systems;latency tail probability;optimization problem;percentile response times;service time distribution;tail latency;Conferences;Encoding;Optimization;Probabilistic logic;Queueing analysis;Servers;Upper bound}, 
doi={10.1109/INFOCOM.2017.8056997}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8056998, 
author={S. Wei and Y. Li and Y. Xu and S. Wu}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={DSC: Dynamic stripe construction for asynchronous encoding in clustered file system}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Nowadays many clustered file systems adopt asynchronous encoding which transforms replicated data into erasure coding to maintain data availability with bounded storage overhead. Existing implementations of asynchronous encoding construct coding stripes with logically sequential data blocks, which suffers from heavy cross-rack traffic and necessitates data block redistribution. Recent work [12] solves this problem by carefully distributing replicated data blocks among racks at the time when they are being written, but it is not applicable to the cases when existing systems have different data layouts or the data layout changes. In this paper, we propose Dynamic Stripe Construction (DSC) to transform N-way replication to erasure coding. DSC does not induce to any cross-rack traffic for encoding, and it does not require data block redistribution after encoding. Besides, DSC is general enough to be applied to any existing CFSes with various erasure codes, and it can also be deployed on a distributed file system in a hot-plugging-in manner. To validate the effectiveness of DSC, we implement it on HDFS. Through extensive testbed experiments in a real storage cluster, we show that DSC can significantly increase the encoding throughput and reduce the foreground user response time over the traditional approach.}, 
keywords={encoding;storage management;DSC;Dynamic Stripe Construction;Dynamic stripe construction;N-way replication;asynchronous encoding;bounded storage overhead;clustered file system;coding stripes;cross-rack traffic;data availability;data block redistribution;data layouts;distributed file system;encoding throughput;erasure codes;erasure coding;foreground user response time;logically sequential data blocks;replicated data blocks;storage cluster;Distributed databases;Ear;Encoding;Fault tolerance;Fault tolerant systems;Layout}, 
doi={10.1109/INFOCOM.2017.8056998}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8056999, 
author={X. Liu and W. Sun and W. Lou and Q. Pei and Y. Zhang}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={One-tag checker: Message-locked integrity auditing on encrypted cloud deduplication storage}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={In this paper, we investigate the problem of integrity auditing for cloud deduplication storage. Specifically, in addition to the outsourced data confidentiality, we also aim to ensure the integrity of the deduplicated cloud storage. With the existing works based on Provable Data Possession (PDP)/Proof of Retrievability (PoR), we are either required to rely on a fully trusted proxy server or inevitably sacrifice the privacy and efficiency. In contrast, we present a novel message-locked integrity auditing scheme without an additional proxy server, which is applicable to both file-level and chunk-level deduplication systems. In particular, our scheme is storage efficient in the sense that apart from eliminating the ciphertext redundancy, we also enable the integrity tag deduplication by a message-derived signing key, which merely incurs minimal client-side computation overhead. Besides, we can still publicly perform the integrity check over any client's cloud storage by incorporating the proxy re-signature technique. We show that the proposed scheme will not disclose the data ownership information and is provably secure under the Computational Diffie-Hellman (CDH) assumption in the random oracle model. Finally, the performance evaluation demonstrates its effectiveness and efficiency.}, 
keywords={cloud computing;data integrity;data privacy;digital signatures;public key cryptography;storage management;CDH assumption;Computational Diffie-Hellman assumption;Provable Data Possession;chunk-level deduplication systems;client-side computation overhead;data ownership information;deduplicated cloud storage;encrypted cloud deduplication storage;file-level deduplication systems;integrity auditing scheme;integrity check;integrity tag deduplication;message-derived signing key;message-locked integrity auditing;outsourced data confidentiality;proxy re-signature technique;proxy server;random oracle model;Cloud computing;Conferences;Encryption;Protocols;Public key;Servers}, 
doi={10.1109/INFOCOM.2017.8056999}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057000, 
author={M. Dinitz and J. Fineman and S. Gilbert and C. Newport}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Load balancing with bounded convergence in dynamic networks}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Load balancing is an important activity in distributed systems. At a high-level of abstraction, this problem assumes processes in the system begin with a potentially uneven assignment of “work” which they must then attempt to spread evenly. There are many different approaches to solving this problem. In this paper, we focus on local load balancing-an approach in which work is balanced in an iterative and distributed manner by having processes exchange work in each round with neighbors in an underlying communication network. The goal with local load balancing is to prove that these local exchanges will lead over time to a global balance. We describe and analyze a new local load balancing strategy called max neighbor, and prove a bound on the number of rounds required for it to obtain a parameterized level of balance, with high probability, in a general dynamic network topology. We then prove this analysis tight (within logarithmic factors) by describing a network and initial work distribution for which max neighbor matches its upper bound, and then build on this to prove that no load balancing algorithm in which every node exchanges work with at most one partner per round can converge asymptotically faster than max neighbor.}, 
keywords={convergence;probability;resource allocation;telecommunication network topology;bounded convergence;distributed systems;general dynamic network topology;load balancing algorithm;local exchanges;local load balancing strategy;max neighbor;node exchanges work;underlying communication network;Communication networks;Conferences;Convergence;Heuristic algorithms;Hypercubes;Load management;Network topology}, 
doi={10.1109/INFOCOM.2017.8057000}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057001, 
author={H. Xu and H. Huang and S. Chen and G. Zhao}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Scalable software-defined networking through hybrid switching}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Traditional networks rely on aggregate routing and decentralized control to achieve scalability. On the contrary, software-defined networks achieve near optimal network performance and policy-based management through per-flow routing and centralized control, which however face scalability challenge due to (1) limited TCAM and on-die memory for storing the forwarding table and (2) per-flow communication/computation overhead at the controller. This paper presents a novel hybrid switching design, which integrates traditional switching and SDN switching for the purpose of achieving both scalability and optimal performance. We show that the integration also leads to unexpected benefits of making both types of switching more efficient under the hybrid design. Numerical evaluation demonstrates the superior performance of hybrid switching when comparing with the state-of-the-art SDN design.}, 
keywords={centralised control;software defined networking;switching networks;telecommunication control;telecommunication network routing;aggregate routing;centralized control;forwarding table;limited TCAM;near optimal network performance;novel hybrid switching design;on-die memory;per-flow routing;policy-based management;scalable software-defined networking;state-of-the-art SDN design;Aggregates;Centralized control;Ports (Computers);Routing;Scalability;Switches}, 
doi={10.1109/INFOCOM.2017.8057001}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057002, 
author={S. M. Irteza and H. M. Bashir and T. Anwar and I. A. Qazi and F. R. Dogar}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Load balancing over symmetric virtual topologies}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Datacenter networks are often structured as multi-rooted trees to provide high bisection bandwidth at low cost. To utilize the available bisection bandwidth, an efficient load balancing algorithm is required. Packet Spraying is known to perform well in symmetric topologies as it provides per-packet load balancing over equal cost paths. However, packet spraying performs poorly in asymmetric topologies. In this paper we ask, “How can we make packet spraying effective in asymmetric topologies while retaining its simplicity?” Towards this end, we propose SAPS, “Symmetric Adaptive Packet Spraying”, an SDN-based scheme that uses packet spraying over symmetric virtual topologies. SAPS is based on the key insight that if we provide each flow with a symmetric view of the network fabric, then packet spraying can produce near-optimal performance. We evaluate SAPS using simulations and testbed experiments. Our results indicate that SAPS performs well for a variety of application workloads and asymmetric network scenarios.}, 
keywords={routing protocols;software defined networking;telecommunication network topology;telecommunication traffic;trees (mathematics);SAPS;SDN-based scheme;Symmetric Adaptive Packet Spraying;asymmetric topologies;bisection bandwidth;load balancing algorithm;multirooted trees;packet spraying;symmetric topologies;symmetric virtual topologies;Bandwidth;Conferences;Load management;Network topology;Spraying;Switches;Topology}, 
doi={10.1109/INFOCOM.2017.8057002}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057003, 
author={S. Roos and M. Byrenheid and C. Deusser and T. Strufe}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={BD-CAT: Balanced dynamic content addressing in trees}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Balancing the load in content addressing schemes for route-restricted networks represents a challenge with a wide range of applications. Solutions based on greedy embeddings maintain minimal state information and enable efficient routing, but any such solutions currently result in either imbalanced content addressing, overloading individual nodes, or are unable to efficiently account for network dynamics. In this work, we propose a greedy embedding in combination with a content addressing scheme that provides balanced content addressing while at the same time enabling efficient stabilization in the presence of network dynamics. We point out the tradeoff between stabilization complexity and maximal permitted imbalance when deriving upper bounds on both metrics for two variants of the proposed algorithms. Furthermore, we substantiate these bounds through a simulation study based on both real-world and synthetic data.}, 
keywords={content-addressable storage;greedy algorithms;resource allocation;telecommunication network routing;trees (mathematics);BD-CAT;balanced content;balanced dynamic content addressing;content addressing scheme;efficient routing;greedy embedding;imbalanced content addressing;individual nodes;maximal permitted imbalance;minimal state information;network dynamics;route-restricted networks;stabilization complexity;trees;Complexity theory;Conferences;Content addressable storage;Heuristic algorithms;Network topology;Routing;Topology}, 
doi={10.1109/INFOCOM.2017.8057003}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057004, 
author={T. Jung and X. Y. Li and W. Huang and J. Qian and L. Chen and J. Han and J. Hou and C. Su}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={AccountTrade: Accountable protocols for big data trading against dishonest consumers}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={We propose AccountTrade, a set of accountable protocols, for big data trading among dishonest consumers. To secure the big data trading environment, our protocols achieve book-keeping ability and accountability against dishonest consumers who may misbehave throughout the dataset transactions. Specifically, we study the responsibilities of the consumers in the dataset trading and design AccountTrade to achieve accountability against the dishonest consumers who may try to deviate from their responsibilities. Specifically, we propose uniqueness index, a new rigorous measurement of the data uniqueness, as well as several accountable trading protocols to enable data brokers to blame the dishonest consumer when misbehavior is detected. We formally define, prove, and evaluate the accountability of our protocols by an automatic verification tool as well as extensive evaluation in real-world datasets. Our evaluation shows that AccountTrade incurs negligible constant storage overhead per file (<;10KB), and it is able to handle 8-1000 concurrent data uploading per server depending on the data types.}, 
keywords={Big Data;electronic commerce;formal verification;transaction processing;AccountTrade;accountable protocols;accountable trading protocols;big data trading environment;book-keeping ability;data brokers;data uniqueness;dataset trading;dishonest consumer;uniqueness index;Big Data;Conferences;Cryptography;Data models;Indexes;Monitoring;Protocols}, 
doi={10.1109/INFOCOM.2017.8057004}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057005, 
author={X. Yao and R. Zhang and Y. Zhang and Y. Lin}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Verifiable social data outsourcing}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Social data outsourcing is an emerging paradigm for effective and efficient access to the social data. In such a system, a third-party Social Data Provider (SDP) purchases complete social datasets from Online Social Network (OSN) operators and then resells them to data consumers who can be any individuals or entities desiring the complete social data satisfying some criteria. The SDP cannot be fully trusted and may return wrong query results to data consumers by adding fake data and deleting/modifying true data in favor of the businesses willing to pay. In this paper, we initiate the study on verifiable social data outsourcing whereby a data consumer can verify the trustworthiness of the social data returned by the SDP. We propose three schemes for verifiable queries over outsourced social data. The three schemes all require the OSN provider to generate some cryptographic auxiliary information, based on which the SDP can construct a verification object for the data consumer to verify the query-result trustworthiness. They differ in how the auxiliary information is generated and how the verification object is constructed and verified. Extensive experiments based on a real Twitter dataset confirm the high efficacy and efficiency of our schemes.}, 
keywords={data privacy;outsourcing;query processing;social networking (online);OSN operators;Online Social Network operators;Twitter dataset;data consumer;fake data;social data trustworthiness;social datasets;third-party SDP;third-party Social Data Provider;true data deletion;true data modification;verifiable social data outsourcing;Conferences;Cryptography;Facebook;Outsourcing;Twitter}, 
doi={10.1109/INFOCOM.2017.8057005}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057006, 
author={P. Giridhar and S. Wang and T. Abdelzaher and R. Ganti and L. Kaplan and J. George}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={On localizing urban events with Instagram}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={This paper develops an algorithm that exploits picture-oriented social networks to localize urban events. We choose picture-oriented networks because taking a picture requires physical proximity, thereby revealing the location of the photographed event. Furthermore, most modern cell phones are equipped with GPS, making picture location, and time metadata commonly available. We consider Instagram as the social network of choice and limit ourselves to urban events (noting that the majority of the world population lives in cities). The paper introduces a new adaptive localization algorithm that does not require the user to specify manually tunable parameters. We evaluate the performance of our algorithm for various real-world datasets, comparing it against a few baseline methods. The results show that our method achieves the best recall, the fewest false positives, and the lowest average error in localizing urban events.}, 
keywords={meta data;social networking (online);Instagram;adaptive localization algorithm;photographed event location;physical proximity;picture location;picture-oriented social networks;time metadata;urban event localization;Algorithm design and analysis;Event detection;Flickr;Metadata;Twitter;Urban areas}, 
doi={10.1109/INFOCOM.2017.8057006}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057007, 
author={Y. Yu and W. Wang and J. Zhang and K. Ben Letaief}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={LRC: Dependency-aware cache management for data analytics clusters}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Memory caches are being aggressively used in today's data-parallel systems such as Spark, Tez, and Piccolo. However, prevalent systems employ rather simple cache management policies - notably the Least Recently Used (LRU) policy - that are oblivious to the application semantics of data dependency, expressed as a directed acyclic graph (DAG). Without this knowledge, memory caching can at best be performed by “guessing” the future data access patterns based on historical information (e.g., the access recency and/or frequency), which frequently results in inefficient, erroneous caching with low hit ratio and a long response time. In this paper, we propose a novel cache replacement policy, Least Reference Count (LRC), which exploits the application-specific DAG information to optimize the cache management. LRC evicts the cached data blocks whose reference count is the smallest. The reference count is defined, for each data block, as the number of dependent child blocks that have not been computed yet. We demonstrate the efficacy of LRC through both empirical analysis and cluster deployments against popular benchmarking workloads. Our Spark implementation shows that, compared with LRU, LRC speeds up typical applications by 60%.}, 
keywords={cache storage;data analysis;directed graphs;LRC;LRU policy;Least Reference Count;application semantics;application-specific DAG information;cache management policies;cache replacement policy;cached data blocks;cluster deployments;data access patterns;data analytics clusters;data dependency;data-parallel systems;dependency-aware cache management;directed acyclic graph;empirical analysis;historical information;hit ratio;memory caches;memory caching;response time;Benchmark testing;Conferences;Data analysis;Random access memory;Semantics;Sparks}, 
doi={10.1109/INFOCOM.2017.8057007}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057008, 
author={Y. Afek and A. Bremler-Barr and L. Shafir}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Network anti-spoofing with SDN data plane}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Traditional DDoS anti-spoofing scrubbers require dedicated middleboxes thus adding CAPEX, latency and complexity in the network. This paper starts by showing that the current SDN match-and-action model is rich enough to implement a collection of anti-spoofing methods. Secondly we develop and utilize advance methods for dynamic resource sharing to distribute the required mitigation resources over a network of switches. None of the earlier attempts to implement anti-spoofing in SDN actually directly exploited the match and action power of the switch data plane. They required additional functionalities on top of the match-and-action model, and are not implementable on an SDN switch as is. Our method builds on the premise that an SDN data path is a very fast and efficient engine to perform low level primitive operations at wire speed. The solution requires a number of flow-table rules and switch-controller messages proportional to the legitimate traffic. To scale when protecting multiple large servers the flow tables of multiple switches are harnessed in a distributed and dynamic network based solution. We have fully implemented all our methods in either Open-Flow1.5 in Open-vSwitch and in P4. The system mitigates spoofed attacks on either the SDN infrastructure itself or on downstream servers.}, 
keywords={computer network security;software defined networking;telecommunication traffic;Open-Flow1.5;Open-vSwitch;P4;SDN data path;SDN data plane;SDN infrastructure;SDN match-and-action model;SDN switch;anti-spoofing methods;distributed network based solution;downstream servers;dynamic network based solution;dynamic resource;flow-table rules;legitimate traffic;low level primitive operations;mitigation resources;multiple switches;network anti-spoofing;switch data plane;switch-controller messages;traditional DDoS anti-spoofing scrubbers;Computer crime;Conferences;Control systems;Electronic mail;Materials handling;Servers;Software}, 
doi={10.1109/INFOCOM.2017.8057008}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057009, 
author={G. Shang and P. Zhe and X. Bin and H. Aiqun and R. Kui}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={FloodDefender: Protecting data and control plane resources under SDN-aimed DoS attacks}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={The separated control and data planes in software-defined networking (SDN) with high programmability introduce a more flexible way to manage and control network traffic. However, SDN will experience long packet delay and high packet loss rate when the communication link between two planes is jammed by SDN-aimed DoS attacks with massive table-miss packets. In this paper, we propose FloodDefender, an efficient and protocol-independent defense framework for SDN/OpenFlow networks to mitigate DoS attacks. It stands between the controller platform and other controller apps, and can protect both the data and control plane resources by leveraging three new techniques: table-miss engineering to prevent the communication bandwidth from being exhausted; packet filter to identify attack traffic and save computational resources of the control plane; and flow rule management to eliminate most of useless flow entries in the switch flow table. All designs of FloodDefender conform to the OpenFlow policy, requiring no additional devices. We implement a prototype of FloodDefender and evaluate its performance in both software and hardware environments. Experimental results show that FloodDefender can efficiently mitigate the SDN-aimed DoS attacks, incurring less than 0.5% CPU computation to handle attack traffic, only 18ms packet delay and 5% packet loss rate under attacks.}, 
keywords={computer network security;protocols;software defined networking;telecommunication network routing;telecommunication traffic;FloodDefender;SDN-aimed DoS attacks;SDN/OpenFlow networks;attack traffic;computational resources;controller apps;controller platform;data planes;flow rule management;high programmability;massive table-miss packets;network traffic control;networking;packet delay;packet filter;packet loss rate;plane resources;protocol-independent defense framework;separated control;switch flow table;table-miss engineering;time 18.0 ms;Bandwidth;Computer crime;Conferences;Delays;Monitoring;Switches}, 
doi={10.1109/INFOCOM.2017.8057009}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057010, 
author={A. Bremler-Barr and E. Brosh and M. Sides}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={DDoS attack on cloud auto-scaling mechanisms}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Auto-scaling mechanisms are an important line of defense against Distributed Denial of Service (DDoS) in the cloud. Using auto-scaling, machines can be added and removed in an on-line manner to respond to fluctuating load. It is commonly believed that the auto-scaling mechanism casts DDoS attacks into Economic Denial of Sustainability (EDoS) attacks. Rather than suffering from performance degradation up to a total denial of service, the victim suffers only from the economic damage incurred by paying for the extra resources required to process the bogus traffic of the attack. Contrary to this belief, we present and analyze the Yo-Yo attack, a new attack against the auto-scaling mechanism, that can cause significant performance degradation in addition to economic damage. In the Yo-Yo attack, the attacker sends periodic bursts of overload, thus causing the auto-scaling mechanism to oscillate between scale-up and scale-down phases. The Yo-Yo attack is harder to detect and requires less resources from the attacker compared to traditional DDoS. We demonstrate the attack on Amazon EC2 [4], and analyze protection measures the victim can take by reconfiguring the auto-scaling mechanism.}, 
keywords={cloud computing;computer network security;Amazon EC2;DDoS attack;EDoS attack;Yo-Yo attack;auto-scaling mechanism;cloud auto-scaling mechanisms;distributed denial of service attack;economic denial of sustainability attack;Adaptive systems;Cloud computing;Computer crime;Conferences;Economics;Google;Sustainable development;Auto-scaling;Cloud attack;Denial-of-service attack;Distributed systems security;Economic-Denial-of-Sustainability attack}, 
doi={10.1109/INFOCOM.2017.8057010}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057011, 
author={M. Jadin and G. Tihon and O. Pereira and O. Bonaventure}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Securing multipath TCP: Design implementation}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={MultiFath TCP (MPTCP) is a recent TCP extension that enables hosts to send data over multiple paths for a single connection. It is already deployed for various use cases, notably on smartphones. In parallel with this, there is a growing deployment of encryption and authentication techniques to counter various forms of security attacks. Tcpcrypt and TLS are some of these security solutions. In this paper, we propose MPTCPsec, a MultiPath TCP extension that closely integrates authentication and encryption inside the protocol itself. Our design relies on an adaptation for the multipath environment of the ENO option that is being discussed within the IETF tcpinc working group. We then detail how MultiPath TCP needs to be modified to authenticate and encrypt all data and authenticate the different TCP options that it uses. Finally, we implement our proposed extension in the reference implementation of MultiPath TCP in the Linux kernel and we evaluate its performance.}, 
keywords={IP networks;Linux;cryptographic protocols;telecommunication security;transport protocols;Linux kernel;MultiPath TCP extension;Tcpcrypt;authentication techniques;encryption;multipath environment;protocol;security attacks;security solutions;smartphones;Authentication;Encryption;Linux;Protocols;Smart phones}, 
doi={10.1109/INFOCOM.2017.8057011}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057012, 
author={M. van der Boor and S. Borst and J. van Leeuwaarden}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Load balancing in large-scale systems with multiple dispatchers}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Load balancing algorithms play a crucial role in delivering robust application performance in data centers and cloud networks. Recently, strong interest has emerged in Join-the-Idle-Queue (JIQ) algorithms, which rely on tokens issued by idle servers in dispatching tasks and outperform power-of-d policies. Specifically, JiQ strategies involve minimal information exchange, and yet achieve zero blocking and wait in the many-server limit. The latter property prevails in a multiple-dispatcher scenario when the loads are strictly equal among dispatchers. For various reasons it is not uncommon however for skewed load patterns to occur. We leverage product-form representations and fluid limits to establish that the blocking and wait then no longer vanish, even for arbitrarily low overall load. Remarkably, it is the least-loaded dispatcher that throttles tokens and leaves idle servers stranded, thus acting as bottleneck. Motivated by the above issues, we introduce two enhancements of the ordinary JIQ scheme where tokens are either distributed non-uniformly or occasionally exchanged among the various dispatchers. We prove that these extensions can achieve zero blocking and wait in the many-server limit, for any subcritical overall load and arbitrarily skewed load profiles. Extensive simulation experiments demonstrate that the asymptotic results are highly accurate, even for moderately sized systems.}, 
keywords={cloud computing;queueing theory;resource allocation;JiQ strategies;Join-the-Idle-Queue algorithms;cloud networks;data centers;dispatching tasks;fluid limits;large-scale systems;leverage product-form representations;load balancing algorithms;load profiles;multiple-dispatcher scenario;skewed load patterns;throttles tokens;zero blocking;Algorithm design and analysis;Conferences;Large-scale systems;Load management;Load modeling;Servers}, 
doi={10.1109/INFOCOM.2017.8057012}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057013, 
author={W. R. KhudaBukhsh and A. Rizk and A. Frömmgen and H. Koeppl}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Optimizing stochastic scheduling in fork-join queueing models: Bounds and applications}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Fork-Join (FJ) queueing models capture the dynamics of system parallelization under synchronization constraints, for example, for applications such as MapReduce, multipath transmission and RAID systems. Arriving jobs are first split into tasks and mapped to servers for execution, such that a job can only leave the system when all of its tasks are executed. In this paper, we provide computable stochastic bounds for the waiting and response time distributions for heterogeneous FJ systems under general parallelization benefit. Our main contribution is a generalized mathematical framework for probabilistic server scheduling strategies that are essentially characterized by a probability distribution over the number of utilized servers, and the optimization thereof. We highlight the trade-off between the scaling benefit due to parallelization and the FJ inherent synchronization penalty. Further, we provide optimal scheduling strategies for arbitrary scaling regimes that map to different levels of parallelization benefit. One notable insight obtained from our results is that different applications with varying parallelization benefits result in different optimal strategies. Finally, we complement our analytical results by applying them to various applications showing the optimality of the proposed scheduling strategies.}, 
keywords={optimisation;queueing theory;statistical distributions;stochastic processes;telecommunication scheduling;FJ inherent synchronization penalty;FJ queueing models;arbitrary scaling regimes;fork-join queueing models;general parallelization benefit;generalized mathematical framework;heterogeneous FJ systems;optimal scheduling strategies;optimal strategies;optimality;probabilistic server scheduling strategies;probability distribution;scaling benefit;stochastic scheduling optimization;synchronization constraints;system parallelization;waiting response time distributions;Computational modeling;Optimal scheduling;Processor scheduling;Servers;Steady-state;Time factors}, 
doi={10.1109/INFOCOM.2017.8057013}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057014, 
author={A. Anand and G. de Veciana}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Measurement-based scheduler for multi-class QoE optimization in wireless networks}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Traditional wireless schedulers have been driven by rate-based criteria, e.g., utility maximizing/proportionally fair, and/or queue-based packet schedulers which do not directly reflect the Quality of Experience (QoE) associated with flow-based transactions and services. This paper proposes, a Measurement-Based Delay Optimal (MBDO) scheduler, which optimizes a cost function of the mean flow delays in a multi-class system, e.g,. web interactive, file downloads, etc. In this context the cost function expresses desired trade-offs amongst traffic classes reflecting heterogeneous QoE sensitivities which are nonlinear in the flow delays and/or system loads. To achieve optimality, MBDO scheduling uses measured system variables and knowledge (or measurement) of class flow-size distributions to adapt a weighted Gittins index scheduler. We show that under mild assumptions, and in a stationary regime, that MBDO scheduling is indeed asymptotically optimal. Perhaps more importantly, MBDO schedulers can self-optimize by adapting to slowly varying traffic loads, mixes and flow size distributions. Our extensive simulations confirm the effectiveness at realizing trade-offs and performance of the proposed approach.}, 
keywords={optimisation;quality of experience;radio networks;telecommunication scheduling;telecommunication traffic;Delay Optimal scheduler;MBDO schedulers;MBDO scheduling;class flow-size distributions;cost function;flow size distributions;flow-based transactions;heterogeneous QoE sensitivities;mean flow delays;measurement-based delay optimal scheduler;multiclass QoE optimization;multiclass system;packet schedulers;quality of experience;system loads;system variables;traffic classes;weighted Gittins index scheduler;wireless networks;wireless schedulers;Base stations;Cost function;Delays;Wireless networks}, 
doi={10.1109/INFOCOM.2017.8057014}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057015, 
author={A. Davydow and P. Chuprikov and S. I. Nikolenko and K. Kogan}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Throughput optimization with latency constraints}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Modern datacenters are increasingly required to deal with latency-sensitive applications. A major question here is how to represent latency in desired objectives. Incorporation of multiple traffic characteristics (e.g., packet values and required processing requirements) significantly increases the complexity of buffer management policies. In this work, we consider weighted throughput optimization (total transmitted value) in the setting where every incoming packet is branded with intrinsic value, required processing, and slack (an offset from the arrival time when a packet should be transmitted), and the buffer is unbounded but effectively bounded by slacks. The main result is a 3-competitive algorithm as the slack-to-work ratio increases. Our results supported by a comprehensive evaluation study on CAIDA network traces.}, 
keywords={Internet;competitive algorithms;computer centres;telecommunication traffic;CAIDA network;buffer management policies;datacenters;latency constraints;multiple traffic characteristics;packet values;slack-to-work ratio increases;throughput optimization;Algorithm design and analysis;Conferences;Optimization;Process control;Scheduling;Throughput;Upper bound}, 
doi={10.1109/INFOCOM.2017.8057015}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057016, 
author={Y. Luo and L. Pu and Y. Zhao and G. Wang and M. Song}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Optimal energy requesting strategy for RF-based energy harvesting wireless communications}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Energy harvesting is emerging as a promising alternative source to power the next generation of wireless networks. This paper introduces a new energy harvesting strategy that uses a dedicated energy source to optimally replenish energy for radio frequency (RF) based wireless communication systems. Specifically, we develop a two-step dual tunnel energy requesting (DTER) strategy that allows an energy harvesting device to effectively obtain energy from a dedicated energy source. While minimizing the system energy consumption, DTER takes into account the practical constraints on both the energy source and the energy harvesting device. Additionally, the overhead issue and the charge characteristics of an energy storage component are examined to make the proposed strategy practical. To solve the nonlinear optimization problem in DTER, we convert the design of optimal energy requesting problem into a classic shortest path problem and thus enable us to find a global optimal solution through dynamic programming algorithms. Theoretical analysis and simulation study verify that DTER outperforms two other schemes in the literature.}, 
keywords={energy harvesting;nonlinear programming;radio networks;telecommunication power supplies;DTER strategy;RF;energy harvesting device;energy harvesting strategy;energy source;energy storage component;nonlinear optimization problem;optimal energy requesting problem;optimal energy requesting strategy;system energy consumption;two-step dual tunnel energy;wireless communication systems;wireless communications;wireless networks;Batteries;Data communication;Energy consumption;Energy harvesting;Erbium;Radio frequency}, 
doi={10.1109/INFOCOM.2017.8057016}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057017, 
author={H. Dai and X. Wang and A. X. Liu and H. Ma and G. Chen}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Optimizing wireless charger placement for directional charging}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Wireless Power Transfer (WPT) technology has witnessed huge development because of its convenience and reliability. This paper concerns the fundamental issue of wireless charger PLacement with Optimized charging uTility (PLOT), that is, given a fixed number of chargers and a set of points on the plane, determining the positions and orientations of chargers such that the overall expected charging utility for all points is maximized. To address PLOT, we propose a 1 - 1/e - ε approximation algorithm. First, we present techniques to approximate the nonlinear charging power and the expected charging utility to make the problem almost linear. Second, we develop a Dominating Coverage Set extraction method to reduce the continuous search space of PLOT to a limited and discrete one without performance loss. Third, we prove that the reformulated problem is essentially maximizing a monotone submodular function subject to a matroid constraint, and propose a greedy algorithm to address this problem. We conduct both simulation and field experiments to validate our theoretical results, and the results show that our algorithm can outperform comparison algorithms by at least 46.3%.}, 
keywords={approximation theory;computational complexity;greedy algorithms;inductive power transmission;optimisation;set theory;ε approximation algorithm;Dominating Coverage Set extraction method;Wireless Power Transfer technology;directional charging;matroid constraint;monotone submodular function;nonlinear charging power approximation;reliability;wireless charger PLOT;wireless charger placement and optimized charging utility;Approximation algorithms;Conferences;Greedy algorithms;Performance evaluation;Reliability;Wireless communication;Wireless sensor networks}, 
doi={10.1109/INFOCOM.2017.8057017}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057018, 
author={F. Wang and X. Zhang}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Secure resource allocations for polarization-enabled cooperative cognitive radio networks with energy harvesting capability}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={We address secure communications over energy-harvesting based OFDMA cooperative cognitive radio networks, where one primary user (PU) cooperates with several secondary users (SUs) in terms of both information transmission and energy harvesting. To improve spectrum utilization and ensure that SU transmitters can harvest as much energy as possible, we suppose that SUs are equipped with orthogonally dual-polarized antennas. Based on these setting-ups, we propose the polarization-enabled two-phase cooperative framework, where SU transmitters first apply power splitting technique to harvest energy from radio frequency signals radiated by the PU transmitter, and then use the harvested energy to concurrently transmit their own and the PU's data. Under the proposed framework, we develop the secure resource allocation schemes for the scenarios when SU receivers are untrusted users, implying that each SU receiver may overhear the PU's and the other SUs' confidential information. For this scenario, which has hardly been studied, we investigate the joint allocation of relays, subcarriers, power splitting ratios, and powers, with the objective to maximize the total secrecy rate of all SUs while guaranteeing the PU's minimum secrecy rate. Finally, we validate and evaluate our proposed cooperative framework and resource allocation schemes through numerical analyses.}, 
keywords={OFDM modulation;antennas;cognitive radio;cooperative communication;energy harvesting;frequency division multiple access;radio networks;radio receivers;radio transmitters;resource allocation;telecommunication security;PU minimum secrecy rate;PU transmitter;SU receiver;SU transmitters;communication security;dual-polarized antennas;energy harvesting capability;energy-harvesting based OFDMA cooperative cognitive radio networks;information transmission;power splitting ratios;power splitting technique;primary user;radio frequency signals;resource allocation schemes;resource allocation security;secondary users;spectrum utilization;untrusted users;Antennas;Energy harvesting;Fading channels;Receivers;Relays;Resource management;Transmitters;Energy harvesting;RF signals;cooperative overlay CRN;orthogonally dual-polarized antennas (ODPAs);resource allocation;secure communication}, 
doi={10.1109/INFOCOM.2017.8057018}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057019, 
author={L. Yan and H. Shen and J. Zhao and C. Xu and F. Luo and C. Qiu}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={CatCharger: Deploying wireless charging lanes in a metropolitan road network through categorization and clustering of vehicle traffic}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={The future generation of transportation system will be featured by electrified public transportation. To fulfill metropolitan transit demands, electric vehicles (EVs) must be continuously operable without recharging downtime. Wireless Power Transfer (WPT) techniques for in-motion EV charging is a solution. It however brings up a challenge: how to deploy charging lanes in a metropolitan road network to minimize the deployment cost while enabling EVs' continuous operability. In this paper, we propose CatCharger, which is the first work that handles this challenge. From a metropolitan-scale dataset collected from multiple sources of vehicles, we observe the diversity of vehicle passing speed and daily visit frequency (called traffic attributes) at intersections (i.e., landmarks), which are important factors for charging lane deployment. To select landmarks for deployment, we first group landmarks with similar traffic attribute values using the entropy minimization clustering method, and choose better candidate landmarks from each group suitable for deployment. To determine the deployment locations from the candidate landmarks, we infer the expected vehicle residual energy at each landmark using a Kernel Density Estimator fed by the vehicles' mobility, and formulate and solve an optimization problem to minimize the total deployment cost while ensuring a certain level of expected residual energy of EVs at each landmark. Our trace-driven experiments demonstrate the superior performance of CatCharger over other methods.}, 
keywords={electric vehicle charging;electric vehicles;inductive power transmission;optimisation;pattern clustering;road traffic;traffic engineering computing;transportation;CatCharger;EVs continuous operability;Wireless Power Transfer techniques;electric vehicles;electrified public transportation;entropy minimization clustering method;group landmarks;in-motion EV charging;kernel density estimator;lane deployment;metropolitan road network;metropolitan transit demands;metropolitan-scale dataset;total deployment cost;traffic attribute values;transportation system;vehicle residual energy;vehicle traffic;wireless charging lanes;Batteries;Charging stations;Conferences;Inductive charging;Optimization;Roads;Wireless power transfer}, 
doi={10.1109/INFOCOM.2017.8057019}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057020, 
author={X. Xia and S. Li and Y. Zhang and L. Li and T. Gu and Y. Liu and Y. Pan}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Surviving screen-off battery through out-of-band Wi-Fi coordination}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={This paper identifies two energy saving opportunities of Wi-Fi interface emerged during smartphone's screen-off periods. Exploiting the opportunities, we propose a new power saving strategy, BackPSM, for screen-off Wi-Fi communications. BackPSM regulates client to send and receive packets in batches and coordinates multiple clients to communicate at different slots (i.e., beacon interval). The core problem in BackPSM is how to coordinate client without incurring extra traffic overheads. To handle the problem, we propose a novel paradigm, Out-of-Band Communication (OBC), for client-to-client direct communications. OBC exploits the TIM (Traffic Indication Map) field of Wi-Fi Beacon to create a free side-channel between clients. It is based upon the observation that a client may control 1 → 0 appearing on TIM bit by locally regulating packet receiving operations. We adopt this 1 → 0 as the basic signal, and leverage the time length in between two signals to encode information. We demonstrate that OBC can be used to convey coordination information with close to 100% accuracy. We have implemented and evaluated BackPSM on a testbed. The results show that BackPSM reduces screen-off energy by up to 60%, and outperforms state-of-the-art strategies by 16%-42%.}, 
keywords={telecommunication power management;telecommunication traffic;wireless LAN;BackPSM;OBC;Out-of-Band Communication;TIM field;Traffic Indication Map;Wi-Fi Beacon;Wi-Fi interface;beacon interval;client-to-client direct communications;coordination information;out-of-band Wi-Fi coordination;packet receiving operations;power saving strategy;screen-off Wi-Fi communications;screen-off battery;Delays;Energy consumption;Scalability;Smart phones;Switches;Wireless fidelity}, 
doi={10.1109/INFOCOM.2017.8057020}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057021, 
author={M. Jin and Y. He and D. Fang and X. Chen and X. Meng and T. Xing}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={iGuard: A real-time anti-theft system for smartphones}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Smartphone theft is a non-negligible problem that causes serious concerns on personal property, privacy, and public security. The existing solutions to this problem either provide only functions like retrieving a phone, or require dedicated hardware to detect thefts. How to protect smartphones from being stolen at all times is still an open problem. In this paper, we propose iGuard, a real-time anti-theft system for smartphones. iGuard utilizes only the inertial sensing data from the smartphone. The basic idea behind iGuard is to distinguish different people holding a smartphone, by identifying the order of the motions during the `take-out' behavior and how each motion is performed. For this purpose, we design a motion segmentation algorithm to detect the transition between two motions from the noisy sensing data. We then leverage the distinct feature contained in each sub-segment of a motion, instead of the entire motion, to estimate the probability that the motion is performed by the smartphone owner himself/herself. Based on such pre-processed data, we propose a Markov Chain based model to track the behavior of a smartphone user. According to this model, iGuard instantly alarms once the tracked data deviate from the smartphone owner's usual habit. We implement iGuard on Android and evaluate its performance in real environments. The experimental results show that iGuard is accurate and robust in various scenarios.}, 
keywords={Markov processes;human factors;security of data;smart phones;Android;Markov chain based model;iGuard system;inertial sensing data;motion segmentation algorithm;noisy sensing data;realtime anti-theft system;smart phone user behavior;smartphone owner;smartphone theft;smartphone user;Acceleration;Feature extraction;Legged locomotion;Real-time systems;Sensors;Smart phones;Tracking}, 
doi={10.1109/INFOCOM.2017.8057021}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057022, 
author={X. Xu and H. Gao and J. Yu and Y. Chen and Y. Zhu and G. Xue and M. Li}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={ER: Early recognition of inattentive driving leveraging audio devices on smartphones}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Real-time driving behavior monitoring is a corner stone to improve driving safety. Most of the existing studies on driving behavior monitoring using smartphones only provide detection results after an abnormal driving behavior is finished, not sufficient for driver alert and avoiding car accidents. In this paper, we leverage existing audio devices on smartphones to realize early recognition of inattentive driving events including Fetching Forward, Picking up Drops, Turning Back and Eating or Drinking. Through empirical studies of driving traces collected in real driving environments, we find that each type of inattentive driving event exhibits unique patterns on Doppler profiles of audio signals. This enables us to develop an Early Recognition system, ER, which can recognize inattentive driving events at an early stage and alert drivers timely. ER employs machine learning methods to first generate binary classifiers for every pair of inattentive driving events, and then develops a modified vote mechanism to form a multi-classifier for all inattentive driving events along with other driving behaviors. It next turns the multi-classifier into a gradient model forest to achieve early recognition of inattentive driving. Through extensive experiments with 8 volunteers driving for about half a year, ER can achieve an average total accuracy of 94.80% for inattentive driving recognition and recognize over 80% inattentive driving events before the event is 50% finished.}, 
keywords={audio signal processing;driver information systems;learning (artificial intelligence);road accidents;road safety;signal classification;smart phones;ER;abnormal driving behavior;audio devices;audio signals;driving behaviors;driving environments;driving safety;early recognition system;inattentive driving recognition;machine learning;multiclassifier;real-time driving behavior monitoring;smartphones;Automobiles;Doppler effect;Erbium;Monitoring;Smart phones;Turning}, 
doi={10.1109/INFOCOM.2017.8057022}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057023, 
author={Y. Gao and Y. Luo and D. Chen and H. Huang and W. Dong and M. Xia and X. Liu and J. Bu}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Every pixel counts: Fine-grained UI rendering analysis for mobile applications}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={For mobile apps, user-perceived delays are critical for user satisfaction. According to our measurement, long delays are commonly caused by network and storage I/O operations while short delays are mainly caused by UI rendering. Short delays are not uncommon, which account for 55.3% in our measurement cases. Previous app performance studies have largely focused on I/O operations but the understanding of UI rendering impact is limited. In this work, we propose DRAW, a system that performs two UI rendering analyses to help app developers pinpoint rendering problems and resolve short delays. The first analysis outlines the wasted rendering time on invisible or covered UI components, namely the overdraw problem. The second analysis is to identify the responsible UI components and rendering operations that cause overall low rendering efficiency. We implement DRAW on Android and apply it to study 1,158 real-world Android apps. Results show that DRAW is helpful as it can pinpoint the responsible UI components and specific rendering operations. Four concrete case studies of real-world apps are further presented to show how DRAW can help developers improve the UI rendering performance of their apps.}, 
keywords={Android (operating system);mobile computing;rendering (computer graphics);smart phones;user interfaces;Android apps;DRAW;I/O operations;UI rendering analyses;UI rendering impact;app performance studies;covered UI components;fine-grained UI rendering analysis;invisible components;low rendering efficiency;mobile applications;responsible UI components;specific rendering operations;user satisfaction;Androids;Delays;Humanoid robots;Mobile applications;Rendering (computer graphics);Smart phones;Time factors}, 
doi={10.1109/INFOCOM.2017.8057023}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057024, 
author={G. Gao and Y. Wen and H. Hu}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={QDLCoding: QoS-differentiated low-cost video encoding scheme for online video service}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Adaptive bitrate (ABR) streaming is the de facto solution in online video services to cope with heterogeneous devices and varying network connections. However, this solution is computation intensive, demanding a large number of servers for encoding videos. Moreover, due to the time-varying nature of video generation, intelligent strategies are required in order to determine the right amount of resources for encoding. The situation is further complicated by the fact that, the two types of co-existing video content, live content and Video-on-Demand (VoD) content, have different QoS requirements for encoding. These observations posit daunting challenges for meeting the heterogeneous QoS requirements with a minimum computing capacity. This paper proposes the QoS-differentiated low-cost video encoding (QDLCoding) scheme to address these challenges. We develop a framework for scheduling the encoding workloads of the two types of videos with statistical QoS guarantees. Each type of videos is specified with a QoS criterion and a QoS loss bound. The objective is to provision the minimum amount of resources while keeping the QoS loss probabilities within the prescribed bounds. We design an online algorithm that can determine the minimum required capacity by learning content arrival distributions. The experiment results demonstrate that our method can greatly reduce the required capacity for encoding online videos while controlling the likelihood of QoS loss precisely.}, 
keywords={probability;quality of service;video coding;video on demand;QDLCoding scheme;QoS loss probabilities;QoS-differentiated low-cost video encoding scheme;Video-on-Demand content;adaptive bitrate streaming;content arrival distributions;facto solution;heterogeneous QoS requirements;heterogeneous devices;live content;low-cost video encoding;online algorithm;online video encoding;online video service;statistical QoS guarantees;time-varying nature;varying network connections;video content;video generation;Capacity planning;Delays;Encoding;Quality of service;Resource management;Servers;Streaming media}, 
doi={10.1109/INFOCOM.2017.8057024}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057025, 
author={M. Tang and S. Wang and L. Gao and J. Huang and L. Sun}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={MOMD: A multi-object multi-dimensional auction for crowdsourced mobile video streaming}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Crowdsourced mobile video streaming enables nearby mobile video users to aggregate their network resources to improve the video streaming performance. However, users are often selfish and may not be willing to cooperate without proper incentives. Designing an incentive mechanism for such a scenario is challenging due to the users' asynchronous downloading behaviors as well as their private valuations for multi-bitrate encoded videos. In this work, we propose a multi-object multi-dimensional auction-based incentive framework, through which users can download multiple video segments with different bitrates for multiple nearby users (and themselves). Based on this incentive framework, we propose a Vickrey-score auction, which is the first multi-object multi-dimensional auction that achieves both truthfulness and efficiency. Simulations with real traces show that crowdsourced mobile streaming outperforms noncooperative streaming by 48.6% (on average) in terms of social welfare. We further implement our proposed auction mechanism in a demostration system, and show that the crowdsourced framework together with the auction mechanism can substantially increase mobile user's welfare and video service stability.}, 
keywords={commerce;incentive schemes;mobile computing;video coding;video streaming;Vickrey-score auction;auction mechanism;crowdsourced framework;crowdsourced mobile streaming outperforms;crowdsourced mobile video streaming;incentive framework;incentive mechanism;mobile video users;multibitrate encoded videos;multidimensional auction;video segments;video streaming performance;Adaptation models;Bit rate;Cost accounting;Mobile communication;Mobile computing;Resource management;Streaming media}, 
doi={10.1109/INFOCOM.2017.8057025}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057026, 
author={K. Matsuzono and H. Asaeda and T. Turletti}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Low latency low loss streaming using in-network coding and caching}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Owing to the rapid growth in high-quality video streaming over the Internet, preserving high-level robustness against data loss and low latency, while maintaining higher data transmission rates, is becoming an increasingly important issue for high-quality real-time delay-sensitive streaming. In this paper, we propose a low latency, low loss streaming mechanism, L4C2, specialized for high-quality delay-sensitive streaming. With L4C2, nodes in the network estimate the acceptable delay and packet loss probability in their uplinks, aiming at retrieving lost data packets from in-network cache and/or coded data packets using in-network coding within an acceptable delay, by extending the Content-Centric Networking (CCN) approach. Further, L4C2 naturally provides multiple paths and multicast technologies to efficiently utilize network resources while sharing network resources fairly with competing data flows by adjusting the video quality when necessary. We validate through comprehensive simulations that L4C2 achieves a high success probability of data transmission considering the acceptable one-way delay, and higher QoE while suppressing the interest and redundant data traffic than the proposed multipath congestion control mechanism in CCN.}, 
keywords={Internet;delays;network coding;probability;telecommunication congestion control;telecommunication traffic;video streaming;Content-Centric Networking approach;L4C2;acceptable delay;data flows;data loss;high success probability;high-level robustness;high-quality delay-sensitive streaming;high-quality real-time delay-sensitive streaming;high-quality video;higher data transmission rates;in-network cache;in-network coding;lost data packets;low latency low loss;low loss streaming mechanism;network estimate;network resources;redundant data traffic;video quality;Delays;Encoding;Quality assessment;Real-time systems;Robustness;Streaming media;Video recording}, 
doi={10.1109/INFOCOM.2017.8057026}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057027, 
author={A. Badr and A. Khisti and W. t. Tan and X. Zhu and J. Apostolopoulos}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={FEC for VoIP using dual-delay streaming codes}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={We introduce a new class of forward error correction (FEC) codes for VoIP communications which support different recovery delay depending on the channel conditions. Specifically, our proposed class of Dual-Delay (DD) codes can recover from challenging long bursts of losses with close to theoretical minimum delay so as to meet playback deadlines for recovered packets. They further improve conversational interactivity by achieving lower recovery delay during periods of random isolated losses. These DD codes are shown to achieve lower residual loss rates when compared to existing codes over a wide range of parameters of the Gilbert-Elliott channel. Experiments over real world packet traces further show performance gains of DD codes in terms of perceptually motivated ITU-T G.107 E-model.}, 
keywords={Internet telephony;delays;dual codes;forward error correction;media streaming;DD codes;Dual-Delay codes;FEC;Gilbert-Elliott channel;VoIP communications;channel conditions;conversational interactivity;dual-delay streaming codes;forward error correction codes;playback deadlines;random isolated losses;recovered packets;recovery delay;residual loss rates;Channel models;Conferences;Decoding;Delays;Forward error correction;Linear codes;Systematics;Application Layer Forward Error Correction (AL-FEC);Burst Erasures;E-model;Interactive Multimedia Applications;VoIP Audio Quality}, 
doi={10.1109/INFOCOM.2017.8057027}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057028, 
author={G. Tang and K. Wu and R. Brunner}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Rethinking CDN design with distributee time-varying traffic demands}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={The content delivery network (CDN) intensively uses cache to push the content close to end users. Over both traditional Internet architecture and emerging cloud-based framework, cache allocation has been the core problem that any CDN operator needs to address. As the first step for cache deployment, CDN operators need to discover or estimate the distribution of user requests in different geographic areas. This step results in a statistical spatial model for the user requests, which is used as the key input to solve the optimal cache deployment problem. More often than not, the temporal information in user requests is omitted to simplify the CDN design. In this paper, we disclose that the spatial request model alone may not lead to truly optimal cache deployment. By considering the temporal information in user requests, we provide a dynamic traffic based solution to this broadly studied problem. Via experiments over the North American ISPs Points of Presence (PoPs) network, our new solution outperforms traditional CDN design method and saves the overall delivery cost by 16% to 20%.}, 
keywords={Internet;cache storage;cloud computing;telecommunication traffic;CDN design method;CDN operator;North American ISP Point-of-Presence network;North American ISP-PoP network;content delivery network;distributee time-varying traffic demands;dynamic traffic based solution;optimal cache deployment problem;spatial request model;statistical spatial model;temporal information;Conferences;Internet;Network topology;Optimization;Servers;Topology}, 
doi={10.1109/INFOCOM.2017.8057028}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057029, 
author={S. Shukla and A. A. Abouzeid}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Proactive retention aware caching}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={We consider the problem of proactive (i.e. predictive) content caching that is aware of the costs of retention of the content in the cache. Prior work on caching (whether proactive or reactive) does not explicitly take into account the storage cost due to the duration of time for which a content is cached. This new problem, which we call retention aware caching, is motivated by two recent technological developments that are described in the paper: cloud storage rental costs and flash memory damage. We consider a hierarchical network consisting of a server connected to a number of cache-enabled nodes, located either at the edge of a network (e.g. base stations) or in the core of a data center. There are two types of network costs: storage cost at the caches and download cost from the server. We formulate the problem of proactive retention aware data caching (PRAC), which minimizes the total cost subject to the node capacity constraints. We first prove that PRAC is NP-Hard in general and then analyze PRAC for two cases: (1) linear storage cost, (2) convex storage cost. We show that PRAC admits efficient polynomial time algorithms when the storage cost is linear in retention times and caches have a large capacity. Furthermore, we derive bounds on the performance of PRAC for the case when the storage cost is a practically motivated convex function. Numerical evaluations demonstrate that PRAC outperforms other state-of-the-art caching policies for a wide range of parameters of interest.}, 
keywords={cache storage;cloud computing;computational complexity;convex programming;flash memories;linear programming;NP-hard problem;PRAC;cache-enabled nodes;caching policies;cloud storage rental costs;content caching;convex storage cost;download cost;flash memory damage;hierarchical network;linear storage cost;network costs;proactive retention aware caching;retention times;Cloud computing;Conferences;Delays;Hardware;Multicast communication;Servers}, 
doi={10.1109/INFOCOM.2017.8057029}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057030, 
author={L. Qiu and G. Cao}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Popularity-aware caching increases the capacity of wireless networks}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={In wireless ad hoc networks, due to the interference between concurrent transmissions, the per node capacity generally decreases with the increasing number of nodes in the network. Caching can help improve the network capacity, as it shortens the content transmission distance and reduces the communication interference. However, current researches on the capacity of wireless ad hoc networks with caching generally assume that content popularity follows uniform distribution. They ignore the fact that contents in reality have skewed popularity, which may lead to totally different capacity results. In this paper, we evaluate how the distribution of the content popularity affects the network capacity, and derive different capacity scaling laws based on the skewness of the content popularity. Our results suggest that for wireless networks with caching, when contents have skewed popularity, increasing the number of nodes monotonically increases the per node capacity.}, 
keywords={ad hoc networks;cache storage;capacity scaling laws;communication interference;concurrent transmissions;content popularity;content transmission distance;network capacity;node capacity;popularity-aware caching;wireless ad hoc networks;Conferences;Interference;Internet;Mobile ad hoc networks;Throughput;Wireless networks}, 
doi={10.1109/INFOCOM.2017.8057030}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057031, 
author={L. E. Chatzieleftheriou and M. Karaliopoulos and I. Koutsopoulos}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Caching-aware recommendations: Nudging user preferences towards better caching performance}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Caching decisions by default seek to maximize some notion of social welfare: the content to be cached is determined so that the maximum possible aggregate demand over all users served by the cache is satisfied. Recommendation systems, on the contrary, are oriented towards user individual preferences: the recommended content should be most appealing to the user so as to elicit further content consumption. In our paper we explore how these, phenomenically conflicting, objectives can be jointly addressed. To this end, we depart radically from current practice with recommender systems, and we approach them as network traffic engineering tools that can actively shape content demand towards optimizing user- and network-centric performance objectives. We formulate the resulting joint theoretical optimization problem of deciding on the cached content and the recommendations to each user so that the cache hit ratio is maximized subject to a maximum tolerable distortion that the recommendation should undergo. We conclude on its complexity, and we propose a practical algorithm for its solution. The algorithm is essentially a form of lightweight control over the user recommendations so that the recommended content is both appealing to the end user and more friendly to the caching system and the network resources.}, 
keywords={cache storage;optimisation;recommender systems;telecommunication traffic;user interfaces;cache hit ratio;cached content;caching decisions;caching performance;caching system;caching-aware recommendations;content consumption;content demand;end user;maximum possible aggregate demand;maximum tolerable distortion;network traffic engineering tools;network-centric performance objective optimization;recommendation systems;recommended content;recommender systems;resulting joint theoretical optimization problem;social welfare;user individual preferences;user preferences;user recommendations;user-centric performance objective optimization;Conferences;Distortion;Mobile computing;Recommender systems;Shape;Wireless networks}, 
doi={10.1109/INFOCOM.2017.8057031}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057032, 
author={M. Xiao and J. Wu and S. Zhang and J. Yu}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Secret-sharing-based secure user recruitment protocol for mobile crowdsensing}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Mobile crowdsensing is a new paradigm in which a requester can recruit a group of mobile users via a platform and coordinate them to perform some sensing tasks by using their smartphones. In mobile crowdsensing, each user might perform multiple tasks with different sensing qualities. An important problem is recruiting the minimum number of users while achieving a satisfactory sensing quality for each task. Meanwhile, in order to ease users' worries about privacy disclosures, the user recruitment process needs to protect each user's sensing quality information from being revealed to other users or to the platform. We prove that this problem is NP-hard. To solve this problem, we first propose a Basic User Recruitment (BUR) protocol based on a greedy strategy, which can recruit nearly the minimum amount of users while ensuring that the total sensing quality of each task is no less than a given threshold. Based on BUR, we further propose a Secure User Recruitment (SUR) protocol by using secret sharing schemes. We analyze the approximation ratio and prove the security of the SUR protocol in the semi-honest model. Moreover, we extend SUR to deal with a more general case where the total sensing quality of each task might be an increasing submodular function. Finally, we demonstrate the significant performance of the proposed protocol through extensive simulations and execution in real smartphones.}, 
keywords={computational complexity;cryptographic protocols;data privacy;greedy algorithms;mobile computing;smart phones;BUR protocol;Basic User Recruitment protocol;NP-hard problem;SUR protocol security;approximation ratio;greedy strategy;mobile crowdsensing;privacy disclosures;secret-sharing-based secure user recruitment protocol;semihonest model;sensing qualities;smartphones;submodular function;user sensing quality information;Cryptography;Mobile communication;Privacy;Protocols;Recruitment;Sensors;Mobile crowdsensing;privacy;secret sharing;sensing quality;user recruitment}, 
doi={10.1109/INFOCOM.2017.8057032}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057033, 
author={S. Liu and Z. Zheng and F. Wu and S. Tang and G. Chen}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Context-aware data quality estimation in mobile crowdsensing}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={With the rapid growth of smart devices, mobile crowdsensing is becoming an important paradigm to acquire information from physical environments. Considering that the sensing data collected by mobile users are normally noisy and imprecise, one of the pressing problems in mobile crowdsensing is to evaluate the data quality in real time and to steer users to acquire data with high quality. However, it is challenging to estimate the data quality without the availability of ground truth data. In this paper, we observe that sensing context has a significant impact on data quality, which motivates us to propose a context-aware data quality estimation scheme. With historical sensing data, we train a context-quality classifier, which captures the relation between context information and data quality, to estimate data quality in an online manner. We apply such a context-aware data quality estimation scheme to guide user recruitment in mobile crowdsensing. We model the process of user recruitment as a stochastic submodular maximization problem, and design a random adaptive greedy algorithm to guarantee a constant approximation ratio. We evaluate our algorithm on a real-world temperature data set. The evaluation results show that our algorithm outperforms other existing techniques, in terms of prediction accuracy.}, 
keywords={approximation theory;information retrieval;mobile computing;optimisation;approximation ratio;context-aware data quality estimation scheme;context-quality classifier;ground truth data;historical sensing data;mobile crowdsensing;real-world temperature data;stochastic submodular maximization problem;user recruitment;Estimation;Legged locomotion;Mobile communication;Noise measurement;Real-time systems;Recruitment;Sensors}, 
doi={10.1109/INFOCOM.2017.8057033}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057034, 
author={Q. Xu and R. Zheng}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={When data acquisition meets data analytics: A distributed active learning framework for optimal budgeted mobile crowdsensing}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={An important category of mobile crowdsensing applications involve collecting sensor measurements from mobile devices and querying mobile users for annotations to build machine learning models for inference and prediction. Trade-offs between inference performance and the costs of data acquisition (both unlabeled and labeled) are not yet well understood. In this paper, we develop, ALSense, a distributed active learning framework for mobile crowdsensing. The goal is to minimize prediction errors for classification-based mobile crowdsensing tasks subject to upload and query cost constraints. Novel stream-based active learning strategies are developed to orchestrate queries of annotation data and the upload of unlabeled data from mobile devices. We evaluate the effectiveness of ALSense through two applications that can benefit from mobile crowdsensing, namely, WiFi fingerprint-based indoor localization and IMU-based human activity recognition. Extensive experiments demonstrate that ALSense can indeed achieve higher classification accuracy given fixed data acquisition budgets for both applications.}, 
keywords={data acquisition;data analysis;learning (artificial intelligence);mobile computing;pattern classification;wireless LAN;ALSense framework;IMU-based human activity recognition;WiFi fingerprint-based indoor localization;active learning strategies;annotation data;data acquisition budgets;data analytics;distributed active learning framework;machine learning models;mobile crowdsensing applications;mobile crowdsensing tasks;mobile devices;optimal budgeted mobile crowdsensing;prediction errors;query cost constraints;querying mobile users;sensor measurements;unlabeled data;Data acquisition;Mobile communication;Mobile handsets;Predictive models;Sensors;Servers}, 
doi={10.1109/INFOCOM.2017.8057034}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057035, 
author={M. H. Cheung and F. Hou and J. Huang}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Make a difference: Diversity-driven social mobile crowdsensing}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={In a mobile crowdsensing (MCS) application, user diversity and social effect are two important phenomena that determine its profitability, where the former improves the sensing quality, while the latter incentives the users' participation. In this paper, we consider a reward mechanism design for the service provider to achieve diversity in the collected data by exploiting the users' social relationship. Specifically, we formulate a two-stage decision problem, where the service provider first optimizes its rewards for profit maximization. The users then decide their effort levels through social network interactions as a participation game. The analysis is particularly challenging due to the users' interplay in both the diversity and social graphs, which leads to a non-convex bilevel optimization problem. Surprisingly, we find that the service provider can focus on one superimposed graph that incorporates the diversity and social relationship and compute the optimal reward as the Katz centrality in closed-form. Simulation results, based on the random graph and a real Facebook trace, show that the availability of network information improves both the service provider's profit and the users' social surplus over the incomplete information cases.}, 
keywords={convex programming;game theory;graph theory;mobile computing;profitability;social networking (online);diversity-driven social mobile crowdsensing;mobile crowdsensing application;network information availability;nonconvex bilevel optimization problem;optimal reward;participation game;profit maximization;profitability;reward mechanism design;sensing quality;service provider;social effect;social graphs;social network interactions;social relationship;two-stage decision problem;user diversity;Cultural differences;Facebook;Games;Mobile communication;Sensors}, 
doi={10.1109/INFOCOM.2017.8057035}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057036, 
author={Y. Sang and B. Ji and G. R. Gupta and X. Du and L. Ye}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Provably efficient algorithms for joint placement and allocation of virtual network functions}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Network Function Virtualization (NFV) has the potential to significantly reduce the capital and operating expenses, shorten product release cycle, and improve service agility. In this paper, we focus on minimizing the total number of Virtual Network Function (VNF) instances to provide a specific service (possibly at different locations) to all the flows in a network. Certain network security and analytics applications may allow fractional processing of a flow at different nodes (corresponding to datacenters), giving an opportunity for greater optimization of resources. Through a reduction from the set cover problem, we show that this problem is NP-hard and cannot even be approximated within a factor of (1 - o(1))lnm (where m is the number of flows) unless P=NP. Then, we design two simple greedy algorithms and prove that they achieve an approximation ratio of (1 - o(1))ln m + 2, which is asymptotically optimal. For special cases where each node hosts multiple VNF instances (which is typically true in practice), we also show that our greedy algorithms have a constant approximation ratio. Further, for tree topologies we develop an optimal greedy algorithm by exploiting the inherent topological structure. Finally, we conduct extensive numerical experiments to evaluate the performance of our proposed algorithms in various scenarios.}, 
keywords={computational complexity;computer network security;greedy algorithms;minimisation;open systems;optimisation;telecommunication network topology;trees (mathematics);virtualisation;NFV;NP-hard problem;Network Function Virtualization;VNF instance minimization;Virtual Network Function instances;analytics applications;constant approximation ratio;fractional processing;inherent topological structure;joint placement;network security;optimal greedy algorithm;product release cycle;provably efficient algorithms;resource optimization;service agility;simple greedy algorithms;total number;tree topologies;virtual network functions allocation;Algorithm design and analysis;Conferences;Greedy algorithms;Heuristic algorithms;Network topology;Resource management;Topology}, 
doi={10.1109/INFOCOM.2017.8057036}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057037, 
author={R. Han and F. Zhang and Z. Wang}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={AccurateML: Information-aggregation-based approximate processing for fast and accurate machine learning on MapReduce}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={The growing demands of processing massive datasets have promoted irresistible trends of running machine learning applications on MapReduce. When processing large input data, it is often of greater values to produce fast and accurate enough approximate results than slow exact results. Existing techniques produce approximate results by processing parts of the input data, thus incurring large accuracy losses when using short job execution times, because all the skipped input data potentially contributes to result accuracy. We address this limitation by proposing AccurateML that aggregates information of input data in each map task to create small aggregated data points. These aggregated points enable all map tasks producing initial outputs quickly to save computation times and decrease the outputs' size to reduce communication times. Our approach further identifies the parts of input data most related to result accuracy, thus first using these parts to improve the produced outputs to minimize accuracy losses. We evaluated AccurateML using real machine learning applications and datasets. The results show: (i) it reduces execution times by 30 times with small accuracy losses compared to exact results; (ii) when using the same execution times, it achieves 2.71 times reductions in accuracy losses compared to existing approximate processing techniques.}, 
keywords={approximation theory;data analysis;learning (artificial intelligence);AccurateML;MapReduce;accuracy loss minimization;aggregated data points;approximate processing techniques;information aggregation;information-aggregation-based approximate processing;machine learning applications;map task;short job execution times;Aggregates;Clustering algorithms;Conferences;Correlation;Indexes;Sparks;MapReduce;approximate processing;information aggregation;machine learning;result accuracy}, 
doi={10.1109/INFOCOM.2017.8057037}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057038, 
author={J. F. Pérez and R. Birke and L. Y. Chen}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={On the latency-accuracy tradeoff in approximate MapReduce jobs}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={To ensure the scalability of big data analytics, approximate MapReduce platforms emerge to explicitly trade off accuracy for latency. A key step to determine optimal approximation levels is to capture the latency of big data jobs, which is long deemed challenging due to the complex dependency among data inputs and map/reduce tasks. In this paper, we use matrix analytic methods to derive stochastic models that can predict a wide spectrum of latency metrics, e.g., average, tails, and distributions, for approximate MapReduce jobs that are subject to strategies of input sampling and task dropping. In addition to capturing the dependency among waves of map/reduce tasks, our models incorporate two job scheduling policies, namely, exclusive and overlapping, and two task dropping strategies, namely, early and straggler, enabling us to realistically evaluate the potential performance gains of approximate computing. Our numerical analysis shows that the proposed models can guide big data platforms to determine the optimal approximation strategies and degrees of approximation.}, 
keywords={Big Data;approximation theory;data analysis;matrix algebra;parallel processing;scheduling;stochastic processes;Big Data analytics scalability;Big Data jobs latency;approximate MapReduce jobs;approximate MapReduce platforms;approximate computing;complex data dependency;data inputs;early task dropping strategy;input sampling;job scheduling policies;latency metrics spectrum;latency-accuracy tradeoff;matrix analytic methods;numerical analysis;optimal approximation strategies;performance gain evaluation;stochastic models;stragger task dropping strategy;task dropping strategies;Analytical models;Big Data;Computational modeling;Conferences;Measurement;Time factors}, 
doi={10.1109/INFOCOM.2017.8057038}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057039, 
author={H. Feng and J. Llorca and A. M. Tulino and D. Raz and A. F. Molisch}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Approximation algorithms for the NFV service distribution problem}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Distributed cloud networking builds on network functions virtualization (NFV) and software defined networking (SDN) to enable the deployment of network services in the form of elastic virtual network functions (VNFs) instantiated over general purpose servers at distributed cloud locations. We address the design of fast approximation algorithms for the NFV service distribution problem (NSDP), whose goal is to determine the placement of VNFs, the routing of service flows, and the associated allocation of cloud and network resources that satisfy client demands with minimum cost. We show that in the case of load-proportional costs, the resulting fractional NSDP can be formulated as a multi-commodity-chain flow problem on a cloud-augmented graph, and design a queue-length based algorithm, named QNSD, that provides an O(ε) approximation in time O (1/ε). We then address the case in which resource costs are a function of the integer number of allocated resources and design a variation of QNSD that effectively pushes for flow consolidation into a limited number of active resources to minimize overall cloud network cost.}, 
keywords={approximation theory;cloud computing;computational complexity;graph theory;resource allocation;software defined networking;virtualisation;NFV service distribution problem;O(ε) approximation;VNF;cloud network cost;cloud-augmented graph;distributed cloud locations;distributed cloud networking;elastic virtual network functions;fast approximation algorithms;general purpose servers;multicommodity-chain flow problem;network resources;network services;queue-length based algorithm;service flows;software defined networking;Algorithm design and analysis;Approximation algorithms;Cloud computing;Network function virtualization;Resource management;Routing;Servers}, 
doi={10.1109/INFOCOM.2017.8057039}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057040, 
author={J. Tapolcai and L. Rónyai and B. Vass and L. Gyimóthi}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={List of shared risk link groups representing regional failures with limited size}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Shared Risk Link Group (SRLG) is a failure the network is prepared for, which contains a set of links subject to a common risk of single failure. During planning a backbone network, the list of SRLGs must be defined very carefully, because leaving out one likely failure event will significantly degrade the observed reliability of the network. Regional failures are manifested at multiple locations of the network, which are physically close to each other. In this paper we show that operators should prepare a network for only a small number of possible regional failure events. In particular, we give a fast systematic approach to generate the list of SRLGs that cover every possible circular disk failure of a given radius r. We show that this list has O((n + x)σr) SRLGs, where n is the number of nodes in the network, x is the number of link crossings, and σr is the maximal number of links that could be hit by a disk failure of radius r. Finally through extensive simulations we show that this list in practice has size of ≈ 1.2 n.}, 
keywords={optical fibre networks;telecommunication network planning;telecommunication network reliability;telecommunication network routing;SRLG;circular disk failure;failure event;link crossings;optical backbone network;regional failures;shared risk link groups;single failure;Computer network reliability;Conferences;Earthquakes;Hurricanes;Reliability;Topology;Tornadoes}, 
doi={10.1109/INFOCOM.2017.8057040}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057041, 
author={Y. Zhang and W. Wu and S. Banerjee and J. M. Kang and M. A. Sanchez}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={SLA-verifier: Stateful and quantitative verification for service chaining}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Network verification has been recently proposed to detect network misconfigurations. Existing work focuses on the reachability. This paper proposes a framework that verifies the Service Level Agreement (SLA) compliance of the network using static verification. This work proposes a quantitative model and a set of algorithms for verifying performance properties of a network with switches and middleboxes, i.e., service chains. We develop SLA-Verifier and evaluate its efficiency using simulation on real-world data and testbed experiments. To improve the SLA violation detection accuracy, our system uses verification results to optimize online monitoring.}, 
keywords={contracts;formal verification;quality of service;telecommunication network management;telecommunication services;SLA violation detection accuracy;SLA-Verifier;SLA-verifier;Service Level Agreement compliance;Stateful verification;network misconfigurations;network verification;quantitative model;quantitative verification;service chaining;static verification;Bandwidth;Load modeling;Middleboxes;Monitoring;Noise measurement;Quality of service}, 
doi={10.1109/INFOCOM.2017.8057041}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057042, 
author={S. Ciavarella and N. Bartolini and H. Khamfroush and T. L. Porta}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Progressive damage assessment and network recovery after massive failures}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={After a massive scale failure, the assessment of damages to communication networks requires local interventions and remote monitoring. While previous works on network recovery require complete knowledge of damage extent, we address the problem of damage assessment and critical service restoration in a joint manner. We propose a polynomial algorithm called Centrality based Damage Assessment and Recovery (CeDAR) which performs a joint activity of failure monitoring and restoration of network components. CeDAR works under limited availability of recovery resources and optimizes service recovery over time. We modified two existing approaches to the problem of network recovery to make them also able to exploit incremental knowledge of the failure extent. Through simulations we show that CeDAR outperforms the previous approaches in terms of recovery resource utilization and accumulative flow over time of the critical services.}, 
keywords={condition monitoring;failure analysis;polynomials;telecommunication network management;telecommunication network reliability;CeDAR;Centrality based Damage Assessment;communication networks;critical service restoration;failure monitoring;massive scale failure;network recovery;polynomial algorithm;progressive damage assessment;recovery resource utilization;remote monitoring;Communication networks;Conferences;Inspection;Knowledge engineering;Maintenance engineering;Monitoring;Schedules}, 
doi={10.1109/INFOCOM.2017.8057042}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057043, 
author={Y. Bejerano and C. Raman and C. N. Yu and V. Gupta and C. Gutterman and T. Young and H. Infante and Y. Abdelmalek and G. Zussman}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={DyMo: Dynamic monitoring of large scale LTE-Multicast systems}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={LTE evolved Multimedia Broadcast/Multicast Service (eMBMS) is an attractive solution for video delivery to very large groups in crowded venues. However, deployment and management of eMBMS systems is challenging, due to the lack of realtime feedback from the User Equipment (UEs). Therefore, we present the Dynamic Monitoring (DyMo) system for low-overhead feedback collection. DyMo leverages eMBMS for broadcasting Stochastic Group Instructions to all UEs. These instructions indicate the reporting rates as a function of the observed Quality of Service (QoS). This simple feedback mechanism collects very limited QoS reports from the UEs. The reports are used for network optimization, thereby ensuring high QoS to the UEs. We present the design aspects of DyMo and evaluate its performance analytically and via extensive simulations. Specifically, we show that DyMo infers the optimal eMBMS settings with extremely low overhead, while meeting strict QoS requirements under different UE mobility patterns and presence of network component failures. For instance, DyMo can detect the eMBMS Signal-to-Noise Ratio (SNR) experienced by the 0.1% percentile of the UEs with Root Mean Square Error (RMSE) of 0.05% with only 5 to 10 reports per second regardless of the number of UEs.}, 
keywords={Long Term Evolution;broadcast communication;feedback;mean square error methods;multicast communication;multimedia communication;quality of service;DyMo leverages eMBMS;Dynamic Monitoring system;LTE evolved multimedia broadcast-multicast service;QoS;RMSE;Stochastic Group Instructions;UE mobility patterns;eMBMS systems;extremely low overhead;large scale LTE-multicast systems;low-overhead feedback collection;network optimization;optimal eMBMS settings;realtime feedback;reporting rates;root mean square error;simple feedback mechanism;strict QoS requirements;video delivery;Estimation;Long Term Evolution;Quality of service;Signal to noise ratio;Sociology;Statistics;Wireless fidelity;Feedback Mechanism;LTE;Multi-cast;Wireless Monitoring;eMBMS}, 
doi={10.1109/INFOCOM.2017.8057043}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057044, 
author={M. Zhang and L. Gao and J. Huang and M. Honig}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Cooperative and competitive operator pricing for mobile crowdsourced internet access}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Mobile Crowdsourced Access (MCA) enables mobile users (MUs) to share their Internet connections by serving as tethers to other MUs, hence can improve the quality of service of MUs as well as the overall utilization of network resources. However, MCA can also reduce the revenue-generating mobile traffic and increase the network congestion for mobile network operators (MNOs), and thus has been blocked by some MNOs in practice. In this work, we reconcile the conflicting objectives of MNOs and MUs by introducing a pricing framework for MCA, where the direct traffic and tethering traffic are charged independently according to a data price and a tethering price, respectively. We derive the optimal data and tethering prices systematically for MUs with the α-fair utility in two scenarios with cooperative and competitive MNOs, respectively. We show that the optimal tethering prices are zero and the optimal usage-based data prices are identical for all MUs, in both the cooperative and competitive scenarios. Such optimal pricing schemes will lead to mutually beneficial results for MNOs and MUs. Our simulation results show that the proposed pricing scheme approximately triples both the MNOs' profit and the MUs' payoff when the MNOs cooperate, comparing to the case where MCA is blocked. Moreover, competition among MNOs will decrease MNOs' profit and further increase the MUs' payoff.}, 
keywords={Internet;cooperative communication;mobile communication;mobile computing;optimisation;pricing;telecommunication services;telecommunication traffic;MCA;MNO profit;MU payoff;competitive MNO;competitive operator pricing;cooperative operator pricing;data price;direct traffic;mobile crowdsourced internet access;mobile network operators;mobile users;network resources;optimal pricing schemes;optimal tethering prices;pricing framework;revenue-generating mobile traffic;tethering price;tethering traffic;Conferences;Downlink;Internet;Mobile communication;Mobile computing;Pricing;Wireless fidelity}, 
doi={10.1109/INFOCOM.2017.8057044}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057045, 
author={D. Bega and M. Gramaglia and A. Banchs and V. Sciancalepore and K. Samdanis and X. Costa-Perez}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Optimising 5G infrastructure markets: The business of network slicing}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={In addition to providing substantial performance enhancements, future 5G networks will also change the mobile network ecosystem. Building on the network slicing concept, 5G allows to “slice” the network infrastructure into separate logical networks that may be operated independently and targeted at specific services. This opens the market to new players: the infrastructure provider, which is the owner of the infrastructure, and the tenants, which may acquire a network slice from the infrastructure provider to deliver a specific service to their customers. In this new context, we need new algorithms for the allocation of network resources that consider these new players. In this paper, we address this issue by designing an algorithm for the admission and allocation of network slices requests that (i) maximises the infrastructure provider's revenue and (ii) ensures that the service guarantees provided to tenants are satisfied. Our key contributions include: (i) an analytical model for the admissibility region of a network slicing-capable 5G Network, (ii) the analysis of the system (modelled as a Semi-Markov Decision Process) and the optimisation of the infrastructure provider's revenue, and (iii) the design of an adaptive algorithm (based on Q-learning) that achieves close to optimal performance.}, 
keywords={5G mobile communication;Markov processes;mobile computing;mobility management (mobile radio);5G infrastructure market optimisation;Q-learning;future 5G networks;infrastructure provider revenue optimisation;logical networks;mobile network ecosystem;network infrastructure;network resources;network slice requests;network slicing;semi-Markov decision process;substantial performance enhancements;5G mobile communication;Adaptation models;Algorithm design and analysis;Base stations;Ecosystems;Mobile computing;Throughput}, 
doi={10.1109/INFOCOM.2017.8057045}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057046, 
author={P. Caballero and A. Banchs and G. de Veciana and X. Costa-Pérez}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Network slicing games: Enabling customization in multi-tenant networks}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Network slicing to enable resource sharing among multiple tenants-network operators and/or services-is considered a key functionality for next generation mobile networks. This paper provides an analysis of a well-known model for resource sharing, the `share-constrained proportional allocation' mechanism, to realize network slicing. This mechanism enables tenants to reap the performance benefits of sharing, while retaining the ability to customize their own users' allocation. This results in a network slicing game in which each tenant reacts to the user allocations of the other tenants so as to maximize its own utility. We show that, under appropriate conditions, the game associated with such strategic behavior converges to a Nash equilibrium. At the Nash equilibrium, a tenant always achieves the same, or better, performance than under a static partitioning of resources, hence providing the same level of protection as such static partitioning. We further analyze the efficiency and fairness of the resulting allocations, providing tight bounds for the price of anarchy and envy-freeness. Our analysis and extensive simulation results confirm that the mechanism provides a comprehensive practical solution to realize network slicing. Our theoretical results also fill a gap in the literature regarding the analysis of this resource allocation model under strategic players.}, 
keywords={game theory;mobile computing;next generation networks;program slicing;resource allocation;Nash equilibrium;multi-tenant networks;multiple tenants-network operators;network slicing game;next generation mobile networks;resource allocation model;resource sharing;share-constrained proportional allocation mechanism;static partitioning;user allocations;Analytical models;Base stations;Conferences;Mobile communication;Mobile computing;Nash equilibrium;Resource management}, 
doi={10.1109/INFOCOM.2017.8057046}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057047, 
author={M. Zou and R. T. B. Ma and X. Wang and Y. Xu}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={On optimal service differentiation in congested network markets}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={As Internet applications have become more diverse in recent years, users having heavy demand for online video services are more willing to pay higher prices for better services than light users that mainly use e-mails and instant messages. This encourages the Internet Service Providers (ISPs) to explore service differentiations so as to optimize their profits and allocation of network resources. Much prior work has focused on the viability of network service differentiation by comparing with the case of a single-class service. However, the optimal service differentiation for an ISP subject to resource constraints has remained unsolved. In this work, we establish an optimal control framework to derive the analytical solution to an ISP's optimal service differentiation, i.e., the optimal service qualities and associated prices. By analyzing the structures of the solution, we reveal how an ISP should adjust the service qualities and prices in order to meet varying capacity constraints and users' characteristics. We also obtain the conditions under which ISPs have strong incentives to implement service differentiation and whether regulators should encourage such practices.}, 
keywords={Internet;optimal control;pricing;quality of service;ISP's optimal service differentiation;Internet Service Providers;Internet applications;congested network markets;e-mails;instant messages;network resource allocation;network resources;network service differentiation;online video services;optimal control framework;optimal service differentiation;optimal service qualities;single-class service;Capacity planning;Conferences;Optimal control;Pricing;Quality of service;Regulators;Resource management}, 
doi={10.1109/INFOCOM.2017.8057047}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057048, 
author={P. Nayak and M. Garetto and E. W. Knightly}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Multi-user downlink with single-user uplink can starve TCP}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={In this paper we present the first cross-layer analysis of wireless LANs operating under downlink multi-user MIMO (MU-MIMO), considering the fundamental role played by closed-loop (TCP) traffic. In particular, we consider an 802.11ac scenario in which the access point transmits on the downlink via MU-MIMO, whereas stations must employ single-user transmissions on the uplink. With the help of analytical models built for the different regimes that can occur in the considered system, we identify and explain crucial performance anomalies that can result in very low throughput in some scenarios, completely offsetting the theoretical gains achievable by MU-MIMO. We discuss solutions to mitigate the risk of this performance degradation and alternative uplink strategies allowing WLANs to approach their maximum theoretical capacity under MU-MIMO.},
keywords={MIMO communication;multiuser detection;telecommunication traffic;transport protocols;wireless LAN;MU-MIMO;closed-loop TCP traffic;multiuser MIMO;multiuser downlink;single-user uplink;wireless LAN;Downlink;MIMO;Servers;Standards;Throughput;Uplink;Wireless LAN}, 
doi={10.1109/INFOCOM.2017.8057048}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057049, 
author={D. Gunatilaka and M. Sha and C. Lu}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Impacts of channel selection on industrial wireless sensor-actuator networks}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Industrial automation has emerged as an important application of wireless sensor-actuator networks (WSANs). To meet stringent reliability requirements of industrial applications, industrial standards such as WirelessHART adopt Time Slotted Channel Hopping (TSCH) as its MAC protocol. Since every link hops through all the channels used in TSCH, a straightforward policy to ensure reliability is to retain a link in the network topology only if it is reliable in all channels used. However, this policy has surprising side effects. While using more channels may enhance reliability due to channel diversity, more channels may also reduce the number of links and route diversity in the network topology. We empirically analyze the impact of channel selection on network topology, routing, and scheduling on a 52-node WSAN testbed. We observe inherent tradeoff between channel diversity and route diversity in channel selection, where using an excessive number of channels may negatively impact routing and scheduling. We propose novel channel and link selection strategies to improve route diversity and network schedulability. Experimental results on two different testbeds show that our algorithms can drastically improve routing and scheduling of industrial WSANs.}, 
keywords={access protocols;diversity reception;routing protocols;telecommunication network topology;telecommunication scheduling;wireless sensor networks;MAC protocol;TSCH;Time Slotted Channel Hopping;WirelessHART;channel diversity;channel selection;industrial WSANs;industrial automation;industrial standards;industrial wireless sensor-actuator networks;network schedulability;network topology;route diversity;IEEE 802.15 Standard;Network topology;Reliability;Routing;Wireless communication;Wireless sensor networks}, 
doi={10.1109/INFOCOM.2017.8057049}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057050, 
author={A. Baiocchi and I. Tinnirello and D. Garlisi and A. L. Valvo}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Random access with repeated contentions for emerging wireless technologies}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={In this paper we propose ReCo, a robust contention scheme for emerging wireless technologies, whose efficiency is not sensitive to the number of contending stations and to the settings of the contention parameters (such as the contention windows and retry limits). The idea is iterating a basic contention mechanism, devised to select a sub-set of stations among the contending ones, in consecutive elimination rounds, before performing a transmission attempt. Elimination rounds can be performed in the time or frequency domain, with different overheads, according to the physical capabilities of the nodes. Closed analytical formulas are given to dimension the number of contention rounds in order to achieve an arbitrary low collision probability. Simulation results and a real implementation for the time-domain solution demonstrate the effectiveness and robustness of this approach in comparison to IEEE 802.11 DCF.}, 
keywords={access protocols;probability;wireless LAN;ReCo;arbitrary low collision probability;closed analytical formulas;contention parameters;contention rounds;contention windows;frequency domain;random access;retry limits;robust contention scheme;time-domain solution;wireless technologies;Frequency-domain analysis;IEEE 802.11 Standard;Multiaccess communication;OFDM;Protocols;Wireless communication}, 
doi={10.1109/INFOCOM.2017.8057050}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057051, 
author={C. F. Shih and R. Sivakumar}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Switch: Enabling transmitter and receiver participation in seamless lightweight control}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Lightweight control planes are techniques that create a control plane in WiFi networks without any additional spectrum requirements. Flash signals are an example of such control signals that exploit the link margin that typically exists in WiFi communication. In this paper we consider the problem of allowing transmitters and receivers of a transmission to exploit such control channels while the communication is ongoing. We present a mechanism called switch that facilitates switches in communication modes (Tx to Rx and vice-versa). We then use switch as the core building block to solve problems in WiFi networks such as starvation due to hidden terminals, early collision termination, and frequency backoffs. We rely on WARP radios to experimentally verify that switch is indeed possible, and use ns-3 simulations to study the impact of using switch to solve the aforementioned WiFi problems.}, 
keywords={access protocols;radio receivers;radio transmitters;telecommunication control;telecommunication switching;telecommunication traffic;wireless LAN;wireless channels;WiFi communication;WiFi networks;collision termination;communication modes;control channels;control signals;flash signals;link margin;receiver participation;seamless lightweight control;transmitter participation;Delays;Radio transmitters;Receivers;Switches;Wireless fidelity}, 
doi={10.1109/INFOCOM.2017.8057051}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057052, 
author={W. Cheng and X. Zhang and H. Zhang}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Pilot-based full-duplex spectrum-sensing and multichannel-MAC over non-time-slotted cognitive radio networks}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={In the non-time-slotted cognitive radio networks (CRNs), the synchronization between PUs and secondary users (SUs) cannot be guaranteed, resulting in two challenging problems: the reactivation-failure of PUs and the frequently unexpected hand-offs among SUs. The reactivation-failure of PUs is the incident that the SUs cannot detect the PUs' reactivation when the SUs are occupying the channels to transmit their data in non-time-slotted CRNs. The frequently unexpected handoffs among SUs are the events that the SU cannot distinguish between the PUs' reactivation and the other SUs' contention, thus causing many unexpected hand-offs among SUs, which severely degrade the achieved throughput of SUs in non-time-slotted CRNs. Employing the energy-detection based wireless full-duplex spectrum sensing schemes, the PUs' reactivation-failure problem can be efficiently solved, thus guaranteeing the required throughput of PUs. However, the key of CRNs is not only the throughput-guarantees for PUs, but also the throughput-boosts for SUs. To optimize the throughput of SUs in multichannel non-time-slotted CRNs, in this paper we develop the pilot-based full-duplex spectrum sensing (PF-SS) scheme and the pilot-based medium access control (P-MAC) protocol to not only guarantee the required throughput of PUs, but also significantly increase the throughput of SUs in multichannel non-time-slotted CRNs. Using the PF-SS scheme, the SUs can identify whether the PUs' signal or the SUs' signal, thus significantly reducing the frequently unexpected hand-offs among SUs. Then, based on the PF-SS scheme, the P-MAC protocol can significantly increase the throughput of SUs. We conduct extensive numerical analyses to show that our developed PF-SS scheme and P-MAC protocol can significantly increase the throughput of SUs while guaranteeing the required throughput for PUs in multichannel non-time-slotted CRNs.}, 
keywords={access protocols;cognitive radio;mobility management (mobile radio);radio networks;radio spectrum management;signal detection;wireless channels;CRNs;P-MAC protocol;PUs reactivation;SUs;frequently unexpected hand-offs;frequently unexpected handoffs;full-duplex spectrum-sensing;multichannel-MAC;nontime-slotted cognitive radio networks;pilot-based medium access control protocol;primary user;reactivation-failure;secondary users;Cognitive radio;Protocols;Sensors;Throughput;Wireless networks;Wireless sensor networks;Non-time-slotted cognitive radio networks;hand-offs among secondary users;pilot-based full-duplex spectrum sensing;pilot-based medium access control protocol}, 
doi={10.1109/INFOCOM.2017.8057052}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057053, 
author={X. Liu and J. Xie}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={A 2D heterogeneous rendezvous protocol for multi-wideband cognitive radio networks}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Ideally, users in cognitive radio networks (CRNs) are capable of sensing and exploiting any potential transmission opportunities in the available spectrum band ranging from 30 KHz to 300 GHz. With the multiple-diverse-band spectrum, the network can provide more radio resources and capacity to a large number of CR users. However, the multiband scenario (e.g., TV band + 2/3G band + 4/5G band) also introduces significant challenges in channel rendezvous, a fundamental operation for users in CRNs to set up their communication link on a common channel. Existing studies on channel rendezvous suffer from unacceptable long delay and high energy consumption when applied to such scenarios. In this paper, we propose a two-dimensional heterogeneous rendezvous (2D-HR) protocol which can support multi-wideband CRNs (MWB-CRNs) with a significantly reduced rendezvous delay and energy consumption for various rendezvous scenarios, such as the pair-wise rendezvous, any-wise rendezvous, and multi-wise rendezvous. The proposed design also performs better than existing efforts even when dealing with traditional single-band rendezvous. The merits of 2D-HR are proved theoretically and validated against extensive simulations. To the best of our knowledge, this is the first work that addresses heterogeneous rendezvous in MWB-CRNs.}, 
keywords={cognitive radio;protocols;radio links;radio spectrum management;wireless channels;2D-HR protocol;CR users;MWB-CRN;TV band;any-wise rendezvous;channel rendezvous;energy consumption;frequency 30.0 kHz to 300.0 GHz;high energy consumption;multiband scenario;multiple-diverse-band spectrum;multiwideband CRN;multiwideband cognitive radio networks;multiwise rendezvous;pair-wise rendezvous;radio resources;reduced rendezvous delay;two-dimensional heterogeneous rendezvous protocol;Algorithm design and analysis;Cognitive radio;Conferences;Delays;Energy consumption;Protocols;Wideband}, 
doi={10.1109/INFOCOM.2017.8057053}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057054, 
author={C. Joo and N. B. Shroff}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={A novel coupled queueing model to control traffic via QoS-aware collision pricing in cognitive radio networks}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={We consider a cognitive radio network, where primary users have priority over the spectrum resources, and secondary users can exploit the unused resources through channel sensing. Due to sensing inaccuracy, the secondary traffic may obstruct the primary traffic. A penalty for collision has been used to protect the primary traffic, which is often designed to provide a fixed per-collision compensation or to restrict the collision rate at an acceptable level. In this work, we develop a framework that can protect the primary traffic taking into account the Quality of Service of the primary traffic. In particular, we pay attention to the delay performance, which is determined not only by the collision rate but also by the amount of traffic in both networks. We design a novel model with coupled queues, and successfully incorporate dynamic interactions between the two systems through the standard optimization problem. We also consider the practical requirement of no direct sharing of the system information between the two networks, and develop a close-to-optimal solution of per-collision price and channel sensing under mild assumptions. We evaluate its performance through simulations.}, 
keywords={cognitive radio;multi-access systems;optimisation;quality of service;queueing theory;telecommunication congestion control;QoS-aware collision pricing;channel sensing;cognitive radio network;control traffic;coupled queueing model;per-collision compensation;per-collision price;primary traffic;primary users;quality of service;secondary traffic;secondary users;spectrum resources;Cognitive radio;Interference;Load management;Quality of service;Sensors;Signal to noise ratio;Throughput}, 
doi={10.1109/INFOCOM.2017.8057054}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057055, 
author={A. Abdelfattah and N. Malouch}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Modeling and performance analysis of Wi-Fi networks coexisting with LTE-U}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={In order to cope with the exponential growth of mobile traffic, mobile operators need to access more spectrum resources. LTE in unlicensed spectrum (LTE-U) has been proposed to extend the usual operation of LTE in licensed spectrum to cover also unlicensed spectrum. However, this extension poses significant challenges especially regarding the coexistence between LTE-U and legacy systems like Wi-Fi. In case of LTE-U adopts Time-Division Multiplexing (TDM) schemes to share the spectrum with Wi-Fi, we expect performance degradations of Wi-Fi networks. In this paper, we quantify the impact of TDM schemes on Wi-Fi performance in a coexistence scenario. We provide detailed analytical models using two different random walk approaches to compute the probability of collision faced by Wi-Fi stations and their throughput performance. Besides, we derive the performance results using an exponential approximation which shows its insufficiency to capture the exact behavior. We implement the coexistence in the NS3 simulator and we show that the models estimate accurately the collision probability and the throughput experienced by Wi-Fi. The models are then used to study and compare different coexistence schemes showing for instance that the Wi-Fi frame size has a non-negligible impact on the performance of Wi-Fi users.}, 
keywords={Long Term Evolution;probability;time division multiplexing;wireless LAN;LTE-U;NS3;TDM schemes;Time-Division Multiplexing schemes;U;Wi-Fi frame size;Wi-Fi networks;Wi-Fi performance;Wi-Fi stations;Wi-Fi users;licensed spectrum;mobile operators;mobile traffic;performance degradations;spectrum resources;unlicensed spectrum;Analytical models;Computational modeling;Long Term Evolution;Markov processes;Media Access Protocol;Throughput;Wireless fidelity;5G;CSAT;Collision Probability;LTE-U;Mobile Communication;Performance Evaluation;Simulation;Wi-Fi}, 
doi={10.1109/INFOCOM.2017.8057055}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057056, 
author={Y. Qin and R. Jin and S. Hao and K. R. Pattipati and F. Qian and S. Sen and B. Wang and C. Yue}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={A control theoretic approach to ABR video streaming: A fresh look at PID-based rate adaptation}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Adaptive bitrate streaming (ABR) has become the de facto technique for video streaming over the Internet. Despite a flurry of techniques, achieving high quality ABR streaming over cellular networks remains a tremendous challenge. ABR streaming can be naturally modeled as a feedback control problem. There has been some initial work on using PID, a widely used feedback control technique, for ABR streaming. Existing studies, however, either use PID control directly without fully considering the special requirements of ABR streaming, leading to suboptimal results, or conclude that PID is not a suitable approach. In this paper, we take a fresh look at PID-based control for ABR streaming. We design a framework called PIA that strategically leverages PID control concepts and incorporates several novel strategies to account for the various requirements of ABR streaming. We evaluate PIA using simulation based on real LTE network traces, as well as using real DASH implementation. The results demonstrate that PIA outperforms state-of-the-art schemes in providing high average bitrate with significantly lower bitrate changes (reduction up to 40%) and stalls (reduction up to 85%), while incurring very small runtime overhead.}, 
keywords={Internet;Long Term Evolution;cellular radio;feedback;three-term control;video streaming;ABR video streaming;DASH implementation;Internet;LTE network traces;PID control concepts;PID-based rate adaptation;adaptive bitrate streaming;control theoretic approach;de facto technique;feedback control problem;feedback control technique;Bandwidth;Bit rate;Cellular networks;PD control;PI control;Streaming media}, 
doi={10.1109/INFOCOM.2017.8057056}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057057, 
author={T. Zhang and F. Ren and W. Cheng and X. Luo and R. Shu and X. Liu}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Modeling and analyzing the influence of chunk size variation on bitrate adaptation in DASH}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Recently, HTTP-based adaptive video streaming has been widely adopted in the Internet. Up to now, HTTP-based adaptive video streaming is standardized as Dynamic Adaptive Streaming over HTTP (DASH), where a client-side video player can dynamically pick the bitrate level according to the perceived network conditions. Actually, not only the available bandwidth is varying, but also the chunk sizes in the same bitrate level significantly fluctuate, which also influences the bitrate adaptation. However, existing bitrate adaptation algorithms do not accurately involve the chunk size variation, leading to performance losses. In this paper, we theoretically analyze the influence of chunk size variation on bitrate adaptation performance. Based on DASH system features, we build a general model describing the playback buffer evolution. Applying stochastic theories, we respectively analyze the influence of the chunk size variation on rebuffering probability and average bitrate level. Furthermore, based on theoretical insights, we provide several recommendations for algorithm designing and rate encoding, and also propose a simple bitrate adaptation algorithm. Extensive simulations verify our insights as well as the efficiency of the proposed recommendations and algorithm.}, 
keywords={Internet;hypermedia;probability;transport protocols;video streaming;DASH;HTTP-based adaptive video streaming;average bitrate level;bitrate adaptation algorithms;bitrate adaptation performance;chunk size variation;client-side video player;dynamic adaptive streaming over HTTP;simple bitrate adaptation algorithm;stochastic theory;Adaptation models;Algorithm design and analysis;Analytical models;Bandwidth;Bit rate;Encoding;Streaming media;Chunk Size;DASH;average video rate;bitrate adaptation;rebuffering}, 
doi={10.1109/INFOCOM.2017.8057057}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057058, 
author={C. Zhang and Q. He and J. Liu and Z. Wang}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Beyond the touch: Interaction-aware mobile gamecasting with gazing pattern prediction}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Recent years have witnessed an explosion of gamecasting applications in the market, in which game players (or gamers in short) broadcast their game scenes in real-time. Such pioneer applications as YouTube Gaming, Twitch, and Mobcrush have attracted a massive number of online broadcasters, and each of them can attract hundreds or thousands of fellow viewers. The growing number however has created significant challenges to the network and end-devices, particularly considering bandwidth- and battery-limited smartphones or tablets are becoming dominating for both gamers and viewers. Yet the unique touch operations of the mobile interface offer opportunities, too. In this paper, our crowdsourced measurement reveals that strong associations exist between the gamers' touch interactions and the viewers' gazing patterns. Motivated by this, we present a novel interaction-aware optimization framework to improve the energy utilization and stream quality for mobile gamecasting (MGC). Our framework incorporates a touch-assisted prediction module to extract association rules for gazing pattern prediction and a tile-based optimization module to utilize energy on mobile devices efficiently. Trace-driven simulations illustrate the effectiveness of our framework in terms of energy consumption and streaming quality. Our user study experiments also demonstrate much improved (3%-13%) quality satisfaction than the state-of-the-art solution with similar network resources.}, 
keywords={computer games;data mining;mobile computing;smart phones;user interfaces;association rules;battery-limited smartphones;crowdsourced measurement;energy consumption;energy utilization;game players;game scenes;gamers;gazing pattern prediction;interaction-aware mobile gamecasting;mobile devices;mobile interface;novel interaction-aware optimization framework;online broadcasters;streaming quality;tablets;tile-based optimization module;touch interactions;touch-assisted prediction module;Games;Land mobile radio;Optimization;Smart phones;YouTube}, 
doi={10.1109/INFOCOM.2017.8057058}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057059, 
author={K. Diab and M. Hefeeda}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={MASH: A rate adaptation algorithm for multiview video streaming over HTTP}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Multiview videos offer unprecedented experience by allowing users to explore scenes from different angles and perspectives. Thus, such videos have been gaining substantial interest from major content providers such as Google and Facebook. Adaptive streaming of multiview videos is, however, challenging because of the Internet dynamics and the diversity of user interests and network conditions. To address this challenge, we propose a novel rate adaptation algorithm for multiview videos (called MASH). Streaming multiview videos is more user centric than single-view videos, because it heavily depends on how users interact with the different views. To efficiently support this interactivity, MASH constructs probabilistic view switching models that capture the switching behavior of the user in the current session, as well as the aggregate switching behavior across all previous sessions of the same video. MASH then utilizes these models to dynamically assign relative importance to different views. Furthermore, MASH uses a new buffer-based approach to request video segments of various views at different qualities, such that the quality of the streamed videos is maximized while the network bandwidth is not wasted. We have implemented a multiview video player and integrated MASH in it. We compare MASH versus the state-of-the-art algorithm used by YouTube for streaming multiview videos. Our experimental results show that MASH can produce much higher and smoother quality than the algorithm used by YouTube, while it is more efficient in using the network bandwidth. In addition, we conduct large-scale experiments with up to 100 concurrent multiview streaming sessions, and we show that MASH maintains fairness across competing sessions, and it does not overload the streaming server.}, 
keywords={Internet;interactive video;social networking (online);video coding;video streaming;MASH constructs probabilistic view;multiview streaming sessions;multiview video player;single-view videos;streamed videos;video segments;Adaptation models;Computational modeling;Heuristic algorithms;Multi-stage noise shaping;Streaming media;Switches;YouTube}, 
doi={10.1109/INFOCOM.2017.8057059}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057060, 
author={I. Markwood and Y. Liu and K. Kwiat and C. Kamhoua}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Electric grid power flow model camouflage against topology leaking attacks}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={The power flow model for DC power grids has been used theoretically to launch false data injection attacks (FDIAs) against state estimation. We recognize FDIAs are just one possible attack using the power flow model and that the grid topology information within the model implies its discovery may also facilitate topology-based attacks. We show attackers can derive the power flow model, and thus the topology also. Indeed, with incomplete data, attackers can accurately reconstruct regions of the model, or topology, all that is necessary to launch an attack. We also illustrate how to cause such attackers to derive instead a convincing fake model by camouflaging the real model. Consequently, no sensitive information will leak, so attacks based on this fake model will be ineffective, rather alerting grid administrators to the attacker's efforts. Using five test cases included in the MATLAB power flow analysis tool MATPOWER, ranging from 9 to 300 buses, an average 67.0% of the topology may be derived with a 69.1% model accuracy. Lastly, we find reconstructions of small portions of the model sufficient for performing FDIAs with 75% success, and that camouflage prevents 93% of them in all but the 9-bus case.}, 
keywords={DC power transmission;load flow control;power grids;power system simulation;power system state estimation;DC power grids;FDIAs;MATLAB power flow analysis tool;MATLAB power flow analysis tool MATPOWER;MATPOWER;convincing fake model;electric grid power flow model camouflage;false data injection attacks;grid topology information;power flow model;state estimation;topology leaking attacks;Admittance;Computational modeling;Data models;Mathematical model;Meters;Power grids;Topology}, 
doi={10.1109/INFOCOM.2017.8057060}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057061, 
author={Z. Lu and M. Wei and X. Lu}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={How they interact? Understanding cyber and physical interactions against fault propagation in smart grid}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={In the smart grid, computer networks (i.e., the cyber domain) are built upon physical infrastructures (i.e., the physical domain) to facilitate advanced functionalities that were considered not possible in legacy systems. It is envisioned that such a cyber-physical paradigm enables intelligent, collaborative controls to prevent faults from propagating along large-scale infrastructures, which is a primary cause for massive blackouts (e.g., Northeast blackout of 2003). Despite this promising vision, how effective cyber and physical interactions are against fault propagation is not yet fully investigated. In this paper, we use analysis and system-level simulations to characterize such interactions during load shedding, which is a process to stop fault propagation by shedding a computed amount of loads based on collaborative communication. Specifically, we model faults happening in the physical domain as a counting process, with each count triggering a load shedding action on the fly in the cyber domain. We show that although global load shedding design is considered optimal by globally coordinating shedding actions in power engineering, its induced failure probability (defined as the one that at least a given number of power lines fail) is scalable to the delay performance and the system size in the cyber domain, thus less likely to stop fault propagation in large systems than local shedding design that sheds loads within a limited system scope. Our study demonstrates that a joint view on cyber and physical factors is essential for failure prevention design in the smart grid.}, 
keywords={load shedding;power system faults;probability;smart power grids;cyber domain;cyber interactions;cyber-physical paradigm;fault propagation;global load shedding design;legacy systems;load shedding action;local shedding design;model faults;physical domain;physical factors;physical infrastructures;physical interactions;shedding actions;smart grid;system-level simulations;Analytical models;Computational modeling;Load modeling;Power system faults;Power system protection;Smart grids;Smart grid;cascading failure;failure prevention;fault propagation;load shedding;modeling and simulations}, 
doi={10.1109/INFOCOM.2017.8057061}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057062, 
author={X. Lou and R. Tan and D. K. Y. Yau and P. Cheng}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Cost of differential privacy in demand reporting for smart grid economic dispatch}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Increasing dynamics of electrical loads presents uncertainty and hence new challenges for power grid controls and optimization. In economic dispatch control (EDC) for minimizing generation cost, demand reporting by customers is a promising approach for managing the uncertainty, but it raises important privacy concerns. Adding random noise to aggregate queries of demand reports can provide differential privacy (DP) for the individual customers. But the noisy query results can adversely impact the EDC's optimality. In this paper, we analyze the privacy cost in demand reporting in terms of how DP-induced noise will increase the total generation cost. Our analysis shows that the noise amounts for different customers are intricately coupled with one another in determining the total cost. In view of the coupling, we apply the principle of Shapley value to attribute fair shares of the total cost to the power grid buses. For efficient sharing of the privacy cost, in a manner scalable to large power systems with many buses, we additionally propose heuristic algorithms to approximate the Shapley value. Trace-driven simulations based on a 5-bus power system model validate our analysis and illustrate the performance of the proposed cost sharing algorithms.}, 
keywords={data privacy;minimisation;power generation dispatch;power generation economics;power grids;smart power grids;DP-induced noise;EDC optimality;Shapley value;cost sharing algorithms;demand reporting;demand reports;differential privacy;economic dispatch control;electrical loads;optimization;power grid buses;power grid control;privacy concerns;privacy cost;smart grid economic dispatch;total generation cost minimisation;Aggregates;Generators;Power grids;Power system dynamics;Privacy;Uncertainty}, 
doi={10.1109/INFOCOM.2017.8057062}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057063, 
author={A. Sehati and M. Ghaderi}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Energy-delay tradeoff for request bundling on smartphones}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={To reduce the energy consumption of a smartphone, multiple data transfer requests from applications can be bundled together and granted at once in order to reduce the time the radio interface is on. The side effect of bundling is the increased delay experienced by mobile applications. While several bundling algorithms have been proposed in the literature, a general and systematic solution to balance the energy-delay tradeoff is missing. In this paper, we formulate bundling as a cost minimization problem, in which the tradeoff between energy and delay is captured by a cost function. We then propose an online algorithm for minimizing the bundling cost and show that the algorithm is 4-competitive with respect to the optimal offline algorithm that knows the entire sequence of data transfer requests a priori. We evaluate the performance of the proposed algorithm and the accuracy of our results in a range of realistic scenarios using both model-driven simulations and real experiments on a smartphone. Our results show that depending on the delay tolerance level of a user, energy savings ranging from zero (delay intolerant) to about 100% (delay tolerant) can be achieved using our algorithm.}, 
keywords={costing;delays;minimisation;smart phones;telecommunication power management;bundling algorithms;bundling cost;cost function;cost minimization problem;delay tolerance level;energy consumption;energy savings;energy-delay tradeoff;mobile applications;multiple data transfer requests;optimal offline algorithm;radio interface;request bundling;smartphone data transfer requests;Algorithm design and analysis;Conferences;Data transfer;Delays;Energy consumption;Mobile communication;Smart phones}, 
doi={10.1109/INFOCOM.2017.8057063}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057064, 
author={L. De Carli and R. Torres and G. Modelo-Howard and A. Tongaonkar and S. Jha}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Botnet protocol inference in the presence of encrypted traffic}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Network protocol reverse engineering of botnet command and control (C&C) is a challenging task, which requires various manual steps and a significant amount of domain knowledge. Furthermore, most of today's C&C protocols are encrypted, which prevents any analysis on the traffic without first discovering the encryption algorithm and key. To address these challenges, we present an end-to-end system for automatically discovering the encryption algorithm and keys, generating a protocol specification for the C&C traffic, and crafting effective network signatures. In order to infer the encryption algorithm and key, we enhance state-of-the-art techniques to extract this information using lightweight binary analysis. In order to generate protocol specifications we infer field types purely by analyzing network traffic. We evaluate our approach on three prominent malware families: Sality, ZeroAccess and Ramnit. Our results are encouraging: the approach decrypts all three protocols, detects 97% of fields whose semantics are supported, and infers specifications that correctly align with real protocol specifications.}, 
keywords={computer network security;cryptographic protocols;invasive software;reverse engineering;statistical analysis;telecommunication traffic;transport protocols;C&C protocols;Ramnit malware;Sality malware;ZeroAccess malware;botnet botnet command and control protocols;botnet protocol inference;encrypted traffic;lightweight binary analysis;network protocol reverse engineering;network signatures;network traffic;Ciphers;Encryption;Malware;Payloads;Protocols}, 
doi={10.1109/INFOCOM.2017.8057064}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057065, 
author={A. A. Chariton and E. Degkleri and P. Papadopoulos and P. Ilia and E. P. Markatos}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={CCSP: A compressed certificate status protocol}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Trust in SSL-based communications is provided by Certificate Authorities (CAs) in the form of signed certificates. Checking the validity of a certificate involves three steps: (i) checking its expiration date, (ii) verifying its signature, and (iii) ensuring that it is not revoked. Currently, such certificate revocation checks are done either via Certificate Revocation Lists (CRLs) or Online Certificate Status Protocol (OCSP) servers. Unfortunately, despite the existence of these revocation checks, sophisticated cyber-attackers, may trick web browsers to trust a revoked certificate, believing that it is still valid. Consequently, the web browser will communicate (over TLS) with web servers controlled by cyber-attackers. Although frequently updated, nonced, and timestamped certificates may reduce the frequency and impact of such cyber-attacks, they impose a very large overhead to the CAs and OCSP servers, which now need to timestamp and sign on a regular basis all the responses, for every certificate they have issued, resulting in a very high overhead. To mitigate this overhead and provide a solution to the described cyber-attacks, we present CCSP: a new approach to provide timely information regarding the status of certificates, which capitalizes on a newly introduced notion called signed collections. In this paper, we present the design, preliminary implementation, and evaluation of CCSP in general, and signed collections in particular. Our preliminary results suggest that CCSP (i) reduces space requirements by more than an order of magnitude, (ii) lowers the number of signatures required by 6 orders of magnitude compared to OCSP-based methods, and (iii) adds only a few milliseconds of overhead in the overall user latency.}, 
keywords={Internet;certification;computer network security;protocols;public key cryptography;CAs;CCSP;Certificate Authorities;Certificate Revocation Lists;OCSP servers;OCSP-based methods;Online Certificate Status Protocol servers;certificate revocation checks;compressed certificate status protocol;signed certificates;sophisticated cyber-attackers;timestamped certificates;web browser;web servers;Browsers;Conferences;Protocols;Public key;Receivers;Web servers}, 
doi={10.1109/INFOCOM.2017.8057065}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057066, 
author={B. Wang and L. Zhang and N. Z. Gong}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={SybilSCAR: Sybil detection in online social networks via local rule based propagation}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Detecting Sybils in online social networks (OSNs) is a fundamental security research problem as adversaries can leverage Sybils to perform various malicious activities. Structure-based methods have been shown to be promising at detecting Sybils. Existing structure-based methods can be classified into two categories: Random Walk (RW)-based methods and Loop Belief Propagation (LBP)-based methods. RW-based methods cannot leverage labeled Sybils and labeled benign users simultaneously, which limits their detection accuracy, and they are not robust to noisy labels. LBP-based methods are not scalable, and they cannot guarantee convergence. In this work, we propose SybilSCAR, a new structure-based method to perform Sybil detection in OSNs. SybilSCAR maintains the advantages of existing methods while overcoming their limitations. Specifically, SybilSCAR is Scalable, Convergent, Accurate, and Robust to label noises. We first propose a framework to unify RW-based and LBP-based methods. Under our framework, these methods can be viewed as iteratively applying a (different) local rule to every user, which propagates label information among a social graph. Second, we design a new local rule, which SybilSCAR iteratively applies to every user to detect Sybils. We compare SybilSCAR with a state-of-the-art RW-based method and a state-of-the-art LBP-based method, using both synthetic Sybils and large-scale social network datasets with real Sybils. Our results demonstrate that SybilSCAR is more accurate and more robust to label noise than the compared state-of-the-art RW-based method, and that SybilSCAR is orders of magnitude more scalable than the state-of-the-art LBP-based method and is guaranteed to converge. To facilitate research on Sybil detection, we have made our implementation of SybilSCAR publicly available on our webpages.}, 
keywords={graph theory;learning (artificial intelligence);security of data;social networking (online);LBP-based methods;OSN;RW-based methods;Sybil detection;SybilSCAR;label information;large-scale social network datasets;local rule based propagation;noisy labels;online social networks;security research problem;synthetic Sybils;Belief propagation;Convergence;Image edge detection;Robustness;Training;Twitter}, 
doi={10.1109/INFOCOM.2017.8057066}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057067, 
author={S. Ji and S. Yang and A. Das and X. Hu and R. Beyah}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Password correlation: Quantification, evaluation and application}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={In this paper, we study the correlation between passwords across different datasets which quantitatively explains the success of existing training-based password cracking techniques. We also study the correlation between a user's password and his/her social profile. This enabled us to develop the first social profile-aware password strength meter, namely SociaLShield. Our quantification techniques and SocialShield have meaningful implications to system administrators, users, and researchers, e.g., helping them quantitatively understand the threats posed by a password leakage incident, defending against emerging profile-based password attacks, and facilitating the research of countermeasures against existing and newly developed training-based password attacks. We validate our proposed quantification techniques and SocialShield through extensive experiments by leveraging real-world leaked passwords. Experimental results demonstrate that our quantification techniques are accurate in measuring correlation among different leaked datasets and that although SocialShield is light-weight, it is effective in defending against profile-based password attacks.}, 
keywords={authorisation;SociaLShield;SocialShield;leaked datasets;password correlation;password cracking techniques;password leakage incident;profile-based password attacks;quantification techniques;social profile-aware password strength meter;training-based password attacks;user passwork;user social profile;Correlation;LinkedIn;Markov processes;Measurement;Meters;Security;Standards}, 
doi={10.1109/INFOCOM.2017.8057067}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057068, 
author={A. Kuhnle and T. Pan and M. A. Alim and M. T. Thai}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Scalable bicriteria algorithms for the threshold activation problem in online social networks}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={We consider the Threshold Activation Problem (TAP): given social network G and positive threshold T, find a minimum-size seed set A that can trigger expected activation of at least T. We introduce the first scalable, parallelizable algorithm with performance guarantee for TAP suitable for datasets with millions of nodes and edges; we exploit the bicriteria nature of solutions to TAP to allow the user to control the running time versus accuracy of our algorithm through a parameter α ϵ (0, 1): given η > 0, with probability 1 - η our algorithm returns a solution A with expected activation greater than T - 2αT, and the size of the solution A is within factor 1-h 4α/T + log(T) of the optimal size. The algorithm runs in time O (α-2 log (n/η) (n + m)|A|), where n, m, refer to the number of nodes, edges in the network. The performance guarantee holds for the general triggering model of internal influence and also incorporates external influence, provided a certain condition is met on the cost-effectivity of seed selection.}, 
keywords={computational complexity;directed graphs;network theory (graphs);parallel algorithms;probability;social networking (online);TAP;expected activation;minimum-size seed set;online social networks;optimal size;positive threshold;running time;scalable bicriteria algorithms;scalable parallelizable algorithm;threshold activation problem;Computational modeling;Conferences;Context modeling;Integrated circuit modeling;Social network services;TV}, 
doi={10.1109/INFOCOM.2017.8057068}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057069, 
author={X. Li and J. D. Smith and T. N. Dinh and M. T. Thai}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Why approximate when you can get the exact? Optimal targeted viral marketing at scale}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={One of the most central problems in viral marketing is Influence Maximization (IM), which finds a set of k seed users who can influence the maximum number of users in online social networks. Unfortunately, all existing algorithms to IM, including the state of the art SSA and IMM, have an approximation ratio of (1 - 1/e - ϵ). Recently, a generalization of IM, Cost-aware Target Viral Marketing (CTVM), asks for the most cost-effective users to influence the most relevant users, has been introduced. The current best algorithm for CTVM has an approximation ratio of (1 - 1/√e - ϵ). In this paper, we study the CTVM problem, aiming to optimally solve the problem. We first highlight that using a traditional two stage stochastic programming to exactly solve CTVM is not possible because of scalability. We then propose an almost exact algorithm TIPTOP, which has an approximation ratio of (1 - ϵ). This result significantly improves the current best solutions to both IM and CTVM. At the heart of TIPTOP lies an innovative technique that reduces the number of samples as much as possible. This allows us to exactly solve CTVM on a much smaller space of generated samples using Integer Programming. While obtaining an almost exact solution, TIPTOP is very scalable, running on billion-scale networks such as Twitter under three hours. Furthermore, TIPTOP lends a tool for researchers to benchmark their solutions against the optimal one in large-scale networks, which is currently not available.}, 
keywords={approximation theory;graph theory;integer programming;marketing data processing;social networking (online);stochastic programming;CTVM problem;Cost-aware Target Viral Marketing;IM;Influence Maximization;TIPTOP exact algorithm;Twitter;approximation ratio;cost-effective users;online social networks;two stage stochastic programming;Approximation algorithms;Conferences;Integrated circuit modeling;Programming;Scalability;Social network services;Algorithms;Influence Maximization;Online Social Networks;Optimization;Viral Marketing}, 
doi={10.1109/INFOCOM.2017.8057069}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057070, 
author={Z. Zhang and Y. Shi and J. Willson and D. Z. Du and G. Tong}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Viral marketing with positive influence}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-8}, 
abstract={One model for viral marketing is the positive influence. In this model, an inactive node is changed into active if and only if at least half of its neighbors are already in active state. The positive influence model can be viewed as a special case of a general threshold model, in which the threshold function at each node has value one if at least a certain fraction of neighbors are in active state, and value 0 otherwise. This function can be proved to be monotonically increasing and nonsubmodular for any predefined fraction. Therefore, given a seed set, the number of influenced nodes is not submodular with respect to the size of the seed set. This fact makes those optimization problems related with positive influence very hard, including the minimum partial positive influence seeding problem: Given a social network G = (V, E) and a number 0 <; p <; 1, find a minimum seed set S which can positively influence at least p|V| nodes. In this paper, we present an O ((log n)2 H ([pn]))-approximation algorithm for the minimum partial positive influence seeding problem, where n is the number of nodes, and H(·) is the Harmonic number.}, 
keywords={computational complexity;marketing;network theory (graphs);optimisation;harmonic number;influenced nodes;minimum partial positive influence seeding problem;positive influence model;seed set;social network;threshold function;viral marketing;Approximation algorithms;Computational modeling;Computer science;Conferences;Electronic mail;Greedy algorithms;Social network services;approximation algorithm;partial positive-influence seeding problem;viral marketing}, 
doi={10.1109/INFOCOM.2017.8057070}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057071, 
author={C. H. Lee and X. X. Do and Y. Eun}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={On the rao-blackwellization and its application for graph sampling via neighborhood exploration}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={We study how the so-called Rao-Blackwellization, which is a variance reduction technique via “conditioning” for Monte Carlo methods, can be judiciously applied for graph sampling through neighborhood exploration. Despite its popularity for Monte Carlo methods, it is little known for Markov chain Monte Carlo methods and has never been discussed for random walk-based graph sampling. We first propose two forms of Rao-Blackwellization that can be used as a swap-in replacement for virtually all (reversible) random-walk graph sampling methods, and prove that the `Rao-Blackwellized' estimators reduce the (asymptotic) variances of their original estimators yet maintain their inherent unbiasedness. The variance reduction can translate into lowering the number of samples required to achieve a desired sampling accuracy. However, the sampling cost for neighborhood exploration, if required, may outweigh such improvement, even leading to higher total amortized cost. Considering this, we provide a generalization of Rao-Blackwellization, which allows one to choose a suitable extent of obtaining Rao-Blackwellized samples in order to achieve a right balance between sampling cost and accuracy. We finally provide simulation results via real-world datasets that confirm our theoretical findings.}, 
keywords={Bayes methods;Markov processes;Monte Carlo methods;graph theory;random processes;sampling methods;Markov chain Monte Carlo methods;Rao-Blackwellized estimators;asymptotic variance reduction;conditioning technique;graph sampling;neighborhood exploration;sampling cost;total amortized cost;virtually-all random-walk graph;Conferences;Markov processes;Monte Carlo methods;Random variables;Sampling methods;Simulation;Social network services}, 
doi={10.1109/INFOCOM.2017.8057071}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057072, 
author={Y. Wu and B. Zhang and S. Yang and X. Yi and X. Yang}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Energy-efficient joint communication-motion planning for relay-assisted wireless robot surveillance}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={In this paper, we consider a surveillance scenario where a team of sensing robots survey a sensitive area and transmit the monitored data to a remote base station through a mobile relay. In this scenario, it is challenging to autonomously adjust the position of the mobile relay for the sake of minimizing the total communication-motion energy consumption of the system, while maintaining the communication quality of the mobile sensing robots. We first derive the asymptotically optimal transmit powers of the mobile relay and of the sensing robots according to the predefined end-to-end packet error rate (PER) requirement. Then, we propose a joint communication-motion planning (JCMP) method for minimizing the total communication-motion energy consumption in both: single- and multi-sensing-robot scenarios, where the trajectories of the sensing robots are rigorously defined. We further consider the scenario where the sensing robots' trajectories are not fixed but can be optimized in restrained areas. The effectiveness of the proposed JCMP is verified by analysis and numerical results for different system configurations, showing that a substantial energy-efficiency improvement may be achieved in comparison with the benchmark that only optimizes the communication energy consumption.}, 
keywords={energy conservation;energy consumption;error statistics;mobile robots;relay networks (telecommunication);telecommunication network planning;telecommunication power management;asymptotically optimal transmit powers;communication quality;energy-efficiency improvement;energy-efficient joint communication-motion planning;joint communication-motion planning method;mobile relay;mobile sensing robots;multisensing-robot scenarios;predefined end-to-end packet error rate requirement;remote base station;surveillance scenario;total communication-motion energy consumption;wireless robot surveillance;Base stations;Energy consumption;Mobile communication;Relays;Robot sensing systems;Joint communication-motion planning;energy-efficient relays;optimization methods;robots;surveillance}, 
doi={10.1109/INFOCOM.2017.8057072}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057073, 
author={T. Shi and S. Cheng and J. Li and Z. Cai}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Constructing connected dominating sets in battery-free networks}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Currently, the limitation of battery has become a serious obstacle for the development of Internet of things (IoTs). Therefore, a new network architecture, named as battery-free network, were proposed. In a typical battery-free network, the battery-free nodes are equipped with any battery and can only gain energy from the environment. Such network extremely expands the scope of the IoT applications, however, it also brings many troubles for some network operations, e.g. data collection, since the energy of each node is quite small. Considering that the Connected Dominating Sets (CDSs) are commonly used to support data collection and network communication in wireless networks, and thus we will also investigate the CDS construction problem in battery-free networks. In this paper, the problem of constructing CDS in a battery-free network is formally defined, and we prove that it is NP-Complete. Thus, four approximation algorithms were proposed to deal with the snapshot, continuous and time-window based CDS construction requirements, respectively. Finally, the extensive experiments were carried out and the results verify that the proposed algorithms have high performance in term of accuracy and efficiency.}, 
keywords={Internet of Things;approximation theory;radio networks;set theory;CDS;CDS construction problem;Internet of things;IoT;approximation algorithms;battery-free network;battery-free nodes;connected dominating sets;network architecture;network communication;network operations;wireless networks;Batteries;Bridges;Conferences;Monitoring;Sensors;Windows;Wireless sensor networks}, 
doi={10.1109/INFOCOM.2017.8057073}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057074, 
author={Y. Gao and W. Dong and X. Zhang and W. Wu}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Universal path tracing for large-scale sensor networks}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Most sensor networks employ dynamic routing protocols so that the routing topology can be dynamically optimized with environmental changes. The routing behaviors can be quite complex with increasing network scale and environmental dynamics. Knowledge on the routing path of each packet is certainly a great help in understanding the complex routing behaviors, allowing effective performance diagnosis and efficient network management. We propose PAT, a universal sensornet path tracing approach. PAT includes an intelligent path encoding scheme that allows efficient decoding at the PC side. To make PAT more scalable, we propose techniques to accurately estimate the degree information by exploiting timing information, allowing more compact path encoding. Moreover, we employ subpath concatenation to infer excessively long paths with a high recovery probability. We carefully evaluate PAT's performance using testbed experiments and and extensive simulations with up to 4,000 nodes. Results show that PAT significantly outperforms existing approaches.}, 
keywords={decoding;encoding;probability;routing protocols;telecommunication network management;telecommunication network topology;wireless sensor networks;PAT;compact path encoding;complex routing behaviors;decoding;dynamic routing protocols;environmental dynamics;intelligent path encoding scheme;large-scale sensor networks;network management;network scale;performance diagnosis;routing path;routing topology;timing information;universal sensornet path tracing approach;Encoding;Monitoring;Network topology;Robot sensing systems;Routing;Topology;Wireless sensor networks}, 
doi={10.1109/INFOCOM.2017.8057074}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057075, 
author={F. Guo and B. Zhou and M. C. Vuran}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={CFOSynt: Carrier frequency offset assisted clock syntonization for wireless sensor networks}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={System-level timing inconsistency and wireless communication delays are the main uncertainties of existing synchronization mechanisms for wireless sensor networks. Existing solutions mainly rely on timestamp exchanges to estimate clock offset and skew, which results in frequent synchronization, high overhead, and high energy consumption to maintain a well-synchronized network. This paper introduces a novel clock syntonization approach to estimate the differences between clock frequencies of network nodes without the need for timestamp exchanges. The carrier frequency offset (CFO) assisted syntonization (CFOSynt) utilizes the carrier information obtained from wireless packet transmission for clock skew compensation. The key idea of CFOSynt is that, in any wireless communication system, where carrier modulation is employed, carrier frequency delivers information about the transmitter RF clock. Consequently, clock frequency offset between a pair of sensor nodes will result in a carrier frequency offset detected by the receiver node. By leveraging the CFO information, CFOSynt can estimate the system clock skew based on digital counter theory. Extensive experiments and numerical analysis have been demonstrated to evaluate clock skew estimation.}, 
keywords={clocks;delays;modulation;synchronisation;wireless sensor networks;CFO assisted syntonization;CFOSynt;carrier frequency offset;carrier information;carrier modulation;clock frequency;clock skew compensation;clock skew estimation;frequent synchronization;high energy consumption;network nodes;novel clock syntonization approach;sensor nodes;synchronization mechanisms;system clock;system-level timing inconsistency;timestamp exchanges;transmitter RF clock;well-synchronized network;wireless communication delays;wireless communication system;wireless packet transmission;wireless sensor networks;Clocks;Delays;Estimation;Radio frequency;Synchronization;Wireless communication;Wireless sensor networks}, 
doi={10.1109/INFOCOM.2017.8057075}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057076, 
author={X. Lin and S. Wang}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Efficient remote radio head switching scheme in cloud radio access network: A load balancing perspective}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Cloud radio access network (C-RAN) is deemed as a promising architecture to meet the exponentially increasing traffic demand in mobile networks, where baseband processing is separated from remote radio heads (RRHs) and performed in a centralized baseband unit (BBU) pool. However, the densely deployed RRHs, as well as the passive optical network which provides high capacity backhauls between the RRHs and the BBU pool, consume a large amount of energy. In this paper, we propose efficient RRH switching schemes to achieve a tradeoff between the system energy saving and the load balance among the RRHs in the C-RAN. We first develop an approximation algorithm to address the intractable user association problem for a given set of RRHs, based on which we introduce efficient local search algorithms to perform RRH selection procedure, which can reduce the load fairness index of the C-RAN by controlling the active/inactive state of each RRH. We also discuss the handover signalling overhead issue and introduce an adaptive trigger mechanism to avoid switching on/off too many RRHs simultaneously so as to keep the signalling overhead of the C-RAN below an acceptable level. Numerical results demonstrate that the proposed RRH switching schemes can improve the system performance of the C-RAN significantly. Moreover, our proposal sheds light on how to design effective and efficient handover schemes for next generation mobile networks.}, 
keywords={cellular radio;cloud computing;energy conservation;mobile radio;mobility management (mobile radio);radio access networks;resource allocation;search problems;telecommunication computing;telecommunication power management;telecommunication traffic;BBU pool;RRH efficiency;RRH selection procedure;RRH switching schemes;baseband processing;centralized baseband unit pool;cloud radio access network;energy saving;handover schemes;load balancing;load fairness index;local search algorithm;next generation mobile networks;passive optical network;remote radio head switching scheme;remote radio heads;traffic demand;Heuristic algorithms;Indexes;Interference;Mobile communication;Optical switches;Power demand}, 
doi={10.1109/INFOCOM.2017.8057076}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057077, 
author={D. Griffith and A. Ben Mosbah and R. Rouil}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Group discovery time in device-to-device (D2D) proximity services (ProSe) networks}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Device-to-device (D2D) communications for Long Term Evolution (LTE) networks relies on a discovery process to enable User Equipment (UE) to determine which D2D applications and services are supported by neighboring UEs. This is especially important for groups of UEs that operate outside the coverage area of any base station. The amount of time required for discovery information to reach every UE in a group depends on the number of UEs in the group and the dimensions of the discovery resource pool associated with the Physical Sidelink Discovery Channel (PSDCH); an additional factor is the half-duplex property of current UEs. In this paper, we use a Markov chain to characterize the performance of Mode 2 direct discovery. The resulting analytical model gives the distribution of the time for a UE to discover all other UEs in its group. We validate the model using Monte Carlo and network simulations.}, 
keywords={Long Term Evolution;Monte Carlo methods;resource allocation;D2D communication;Device-to-device communications;LTE network;Long Term Evolution networks;Mode 2 direct discovery;Monte Carlo;Physical Sidelink Discovery Channel;device-to-device proximity services;discovery information;discovery resource pool;group discovery time;neighboring UEs;Analytical models;Device-to-device communication;Indexes;Long Term Evolution;Markov processes;Mathematical model;Superluminescent diodes}, 
doi={10.1109/INFOCOM.2017.8057077}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057078, 
author={R. Calvo-Palomino and D. Giustiniano and V. Lenders and A. Fakhreddine}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Crowdsourcing spectrum data decoding}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Crowdsourced signal monitoring systems are gaining attention for capturing the wireless spectrum at large geographical scale. Yet, most of the current systems are still limited to simple power spectrum measurements reported by each sensor. Our objective is to enhance such systems with signal decoding capabilities performed in the backend while retaining the original vision of a low-cost and crowdsourced setup. We propose a distributed system architecture for collaborative radio signal monitoring and decoding that builds on $12 low-cost radio frequency (RF) frontends and embedded boards and that takes into consideration the limited network bandwidth from the sensors to the backend. We present a distributed time multiplexing mechanism to sample the spectrum in a coordinated fashion that exploits the similarity of the radio signal received by more than one RF frontend in the same radio coverage. We address the strict time synchronization required among sensors to reconstruct the signal from the samples they receive when in the same radio coverage. We study and implement techniques to identify and overcome errors in the timing information in the presence of noise sources and decode the data in the backend. We provide an evaluation based on simulations and on real signals transmitted by Long-Term Evolution (LTE) base stations. Our results show that we can reliably reconstruct and decode radio signals received by low-cost crowdsourced sensors.}, 
keywords={cognitive radio;crowdsourcing;decoding;radio receivers;synchronisation;backend;collaborative radio signal monitoring;consideration the limited network bandwidth;crowdsourced sensors;crowdsourced signal monitoring systems;decode radio signals;distributed system architecture;distributed time;geographical scale;radio coverage;signal decoding capabilities;spectrum data;time synchronization;wireless spectrum;Bandwidth;Decoding;Economic indicators;Internet;Monitoring;Radio frequency;Synchronization}, 
doi={10.1109/INFOCOM.2017.8057078}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057079, 
author={Y. Zhang and L. Deng and M. Chen and P. Wang}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Joint bidding and geographical load balancing for datacenters: Is uncertainty a blessing or a curse?}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={We consider the scenario where a cloud service provider (CSP) operates multiple geo-distributed datacenters to provide Internet-scale service. Our objective is to minimize the total electricity and bandwidth cost by jointly optimizing electricity procurement from wholesale markets and geographical load balancing (GLB), i.e., dynamically routing workloads to locations with cheaper electricity. Under the ideal setting where exact values of market prices and workloads are given, this problem reduces to a simple LP and is easy to solve. However, under the realistic setting where only distributions of these variables are available, the problem unfolds into a non-convex infinite-dimensional one and is challenging to solve. Our main contribution is to develop an algorithm that is proven to solve the challenging problem optimally and efficiently, by exploring the full design space of strategic bidding. Trace-driven evaluations corroborate our theoretical results, demonstrate fast convergence of our algorithm, and show that it can reduce the cost for the CSP by up to 20% as compared to baseline alternatives. Our study highlights the intriguing role of uncertainty. While variability in workloads deteriorates the cost-saving performance of joint electricity procurement and GLB, counter-intuitively, variability in market prices can be exploited to achieve a cost reduction even larger than the setting without price variability.}, 
keywords={Internet;cloud computing;computer centres;concave programming;convex programming;cost reduction;power aware computing;power markets;pricing;resource allocation;tendering;CSP;GLB;Internet-scale service;bandwidth cost minimization;cloud service provider;cost reduction;cost-saving performance;dynamic routing workloads;joint bidding and geographical load balancing;joint electricity procurement optimisation;market prices;multiple geo-distributed datacenters;nonconvex infinite-dimensional problem;price variability;strategic bidding;total electricity minimization;trace-driven evaluations;wholesale markets;Conferences;Electricity supply industry;Load management;Procurement;Real-time systems;Standards;Uncertainty}, 
doi={10.1109/INFOCOM.2017.8057079}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057080, 
author={A. Kabbani and M. Sharif}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Flier: Flow-level congestion-aware routing for direct-connect data centers}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Various topologies have been proposed in the context of high-performance computing and data center networking. Direct-connect topologies generally offer large capacity with high path diversity and are highly cost effective for general data center traffic patterns. However, the lack of simple yet efficient load balancing techniques for direct-connect fabrics has hindered these networks from gaining traction in data centers. This paper presents the design, implementation, and evaluation of Flicr, a light-weight host-based load balancing mechanism for direct-connect data centers. Flicr dynamically reroutes traffic through minimal and non-minimal routes to avoid congesting the fabric. This enables Flicr to efficiently minimize networking resource consumption while exploiting high path diversity in direct-connect fabrics to balance the network and gracefully handle link failures. Flicr requires only a simple kernel modification and is readily deployable in commodity data centers today. Our evaluations show that Flicr consistently outperforms other state-of-the-art load balancing designs, achieving 25-60% lower average flow completion time compared to adaptive routing. Flicr is also more robust against link failures and has 5-8 χ better performance relative to other schemes in the presence of link failures.}, 
keywords={computer centres;computer networks;data communication;telecommunication network routing;telecommunication network topology;telecommunication traffic;Flicr;Flow-level congestion-aware routing;commodity data centers today;data center networking;direct-connect topologies;fabrics;general data center traffic patterns;high path diversity;high-performance computing;load balancing mechanism;networking resource consumption;Fabrics;Load management;Network topology;Robustness;Routing;Topology}, 
doi={10.1109/INFOCOM.2017.8057080}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057081, 
author={R. Zhu and D. Niu and B. Li and Z. Li}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Optimal multicast in virtualized datacenter networks with software switches}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Virtualized datacenter networks have been deployed in production platforms, e.g., Amazon VPC and VMware's NVP, to offer the flexibility of network management to enterprise-level clients. A common characteristic of these platforms is that they adopt software switches, such as Open vSwitch (OvS), instead of hardware switches to transfer data between VMs. Although group communication is common in enterprise applications, the unique characteristics of software switches have posed new challenges to the design of multicast protocols. How logical multicast can be optimally performed with software switches is still not well understood. In this paper, we observe that unlike hardware switches, the per-stream output rate in a software switch critically depends on the packet processing overhead of flow cloning. We study the optimal OvS multicast topology with or without the help of additional dedicated software switches called service nodes, and formulate the throughput maximization as a new class of degree-supervised combinatorial graph problems due to the presence of flow cloning costs. We propose a lineartime optimal solution that translates into simple forwarding rules installed at each software switch. Through emulation-based OvS profiling and extensive simulation results, we demonstrate that our proposed logical multicast solutions can significantly improve session throughput with the ability to handle load balancing and latency issues, as compared to the state-of-the-art in the literature.}, 
keywords={computer centres;computer network management;graph theory;multicast protocols;telecommunication network topology;telecommunication switching;virtualisation;Open vSwitch;OvS multicast topology;degree-supervised combinatorial graph problem;hardware switches;multicast protocols;network management;software switches;virtualized datacenter networks;Cloning;Hardware;Network topology;Software;Throughput;Topology;Virtual machine monitors}, 
doi={10.1109/INFOCOM.2017.8057081}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057082, 
author={Z. Li and W. Bai and K. Chen and D. Han and Y. Zhang and D. Li and H. Yu}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Rate-aware flow scheduling for commodity data center networks}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Flow completion times (FCTs) are critical for many cloud applications. To minimize the average FCT, recent transport designs, such as pFabric, PASE, and PIAS, approximate the Shortest Remaining Time First (SRTF) scheduling. A common, implicit assumption of these solutions is that the remaining time is only determined by the remaining flow size. However, this assumption does not hold in many real-world scenarios where applications generate data at diverse rates that are smaller than the network capacity. In this paper, we look into this issue from system perspective and find that the operating system (OS) kernel can be exploited to better estimate the remaining time of a flow. In particular, we use the rate of copying data from user space to kernel space to measure the data generation rate. We design RAX, a rate aware flow scheduling method, that calculates the remaining time of a flow more accurately, based on not only the flow size but also the data generation rate. We have implemented a RAX prototype in Linux kernel and evaluated it through testbed experiments and ns-2 simulations. Our testbed results show that RAX reduces FCT by up to 14.9%/41.8% and 7.8%/22.9% over DCTCP and PIAS for all/medium flows respectively.}, 
keywords={Linux;computer centres;computer networks;telecommunication scheduling;transport protocols;FCT;Linux kernel;RAX prototype;SRTF scheduling;cloud applications;commodity data center networks;data generation rate;diverse rates;flow completion times;network capacity;ns-2 simulations;operating system kernel;rate aware flow scheduling method;shortest remaining time first schedulling approximation;Conferences;Hard disks;Kernel;Schedules;Servers;Throughput}, 
doi={10.1109/INFOCOM.2017.8057082}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057083, 
author={Y. Li and W. Gao}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Interconnecting heterogeneous devices in the personal mobile cloud}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Recent diversification of mobile computing devices allows a mobile user to own multiple types of devices for different application scenarios, but also results in various restrictions on the performance and usability of these devices. A viable solution to such restriction is to incorporate and interconnect mobile devices towards a personal mobile cloud where these devices can complement each other via cooperative resource sharing, but is challenging due to the heterogeneity of mobile devices in both hardware and software aspects. In this paper, we propose a novel design of resource sharing framework to address these challenges and generically interconnect heterogeneous mobile devices. Our basic idea is to mask the hardware and software heterogeneity in mobile systems by exploiting the existing mobile OS services as the interface of resource sharing, and further develop the resource sharing framework as a middleware in the mobile OS. We have implemented our design over various mobile platforms with diverse characteristics and resource limits, and demonstrated that our design can efficiently support generic resource sharing among heterogeneous mobile devices without incurring significant system overhead or requiring individual system modification.}, 
keywords={cloud computing;middleware;mobile computing;operating systems (computers);resource allocation;cooperative resource sharing;generically interconnect heterogeneous mobile devices;hardware heterogeneity;heterogeneous devices;interconnect mobile devices;middleware;mobile OS;mobile computing devices;mobile platforms;mobile systems;mobile user;personal mobile cloud;resource sharing framework;software aspects;software heterogeneity;Androids;Hardware;Humanoid robots;Mobile applications;Mobile communication;Mobile handsets;Resource management},
doi={10.1109/INFOCOM.2017.8057083}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057084, 
author={L. Chen and H. Shen}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Considering resource demand misalignments to reduce resource over-provisioning in cloud datacenters}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Previous resource provisioning strategies in cloud datacenters allocate physical resources to virtual machines (VMs) based on the predicted resource utilization pattern of VMs. The pattern for VMs of a job is usually derived from historical utilizations of multiple VMs of the job. We observed that these utilization curves are usually misaligned in time, which would lead to resource over-prediction and hence over-provisioning. Since this resource utilization misalignment problem has not been revealed and studied before, in this paper, we study the VM resource utilization from public datacenter traces to verify the existence of the utilization misalignments. Then, to reduce resource over-provisioning, we propose three VM resource utilization pattern refinement algorithms to improve the original generated pattern by lowering the cap of the pattern, reducing cap provision duration and varying the minimum value of the pattern. These algorithms can be used in any resource provisioning strategy that considers predicted resource utilizations of VMs of a job. We then adopt these refinement algorithms in an initial VM allocation mechanism and test them in trace-driven experiments and real-world cluster experiments. The experimental results show that each improved mechanism can increase resource efficiency up to 74%, and reduce the number of PMs needed to satisfy tenant requests up to 47% while conforming the SLO requirement.}, 
keywords={cloud computing;computer centres;resource allocation;virtual machines;SLO requirement;VM allocation mechanism;VM resource utilization;cap provision duration;cloud datacenters;historical utilizations;physical resources;predicted resource utilization pattern;public datacenter;resource demand misalignments;resource efficiency;resource over-provisioning reduction;resource provisioning strategy;resource utilization misalignment problem;resource utilizations;utilization curves;Clustering algorithms;Correlation;Extraterrestrial measurements;Google;Prediction algorithms;Resource management}, 
doi={10.1109/INFOCOM.2017.8057084}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057085, 
author={J. P. Champati and B. Liang}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Efficient minimization of sum and differential costs on machines with job placement constraints}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={We revisit the problem of assigning n jobs to m machines/servers. We study this problem under more general settings, which capture important aspects of applications that arise in networking and information systems. In particular, we consider jobs that have placement constraints and machines that are heterogeneous. The cost incurred at a machine is given by any general convex function on the number of jobs assigned to it. We aim to minimize the sum cost and the maximum differential cost. Through a network-flow equivalence transformation, we observe how these two objectives are fundamentally related, showing that sum-cost minimization implies maximum-differential-cost minimization. We propose an efficient algorithm termed Maximum Edge-Cost Cycle Cancelling (MEC3) to solve the sum-cost minimization problem with O(n2 m2) time complexity. Furthermore, for applications where only the maximum differential cost is of concern, we further improve the efficiency of MEC3 by proposing an early stop condition. We implement MEC3 and two other algorithms from the literature. Using benchmark input instances, we show that MEC3 has substantially lower run time than the other algorithms.}, 
keywords={computational complexity;convex programming;graph theory;minimisation;scheduling;MEC3;Maximum Edge-Cost Cycle Cancelling;O(n2 m2) time complexity;differential cost minimization;general convex function;job assignment;job placement constraints;maximum-differential-cost minimization;network-flow equivalence transformation;sum-cost minimization problem;Algorithm design and analysis;Benchmark testing;Conferences;Cost function;Information systems;Minimization;Time complexity}, 
doi={10.1109/INFOCOM.2017.8057085}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057086, 
author={W. Jiang and Z. Yin and S. M. Kim and T. He}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Transparent cross-technology communication over data traffic}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Cross-technology communication (CTC) techniques are introduced in recent literatures to explore the opportunities of collaboration between heterogeneous wireless technologies, such as WiFi and ZigBee. Their applications include context-aware services and global channel coordination. However, state-of-the-art CTC schemes either suffer from channel inefficiency, low throughput, or disruption to existing networks. This paper presents the CTC via data packets (DCTC), which takes advantage of abundant existing data packets to construct recognizable energy patterns. DCTC features (i) a significant enhancement in CTC throughput while (ii) keeping transparent to upper layer protocols and applications. Our design also features advanced functions including multiplexing to support concurrent transmissions of multiple DCTC senders and adaptive rate control according to the traffic volume. Testbed implementations across WiFi and ZigBee platforms demonstrate reliable bidirectional communication of over 95% in accuracy while achieving throughput 2.3x of the state of the art. Meanwhile, experiment results show that DCTC has little and bounded impact on the delay and throughput of original data traffic.}, 
keywords={Zigbee;multiplexing;protocols;telecommunication network reliability;telecommunication traffic;wireless LAN;wireless channels;WiFi;ZigBee;adaptive rate control;bidirectional communication;context-aware services;cross-technology communication techniques;data traffic;global channel coordination;heterogeneous wireless technologies;Delays;Protocols;Throughput;Wireless communication;Wireless fidelity;Wireless sensor networks;ZigBee}, 
doi={10.1109/INFOCOM.2017.8057086}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057087, 
author={J. Ren and L. Gao and H. Wang and Z. Wang}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Optimise web browsing on heterogeneous mobile platforms: A machine learning based approach}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Web browsing is an activity that billions of mobile users perform on a daily basis. Battery life is a primary concern to many mobile users who often find their phone has died at most inconvenient times. The heterogeneous multi-core architecture is a solution for energy-efficient processing. However, the current mobile web browsers rely on the operating system to exploit the underlying hardware, which has no knowledge of individual web contents and often leads to poor energy efficiency. This paper describes an automatic approach to render mobile web workloads for performance and energy efficiency. It achieves this by developing a machine learning based approach to predict which processor to use to run the web rendering engine and at what frequencies the processors should operate. Our predictor learns offline from a set of training web workloads. The built predictor is then integrated into the browser to predict the optimal processor configuration at runtime, taking into account the web workload characteristics and the optimisation goal: whether it is load time, energy consumption or a trade-off between them. We evaluate our approach on a representative ARM big.LITTLE mobile architecture using the hottest 500 webpages. Our approach achieves 80% of the performance delivered by an ideal predictor. We obtain, on average, 45%, 63.5% and 81% improvement respectively for load time, energy consumption and the energy delay product, when compared to the Linux heterogeneous multi-processing scheduler.}, 
keywords={Internet;Linux;learning (artificial intelligence);microprocessor chips;mobile computing;multiprocessing systems;online front-ends;power aware computing;search engines;telecommunication power management;telecommunication scheduling;ARM big.LITTLE mobile architecture;Linux heterogeneous multiprocessing scheduler;Web browsing;Web contents;Web workload characteristics;battery life;energy consumption;energy delay product;energy efficiency;energy-efficient processing;heterogeneous mobile platforms;heterogeneous multicore architecture;machine learning;mobile Web browsers;mobile architecture;mobile users;operating system;optimal processor configuration;web rendering engine;Browsers;Energy consumption;Feature extraction;Measurement;Mobile communication;Optimization;Rendering (computer graphics);Energy Optimisation;Mobile Web Browsing;Mobile Workloads;big.LITTLE}, 
doi={10.1109/INFOCOM.2017.8057087}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057088, 
author={Q. Xiao and Y. Zhou and S. Chen}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Better with fewer bits: Improving the performance of cardinality estimation of large data streams}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Cardinality estimation is the task of determining the number of distinct elements (or the cardinality) in a data stream, under a stringent constraint that the input data stream can be scanned by just a single pass. This is a fundamental problem with many practical applications, such as traffic monitoring of high-speed networks and query optimization of Internetscale database. To solve the problem, we propose an algorithm named HLL-TailCut+, which implements the estimation standard error 1.0/√m using the memory units of three bits each, whose cost is much smaller than the five-bit memory units used by HyperLogLog, the best previously known cardinality estimator. This makes it possible to reduce the memory cost of HyperLogLog by 45%. For example, when the target estimation error is 1.1%, state-of-the-art HyperLogLog needs 5.6 kilobytes memory. By contrast, our new algorithm only needs 3 kilobytes memory consumption for attaining the same accuracy. Additionally, our algorithm is able to support the estimation of very large stream cardinalities, even on the Tera and Peta scale.}, 
keywords={data handling;storage management;HyperLogLog;cardinality estimation;data stream;estimation standard error;five-bit memory units;high-speed networks;memory consumption;memory cost;memory size 3.0 KByte;memory size 5.6 KByte;query optimization;stream cardinalities;stringent constraint;target estimation error;word length 3.0 bit;Algorithm design and analysis;Estimation;Google;Memory management;Monitoring;Radiation detectors;Registers}, 
doi={10.1109/INFOCOM.2017.8057088}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057089, 
author={A. Fumo and M. Fiore and R. Stanica}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Joint spatial and temporal classification of mobile traffic demands}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Mobile traffic data collected by network operators is a rich source of information about human habits, and its analysis provides insights relevant to many fields, including urbanism, transportation, sociology and networking. In this paper, we present an original approach to infer both spatial and temporal structures hidden in the mobile demand, via a first-time tailoring of Exploratory Factor Analysis (EFA) techniques to the context of mobile traffic datasets. Casting our approach to the time or space dimensions of such datasets allows solving different problems in mobile traffic analysis, i.e., network activity profiling and land use detection, respectively. Tests with real-world mobile traffic datasets show that, in both its variants above, the proposed approach (i) yields results whose quality matches or exceeds that of state-of-the-art solutions, and (ii) provides additional joint spatiotemporal knowledge that is critical to result interpretation.}, 
keywords={mobile computing;spatiotemporal phenomena;statistical analysis;telecommunication traffic;EFA techniques;Exploratory Factor Analysis techniques;first-time tailoring;joint spatial and temporal classification;land use detection;mobile demand;mobile traffic analysis;mobile traffic data;mobile traffic demands;network activity;original approach;real-world mobile traffic datasets;spatial structures;temporal structures;Conferences;Loading;Mathematical model;Maximum likelihood estimation;Mobile communication;Mobile computing;Sociology}, 
doi={10.1109/INFOCOM.2017.8057089}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057090, 
author={J. Wang and J. Tang and Z. Xu and Y. Wang and G. Xue and X. Zhang and D. Yang}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Spatiotemporal modeling and prediction in cellular networks: A big data enabled deep learning approach}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={In this paper, we propose to leverage the emerging deep learning techniques for spatiotemporal modeling and prediction in cellular networks, based on big system data. First, we perform a preliminary analysis for a big dataset from China Mobile, and use traffic load as an example to show non-zero temporal autocorrelation and non-zero spatial correlation among neighboring Base Stations (BSs), which motivate us to discover both temporal and spatial dependencies in our study. Then we present a hybrid deep learning model for spatiotemporal prediction, which includes a novel autoencoder-based deep model for spatial modeling and Long Short-Term Memory units (LSTMs) for temporal modeling. The autoencoder-based model consists of a Global Stacked AutoEncoder (GSAE) and multiple Local SAEs (LSAEs), which can offer good representations for input data, reduced model size, and support for parallel and application-aware training. Moreover, we present a new algorithm for training the proposed spatial model. We conducted extensive experiments to evaluate the performance of the proposed model using the China Mobile dataset. The results show that the proposed deep model significantly improves prediction accuracy compared to two commonly used baseline methods, ARIMA and SVR. We also present some results to justify effectiveness of the autoencoder-based spatial model.}, 
keywords={cellular radio;learning (artificial intelligence);recurrent neural nets;telecommunication computing;Base Stations;China Mobile dataset;Global Stacked AutoEncoder;LSAE;Long Short-Term Memory units;big data;big system data;cellular networks;deep learning approach;hybrid deep learning model;multiple local SAE;spatial dependencies;spatial model;spatial modeling;spatiotemporal modeling;spatiotemporal prediction;temporal dependencies;temporal modeling;Correlation;Data models;Load modeling;Machine learning;Mobile communication;Predictive models;Spatiotemporal phenomena;Autoencoder;Big Data;Cellular Network;Deep Learning;Recurrent Neural Network;Spatiotemporal Modeling}, 
doi={10.1109/INFOCOM.2017.8057090}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057091, 
author={N. Bartolini and T. He and H. Khamfroush}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Fundamental limits of failure identifiability by boolean network tomography}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Boolean network tomography is a powerful tool to infer the state (working/failed) of individual nodes from path-level measurements obtained by egde-nodes. We consider the problem of optimizing the capability of identifying network failures through the design of monitoring schemes. Finding an optimal solution is NP-hard and a large body of work has been devoted to heuristic approaches providing lower bounds. Unlike previous works, we provide upper bounds on the maximum number of identifiable nodes, given the number of monitoring paths and different constraints on the network topology, the routing scheme, and the maximum path length. The proposed upper bounds represent a fundamental limit on the identifiability of failures via Boolean network tomography. This analysis provides insights on how to design topologies and related monitoring schemes to achieve the maximum identifiability under various network settings. Through analysis and experiments we demonstrate the tightness of the bounds and efficacy of the design insights for engineered as well as real networks.}, 
keywords={Boolean functions;optimisation;telecommunication network topology;NP;NP-hard;boolean network tomography;egde-nodes;failure identifiability;identifiable nodes;maximum identifiability;monitoring paths;monitoring schemes;network failures;network settings;network topology;path-level measurements;Encoding;Monitoring;Network topology;Routing;Testing;Tomography;Upper bound}, 
doi={10.1109/INFOCOM.2017.8057091}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057092, 
author={S. I. Nikolenko and K. Kogan and A. F. Anta}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Network simplification preserving bandwidth and routing capabilities}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={We introduce structural transformations that allow simplifying a given network while preserving its original “bandwidth” and “routing” capabilities, transparently to specific allocations. We minimize a certain objective such as the aggregate capacity of network links, number of nodes, or number of links, in such a way that all the bandwidth that could be routed in the original network can also be routed in the reduced one. This improves cost-efficiency for both inter- and intra-datacenter connections and simplifies network management. We also identify a fundamental tradeoff between extra added capacity and simplicity of representation for a given network. Our analytic results are supported by extensive simulation results on hundreds of real network topologies. One result is that by adding 10-30% extra capacity to evaluated real-world networks one can simplify them down to a star topology with a single switch, while all routing and bandwidth allocation decisions on the simplified topology can be mapped back to the original network. This is an important step towards simplifying network management via a reduced virtualized network infrastructure.},
keywords={bandwidth allocation;computer centres;computer networks;quality of service;telecommunication network management;telecommunication network routing;telecommunication network topology;telecommunication traffic;aggregate capacity;extensive simulation;extra added capacity;given network;intra-datacenter connections;network links;network management;network simplification preserving bandwidth;network topologies;original bandwidth;real-world networks;reduced virtualized network infrastructure;routing capabilities;structural transformations;Bandwidth;Capacity planning;Channel allocation;Conferences;Network topology;Routing;Topology}, 
doi={10.1109/INFOCOM.2017.8057092}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057093, 
author={J. Doncel and S. Aalto and U. Ayesta}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Economies of scale in parallel-server systems}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={We consider a parallel-server system with K homogeneous servers where incoming tasks, arriving at rate λ, are dispatched by n dispatchers. Servers are FCFS queues and dispatchers implement a size-based policy such that the servers are equally loaded. We compare the performance of a system with n> 1 dispatchers and of a system with a single dispatcher. Every dispatcher handles a fraction 1/n of the incoming traffic and balances the load to K/n servers. We show that the performance of a system with n dispatchers, K servers and arrival rate λ coincides with that of a system with one dispatcher, K/n servers and arrival rate λ/n. Therefore, the performance comparison can be interpreted as the economies of scale in a system with one dispatcher when we scale up the number of servers and the arrival rate proportionately. We consider two continuous service time distributions: uniform and Bounded Pareto that have increasing and decreasing failure rates, respectively; and a discrete distribution with two values, which is the distribution that maximizes the variance for a given mean. We show that the performance degradation is small for uniformly distributed job sizes, but that for Bounded Pareto and two points distributions it can be unbounded.}, 
keywords={Pareto distribution;queueing theory;resource allocation;telecommunication traffic;FCFS queues;K homogeneous servers;bounded Pareto distribution;continuous service time distributions;discrete distribution;failure rates;incoming traffic;load balancing;n dispatchers;parallel-server system;single dispatcher;size-based policy;uniform distribution;variance maximization;Conferences;Degradation;Economies of scale;Routing;Servers;Time factors}, 
doi={10.1109/INFOCOM.2017.8057093}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057094, 
author={X. Fu and Z. Xu and Q. Peng and L. Fu and X. Wang}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Complexity vs. optimality: Unraveling source-destination connection in uncertain graphs}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Determination of source-destination connectivity in networks has long been a fundamental problem, where most existing works are based on deterministic graphs that overlook the inherent uncertainty in network links. To overcome such limitation, this paper models the network as an uncertain graph where each edge e exists independently with some probability p(e). The problem examined is that of determining whether a given pair of nodes, a source s and a destination t, are connected by a path or separated by a cut. Assuming that during each determining process we are associated with an underlying graph, the existence of each edge can be unraveled through edge testing at a cost of c(e). Our goal is to find an optimal strategy incurring the minimum expected testing cost with the expectation taken over all possible underlying graphs that form a product distribution. Formulating it into a combinatorial optimization problem, we first characterize the computational complexity of optimally determining source-destination connectivity in uncertain graphs. Specifically, through proving the NP-hardness of two closely related problems, we show that, contrary to its counterpart in deterministic graphs, this problem cannot be solved in polynomial time unless P=NP. Driven by the necessity of designing an exact algorithm, we then apply the Markov Decision Process framework to give a dynamic programming algorithm that derives the optimal strategies. As the exact algorithm may have prohibitive time complexity in practical situations, we further propose two more efficient approximation schemes compromising the optimality. The first one is a simple greedy approach with linear approximation ratio. Interestingly, we show that naive as it is, it has comparable performance than some other seemingly more sophisticated algorithms. Second, by harnessing the sub-modularity of the problem, we further design a more elaborate algorithm with better approximation ratio. The effectiveness of the propos- d algorithms are justified through extensive simulations on three real network datasets, from which we demonstrate that the proposed algorithms yield strategies with smaller expected cost than conventional heuristics.}, 
keywords={Markov processes;approximation theory;computational complexity;dynamic programming;graph theory;greedy algorithms;optimisation;probability;Markov Decision Process framework;approximation scheme;combinatorial optimization problem;computational complexity;deterministic graphs;dynamic programming algorithm;edge testing;exact algorithm;linear approximation ratio;network datasets;network links;prohibitive time complexity;smaller expected cost;source-destination connection;source-destination connectivity;uncertain graph;Algorithm design and analysis;Approximation algorithms;Dynamic programming;Heuristic algorithms;Markov processes;Reliability;Testing}, 
doi={10.1109/INFOCOM.2017.8057094}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057095, 
author={K. Chen and G. Tan}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={SatProbe: Low-energy and fast indoor/outdoor detection based on raw GPS processing}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Indoor-outdoor (IO) detection provides very useful hints for a mobile device to perform context-aware services. To that end, GPS presents a viable solution by relating a device's IO status with its positioning performance, which depends on the device's exposure to the open sky. This approach, however, is prohibitively expensive in terms of energy consumption and response time. Recent work has thus been focused on exploiting low-energy sensors such as light, cellular, and magnetic sensors to infer the IO status indirectly, at the cost of reduced adaptability or explicit user involvement. In this paper, we propose an improving solution to this problem. Our method, called SatProbe, reverts to the GPS approach for its directness and robustness, but avoids its drawback by extracting only the number of visible satellites from the raw GPS data, instead of going through extensive computation to obtain a final position. This metric provides a clear indicator of the IO status, yet can be obtained with great efficiency. Experiments on 79 raw GPS traces with 2595 detection points across a variety of environments show that SatProbe produces higher detection accuracy than previous solutions, with more than an order of magnitude reductions in energy consumption and detection time.}, 
keywords={Global Positioning System;magnetic sensors;mobile computing;ubiquitous computing;IO status;SatProbe;context-aware services;fast indoor/outdoor detection;indoor-outdoor detection;mobile device;raw GPS processing;Correlation;Global Positioning System;Magnetic sensors;Multiaccess communication;Receivers;Satellites}, 
doi={10.1109/INFOCOM.2017.8057095}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057096, 
author={J. Wang and N. Tan and J. Luo and S. J. Pan}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={WOLoc: WiFi-only outdoor localization using crowdsensed hotspot labels}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Given the ever-expanding scale of WiFi deployments in metropolitan areas, we have reached the point where accurate GPS-free outdoor localization becomes possible by relying solely on the WiFi infrastructure. Nevertheless, the existing industrial practices do not seem to have the right implementation to achieve an adequate accuracy, while the academic researches that are mostly attracted by indoor localization have largely neglected this outdoor aspect. In this paper, we propose WOLoc (WiFi-only Outdoor Localization) as a solution that offers meter-level accuracy, by holistically treating the large number of WiFi hotspot labels gather by crowdsensing. On one hand, we do not take these labels as fingerprints as it is almost impossible to extend indoor localization mechanisms by fingerprinting metropolitan areas. On the other hand, we avoid the over-simplified local synthesis methods (e.g., centroid) that significantly lose the information contained in the labels. Instead, we accommodate all the labeled and unlabeled data for a given area using a semi-supervised manifold learning technique, and the output concerning the unlabeled part will become the estimated locations for both users and WiFi hotspots. We conduct extensive experiments with WOLoc in several outdoor areas, and the results have strongly indicated the efficacy of our solution.}, 
keywords={Global Positioning System;learning (artificial intelligence);wireless LAN;GPS-free outdoor localization;WOLoc;WiFi deployments;WiFi hotspot labels;WiFi infrastructure;WiFi-only Outdoor Localization;WiFi-only outdoor localization;crowdsensed hotspot labels;indoor localization mechanisms;meter-level accuracy;metropolitan areas;Conferences;Estimation;Global Positioning System;Manifolds;Roads;Urban areas;Wireless fidelity}, 
doi={10.1109/INFOCOM.2017.8057096}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057097, 
author={R. Margolies and R. Becker and S. Byers and S. Deb and R. Jana and S. Urbanek and C. Volinsky}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Can you find me now? Evaluation of network-based localization in a 4G LTE network}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={User location is of critical importance to cellular network operators. It is often used for network capacity planning and to aid in the analysis of service and network diagnostics. However, existing localization techniques rely on user-provided information (e.g., Angle-of-Arrival), which are not available to the operator, and often require a significant effort to collect training data. Our main contribution is the design and evaluation of the Network-Based Localization (NBL) System for localizing a user in a 4G LTE network. The NBL System consists of 2 stages. In an offline stage, we develop RF coverage maps based on a large-scale crowd-sourced channel measurement campaign. Then, in an online stage, we present a localization algorithm to quickly match RF measurements (which are already collected as part of normal network operation) to coverage map locations. The system is more practical than related works, as it does not make any assumptions about user mobility, nor does it require expensive manual training measurements. Despite the realistic assumptions, our extensive evaluations in a national 4G LTE network show that the NBL System achieves a localization accuracy which is comparable to related works (i.e., a median accuracy of 5% of the cell's coverage region).}, 
keywords={4G mobile communication;Long Term Evolution;cellular radio;mobility management (mobile radio);telecommunication network planning;wireless channels;4G LTE network;RF coverage maps;RF measurements;cellular network operators;crowd-sourced channel measurement;localization techniques;network capacity planning;network-based localization system;user mobility;Area measurement;Cellular networks;Long Term Evolution;Radio frequency;Training;Training data;Localization;crowd-sourcing;wireless networks}, 
doi={10.1109/INFOCOM.2017.8057097}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057098, 
author={M. R. Fida and A. Lutu and M. K. Marina and Ö. Alay}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={ZipWeave: Towards efficient and reliable measurement based mobile coverage maps}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={The accuracy of measurement-driven mobile coverage maps depends on the quality, density and pattern of the signal strength observations. Thus, identifying an efficient measurement data collection methodology is essential, especially when considering the cost associated with the measurement collection approaches (e.g., drive tests, crowd approaches). We propose ZipWeave, a novel measurement data collection and fusion framework for building efficient and reliable measurement-based mobile coverage maps. ZipWeave incorporates a novel nonuniform sampling strategy to achieve reliable coverage maps with reduced sample size. Assuming prior knowledge of the propagation characteristics of the region of interest, we first examine the potential gains of this non-uniform sampling strategy in different cases via a measurement-based statistical analysis methodology; this involves irregular spatial tessellation of the region of interest into sub-regions with internally similar radio propagation characteristics and sampling based on these sub-regions. We then present a practical form of ZipWeave nonuniform sampling strategy that can be used even without any prior information. In all our evaluations, we show that the ZipWeave non-uniform sampling approach reduces the samples by half compared to the common systematic-random sampling, while maintaining similar accuracy. Moreover, we show that the other key feature of ZipWeave to combine high-quality controlled measurements (that present limited geographic footprint similar to drive tests) with crowdsourced measurements (that cover a wider footprint) leads to more reliable mobile coverage maps overall.}, 
keywords={mobile radio;quality control;radiowave propagation;sampling methods;signal sampling;statistical analysis;ZipWeave nonuniform sampling strategy;crowd approaches;crowdsourced measurements;drive tests;fusion framework;high-quality controlled measurements;measurement collection approaches;measurement data collection;measurement data collection methodology;measurement-driven mobile coverage maps;nonuniform sampling strategy;radio propagation characteristics;reduced sample size;reliable measurement;reliable mobile coverage maps;signal strength observations;statistical analysis methodology;systematic-random sampling;Atmospheric measurements;Battery charge measurement;Mobile communication;Monitoring;Particle measurements;Reliability;Urban areas}, 
doi={10.1109/INFOCOM.2017.8057098}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057099, 
author={J. Tan and C. T. Nguyen and X. Wang}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={SilentTalk: Lip reading through ultrasonic sensing on mobile phones}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={The recently enhanced computing capability and rich sensing functionality on mobile devices lead to the ubiquitous application of speech recognition. Traditional speech recognition records acoustic signals or visual images to interpret speech. However, the acoustic based scheme has many drawbacks. It is easily affected by the environmental noise when users are in the factory or market, and can not be used in a place where people need to be quite such as library. Specifically the current design is not suitable for people with speaking or hearing difficulties. Unfortunately, the visual-based approach is sensitive to fight conditions which shows poor performance in the dark area. As a result, it is necessary to provide an new human-computer interaction channel to assist speech recognition. This paper presents SilentTalk, a non-invasive lip reading system based on ultrasonic Doppler effect The main idea is to generate ultrasonic signals from a mobile phone, then capture the reflections and analyze the fine-grained frequency shift caused by mouth movements. A Frequency Shift Detection Model (FSDM) is proposed to quantify the correlation between frequency variations and mouth movements that form different syllables. SilentTalk then applies a Continuous Lip Reading Model (CLRM) on top of FSDM to realize continuous lip reading. Based on Markov assumption, CLRM effectively combines pronunciation rules and context knowledge to connect isolated syllables to words and sentences. Experiments show that SilentTalk can identify 12 basic mouth motions up to 95.4% accuracy in English. The system can also recognize short sentences up to six words with an average accuracy of 74.8%.}, 
keywords={Doppler effect;feature extraction;mobile handsets;speech recognition;Continuous Lip Reading Model;FSDM;Frequency Shift Detection Model;Markov assumption;SilentTalk;acoustic based scheme;environmental noise;fine-grained frequency shift;frequency variations;human-computer interaction channel;mobile devices;mobile phone;mouth movements;noninvasive lip reading system;rich sensing functionality;speaking hearing difficulties;speech recognition;ubiquitous application;ultrasonic Doppler effect;ultrasonic sensing;ultrasonic signals;visual images;visual-based approach;Conferences}, 
doi={10.1109/INFOCOM.2017.8057099}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057100, 
author={Y. L. Wei and H. I. Wu and H. C. Wang and H. M. Tsai and K. C. J. Lin and R. Boubezari and H. Le Minh and Z. Ghassemlooy}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={LiCompass: Extracting orientation from polarized light}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Accurate orientation information is the key in many applications, ranging from map reconstruction with crowdsourcing data, location data analytics, to accurate indoor localization. Many existing solutions rely on noisy magnetic and inertial sensor data, leading to limited accuracy, while others leverage multiple, dense anchor points to improve the accuracy, requiring significant deployment efforts. This paper presents LiCompass, the first system that enables a commodity camera to accurately estimate the object orientation using just a single optical anchor. Our key idea is to allow a camera to observe varying intensity level of polarized light when it is in different orientations and, hence, perform estimation directly from image pixel intensity. As the estimation relies only on pixel intensity, instead of the location of the anchor in an image, the system performs reliably at long distance, with low resolution images, and with large perspective distortion. LiCompass' core designs include an elaborate optical anchor design and a series of signal processing techniques based on trigonometric properties, which extend the range of orientation estimation to full 360 degrees. Our prototype evaluation shows that LiCompass produces very accurate estimates with median errors of merely 2.5 degrees at 5 meters and 7.4 degrees at 2.5 meters with an irradiance angle of 55 degrees.}, 
keywords={cameras;image resolution;image sensors;light polarisation;navigation;LiCompass core design;accurate indoor localization;commodity camera;image pixel intensity;low resolution images;noisy magnetic sensor data;object orientation;optical anchor design;orientation estimation;polarized light;single optical anchor;varying intensity level;Adaptive optics;Cameras;Estimation;Optical distortion;Optical imaging;Optical polarization;Optical receivers}, 
doi={10.1109/INFOCOM.2017.8057100}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057101, 
author={H. Chen and F. Li and Y. Wang}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={EchoTrack: Acoustic device-free hand tracking on smart phones}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={This paper explores the limits of acoustic ranging on smart phone in the scenario of device-free hand tracking. Tracking the hand is challenging since it requires continuously locating the moving hand in the air with fine resolution. Existing work on hand tracking relies on special hardware or requires users hold the mobile device. This paper presents EchoTrack, which continuously locates the hand by leveraging mobile audio hardware advances without special infrastructure supported. EchoTrack measures the distance from the hand to the speaker array embedded in smart phone via the chirp's Time of Flight (TOF). The speaker array and hand yield a unique triangle. The hand can be located with this triangular geometry. The trajectory accuracy can be improved with the method of Doppler shift compensation and trajectory correction (i.e., roughness penalty smoothing method). We implement a prototype on smart phone and the evaluation shows that EchoTrack can achieve tracking accuracy within about three centimeters of 76% and two centimeters of 48%.}, 
keywords={Doppler shift;gesture recognition;human computer interaction;smart phones;Acoustic device-free hand tracking;Doppler shift compensation;EchoTrack;acoustic ranging;mobile audio hardware;mobile device;moving hand;smart phone;speaker array;tracking accuracy;Chirp;Distance measurement;Doppler shift;Hardware;Microphones;Smart phones;Trajectory}, 
doi={10.1109/INFOCOM.2017.8057101}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057102, 
author={H. Xu and Z. Yang and Z. Zhou and K. Yi and C. Peng}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={TUM: Towards ubiquitous multi-device localization for cross-device interaction}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Cross-device interaction is becoming an increasingly hot topic as we often have multiple devices at our immediate disposal in this era of mobile computing. Various cross-device applications such as file sharing, multi-screen display, and cross-device authentication have been proposed and investigated. However, one of the most fundamental enablers remains unsolved: How to achieve ubiquitous multi-device localization? Though pioneer efforts have resorted to gesture-assisted or sensing-assisted localization, they either require extensive user participation or impose some strong assumptions on device sensing abilities. This introduces extra costs and constraints, and thus degrades their practicality. To overcome these limitations, we propose TUM, an acoustic-assisted localization scheme Towards Ubiquitous Multi-device localization. The basic idea of TUM is to utilize the dual-microphones and speakers to obtain distance cues among devices. At the same time it resolves the location ambiguity with the help of MEMS sensors. We devise techniques for distance constraint extraction, static localization, continuous localization, and multi-device localization, and build a prototype that runs on commodity devices. Extensive experiments show that TUM provides a real-time 3D relative localization service under 10cm mean error for both static and continuous localization.}, 
keywords={array signal processing;microphones;mobile computing;peer-to-peer computing;sensors;ubiquitous computing;wireless sensor networks;MEMS sensors;TUM;acoustic-assisted localization scheme;continuous localization;cross-device authentication;cross-device interaction;device sensing abilities;distance constraint extraction;dual-microphones;multidevice localization;static localization;ubiquitous multidevice localization;Acoustics;Clocks;Distance measurement;Microphones;Sensors;Smart devices;Three-dimensional displays}, 
doi={10.1109/INFOCOM.2017.8057102}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057103, 
author={W. Jiang and J. Wu}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Active opinion-formation in online social networks}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Recommendation systems usually try to “guess” a user's preferences from the system's view. We study another side of recommendation: active opinion-formation from the perspective of the user. In real life, a user's opinion evolves with time and refines when new evidence occurs. Then, how does an online user form his/her own opinion actively in large social networks? The problem has three challenges: the factor, the effect and the open environment. To address those challenges, we investigate: (1) what factors or channels a user will consider, (2) how those channels will take effect, and (3) an incremental approach to incorporate multiple channels. We explore three types of channels: the internal opinion of an individual user, influences from trusted friends, and influences from public channels. A novel simulator, OpinionFormer, is proposed to incorporate those channels incrementally. It differentiates the effects of friends and public channels as well as positive and negative opinions. We validate the performance of OpinionFormer by predicting users' opinions using real-world data sets. Experimental results show that our model can improve accuracy over other models that ignore some channels or that neglect the evolving features.}, 
keywords={recommender systems;social networking (online);OpinionFormer simulator;active opinion-formation;friends;individual user;internal opinion;negative opinions;online social networks;online user;positive opinions;public channels;recommendation systems;user preferences;Conferences;Containers;Frequency modulation;Open systems;Social network services;Temperature measurement;fluid dynamics;internal opinion;opinion formation;public channel;trusted friend}, 
doi={10.1109/INFOCOM.2017.8057103}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057104, 
author={A. Zhiyuli and X. Liang and Z. Xu}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Learning distributed representations for large-scale dynamic social networks}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Learning distributed representations of symbolic data were introduced by Hinton[1], and first developed in modeling networks for learning the node vectors by Perozzi et al (2014). In this work, we proposed Dnps, a novel nodes embedding approach for acquiring distributed representations of large-scale dynamic social networks. Dnps is suitable for many types of social networks: dynamic/static, directed/undirected, and weighted/unweighted. Recently, several works of nodes embedding were proposed. However, they were designed for static networks, such as language networks. To address this problem, first, we develop a damping based positive sampling (DpS) algorithm to learn the hierarchical structure of social networks. Then, we devise a local search based DpS algorithm to obtain incremental information of network evolution. Finally, we show Dnps's potentials on future link prediction task for three real-life large-scale dynamic social networks. The results show that Dnps consistently outperforms all baseline methods and exhibits an improvement of 12%, 6%, 4% on Digg, Flickr and YouTube over the second-highest level, respectively. Moreover, Dnps is also scalable. For example, Dnps can speed up the training process in 2 ~ 36 times compared with benchmarks on Flickr network. The source codes of the project is available online1.}, 
keywords={computational complexity;graph theory;learning (artificial intelligence);search problems;social networking (online);speech processing;statistical distributions;Digg;Dnps;DpS algorithm;Flickr network;YouTube;damping based positive sampling algorithm;distributed representations;language networks;large-scale dynamic social networks;network evolution;node embedding;node vectors;static networks;symbolic data;Algorithm design and analysis;Conferences;Damping;Heuristic algorithms;Prediction algorithms;Social network services;Training}, 
doi={10.1109/INFOCOM.2017.8057104}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057105, 
author={W. Chen and C. G. Brinton and D. Cao and M. Chiang}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Behavior in social learning networks: Early detection for online short-courses}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={We study learning outcome prediction for online courses. Whereas prior work has focused on semester-long courses with frequent student assessments, we focus on short-courses that have single outcomes assigned by instructors at the end. The lack of performance data makes the behavior of learners, captured as they interact with course content and with one another in Social Learning Networks (SLN), essential for prediction. Our method defines several (machine) learning features based on behaviors collected on the modes of (human) learning in a course, and uses them in appropriate classifiers. Through evaluation on data captured from three two-week courses hosted through our delivery platforms, we make three key observations: (i) behavioral data is predictive of learning outcomes in short-courses (our classifiers achieving AUCs ≥ 0.8 after the two weeks), (ii) it has an early detection capability (AUCs ≥ 0.7 with the first week of data), and (iii) the content features have an “earliest” detection capability (with higher AUC in the first few days), while the SLN features become the more predictive set over time, as the network matures. We also discuss how our method can generate behavioral analytics for instructors.}, 
keywords={computer aided instruction;distance learning;educational courses;learning (artificial intelligence);pattern classification;social networking (online);behavioral analytics;behavioral data;course content;early detection capability;online short-courses;outcome prediction;performance data;semester-long courses;social learning networks;student assessment;two-week courses;Computational modeling;Conferences;Data models;Discussion forums;Feature extraction;Programmable logic arrays;Training}, 
doi={10.1109/INFOCOM.2017.8057105}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057106, 
author={G. Liu and Q. Chen and Q. Yang and B. Zhu and H. Wang and W. Wang}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={OpinionWalk: An efficient solution to massive trust assessment in online social networks}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Massive trust assessment (MTA) in an Online Social Network (OSN), i.e., computing the trustworthiness of all users in the network, is crucial in various OSN-related applications. Existing solutions are either too slow or inaccurate in addressing the MTA problem. We propose the OpinionWalk algorithm that accurately and efficiently conducts MTA in an OSN. OpinionWalk models trust by the Dirichlet distribution and uses a matrix to represent the direct trust relations among users. From the perspective of a user, other users' trustworthiness are stored in a column vector that is iteratively updated when the algorithm “walks” through the network, in a breadth-first search manner. We identify the overlapping subproblems property in MTA and prove OpinionWalk is a more efficient solution. The accuracy and execution time of OpinionWalk are evaluated and compared to benchmark algorithms including EigenTrust, TrustRank, MoleTrust, TidalTrust and AssessTrust, using two real-world datasets (Advogato and Pretty Good Privacy). Experimental results indicate that OpinionWalk is an efficient and accurate solution to MTA, compared to previous algorithms.}, 
keywords={data privacy;security of data;social networking (online);trusted computing;Advogato;AssessTrust;Dirichlet distribution;EigenTrust;MTA problem;MoleTrust;OSN;OSN-related applications;OpinionWalk algorithm;OpinionWalk models trust;Pretty Good Privacy;TidalTrust;TrustRank;benchmark algorithms;breadth-first search;column vector;direct trust relations;massive trust assessment;online social networks;Algorithm design and analysis;Computational modeling;Conferences;Network topology;Social network services;Statistical distributions;Time complexity;Massive trust assessments;online social networks;threes-valued subjective logic;trust model}, 
doi={10.1109/INFOCOM.2017.8057106}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057107, 
author={Z. Yin and W. Jiang and S. M. Kim and T. He}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={C-Morse: Cross-technology communication with transparent Morse coding}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Recent research on CTC (cross-technology communication) demonstrates the viability of direct coordination among heterogeneous devices (e.g., WiFi and ZigBee) with incompatible physical layers. Although encouraging, current solutions suffer from either severe inefficiency in channel utilization or low throughput using limited beacons. To address these limitations, this paper presents C-Morse, which leverages all traffic (such as through data packets, beacons and other control frames) to achieve a high cross-technology communication throughput. The key idea of C-Morse is to slightly perturb the transmission timing of existing WiFi packets to construct recognizable radio energy patterns without introducing noticeable delays to upper layers. At the receiver side, ZigBee captures such patterns by sensing the RSSI value, and then decodes the transmitted symbols. C-Morse also introduces a novel timing-based multiplexing technique to allow the coexistence of multiple C-Morse access points and reject other interference, showing a reliable symbol delivery ratio. As a result, C-Morse achieves a free side-channel, whose CTC throughput is as much as 9 χ of the present state of the art, while maintaining the through traffic within a negligible delay that goes unnoticed by applications and end-users.}, 
keywords={Zigbee;channel coding;decoding;interference suppression;multiplexing;radiofrequency interference;telecommunication traffic;wireless LAN;wireless channels;wireless sensor networks;CTC;RSSI;WiFi packets;ZigBee;channel utilization;cross-technology communication;data packets;decoding;free side-channel;heterogeneous devices;interference;multiple C-Morse access points;radio energy pattern;recognizable radio energy patterns;symbol delivery ratio;telecommunication traffic;timing-based multiplexing technique;transparent Morse coding;Delays;Throughput;Wireless communication;Wireless fidelity;Wireless sensor networks;ZigBee}, 
doi={10.1109/INFOCOM.2017.8057107}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057108, 
author={X. Guo and X. Zheng and Y. He}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={WiZig: Cross-technology energy communication over a noisy channel}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={The proliferation of loT applications drives the need of ubiquitous connections among heterogeneous wireless devices. Cross-Technology Communication (CTC) is a significant technique to directly exchange information among heterogeneous devices that follow different standards. By exploiting a side-channel like frequency, amplitude or temporal modulation, the existing works enable CTC but have limited performance under channel noise. In this paper, we propose WiZig, a novel CTC technique that employs modulation techniques in both the amplitude and temporal dimensions to optimize the throughput over a noisy channel. We establish a theoretical model of the energy communication channel to clearly understand the channel capacity. We then devise an online rate adaptation algorithm to adjust the modulation strategy according to the channel condition. Based on the theoretical model, WiZig can accurately control the number of encoded energy amplitudes and the length of a receiving window, so as to optimize the CTC throughput. We implement a prototype of WiZig on a software radio platform and a commercial ZigBee device. The evaluation show that WiZig achieves a throughput of 153.85 bps with less than 1 % symbol error rate in a real environment. The results demonstrate that WiZig realizes efficient and reliable CTC under varied channel conditions.}, 
keywords={Zigbee;channel capacity;error statistics;modulation;software radio;telecommunication power management;wireless channels;CTC throughput;Cross-technology energy communication;WiZig;channel capacity;channel condition;channel noise;commercial ZigBee device;encoded energy amplitudes;energy communication channel;heterogeneous devices;heterogeneous wireless devices;loT applications;modulation strategy;modulation techniques;noisy channel;novel CTC technique;online rate adaptation algorithm;symbol error rate;temporal dimensions;temporal modulation;theoretical model;ubiquitous connections;varied channel conditions;Error analysis;Modulation;Noise measurement;Receivers;Throughput;Wireless communication;Wireless sensor networks}, 
doi={10.1109/INFOCOM.2017.8057108}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057109, 
author={Z. Chi and Z. Huang and Y. Yao and T. Xie and H. Sun and T. Zhu}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={EMF: Embedding multiple flows of information in existing traffic for concurrent communication among heterogeneous IoT devices}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={The exponentially increasing number of IoT devices makes the unlicensed industrial, scientific, and medical (ISM) radio bands (e.g., 2.4 GHz) extremely crowded. Currently, there is no efficient solution to coordinate the large amount heterogeneous IoT devices that have different communication technologies (e.g., WiFi and ZigBee). To fill this gap, in this paper, we introduce embedded multiple flows (EMF) communication method, which (i) embeds different pieces of information in existing traffic and (ii)concurrently sends out these information from one IoT sender to multiple IoT receivers that have a different communication technology from the sender. By doing this, our EMF method (i) enables cross-technology communication among heterogeneous IoT devices, (ii) does not introduce any extra control traffic, and (iii) is transparent to the higher layer applications. Our approach is implemented on USRPs and commercial off-the-shelf (COTS) ZigBee devices. We also conducted extensive experiments to evaluate our approach in real-world settings. The evaluation results show that EMF's throughput is more than 14 times higher than the latest cross-technology communication technique (i.e. FreeBee[1]).}, 
keywords={Internet of Things;Zigbee;embedded systems;EMF method;Embedding multiple flows;ISM bands;IoT sender;USRP;commercial off-the-shelf ZigBee devices;concurrent communication;cross-technology communication technique;embedded multiple flows communication method;frequency 2.4 GHz;heterogeneous IoT devices;multiple IoT receivers;unlicensed industrial scientific and medical radio bands;Bit error rate;Logic gates;Modulation;Receivers;Throughput;Wireless fidelity;ZigBee}, 
doi={10.1109/INFOCOM.2017.8057109}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057110, 
author={T. Osuki and K. Sakai and S. Fukumoto}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Contact avoidance routing in delay tolerant networks}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Delay tolerant networks (DTNs) are widely adopted to many network applications, such as disaster recovery and battlefield communications. Such critical network scenarios call for an outright prevention mechanism against contact-based attacks, e.g., blackmailing a legitimate user to compromise sensitive information at a contact. To the best of our knowledge, there is no work on secure routing protocol against contact-based attacks in DTNs. Therefore, in this paper, we first formulate the problem of contact avoidance routing, in which the node holding a message tries to avoid having a contact with an adversary. By applying the phase-type distribution, we build the secure opportunistic path model, which integrates the delivery probability within the deadline and the safety of opportunistic paths. Then, we propose a contact avoidance routing (CAR) protocol to securely deliver a message to its destination against the contact-based compromise attack. In addition, we further propose an adaptive CAR (A-CAR) to accommodate complicated network scenarios, where the capabilities of adversaries are parameterized. The extensive simulations using real traces as well as random graphs demonstrate that the proposed CAR and A-CAR protocols achieve their design goals.}, 
keywords={computer network security;delay tolerant networks;probability;routing protocols;DTNs;contact avoidance routing protocol;contact-based attacks;critical network scenarios;delay tolerant networks;network applications;outright prevention mechanism;phase-type distribution;secure opportunistic path model;secure routing protocol;Adaptation models;Automobiles;Delays;Routing;Routing protocols;Security;Contact avoidance routing;DTNs;delay tolerant networks;network security}, 
doi={10.1109/INFOCOM.2017.8057110}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057111, 
author={H. Jin and L. Su and K. Nahrstedt}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={CENTURION: Incentivizing multi-requester mobile crowd sensing}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={The recent proliferation of increasingly capable mobile devices has given rise to mobile crowd sensing (MCS) systems that outsource the collection of sensory data to a crowd of participating workers that carry various mobile devices. Aware of the paramount importance of effectively incentivizing participation in such systems, the research community has proposed a wide variety of incentive mechanisms. However, different from most of these existing mechanisms which assume the existence of only one data requester, we consider MCS systems with multiple data requesters, which are actually more common in practice. Specifically, our incentive mechanism is based on double auction, and is able to stimulate the participation of both data requesters and workers. In real practice, the incentive mechanism is typically not an isolated module, but interacts with the data aggregation mechanism that aggregates workers' data. For this reason, we propose CENTURION, a novel integrated framework for multi-requester MCS systems, consisting of the aforementioned incentive and data aggregation mechanism. CENTURION's incentive mechanism satisfies truthfulness, individual rationality, computational efficiency, as well as guaranteeing non-negative social welfare, and its data aggregation mechanism generates highly accurate aggregated results. The desirable properties of CENTURION are validated through both theoretical analysis and extensive simulations.}, 
keywords={data aggregation;mobile computing;sensor fusion;CENTURION incentive mechanism;data aggregation mechanism;mobile crowd sensing systems;mobile devices;multirequester MCS systems;multirequester mobile crowd sensing systems;sensory data collection;Conferences;Data aggregation;Mobile communication;Mobile handsets;Reliability;Sensors}, 
doi={10.1109/INFOCOM.2017.8057111}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057112, 
author={C. Huang and D. Wang and S. Zhu}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Where are you from: Home location profiling of crowd sensors from noisy and sparse crowdsourcing data}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Crowdsourcing has emerged as an important data collection paradigm in participatory and human-centric sensing applications. While many crowdsourcing studies focus on sensing and recovering the status of the physical world, this paper investigates the problem of profiling the crowd sensors (i.e., humans). In particular, we study the problem of accurately inferring the home locations of people from the noisy and sparse crowdsourcing data they contribute. In this study, we propose a semi-supervised framework, Where Are You From (WAYF), to accurately infer the home locations of people by explicitly exploring the localness of people and the dependency between people based on their check-in behaviors under a rigorous analytical framework. We perform extensive experiments to evaluate the performance of our scheme and compared it to the state-of-the-art techniques using three real world data traces collected from Foursquare. The results showed the effectiveness of our scheme in accurately profiling the home locations of people.}, 
keywords={mobile computing;sensor fusion;social networking (online);Foursquare;Home location profiling;WAYF;crowd sensors;data collection;home locations;human-centric sensing;noisy crowdsourcing data;participatory;sparse crowdsourcing data;where are you from;Conferences;Crowdsourcing;Manganese;Noise measurement;Sensors;Social network services;Urban areas;Crowdsourcing;Home Location Profiling;Location Based Social Networks (LBSN)}, 
doi={10.1109/INFOCOM.2017.8057112}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057113, 
author={A. Chakraborty and M. S. Rahman and H. Gupta and S. R. Das}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={SpecSense: Crowdsensing for efficient querying of spectrum occupancy}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={We describe an end-to-end platform called SpecSense to support large scale spectrum monitoring. SpecSense crowdsources spectrum monitoring to low-cost, low-power commodity SDR/embedded platforms and provides necessary analytics support in a central spectrum server. In this work, we describe SpecSense and address specific challenges related to accurately estimate spectrum occupancy on demand with low overhead. To address the accuracy question, we augment state-of-the-art spatial interpolation techniques to accommodate scenarios where RF propagation characteristics change across space. To address the overhead question, we solve the sensor selection problem to select the minimum number of spectrum sensors that can best estimate the spectrum at the requested locations.}, 
keywords={cognitive radio;crowdsourcing;embedded systems;interpolation;query processing;radio spectrum management;software radio;telecommunication computing;RF propagation characteristics;SpecSense;central spectrum server;efficient querying;embedded platforms;end-to-end platform;low-cost low-power commodity SDR;sensor selection problem;spatial interpolation techniques;spectrum monitoring;spectrum occupancy estimation;spectrum sensors;Conferences;Crowdsourcing;Interpolation;Monitoring;RF signals;Radio frequency;Sensors}, 
doi={10.1109/INFOCOM.2017.8057113}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057114, 
author={C. Miao and L. Su and W. Jiang and Y. Li and M. Tian}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={A lightweight privacy-preserving truth discovery framework for mobile crowd sensing systems}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={The recent proliferation of human-carried mobile devices has given rise to the mobile crowd sensing (MCS) systems. However, the sensory data provided by the participating workers are usually not reliable. As an efficient technique to extract truthful information from unreliable data, truth discovery has drawn significant attention. Currently, the privacy concern of the participating workers poses a major challenge on the design of truth discovery mechanisms. Although the existing mechanism can conduct truth discovery with high accuracy and strong privacy guarantee, tremendous overhead is incurred on the worker side. In this paper, we propose a novel lightweight privacy preserving truth discovery framework, L-PPTD, which is implemented by involving two non-colluding cloud platforms and adopting additively homomorphic cryptosystem. This framework not only achieves the protection of each worker's sensory data and reliability information but also introduces little overhead to the workers. In order to further reduce each worker's overhead in the scenarios where only the sensory data need to be protected, we propose another more lightweight framework named L2-PPTD. The desirable performance of the proposed frameworks is verified through extensive experiments conducted on real world MCS systems.}, 
keywords={cloud computing;data privacy;mobile computing;homomorphic cryptosystem;human-carried mobile devices;lightweight privacy-preserving truth discovery framework;mobile crowd sensing systems;noncolluding cloud platforms;participating workers;privacy concern;reliability information;sensory data;strong privacy guarantee;truth discovery mechanisms;unreliable data;worker side;world MCS systems;Cryptography;Mobile handsets;Privacy;Reliability;Sensors;Zinc}, 
doi={10.1109/INFOCOM.2017.8057114}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057115, 
author={J. Li and Y. Zhu and J. Yu and C. Long and G. Xue and S. Qian}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Online auction for IaaS clouds: Towards elastic user demands and weighted heterogeneous VMs}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Auctions have been adopted by many major cloud providers, such as Amazon EC2. Unfortunately, only simple auctions have been implemented. Such simple auction has serious limitations, such as being unable to accept elastic user demands and having to allocate different types of VMs independently. These limitations create a big gap between the real needs of cloud users and the available services of cloud providers. In response to the limitations of the existing auction mechanisms, this paper proposes a novel online auction mechanism for IaaS clouds, with the unique features of an elastic model for inputting time-varying user demands and a unified model for requesting heterogeneous VMs together. However, several major challenges should be addressed, such as NP hardness of optimal VM allocation, time-varying user demands and potential misreports of private information of cloud users. We propose a truthful online auction mechanism for maximizing the profit of the cloud provider in IaaS clouds, which is composed of a price-based allocation rule and a payment rule. In the allocation rule, the online auction mechanism determines the number of VMs of each type to each user. In the payment rule, by introducing a marginal price function for each type of VMs, the mechanism determines how much the cloud provider should charge each cloud user. With solid theoretical analysis and trace-driven simulations, we demonstrate that our mechanism is truthful and individually rational, and has a polynomial-time complexity.}, 
keywords={cloud computing;computational complexity;electronic commerce;pricing;resource allocation;virtual machines;IaaS clouds;cloud provider;cloud user;elastic user demands;marginal price function;online auctions;payment rule;price-based allocation rule;time-varying user demands;unified model;weighted heterogeneous VM;Cloud computing;Computational modeling;Cost accounting;Pricing;Resource management;Servers;Upper bound;IaaS Clouds;elastic user demands;online auctions;profit maximization;time-varying;weighted heterogeneous VMs}, 
doi={10.1109/INFOCOM.2017.8057115}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057116, 
author={H. Tan and Z. Han and X. Y. Li and F. C. M. Lau}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Online job dispatching and scheduling in edge-clouds}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={In edge-cloud computing, a set of edge servers are deployed near the mobile devices such that these devices can offload jobs to the servers with low latency. One fundamental and critical problem in edge-cloud systems is how to dispatch and schedule the jobs so that the job response time (defined as the interval between the release of a job and the arrival of the computation result at its device) is minimized. In this paper, we propose a general model for this problem, where the jobs are generated in arbitrary order and times at the mobile devices and offloaded to servers with both upload and download delays. Our goal is to minimize the total weighted response time over all the jobs. The weight is set based on how latency sensitive the job is. We derive the first online job dispatching and scheduling algorithm in edge-clouds, called OnDisc, which is scalable in the speed augmentation model; that is, OnDisc is (1 + ε)-speed O(1/ε)-competitive for any constant ε ϵ (0,1). Moreover, OnDisc can be easily implemented in distributed systems. Extensive simulations on a real-world data-trace from Google show that OnDisc can reduce the total weighted response time dramatically compared with heuristic algorithms.}, 
keywords={cloud computing;mobile computing;telecommunication scheduling;OnDisc;distributed systems;download delays;edge servers;edge-cloud computing;edge-cloud systems;job response time;mobile devices;online job dispatching;scheduling algorithm;speed augmentation model;total weighted response time;upload delays;Algorithm design and analysis;Cloud computing;Dispatching;Mobile communication;Mobile handsets;Servers;Time factors}, 
doi={10.1109/INFOCOM.2017.8057116}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057117, 
author={R. Zhu and D. Niu and Z. Li}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Robust web service recommendation via quantile matrix factorization}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={We study the problem of personalized Quality of Service (QoS) estimation for web services. State-of-the-art methods use matrix factorization or collaborative prediction to estimate web service response times and throughput for each user based on partial measurements collected from past invocations. We point out that in reality, both the response times and through-put of web services follow highly skewed distributions. In this case, the conditional mean QoS estimates generated by traditional matrix completion approaches can be heavily biased toward a few outliers, leading to poor web service recommendation performance. In this paper, we propose the Quantile Matrix Factorization (QMF) technique for web service recommendation by introducing quantile regression into the matrix factorization framework. We propose a novel and efficient algorithm based on Iterative Reweighted Least Squares (IRLS) to solve the QMF problem involving a non-smooth objective function. We further extend the proposed QMF approach to take into account user and service side attributes. Extensive evaluation based on a large-scale QoS dataset has shown that our schemes significantly outperform various state-of-the-art web service QoS estimation schemes in terms of personalized recommendation performance.}, 
keywords={Web services;iterative methods;least squares approximations;matrix decomposition;quality of service;recommender systems;regression analysis;IRLS;QMF problem;Quantile Matrix Factorization technique;conditional mean QoS estimation;iterative reweighted least squares;matrix completion approaches;nonsmooth objective function;quality of service estimation;quantile regression;robust web service recommendation;service side attribute;throughput estimation;user side attribute;web service response times;Collaboration;Estimation;Measurement;Quality of service;Throughput;Time factors;Web services}, 
doi={10.1109/INFOCOM.2017.8057117}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057118, 
author={X. Zhang and C. Wu and Z. Li and F. C. M. Lau}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Proactive VNF provisioning with multi-timescale cloud resources: Fusing online learning and online optimization}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Network Function Virtualization (NFV) represents a new paradigm of network service provisioning. NFV providers acquire cloud resources, install virtual network functions (VNFs), assemble VNF service chains for customer usage, and dynamically scale VNF deployment against input traffic fluctuations. While existing literature on VNF scaling mostly adopts a reactive approach, we target a proactive approach that is more practical given the time overhead for VNF deployment. We aim to effectively estimate upcoming traffic rates and adjust VNF deployment a priori, for flow service quality assurance and resource cost minimization. We adapt online learning techniques for predicting future service chain workloads. We further combine the online learning method with a multi-timescale online optimization algorithm for VNF scaling, through minimization of the regret due to inaccurate demand prediction and minimization of the cost incurred by sub-optimal online decisions in a joint online optimization framework. The resulting proactive online VNF provisioning algorithm achieves a good performance guarantee, as shown by both theoretical analysis and simulation under realistic settings.}, 
keywords={cloud computing;learning (artificial intelligence);optimisation;telecommunication traffic;virtualisation;NFV;Network Function Virtualization;VNF scaling;dynamically scale VNF deployment;flow service quality assurance;future service chain workloads;input traffic fluctuations;joint online optimization framework;multitimescale cloud resources;network service provisioning;online learning method;online learning techniques;proactive approach;proactive online VNF provisioning algorithm;reactive approach;resource cost minimization;sub-optimal online decisions;traffic rates;Algorithm design and analysis;Cloud computing;Dynamic scheduling;Hardware;Minimization;Optimization;Prediction algorithms}, 
doi={10.1109/INFOCOM.2017.8057118}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057119, 
author={S. Sharifian and F. Lin and R. Safavi-Naini}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Secret key agreement using a virtual wiretap channel}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Key agreement using physical layer properties of communication channels is a well studied problem. iJam is a physical layer key agreement protocol that achieves security by creating a “virtual” wiretap channel for the adversary through a subprotocol between the sender and the receiver that uses self-jamming by the receiver. The protocol was implemented and its security was shown through extensive experiments. The self-jamming subprotocol of iJam was later modelled as a wiretap channel and used for designing a secure message transmission protocol with provable security. We use the same wiretap model of the subprotocol to design secret key agreement protocols with provable security. We propose two protocols that use the wiretap channel once from Alice to Bob, and a protocol that uses two wiretap channels, one from Alice to Bob, and one in the opposite direction. We provide security proof and efficiency analysis for the protocols. The protocols effectively give physical layer security protocols that can be implemented and have provable security. We discuss our results and propose directions for future research.}, 
keywords={cryptographic protocols;jamming;telecommunication security;communication channels;iJam;physical layer key agreement protocol;physical layer properties;physical layer security protocols;provable security;secret key agreement protocols;secure message transmission protocol;security proof;self-jamming subprotocol;virtual wiretap channel;wiretap model;Jamming;Physical layer;Protocols;Random variables;Receivers;Security;Wireless communication}, 
doi={10.1109/INFOCOM.2017.8057119}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057120, 
author={P. Wang and R. Safavi-Naini}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Interactive message transmission over adversarial wiretap channel II}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={In Wyner wiretap II model of communication, Alice and Bob are connected by a channel that can be eavesdropped by an adversary with unlimited computation who can select a fraction of communication to view, and the goal is to provide perfect information theoretic security. Information theoretic security is increasingly important because of the threat of quantum computers that can effectively break algorithms and protocols that are used in today's public key infrastructure. We consider interactive protocols for wiretap II channel with active adversary who can eavesdrop and add adversarial noise to the eavesdropped part of the codeword. These channels capture wireless setting where malicious eavesdroppers at reception distance of the transmitter can eavesdrop the communication and introduce jamming signal to the channel. We derive a new upperbound R ≤ 1 - ρ for the rate of interactive protocols over two-way wiretap II channel with active adversaries, and construct a perfectly secure protocol family with achievable rate 1 - 2ρ + ρ2. This is strictly higher than the rate of the best one round protocol which is 1 - 2ρ, hence showing that interaction improves rate. We also prove that even with interaction, reliable communication is possible only if ρ <; 1/2. An interesting aspect of this work is that our bounds will also hold in network setting when two nodes are connected by n paths, a ρ of which is corrupted by the adversary. We discuss our results, give their relations to the other works, and propose directions for future work.}, 
keywords={cryptographic protocols;jamming;telecommunication security;wireless channels;Wyner wiretap II model;active adversary;adversarial noise;adversarial wiretap channel II;interactive message transmission;interactive protocols;jamming signal;perfect information theoretic security;perfectly secure protocol family;public key infrastructure;quantum computers;two-way wiretap II channel;wireless setting;Conferences;Mobile communication;Protocols;Random variables;Reliability;Security;Upper bound}, 
doi={10.1109/INFOCOM.2017.8057120}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057121, 
author={K. Jiang and T. Jing and Z. Li and Y. Huo and F. Zhang}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Analysis of secrecy performance in fading multiple access wiretap channel with SIC receiver}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Recently, a new paradigm of multiple access (MAC) along with one eavesdropper to achieve secrecy transmissions has been getting in focus. However, all existing work on such multiple access wiretap channel (MAC-WT) mainly concentrates on the secrecy performance of the system as a whole from an information theoretic perspective. In this work, we investigate the secrecy performance of a single transmitter in the quasi-static Rayleigh fading MAC-WT on basis of two decoding methods, zero-forcing (ZF) and minimum mean-square error (MMSE), jointly with successive interference cancellation (SIC). We evaluate the secrecy performance in three metrics: positive secrecy capacity probability, secrecy outage probability and effective secrecy throughput. The analytical and simulation results show that, 1) the SIC order has great impacts on the secrecy performance for both methods; 2) MMSE-SIC outperforms ZF-SIC, while the performance gap can be overcome via adjusting SIC order, or increasing SNR, or enhancing the spatial diversity gain; 3) in high SNR regime, the secrecy performance is only determined by the relative distance to eavesdropper over legitimate receiver rather than the SNR.}, 
keywords={Rayleigh channels;channel capacity;decoding;fading channels;least mean squares methods;multi-access systems;radio receivers;telecommunication security;wireless channels;SIC;effective secrecy throughput;multiple access wiretap channel;positive secrecy capacity probability;quasistatic Rayleigh fading MAC-WT;secrecy outage probability;secrecy performance;secrecy transmissions;Decoding;Fading channels;Interference;Receivers;Signal to noise ratio;Silicon carbide;Transmitters}, 
doi={10.1109/INFOCOM.2017.8057121}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057122, 
author={M. Bradbury and A. Jhumka}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Understanding source location privacy protocols in sensor networks via perturbation of time series}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Source location privacy (SLP) is becoming an important property for a large class of security-critical wireless sensor network applications such as monitoring and tracking. Much of the previous work on SLP has focused on the development of various protocols to enhance the level of SLP imparted to the network, under various attacker models and other conditions. Other work has focused on analysing the level of SLP being imparted by a specific protocol. In this paper, we adopt a different approach where we model the attacker movement as a time series and use information theoretic concepts to infer the properties of a routing protocol that imparts high levels of SLP. We propose the notion of a properly competing path that causes an attacker to “stall” when moving towards the source. This concept provides the basis for developing a perturbation model, similar to those in privacy-preserving data mining. We then show how to use properly competing paths to develop properties of an SLP-aware routing protocol. Further, we show how different SLP-aware routing protocols can be obtained through different instantiations of the framework. Those instantiations are obtained based on a notion of information loss achieved through the use of the perturbation model proposed.}, 
keywords={data mining;data privacy;routing protocols;telecommunication security;time series;wireless sensor networks;SLP-aware routing protocol;attacker movement;information theoretic concepts;perturbation model;privacy-preserving data mining;properly-competing path;security-critical wireless sensor network applications;source location privacy protocols;time series;Phantoms;Privacy;Routing;Routing protocols;Time series analysis;Wireless sensor networks;Entropy;Mutual Information;Source Location Privacy;Time Series;Wireless Sensor Networks}, 
doi={10.1109/INFOCOM.2017.8057122}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057123, 
author={L. Zheng and C. Joe-Wong and J. Chen and C. G. Brinton and C. W. Tan and M. Chiang}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Economic viability of a virtual ISP}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Growing mobile data usage has led to end users paying substantial data costs, while Internet service providers (ISPs) struggle to upgrade their networks to keep up with demand and maintain high quality-of-service (QoS). This problem is particularly severe for smaller ISPs with less capital. Instead of simply upgrading their network infrastructure, ISPs can pool their networks to provide a good QoS and attract more users. Such a vISP (virtual ISP), for example, Google's Project Fi, allows users to access any of its partner ISPs' networks. We provide the first systematic analysis of a vISP's economic impact, showing that the vISP provides a viable solution for smaller ISPs attempting to attract more users, but may not maintain a positive profit if users' data demands evolve. To do so, we consider users' decisions of whether to defect from their current ISP to the vISP, as well as ISPs' decisions on whether to partner with the vISP. We derive the vISP's dependence on user behavior and partner ISPs: users with very light or very heavy usage are the most likely to defect, while ISPs with heavy-usage customers can benefit from declining to partner with the vISP. Our analytical results are verified with extensive numerical simulations.}, 
keywords={Internet;microeconomics;mobile radio;quality of service;Google Project Fi;ISP decisions;ISP networks;Internet service providers struggle;QoS;current ISP;economic viability;end users;mobile data usage;network infrastructure;quality-of-service;virtual ISP;Economics;Mobile communication;Pricing;Quality of service;Switches;Throughput;Wireless fidelity}, 
doi={10.1109/INFOCOM.2017.8057123}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057124, 
author={D. Mitra and Q. Wang and A. Hong}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Emerging internet content and service providers' relationships: Models and analyses of engineering, business and policy impact}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={We study engineering and business relationships between Content Providers and broadband access ISPs in various organizational and policy environments. We focus on pricing and capacity decisions for bandwidth and caches for delivery of the CP's content over the “last mile” of the ISP's infrastructure. We model the CP-ISP interaction by the Stackelberg “leader-follower” game and the Integrated Operations model. We consider cases where premium bandwidth is offered to subscribers of the CP's service over the last mile, and cases where this is prohibited by Net Neutrality regulations. We develop a uniform solution procedure for all four resulting models. We explore the connections between optimal bandwidth and cache deployments, and, together with fees, their impact on the number of users, and related business and policy topics. We show that the decrease in profitability of caching due to Net Neutrality regulations is greater than the decrease from Integrated Operations to the Stackelberg game. In the Stackelberg game we prove that if a certain condition is satisfied, then with Net Neutrality the ISP will increase the cache price so that it is unprofitable for the CP to use caches. Moreover, this condition is satisfied in a typical case studied in detail.}, 
keywords={Internet;game theory;pricing;CP content;CP service;CP-ISP interaction;Integrated Operations model;Internet content providers;Internet service providers;Net Neutrality regulations;Stackelberg leader-follower game;broadband access ISP;cache deployments;cache price;capacity decisions;optimal bandwidth;policy impact;pricing decisions;Artificial neural networks;Bandwidth;Business;Elasticity;Internet;Network neutrality}, 
doi={10.1109/INFOCOM.2017.8057124}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057125, 
author={J. Z. F. Pang and H. Fu and W. I. Lee and A. Wierman}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={The efficiency of open access in platforms for networked cournot markets}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={This paper studies how the efficiency of an online platform is impacted by the degree to which access of platform participants is open or controlled. The study is motivated by an emerging trend within platforms to impose increasingly fine-grained control over the options available to platform participants. While early online platforms allowed open access, e.g., Ebay allows any seller to interact with any buyer; modern platforms often impose matches directly, e.g., Uber directly matches drivers to riders. This control is performed with the goal of achieving more efficient market outcomes. However, the results in this paper highlight that imposing matches may create new strategic incentives that lead to increased inefficiency. In particular, in the context of networked Cournot competition, we prove that open access platforms guarantee social welfare within 7/16 of the optimal; whereas controlled allocation platforms can have social welfare unboundedly worse than optimal.}, 
keywords={Internet;controlled allocation platforms;fine-grained control;networked cournot markets;online platform;open access platforms;platform participants;Companies;Conferences;Economics;Market research;Open Access;Production;Resource management}, 
doi={10.1109/INFOCOM.2017.8057125}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057126, 
author={D. X. Mendes and E. de Souza e Silva and D. Menasché and R. Leão and D. Towsley}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={An experimental reality check on the scaling laws of swarming systems}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Swarming systems, such as BitTorrent, are one of the most common solutions for scalable, robust and inexpensive content distribution. Although the service capacity of swarming systems has been studied for decades through modeling and analysis, there is a lack of experimental evidence about how the throughput of such systems behaves in under-provisioned regimes. The aim of this paper is to fill this gap. In this paper, we consider a closed-loop model to assess the throughput of peer-to-peer systems. Then, we show through controlled experiments using BitTorrent clients that some analytical findings recently reported in the literature, such as the missing piece syndrome, occur in practice. In particular, we indicate that when seeds have a small effective service capacity, or when seeds are intermittent, the throughput saturates as the population size grows. Finally, we discuss the implications of such findings on the modeling and design of swarming systems.}, 
keywords={peer-to-peer computing;BitTorrent clients;closed-loop model;content distribution;missing piece syndrome;modeling analysis;peer-to-peer systems;scaling laws;swarming systems;Analytical models;Computational modeling;Peer-to-peer computing;Scalability;Sociology;Statistics;Throughput}, 
doi={10.1109/INFOCOM.2017.8057126}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057127, 
author={C. H. Lin and Y. T. Chen and K. C. J. Lin and W. T. Chen}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={acPad: Enhancing channel utilization for 802.11ac using packet padding}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Multi-User Multiple Input Multiple Output (MU-MIMO) enables a multi-antenna access point (AP) to serve multiple users simultaneously, and has been adopted as the IEEE 802.11ac standard. While several PHY-MAC designs have recently been proposed to improve the throughput performance of a MU-MIMO WLAN, they, however, usually assume that all the concurrent streams are of roughly equal length. In reality, users usually have frames with heterogeneous lengths even after aggregation, leading to different lengths of transmission time. Hence, the concurrent transmission opportunities might not always be fully utilized when some streams finish earlier than the others in a transmission opportunity (TXOP). To resolve this inefficiency, this paper presents acPad, a PHY-MAC design that adds additional frames to fill up the idle channel time and better utilize the spatial multiplexing gain. Our acPad identifies proper users as the padding so as to improve the padding gain, while preventing this padding from harming all the ongoing streams. Our evaluation via large-scale trace-driven simulations demonstrates that acPad improves the throughput by up to 2.83×, or by 1.36× on average, as compared to the conventional 802.11ac.}, 
keywords={MIMO communication;access protocols;antenna arrays;multiplexing;wireless LAN;wireless channels;IEEE 802.11ac standard;MU-MIMO WLAN;MultiUser Multiple Input Multiple Output;PHY-MAC design;acPad;channel utilization;multiantenna access point;packet padding;spatial multiplexing gain;Array signal processing;Interference;MIMO;Precoding;Signal to noise ratio;Standards;Throughput}, 
doi={10.1109/INFOCOM.2017.8057127}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057128, 
author={S. Kim and J. Yi and Y. Son and S. Yoo and S. Choi}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Quiet ACK: ACK transmit power control in IEEE 802.11 WLANs}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={With increasing demand for wireless connectivity, IEEE 802.11 WLANs have become ubiquitous and continue to grow in number. This leads to high density of basic service sets with the significant co-channel interference (CCI) among them. This paper sheds light on the CCI caused by 802.11 MAC ACK frames, which has been less studied than the CCI caused by data frames. Based on stochastic geometry analysis, we propose Quiet ACK (QACK), a dynamic transmit power control algorithm for ACK frames. Fine-grained transmit power adjustment is enabled by CCI detection and CCI power estimation in the middle of a data frame reception. Our prototype using software-defined radio shows the feasibility and performance gain of QACK, i.e., 1.5x higher throughput than the legacy 802.11 WLANs. The performance of QACK is further evaluated in more general WLAN environments via extensive simulations using ns-3.}, 
keywords={access protocols;cochannel interference;power control;wireless LAN;ACK transmit power control;CCI detection;IEEE 802.11 WLAN;MAC ACK frames;QACK;Quiet ACK;co-channel interference;data frame reception;dynamic transmit power control algorithm;fine-grained transmit power adjustment;software-defined radio;wireless connectivity;Heuristic algorithms;IEEE 802.11 Standard;Interference;Power control;Power system reliability;Probability;Wireless LAN}, 
doi={10.1109/INFOCOM.2017.8057128}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057129, 
author={G. Lee and Y. Shin and J. Koo and J. Choi and S. Choi}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={ACT-AP: ACTivator access point for multicast over WLAN}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Multicast is a major solution in supporting the explosive growth of the wireless video traffic demand. Also, power saving is a key technology to extend battery life of mobile devices. To meet both purposes, IEEE 802.11 wireless local area network (WLAN) supports power save mode (PSM) for station (STA) while receiving multicast packets. According to recent studies, off-the-shelf chipsets configured to use PSM show un-desired functions, thus resulting in many multicast packet losses. From our extensive measurement, we also verify degradation of multicast performance with widely-used off-the-shelf chipsets using PSM, and present previously-unknown undesired functions. Without modification of the chipsets, STA in PSM cannot enjoy reliable multicast service. Given this, we develop a practical and readily-applicable AP-side solution, called ACT-AP, which avoids multicast packet losses by preventing STA from operating in PSM. Our prototype implementation with off-the-shelf chipsets demonstrates that ACT-AP improves packet delivery ratio by up to 216% with little additional protocol overhead. To our best knowledge, ACT-AP is the first practical effort to support multicast to real devices with undesired functions in PSM.}, 
keywords={mobile radio;multicast communication;protocols;telecommunication power management;telecommunication traffic;wireless LAN;ACT-AP;ACTivator access point;IEEE 802.11 wireless local area network;PSM;WLAN;mobile devices;multicast packet losses;packet delivery ratio;power save mode;readily-applicable AP-side;reliable multicast service;wireless video traffic demand;IEEE 802.11 Standard;Packet loss;Power system management;Semiconductor device measurement;Unicast;Wireless LAN}, 
doi={10.1109/INFOCOM.2017.8057129}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057130, 
author={R. K. Sheshadri and D. Koutsonikolas}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={On packet loss rates in modern 802.11 networks}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={The knowledge of link packet loss rates (PLRs) at different PHY layer configurations is vital for a number of wireless network optimization schemes. However, the very large number of PHY layer configurations offered by modern 802.11 n/ac networks has made probing-based PLR estimation at each available configuration extremely challenging. In this paper, we seek to answer the question “How to estimate the PLRs at each available PHY layer configuration with minimal overhead?” Our analysis of the PLR datasets collected from three 802.11 n/ac testbeds reveals that, for any given link, there are several configurations with similar PLR. However, capturing this similarity using well-known link quality indicators like RSSI, or PHY layer features such as MCS or number of MIMO streams is hard. Consequently, we explore the approach of clustering the available PHY layer configurations into a small number of clusters with similar PLR, independent of any other parameter, and only probe one representative configuration in each cluster. Using two real-world case studies - rate adaptation and multihop routing, we show that the proposed clustering-based PLR estimation helps network optimization schemes to reach optimal configurations faster leading to significant performance improvements.}, 
keywords={optimisation;telecommunication network routing;wireless LAN;802.11 n/ac networks;PHY layer configurations;PHY layer features;PLR estimation;link packet loss rates;link quality indicators;multihop routing;rate adaptation;Bit rate;Estimation;IEEE 802.11n Standard;MIMO;Probes}, 
doi={10.1109/INFOCOM.2017.8057130}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057131, 
author={A. Marcone and M. Pierobon and M. Magarini}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={A parity check analog decoder for molecular communication based on biological circuits}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Molecular Communication (MC) is an enabling paradigm for the interconnection of future devices and networks in the biological environment, with applications ranging from bio-medicine to environmental monitoring and control. The engineering of biological circuits, which allows to manipulate the molecular information processing abilities of biological cells, is a candidate technology for the realization of MC-enabled devices. In this paper, inspired by recent studies favoring the efficiency of analog computation over digital in biological cells, an analog decoder design is proposed based on biological circuit components. In particular, this decoder computes the a-posteriori log-likelihood ratio of parity-check-encoded bits from a binary-modulated concentration of molecules. The proposed design implements the required L-value and the box-plus operations entirely in the biochemical domain by using activation and repression of gene expression, and reactions of molecular species. Each component of the circuit is designed and tuned in this paper by comparing the resulting functionality with that of the corresponding analytical expression. Despite evident differences with classical electronics, biochemical simulation data of the resulting biological circuit demonstrate very close performance in terms of Mean Squared Error (MSE) and Bit Error Rate (BER), and validate the proposed approach for the future realization of MC components.},
keywords={computational complexity;decoding;error statistics;mean square error methods;molecular communication (telecommunication);parity check codes;BER;L-value;MC components;MSE;a-posteriori log-likelihood ratio;analog computation;binary-modulated concentration;biochemical simulation data;biological cells;biological circuits;biological environment;biomedicine;bit error rate;box-plus operations;environmental monitoring;gene expression;mean squared error;molecular communication;molecular information processing abilities;molecular species;parity check analog decoder;Biological information theory;Cells (biology);Decoding;Mathematical model;Proteins;Receivers;Transmitters}, 
doi={10.1109/INFOCOM.2017.8057131}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057132, 
author={S. Basagni and V. Di Valerio and P. Gjanci and C. Petrioli}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Finding MARLIN: Exploiting multi-modal communications for reliable and low-latency underwater networking}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={This paper concerns the smart exploitation of multimodal communication capabilities of underwater nodes to enable reliable and swift underwater networking. To contrast adverse and highly varying channel conditions we define a smart framework enabling nodes to acquire knowledge on the quality of the communication to neighboring nodes over time. Following a model-based reinforcement learning approach, our framework allows senders to select the best forwarding relay for its data jointly with the best communication device to reach that relay. We name the resulting forwarding method MARLIN, for MultimodAl Reinforcement Learning-based RoutINg. Applications can choose whether to seek reliable routes to the destination, or whether faster packet delivery is more desirable. We evaluate the performance of MARLIN in varying networking scenarios where nodes communicate through two acoustic modems with widely different characteristics. MARLIN is compared to state-of-the-art forwarding protocols, including a channel-aware solution, a machine learning-based solution and to a flooding protocol extended to use multiple modems. Our results show that a smartly learned selection of relay and modem is key to obtain a packet delivery ratio that is twice as much that of other protocols, while maintaining low latencies and energy consumption.}, 
keywords={learning (artificial intelligence);marine communication;modems;radio networks;routing protocols;MARLIN;MultimodAl Reinforcement Learning-based RoutINg;acoustic modems;channel conditions;channel-aware solution;communication device;faster packet delivery;flooding protocol;forwarding method;forwarding relay;machine learning;multimodal communications;multiple modems;neighboring nodes;networking scenarios;packet delivery ratio;reinforcement learning approach;reliable networking;reliable routes;smart framework;smartly learned selection;state-of-the-art forwarding protocols;swift underwater networking;underwater nodes;Computer network reliability;Modems;Protocols;Quality of service;Relays;Reliability;Routing;Underwater Wireless Sensor Networks;multimodal communications;reinforcement learning-based routing}, 
doi={10.1109/INFOCOM.2017.8057132}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057133, 
author={G. E. Santagati and T. Melodia}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={An implantable low-power ultrasonic platform for the Internet of Medical Things}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Wirelessly networked systems of implantable medical devices endowed with sensors and actuators will be the basis of many innovative, sometimes revolutionary therapies. The biggest obstacle in realizing this vision of networked implantable devices is posed by the dielectric nature of the human body, which strongly attenuates radio-frequency (RF) electromagnetic waves. In this paper we present the first hardware and software architecture of an Internet of Medical Things (IoMT) platform with ultrasonic connectivity for intra-body communications that can be used as a basis for building future IoT-ready medical implantable and wearable devices. We show that ultrasonic waves can be efficiently generated and received with low-power and mm-sized components, and that despite the conversion loss introduced by ultrasonic transducers the gap in attenuation between 2.4 GHz RF and ultrasonic waves is still substantial, e.g., ultrasounds offer 70 dB less attenuation over 10 cm. We show that the proposed IoMT platform requires much lower transmission power compared to 2.4 GHz RF with equal reliability in tissues, e.g., 35 dBm lower over 12 cm for 10-3 Bit Error Rate (BEr) leading to lower energy per bit and longer device lifetime. Finally, we show experimentally that 2.4 GHz RF links are not functional at all above 12 cm, while ultrasonic links achieve a reliability of 10-6 up to 20 cm with less than 0 dBm transmission power.}, 
keywords={Internet of Things;biomedical communication;biomedical ultrasonics;error statistics;low-power electronics;prosthetics;ultrasonic transducers;Internet of Medical Things platform;IoMT platform;RF links;actuators;attenuation;dielectric nature;frequency 2.4 GHz;human body;implantable medical devices;intrabody communications;longer device lifetime;low-power ultrasonic platform;lower transmission power;mm-sized components;networked implantable devices;noise figure 70.0 dB;radio-frequency electromagnetic waves;revolutionary therapies;sensors;size 10.0 cm;size 12.0 cm;size 20.0 cm;software architecture;transmission power;ultrasonic connectivity;ultrasonic links;ultrasonic transducers the gap;ultrasonic waves;wearable devices;wirelessly networked systems;Acoustics;Field programmable gate arrays;Hardware;Implants;Radio frequency;Sensors;Wireless communication}, 
doi={10.1109/INFOCOM.2017.8057133}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057134, 
author={Z. Zhang and P. Kumar}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={mEEC: A novel error estimation code with multi-dimensional feature}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Error estimation code estimates the bit error ratio of the received data bits with low overhead. It has many applications, especially in estimating the number of errors in a packet transmitted over a wireless link. In this paper, we propose a novel error estimation code, mEEC, that outperforms the existing code by more than 10%-20% depending on the packet sizes, at the same time being less biased. mEEC is mainly based on the idea of grouping multiple blocks of sampled data bits into a super-block, thus creating a multi-dimensional feature. It then compresses these features into a single number, called the color, as the coded bits. Through an intelligent coloring scheme, the blocks in a super-block share the cost of covering low probability events, which allows the decoder to recover the actual feature values from the color even in the presence of error. mEEC also adopts a lightweight redistribution step, which is guided by the solution of an optimization problem and further reduces the estimation errors and bias. We also show that mEEC can be implemented with reasonable storage sizes and low time complexity.}, 
keywords={error correction codes;estimation theory;bit error ratio;coded bits;data bits;error estimation code;estimation errors;mEEC;multidimensional feature;Conferences}, 
doi={10.1109/INFOCOM.2017.8057134}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057135, 
author={H. Pan and G. Xie and Z. Li and P. He and L. Mathy}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={FlowConvertor: Enabling portability of SDN applications}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Software-Defined Networking (SDN) provides network administrators opportunities to control network devices more simply and easily than in traditional networking. However, heterogeneity in switch hardware, especially in forwarding pipeline architecture, renders the task of network application developers and network administrators tedious, by hampering portability across switch models. In this paper, we propose FlowConvertor, an algorithm capable of converting rules from any forwarding pipeline to any other different forwarding pipeline, as long as both pipelines offer compatible operations. More precisely, FlowConvertor is an online algorithm that operates on flow updates issued to the origin pipeline and computes the corresponding updates for the target pipeline in real time. Performance evaluation shows that the latency introduced by FlowConvertor on the path between the SDN controller and the target switch is of the order of 1ms in most cases, and is thus acceptable for practical deployment.}, 
keywords={computer network management;pipelines;software defined networking;FlowConvertor;SDN applications;SDN controller;Software-Defined Networking;flow updates;forwarding pipeline architecture;network administrators opportunities;network application developers;network devices;online algorithm;origin pipeline;switch hardware;switch models;target pipeline;target switch;time 1.0 ms;traditional networking;Engines;Hardware;Metadata;Pipeline processing;Pipelines;Ports (Computers);Switches}, 
doi={10.1109/INFOCOM.2017.8057135}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057136, 
author={K. Poularakis and G. Iosifidis and G. Smaragdakis and L. Tassiulas}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={One step at a time: Optimizing SDN upgrades in ISP networks}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Nowadays, there is a fast-paced shift from legacy telecommunication systems to novel Software Defined Network (SDN) architectures that can support on-the-fly network reconfiguration, therefore, empowering advanced traffic engineering mechanisms. Despite this momentum, migration to SDN cannot be realized at once especially in high-end cost networks of Internet Service Providers (ISPs). It is expected that ISPs will gradually upgrade their networks to SDN over a period that spans several years. In this paper, we study the SDN upgrading problem in an ISP network: which nodes to upgrade and when. We consider a general model that captures different migration costs and network topologies, and two plausible ISP objectives; first, the maximization of the traffic that traverses at least one SDN node, and second, the maximization of the number of dynamically selectable routing paths enabled by SDN nodes. We leverage the theory of submodular and supermodular functions to devise algorithms with provable approximation ratios for each objective. Using real-world network topologies and traffic matrices, we evaluate the performance of our algorithms and show up to 54% gains over state-of-the-art methods. Moreover, we describe the interplay between the two objectives; maximizing one may cause a factor of 2 loss to the other.}, 
keywords={Internet;optimisation;software defined networking;telecommunication network routing;telecommunication network topology;telecommunication traffic;ISP network;Internet Service Providers;Software Defined Network architectures;advanced traffic engineering mechanisms;network topologies;on-the-fly network reconfiguration;telecommunication systems;traffic maximization;Approximation algorithms;Conferences;Network topology;Optimization;Routing;Routing protocols}, 
doi={10.1109/INFOCOM.2017.8057136}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057137, 
author={H. Wang and A. Srivastava and L. Xu and S. Hong and G. Gu}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Bring your own controller: Enabling tenant-defined SDN apps in IaaS clouds}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={The need of customized network functions for enterprises in Infrastructure-as-a-Service (IaaS) clouds is emerging. However, existing network functions in IaaS clouds are very limited, inflexible, and hard to control by the tenants. Recently, the introduction of Software-Defined Networking (SDN) technology brings the hope of flexible control of network flows and creation of diverse network functions. Unfortunately, enterprises lose access to the SDN controller when they move to clouds. Moreover, the cloud SDN controller is only managed by the provider administrators for security and performance reasons. To allow enterprise tenants to develop and deploy their own SDN apps in the cloud, in this paper, we introduce a new cloud usage paradigm: Bring Your Own Controller (BYOC). BYOC offers each tenant an individual SDN controller, where tenants can deploy SDN apps to manage their network. To manage these tenant SDN controllers, we propose BYOC-Visor, a new SDN-based virtualization platform. BYOC-VISOR addresses several security and performance challenges which are specific to IaaS clouds. We show that BYOC-Visor supports different controller platforms and diverse SDN security applications such as firewall, IDS, and access control. We implement a prototype system and the performance evaluation results show that our system has low overhead.}, 
keywords={Bring Your Own Device;cloud computing;computer network management;computer network security;software defined networking;virtualisation;BYOC;BYOC-Visor;Bring Your Own Controller;IaaS clouds;SDN-based virtualization platform;Software-Defined Networking technology;cloud SDN controller;cloud usage paradigm;customized network functions;diverse SDN security applications;diverse network functions;enterprise tenants;flexible network flow control;individual SDN controller;infrastructure-as-a-service clouds;network management;performance evaluation;prototype system;tenant SDN controllers;tenant-defined SDN apps;Cloud computing;Control systems;Network topology;Prototypes;Security;Topology;Virtualization}, 
doi={10.1109/INFOCOM.2017.8057137}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057138, 
author={H. Mekky and F. Hao and S. Mukherjee and T. V. Lakshman and Z. L. Zhang}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Network function virtualization enablement within SDN data plane}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Software Defined Networking (SDN) can benefit a Network Function Virtualization solution by chaining a set of network functions (NF) to create a network service. Currently, control on NFs is isolated from the SDN, which creates routing inflexibility, flow imbalance and choke points in the network as the controller remains oblivious to the number, capacity and placement of NFs. Moreover, a NF may modify packets in the middle, which makes flow identification at a SDN switch challenging. In this paper, we postulate native NFs within the SDN data plane, where the same logical controller controls both network services and routing. This is enabled by extending SDN to support stateful flow handling based on higher layers in the packet beyond layers 2-4. As a result, NF instances can be chained on demand, directly on the data plane. We present an implementation of this architecture based on Open vSwitch, and show that it enables popular NFs effectively using detailed evaluation and comparison with other alternative solutions.}, 
keywords={computer networks;software defined networking;virtualisation;SDN data plane;SDN switch;Software Defined Networking;flow identification;flow imbalance;logical controller;network function virtualization;network functions;network service;network services;routing inflexibility;Inductors;Kernel;Noise measurement;Routing;Switches}, 
doi={10.1109/INFOCOM.2017.8057138}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057139, 
author={C. Q. Wu}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Bandwidth scheduling in overlay networks with linear capacity constraints}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={An increasing number of high-performance networks are built over the existing IP network infrastructure to provision dedicated channels for big data transfer. The links in these overlay networks correspond to underlying paths and may share lower-level link segments. We consider a model of overlay networks that incorporates correlated link capacities and linear capacity constraints (LCCs) to formulate such shared bottleneck components. The overlay links are typically shared by multiple users through advance reservations, resulting in varying bandwidth availability in future time. Therefore, efficient bandwidth scheduling algorithms are needed to improve the network resource utilization and also meet the user's transport requirements. We investigate two advance scheduling problems in overlay networks with LCCs: Fixed-Bandwidth Path and Varying-Bandwidth Path, with the objective to minimize the data transfer end time for a given data size. We prove that both problems are NP-complete and non-approximable, and propose heuristic algorithms using a gradual relaxation procedure on the maximum number of links from each LCC allowed for path computation. The performance superiority of these heuristics is verified by extensive simulation results in comparison with optimal and greedy strategies.}, 
keywords={Big Data;IP networks;computational complexity;data communication;greedy algorithms;minimisation;overlay networks;telecommunication channels;telecommunication links;telecommunication scheduling;Big Data transfer;Fixed-Bandwidth Path;IP network infrastructure;LCC;NP-complete;Varying-Bandwidth Path;advance scheduling problems;bandwidth scheduling algorithms;data transfer end time minimisation;gradual relaxation procedure;greedy strategy;heuristic algorithms;high-performance networks;linear capacity constraints;link capacities;lower-level link segments;network resource utilization;overlay links;overlay networks;Approximation algorithms;Bandwidth;Data transfer;IP networks;Overlay networks;Processor scheduling;Scheduling;approximate algorithm;bandwidth scheduling;overlay networks}, 
doi={10.1109/INFOCOM.2017.8057139}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057140, 
author={P. Rahimzadeh and C. Joe-Wong and K. Shin and Y. Im and J. Lee and S. Ha}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={SVC-TChain: Incentivizing good behavior in layered P2P video streaming}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Video streaming applications based on Peer-to-Peer (P2P) systems are popular for their scalability, which is hard to achieve with traditional client-server approaches. In particular, layered video streaming has been much-studied due to its ability to differentiate users' streaming qualities in heterogeneous user environments. Previous work, however, has shown that user misbehavior (e.g., free-riding and protocol deviation) poses a serious threat to P2P systems that are not equipped with proper incentive mechanisms. We propose a method to disincentivize such misbehavior. Our SVC-TChain is a layered P2P video streaming method based on scalable video coding (SVC), which uses the recently proposed T-Chain incentive mechanism to discourage free-riding. After introducing T-Chain, we present the first analytical framework to study SVC piece selection with multiple video layers, using it to efficiently choose SVC-TChain's optimal piece selection parameters and thus discourage deviations from the piece selection policy. Extensive experimental results show that SVC-TChain outperforms layered extensions of BiTos and Give-to-Get, two popular P2P video streaming approaches, both in the absence of user misbehavior and when some users misbehave.}, 
keywords={peer-to-peer computing;video coding;video streaming;P2P video streaming approaches;Peer-to-Peer systems;SVC piece selection;T-Chain incentive mechanism;free-riding;heterogeneous user environments;incentive mechanisms;layered P2P video streaming method;layered video streaming;multiple video layers;particular video streaming;piece selection policy;protocol deviation;scalable video coding;traditional client-server approaches;user misbehavior;video streaming applications;Encryption;Peer-to-peer computing;Standards;Static VAr compensators;Streaming media;Video coding}, 
doi={10.1109/INFOCOM.2017.8057140}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057141, 
author={O. Bilgen and A. B. Wagner}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={A new stable peer-to-peer protocol with non-persistent peers}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Recent studies have suggested that the stability of peer-to-peer networks may rely on persistent peers, who dwell on the network after they obtain the entire file. In the absence of such peers, one piece becomes extremely rare in the network, which leads to instability. Technological developments, however, are poised to reduce the incidence of persistent peers, giving rise to a need for a protocol that guarantees stability with nonpersistent peers. We propose a novel peer-to-peer protocol, the group suppression protocol, to ensure the stability of peer-to-peer networks under the scenario that all the peers adopt non-persistent behavior. Using a suitable Lyapunov potential function, the group suppression protocol is proven to be stable when the file is broken into two pieces, and detailed experiments demonstrate the stability of the protocol for arbitrary number of pieces. Straightforward incorporation of the group suppression protocol into BitTorrent while retaining most of BitTorrent's core mechanisms is also presented. Subsequent simulations show that under certain assumptions, BitTorrent with the official protocol cannot escape from the missing piece syndrome, but BitTorrent with group suppression does.}, 
keywords={Lyapunov methods;peer-to-peer computing;protocols;BitTorrent;Lyapunov potential function;group suppression protocol;nonpersistent peers;peer-to-peer network stability;peer-to-peer protocol;persistent peers;Conferences;Mobile communication;Peer-to-peer computing;Protocols;Servers;Stability analysis;Wireless communication}, 
doi={10.1109/INFOCOM.2017.8057141}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057142, 
author={B. Spang and A. Sabnis and R. Sitaraman and D. Towsley and B. DeCleene}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={MON: Mission-optimized overlay networks}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Large organizations often have users in multiple sites which are connected over the Internet. Since resources are limited, communication between these sites needs to be carefully orchestrated for the most benefit to the organization. We present a Mission-optimized Overlay Network (MON), a hybrid overlay network architecture for maximizing utility to the organization. We combine an offline and an online system to solve non-concave utility maximization problems. The offline tier, the Predictive Flow Optimizer (PFO), creates plans for routing traffic using a model of network conditions. The online tier, MONtra, is aware of the precise local network conditions and is able to react quickly to problems within the network. Either tier alone is insufficient. The PFO may take too long to react to network changes. MONtra only has local information and cannot optimize non-concave mission utilities. However, by combining the two systems, MON is robust and achieves near-optimal utility under a wide range of network conditions. While best-effort overlay networks are well studied, our work is the first to design overlays that are optimized for mission utility.}, 
keywords={Internet;optimisation;overlay networks;peer-to-peer computing;telecommunication network routing;telecommunication traffic;Internet;MON;MONtra online tier;PFO offline tier;Predictive Flow Optimizer;hybrid overlay network architecture;local network conditions;mission-optimized overlay networks;near-optimal utility;nonconcave mission utilities optimization;nonconcave utility maximization problems;precise local network conditions;routing traffic;Computer architecture;Internet telephony;Optimization;Organizations;Overlay networks;Routing}, 
doi={10.1109/INFOCOM.2017.8057142}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057143, 
author={L. Toka and B. Lajtha and É. Hosszu and B. Formanek and D. Géhberger and J. Tapolcai}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={A resource-aware and time-critical IoT framework}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Internet of Things (IoT) systems produce great amount of data, but usually have insufficient resources to process them in the edge. Several time-critical IoT scenarios have emerged and created a challenge of supporting low latency applications. At the same time cloud computing became a success in delivering computing as a service at affordable price with great scalability and high reliability. We propose an intelligent resource allocation system that optimally selects the important IoT data streams to transfer to the cloud for processing. The optimization runs on utility functions computed by predictor algorithms that forecast future events with some probabilistic confidence based on a dynamically recalculated data model. We investigate ways of reducing specifically the upload bandwidth of IoT video streams and propose techniques to compute the corresponding utility functions. We built a prototype for a smart squash court and simulated multiple courts to measure the efficiency of dynamic allocation of network and cloud resources for event detection during squash games. By continuously adapting to the observed system state and maximizing the expected quality of detection within the resource constraints our system can save up to 70% of the resources compared to the naive solution.}, 
keywords={Internet of Things;cloud computing;resource allocation;sport;video streaming;Internet of Things;IoT data streams;IoT video streams;cloud computing;dynamic cloud resource allocation;dynamic network resource allocation;dynamically recalculated data model;intelligent resource allocation system;resource-aware time-critical IoT scenarios;smart squash court;upload bandwidth;utility functions;Bandwidth;Cameras;Cloud computing;Quality of service;Resource management;Streaming media;Uplink;Internet of Things;QoE;QoS;adaptive;cloud computing;cloud control;dynamic;resource provisioning}, 
doi={10.1109/INFOCOM.2017.8057143}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057144, 
author={D. Trihinas and G. Pallis and M. D. Dikaiakos}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={ADMin: Adaptive monitoring dissemination for the Internet of Things}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={As more knowledge is vastly added to the devices fuelling the Internet of Things (IoT) energy efficiency and real-time data processing are great challenges that must be tackled. In this paper, we introduce ADMin, a low-cost IoT framework that reduces on device energy consumption and the volume of data disseminated across the network. This is achieved by efficiently adapting the rate at which IoT devices disseminate monitoring streams based on run-time knowledge of the stream evolution, variability and seasonal behavior. Rather than transmitting the entire stream, ADMin favors sending updates for its estimation model from which values can be inferred, triggering dissemination only when shifts in the stream evolution are detected. Results on real-life testbeds, show that ADMin is able to reduce energy consumption by at least 83%, data volume by 71%, shift detection delays by 61% while maintaining accuracy above 91% in comparison to other IoT frameworks.}, 
keywords={Internet of Things;data dissemination;energy consumption;telecommunication power management;ADMin;Internet of Things energy efficiency;IoT devices;IoT frameworks;adaptive monitoring dissemination;data volume;device energy consumption reduction;low-cost IoT framework;real-time data processing;run-time knowledge;triggering dissemination;Adaptation models;Energy consumption;Estimation;Measurement;Monitoring;Receivers;Sensors}, 
doi={10.1109/INFOCOM.2017.8057144}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057145, 
author={J. Zhang and Z. Wang and Z. Yang and Q. Zhang}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Proximity based IoT device authentication}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Internet of Things (IoT) devices are largely embedded devices which lack a sophisticated user interface, e.g., touch screen, keyboard, etc. As a consequence, traditional Pre-Shared Key (PSK) based authentication for mobile devices becomes difficult to apply. For example, according to our study on home automation devices which leverage smartphone for PSK input, the current process does not protect against active impersonating attack and also leaks the Wi-Fi password to eavesdroppers, i.e., currently these IoT devices can be exploited to enter into critical infrastructures, e.g., home networks. Motivated by this real-world security vulnerability, in this paper we propose a novel proximity-based mechanism for IoT device authentication, called Move2Auth, for the purpose of enhancing IoT device security. In Move2Auth, we require user to hold smartphone and perform one of two hand-gestures (moving towards and away, and rotating) in front of IoT device. By combining (1) large RSS-variation and (2) matching between RSS-trace and smartphone sensor-trace, Move2Auth can reliably detect proximity and authenticate IoT device accordingly. Based on our implementation on Samsung Galaxy smartphone and commodity Wi-Fi adapter, we prove Move2Auth can protect against powerful active attack, i.e., the false-positive rate is consistently lower than 0.5%.}, 
keywords={Internet;Internet of Things;computer network security;cryptographic protocols;message authentication;mobile computing;security of data;smart phones;telecommunication security;user interfaces;wireless LAN;IoT device authentication;IoT device security;Key based authentication;Wi-Fi password;authenticate IoT device;called Move2Auth;home automation devices;mobile devices;proximity;smartphone sensor-trace;Authentication;Cryptography;Home automation;Performance evaluation;Phase shift keying;Wireless fidelity}, 
doi={10.1109/INFOCOM.2017.8057145}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057146, 
author={S. Y. Park and S. Lim and D. Jeong and J. Lee and J. S. Yang and H. Lee}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={PUFSec: Device fingerprint-based security architecture for Internet of Things}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={A low-end embedded platform for Internet of Things (IoT) often suffers from a critical trade-off dilemma between security enhancement and computation overhead. We propose PUFSec, a new device fingerprint-based security architecture for IoT devices. By leveraging intrinsic hardware characteristics, we aim to design a computationally lightweight security software system architecture so that complex cryptography computation can dramatically be prohibited. We exploit the innovative idea of Public Physical Unclonable Functions (PPUFs) that fundamentally protects attackers from recovering the secret key from public gate delay information. We implement its hardware logic in a real-world FPGA board. On top of the PPUF fingerprint hardware, we present an adaptive security control mechanism consisting of adaptive key generation and key exchange protocol, which adjusts security strength depending on system load dynamics. We demonstrate that our PPUF FPGA implementation embeds distinctive variability enough to distinguish between two different PPUFs with high fidelity. We validate our PUFSec architecture by implementing necessary algorithms and protocols in a real-world IoT platform, and performing empirical evaluation in terms of computation and memory usages, proving its practical feasibility.}, 
keywords={Internet of Things;cryptographic protocols;embedded systems;field programmable gate arrays;private key cryptography;public key cryptography;telecommunication security;Internet of Things;IoT devices;PPUF FPGA implementation;PPUF fingerprint hardware;PUFSec architecture;Public Physical Unclonable Functions;adaptive key generation;adaptive security control mechanism;complex cryptography computation;computation overhead;computationally lightweight security software system architecture;device fingerprint-based security architecture;hardware logic;intrinsic hardware characteristics;key exchange protocol;low-end embedded platform;public gate delay information;real-world FPGA board;real-world IoT platform;secret key;security enhancement;security strength;system load dynamics;Cryptography;Delays;Hardware;Logic gates;Radiation detectors;Software}, 
doi={10.1109/INFOCOM.2017.8057146}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057147, 
author={K. Sucipto and D. Chatzopoulos and S. Kosta± and P. Hui}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Keep your nice friends close, but your rich friends closer #x2014; Computation offloading using NFC}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={The increasing complexity of smartphone applications and services necessitate high battery consumption but the growth of smartphones' battery capacity is not keeping pace with these increasing power demands. To overcome this problem, researchers gave birth to the Mobile Cloud Computing (MCC) research area. In this paper we advance on previous ideas, by proposing and implementing the first known Near Field Communication (NFC)-based computation offloading framework. This research is motivated by the advantages of NFC's short distance communication, with its better security, and its low battery consumption. We design a new NFC communication protocol that overcomes the limitations of the default protocol; removing the need for constant user interaction, the one-way communication restraint, and the limit on low data size transfer. We present experimental results of the energy consumption and the time duration of two computationally intensive representative applications: (i) RSA key generation and encryption, and (ii) gaming/puzzles. We show that when the helper device is more powerful than the device offloading the computations, the execution time of the tasks is reduced. Finally, we show that devices that offload application parts considerably reduce their energy consumption due to the low-power NFC interface and the benefits of offloading.}, 
keywords={cloud computing;cryptography;mobile computing;near-field communication;smart phones;Mobile Cloud Computing research area;NFC communication protocol;NFC short distance communication;NFC-based computation offloading framework;RSA key generation;computationally intensive representative applications;constant user interaction;device offloading;encryption;energy consumption;low data size transfer;low-power NFC interface;near field communication based computation offloading framework;offload application parts;one-way communication restraint;power demands;smartphone application complexity;smartphone applications;Androids;Batteries;Bluetooth;Humanoid robots;Mobile communication;Protocols;Wireless fidelity}, 
doi={10.1109/INFOCOM.2017.8057147}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057148, 
author={S. Jošilo and G. Dán}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={A game theoretic analysis of selfish mobile computation offloading}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Offloading computation to a mobile cloud is a promising approach for enabling the use of computationally intensive applications by mobile devices. In this paper we consider autonomous devices that maximize their own performance by choosing one of many wireless access points for computation offloading. We develop a game theoretic model of the problem, prove the existence of pure strategy Nash equilibria, and provide a polynomial time algorithm for computing an equilibrium. For the case when the cloud computing resources scale with the number of mobile devices we show that all improvement paths are finite. We provide a bound on the price of anarchy of the game, thus our algorithm serves as an approximation algorithm for the global computation offloading cost minimization problem. We use extensive simulations to provide insight into the performance and the convergence time of the algorithms in various scenarios. Our results show that the equilibrium cost may be close to optimal, and the convergence time is almost linear in the number of mobile devices.}, 
keywords={cloud computing;game theory;minimisation;mobile computing;polynomial approximation;approximation algorithm;autonomous devices;cloud computing resources;computationally intensive applications;convergence time;game theoretic analysis;game theoretic model;global computation offloading cost minimization problem;mobile cloud;mobile devices;polynomial time algorithm;pure strategy Nash equilibria;selfish mobile computation offloading;wireless access points;Approximation algorithms;Cloud computing;Computational modeling;Games;Mobile communication;Mobile handsets;Performance evaluation}, 
doi={10.1109/INFOCOM.2017.8057148}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057149, 
author={J. P. Champati and B. Liang}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Single restart with time stamps for computational offloading in a semi-online setting}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={We study the problem of scheduling n tasks on m + m' parallel processors, where the processing times on m processors are known while those on the remaining m' processors are not known a priori. This semi-online model is an abstraction of certain heterogeneous computing systems, e.g., with the m known processors representing local CPU cores and the unknown processors representing remote servers with uncertain availability of computing cycles. Our objective is to minimize the makespan of all tasks. We initially focus on the case m' = 1 and propose a semi-online algorithm termed Single Restart with Time Stamps (SRTS), which has time complexity O(n log n). We derive its competitive ratio in comparison with the optimal offline solution. If the unknown processing times are deterministic, the competitive ratio of SRTS is shown to be either always constant or asymptotically constant in practice, respectively in cases where the processing times are independent and dependent on m. A similar result is obtained when the unknown processing times are random. Furthermore, extending the ideas of SRTS, we propose a heuristic algorithm termed SRTS-Multiple (SRTS-M) for the case m' > 1. Besides the proven competitive ratios, simulation results further suggest that SRTS and SRTS-M give superior performance on average over randomly generated task processing times, substantially reducing the makespan over the best known alternatives. Interestingly, the performance gain is more significant for task processing times sampled from heavy-tailed distributions.}, 
keywords={computational complexity;processor scheduling;O(n log n) time complexity;SRTS-M;SRTS-multiple heuristic algorithm;competitive ratio;computational offloading;computing cycles;heterogeneous computing systems;local CPU cores;parallel processors;randomly generated task processing times;semionline algorithm;single restart-with-time stamps;task makespan minimization;task scheduling;unknown processing times;unknown processors;Cloud computing;Computational modeling;Heuristic algorithms;Processor scheduling;Program processors;Servers}, 
doi={10.1109/INFOCOM.2017.8057149}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057150, 
author={M. H. Chen and B. Liang and M. Dong}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Joint offloading and resource allocation for computation and communication in mobile cloud with computing access point}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={We consider a general multi-user mobile cloud computing system with a computing access point (CAP), where each mobile user has multiple independent tasks that may be processed locally, at the CAP, or at a remote cloud server. The CAP serves both as the network access gateway and a computation service provider to the mobile users. We aim to jointly optimize the offloading decisions of all users' tasks as well as the allocation of computation and communication resources, to minimize the overall cost of energy, computation, and delay for all users. This problem is NP-hard in general. We propose an efficient three-step algorithm comprising of semidefinite relaxation (SDR), alternating optimization (AO), and sequential tuning (ST). It is shown to always compute a locally optimal solution, and give nearly optimal performance under a wide range of parameter settings. Through evaluating the performance of different combinations of the three components of this SDR-AO-ST algorithm, we provide insights into their roles and contributions in the overall solution. We further compare the performance of SDR-AO-ST against a lower bound to the minimum cost, purely local processing, purely cloud processing, and hybrid local-cloud processing without using the CAP. Our numerical results demonstrate the effectiveness of the proposed algorithm in the joint management of computation and communication resources in mobile cloud computing systems with a CAP.}, 
keywords={cloud computing;mathematical programming;mobile computing;resource allocation;CAP;NP-hard problem;SDR-AO-ST algorithm;alternating optimization;communication resource allocation;computation resource allocation;computation service provider;computing access point;hybrid local-cloud processing;joint offloading;locally optimal solution;lower bound;mobile user;multiple independent tasks;multiuser mobile cloud computing system;network access gateway;offloading decisions;optimal performance;overall cost of energy minimization;purely cloud processing;purely local processing;remote cloud server;resource allocation;semidefinite relaxation;sequential tuning;Cloud computing;Delays;Mobile communication;Mobile handsets;Resource management;Servers}, 
doi={10.1109/INFOCOM.2017.8057150}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057151, 
author={Y. Zhang and D. Li and T. Tian and P. Zhong}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={CubeX: Leveraging glocality of cube-based networks for RAM-based key-value store}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={RAM-based storage aggregates the RAM of servers in data center networks (DCN) to provide extremely high storage performance. For quick recovery of storage server failures, Mem-Cube [1] exploits the proximity of the BCube network to limit the recovery traffic to the recovery servers' 1-hop neighborhood. However, previous design is applicable only to BCube, and has suboptimal recovery performance due to congestion and contention. To address these problems, in this paper we propose CubeX, which generalizes the “1-hop” principle of MemCube for all cube-based networks, and improves the throughput and recovery performance of RAM-based key-value (KV) store via cross-layer optimizations. At the core of CubeX is to leverage the glocality (= globality + locality) of cube-based networks: it scatters backup data across a large number of disks globally distributed throughout the cube, and restricts all recovery traffic within the small local range of each server node. Our evaluation shows that CubeX efficiently supports RAM-based KV store for cube-based networks, and CubeX remarkably outperforms MemCube in both throughput and recovery time.}, 
keywords={computer centres;hypercube networks;optimisation;random-access storage;storage management;telecommunication traffic;BCube network;CubeX;Mem-Cube;RAM-based key-value store;cross-layer optimizations;cube-based networks;data center networks;glocality;recovery servers;recovery traffic;storage server failures;Bandwidth;Conferences;Hypercubes;Random access memory;Servers;Throughput}, 
doi={10.1109/INFOCOM.2017.8057151}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057152, 
author={F. Wang and L. Gao and S. Xiaozhe and H. Harai and K. Fujikawa}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Towards reliable and lightweight source switching for datacenter networks}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={A low-latency and reliable message switching network is critical for constructing high-speed datacenter networks. In this paper, we present the design, implementation, and evaluation of a novel Location basEd Source Switching (LESS) for datacenter networks. LESS enables lightweight source switching through a location-based addressing scheme. Each switch and host can independently derive a source route to reach a destination without requiring the full knowledge of the network topology. We demonstrate that using location-based source routes as forwarding labels allows LESS to eliminate the need for routing tables and integrate with minimum required functionality for packet forwarding. Moreover, we propose a fast rerouting solution to address the issue of fault tolerance in source routing. Each switch can locally derive an alternative source route during a failure. The paper evaluates the performance of LESS. Our evaluation results suggest that LESS improves the performance of datacenter networks in terms of latency, throughput, and reliability.}, 
keywords={computer centres;fault tolerance;telecommunication network routing;telecommunication network topology;telecommunication switching;LESS;alternative source route;fault tolerance;high-speed datacenter networks;lightweight source switching;location-based source routes;network topology;novel Location basEd Source Switching;packet forwarding;reliable message switching network;routing tables;source routing;IP networks;Network topology;Ports (Computers);Reliability;Routing;Switches;Topology}, 
doi={10.1109/INFOCOM.2017.8057152}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057153, 
author={J. Fan and C. Guan and Y. Zhao and C. Qiao}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Availability-aware mapping of service function chains}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Network Function Virtualization (NFV) is a promising technique to greatly improve the effectiveness and flexibility of network services through a process named Service Function Chain (SFC) mapping, with which different network services are deployed over virtualized and shared platforms in data centers. However, such an evolution towards software-defined network functions introduces new challenges to network services which require high availability. One effective way of protecting the network services is to use sufficient redundancy. By doing so, however, the efficiency of physical resources may be greatly decreased. To address such an issue, this paper defines an optimal availability-aware SFC mapping problem and presents a novel online algorithm that can minimize the physical resources consumption while guaranteeing the required high availability within a polynomial time. Simulation results show that our proposed algorithm can significantly improve SFC mapping request acceptance ratio and reduce resource consumption.}, 
keywords={computer centres;computer network management;virtualisation;Network Function Virtualization;SFC mapping request acceptance ratio;Service Function Chain mapping;availability-aware SFC mapping problem;availability-aware mapping;data centers;resource consumption;service function chains;software-defined network functions;virtualized shared platforms;Approximation algorithms;Bandwidth;Computational modeling;Conferences;Delays;Redundancy}, 
doi={10.1109/INFOCOM.2017.8057153}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057154, 
author={H. Saito and H. Honda and R. Kawahara}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Disaster avoidance control against heavy rainfall}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={This paper proposes a disaster avoidance control method for use against heavy rainfall and discusses its effectiveness through actual weather data. The proposed control method uses geographical information data including weather data, hazard area data, and physical network data. By applying technologies related to meteorology, erosion control, and civil engineering to such data, the proposed method can evaluate the risk of a physical network being disconnected. On the basis of the evaluated risk, the proposed method reconfigures a logical network to reduce service disruption. The proposed method is applied to a cloud computing service network where, in addition to route changes, the relocation of virtual machines is possible, increasing its effectiveness. By using empirical data, we show that the proposed method reduces the probability of service disconnection to almost zero even for heavy rainfall causing landslides. Finally, an experimental system of the proposed method was implemented through software defined network technology and successfully controlled the experimental network.}, 
keywords={cloud computing;disasters;emergency management;erosion;geographic information systems;geomorphology;rain;software defined networking;virtual machines;civil engineering;cloud computing service network;disaster avoidance control method;erosion control;geographical information data;hazard area data;heavy rainfall;landslides;logical network;meteorology;physical network data;service disconnection;service disruption;software defined network technology;virtual machines relocation;weather data;Conferences;Control systems;Hazards;Measurement;Rain;Terrain factors}, 
doi={10.1109/INFOCOM.2017.8057154}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057155, 
author={M. Ashour and J. Wang and C. Lagoa and N. Aybat and H. Che}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Non-concave network utility maximization: A distributed optimization approach}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={This paper proposes an algorithm for optimal decentralized traffic engineering in communication networks. We aim at distributing the traffic among the available routes such that the network utility is maximized. In some practical applications, modeling network utility using non-concave functions is of particular interest, e.g., video streaming. Therefore, we tackle the problem of optimizing a generalized class of non-concave utility functions. The approach used to solve the resulting non-convex network utility maximization (NUM) problem relies on designing a sequence of convex relaxations whose solutions converge to that of the original problem. A distributed algorithm is proposed for the solution of the convex relaxation. Each user independently controls its traffic in a way that drives the overall network traffic allocation to an optimal operating point subject to network capacity constraints. All computations required by the algorithm are performed independently and locally at each user using local information and minimal communication overhead. The only non-local information needed is binary feedback from congested links. The robustness of the algorithm is demonstrated, where the traffic is shown to be automatically rerouted in case of a link failure or having new users joining the network. Numerical simulation results are presented to validate our findings.}, 
keywords={concave programming;convex programming;distributed algorithms;radio networks;telecommunication network routing;telecommunication traffic;binary feedback;communication networks;convex relaxation;convex relaxations;distributed algorithm;distributed optimization approach;network capacity constraints;network traffic allocation;nonconcave network utility maximization;nonconcave utility functions;nonconvex network utility maximization problem;optimal decentralized traffic engineering;Communication networks;Conferences;Distributed algorithms;Optimization;Quality of service;Resource management;Streaming media;Distributed optimization;non-concave utility maximization;traffic engineering}, 
doi={10.1109/INFOCOM.2017.8057155}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057156, 
author={P. J. Wan and F. Al-dhelaan and H. Yuan and S. Ji}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Fractional wireless link scheduling and polynomial approximate capacity regions of wireless networks}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Fractional Link scheduling is one of the most fundamental problems in wireless networks. The prevailing approach for shortest fractional link scheduling is based on a reduction to the maximum-weighted independent set problem, which itself may not admit efficient approximation algorithms. In addition, except for the wireless networks under the protocol interference model, none of the existing scheduling algorithms can produce a link schedule with explicit upper bounds on its length in terms of the link demands. As the result, the polynomial approximate capacity regions in these networks remain blank. This paper develops a purely combinatorial paradigm for fractional link scheduling in wireless networks. In addition to the superior efficiency, it is able to provide explicit upper bounds on the lengths of the produced link schedule. By exploiting these upper bounds, polynomial approximate capacity regions are derived. The effectiveness of this new paradigm is demonstrated by its applications in wireless networks under the physical interference model and wireless MIMO networks under the protocol interference model.}, 
keywords={MIMO communication;approximation theory;computational complexity;polynomial approximation;protocols;radio links;radiofrequency interference;scheduling;set theory;telecommunication scheduling;approximation algorithms;combinatorial paradigm;explicit upper bounds;fractional wireless link scheduling;maximum-weighted independent set problem;polynomial approximate capacity regions;protocol interference model;shortest fractional link scheduling;wireless MIMO networks;Approximation algorithms;Games;Interference;Protocols;Schedules;Upper bound;Wireless networks}, 
doi={10.1109/INFOCOM.2017.8057156}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057157, 
author={H. Yu and M. J. Neely}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={A new backpressure algorithm for joint rate control and routing with vanishing utility optimality gaps and finite queue lengths}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={The backpressure algorithm has been widely used as a distributed solution to the problem of joint rate control and routing in multi-hop data networks. By controlling a parameter V in the algorithm, the backpressure algorithm can achieve an arbitrarily small utility optimality gap. However, this in turn brings in a large queue length at each node and hence causes large network delay. This phenomenon is known as the fundamental utility-delay tradeoff. The best known utility-delay tradeoff for general networks is [O(1/V), O(V)] and is attained by a backpressure algorithm based on a drift-pluspenalty technique. This may suggest that to achieve an arbitrarily small utility optimality gap, the existing backpressure algorithms necessarily yield an arbitrarily large queue length. However, this paper proposes a new backpressure algorithm that has a vanishing utility optimality gap, so utility converges to exact optimality as the algorithm keeps running, while queue lengths are bounded throughout by a finite constant. The technique uses backpressure and drift concepts with a new method for convex programming.}, 
keywords={data communication;delays;distributed algorithms;optimisation;queueing theory;telecommunication network routing;backpressure algorithm;convex programming;drift-pluspenalty technique;finite queue lengths;joint rate control and routing;multihop data networks;network delay;queue length;small utility optimality gap;utility-delay tradeoff;Approximation algorithms;Conferences;Delays;Heuristic algorithms;Network topology;Optimization;Routing}, 
doi={10.1109/INFOCOM.2017.8057157}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057158, 
author={S. Vargaftik and I. Keslassy and A. Orda}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Stable user-defined priorities}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Network providers now want to enable users to define their own flow priorities, and commercial devices already implement this ability. However, it has been shown that directly applying arbitrary user-defined priorities can fundamentally destabilize a network. In this paper, we show that it is possible to apply user-defined priorities while keeping the network stable. We introduce U-BP, a scalable approach that extends backpressure-based scheduling techniques to service user-defined flow priorities and rates while maintaining throughput optimality and strong network performance. We explain how our approach relies on a dual-layer scheme with an exponential convergence to requested priorities. We further prove analytically the network stability of our solution, and show how it achieves a strong performance for high-priority flows.}, 
keywords={quality of service;radio networks;telecommunication scheduling;U-BP;arbitrary user-defined priorities;backpressure-based scheduling techniques;commercial devices;high-priority flows;network performance;network providers;network stable;requested priorities;scalable approach;user-defined flow priorities;Conferences;Delays;Heuristic algorithms;Stability analysis;Standards;Throughput;Topology}, 
doi={10.1109/INFOCOM.2017.8057158}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057159, 
author={J. Liu and M. Chen and S. Chen and Q. Pan and L. Chen}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Tag-compass: Determining the spatial direction of an object with small dimensions}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Identifying an object's spatial direction (or orientation) plays a fundamental role in a variety of applications, such as automatic assembly, indoor navigation, and robot driving. In this paper, we design a fine-grained direction finding system called Tag-Compass that attaches a single tag to an object (whose size may be small) and identifies the tagged object's orientation by determining the spatial direction of the tag. We exploit the polarization properties of the RF waves used in the communications between an RFID reader and the tag on the object. Polarization mismatch between the tag and the reader's antenna affects the received signal strength at the reader. From the measured signal strength values, we are able to deduce the tag's direction through a series of transformations and deviation minimization. We propose a system design for Tag-Compass and implement a prototype. We evaluate the performance of TagCompass through extensive experiments using the prototype. The experimental results show that Tag-Compass provides accurate direction estimates with a median error of just 2.5° when the tag's position is known and a median error of 3.8° when the tag's position is unknown.}, 
keywords={RSSI;electromagnetic wave polarisation;radiofrequency identification;Tag-Compass;Tag-compass;fine-grained direction finding system;spatial direction;tagged object;Directive antennas;Object recognition;Radio frequency;Radiofrequency identification;Receivers;Robots;Transmitters}, 
doi={10.1109/INFOCOM.2017.8057159}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057160, 
author={S. Zhang and X. Liu and J. Wang and J. Cao}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Tag size profiling in multiple reader RFID systems}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={In this paper, we study the tag size profiling (TSP) problem in RFID systems with multiple readers, which is to estimate the number of tags in every subregion in the system covered by different set of readers. The TSF problem is vitally important to reader scheduling and many other related operations in large scale multi-reader RFID systems. To our knowledge, however, it is not well solved in previous researches. We propose a novel approach to the TSF problem. The key idea is to treat the size of subregions as variables and construct a linear system in these variables to solve them. We theoretically prove that for any multi-reader RFID system, a linear system that can be used to uniquely solve the variables corresponding to the subregion sizes can always be constructed. We then propose a time-efficient algorithm that uses two heuristics to quickly find enough linearly independent equations to construct the linear system. Extensive simulation results show that the proposed approach achieves very high accuracy. When the estimation results of individual readers contain 5% errors, our approach achieves median estimation error of smaller than 0.02 and 90-percentile estimation error of smaller than 0.04 in large systems containing more than one hundred readers.}, 
keywords={radiofrequency identification;telecommunication scheduling;TSF problem;TSP;individual readers;linear system;linearly independent equations;multiple reader RFID systems;multiple readers;multireader RFID system;reader scheduling;scale multireader RFID systems;subregion sizes;tag size profiling problem;Conferences;Multiple Readers;RFID Estimation;Radio Frequency Identification;Reader Scheduling}, 
doi={10.1109/INFOCOM.2017.8057160}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057161, 
author={C. Duan and X. Rao and L. Yang and Y. Liu}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Fusing RFID and computer vision for fine-grained object tracking}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={In recent years, both the RFID and computer vision technologies have been widely employed in indoor scenarios aimed at different goals while faced with respective limitations. For example, the RFID-based EAS system is useful in quickly identifying tagged objects but the accompanying false alarm problem is troublesome and hard to tackle with except that the accurate trajectory of the target tag can be easily acquired. On the other side, the CV system performs fairly well in tracking multiple moving objects precisely while finding it difficult to screen out the specific target among them. To overcome the above limitations, we present TagVision, a hybrid RFID and computer vision system for fine-grained localization and tracking of tagged objects. A fusion algorithm is proposed to organically combine the position information given by the CV subsystem, and phase data output by the RFID subsystem. In addition, we employ the probabilistic model to eliminate the measurement error caused by thermal noise and device diversity. We have implemented TagVision with COTS camera and RFID devices and evaluated it extensively in our lab environment. Experimental results show that TagVision can achieve 98% blob matching accuracy and 10.33mm location tracking precision.}, 
keywords={computer vision;image fusion;motion estimation;object detection;object tracking;probability;radiofrequency identification;COTS camera;CV system;RFID-based EAS system;TagVision;blob matching accuracy;computer vision system fusion;device diversity;fine-grained localization;fine-grained object tracking;hybrid RFID system fusion;indoor scenarios;multiple moving objects;probabilistic model;tagged objects;thermal noise;Cameras;Computer vision;Optical imaging;Radiofrequency identification;Target tracking;Trajectory;RFID;TagVision;computer vision;tracking}, 
doi={10.1109/INFOCOM.2017.8057161}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057162, 
author={C. Yang and J. Gummeson and A. Sample}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Riding the airways: Ultra-wideband ambient backscatter via commercial broadcast systems}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Communication costs dominate the energy consumption, and ultimately limit the utility, of low power devices and sensor nodes. Backscatter communication based on deliberate and ambient sources has the potential to radically alter this paradigm by offering two to three orders of magnitude better communication efficiency (in terms of nJ/Bit) then conventional radio architectures. Initial work on ambient backscatter shows promising results but has focused on narrow band operation in well controlled laboratory settings. The goal of this work is to enable the ubiquitous deployment of ultra-low power nodes that communicate via ambient backscatter to wired Universal Backscatter Readers, in real-world environments. This is accomplished through ultra-wideband backscatter techniques that leverage the breath of commercial broadcast signals in the 80 MHz to 900 MHz range from FM radios, digital TVs, and cellular networks. Additionally the use of powered Universal Backscatter Readers allows a network of ultra-low power nodes to operate on ambient carriers as low as -80 dBm, which is typical for indoor home and office environments. For the first time we demonstrate the simultaneous use of 17 ambient signal sources to achieve node-to-reader communication distances of 50 meters, with data rates up to 1 kbps.}, 
keywords={backscatter;cellular radio;digital television;radio broadcasting;Backscatter communication;FM radios;Ultra-wideband ambient backscatter;ambient carriers;ambient signal sources;ambient sources;cellular networks;commercial broadcast signals;commercial broadcast systems;communication costs;digital TV;energy consumption;frequency 80.0 MHz to 900.0 MHz;initial work;low power devices;narrow band operation;node-to-reader communication distances;radio architectures;sensor nodes;ultra-low power nodes;ultra-wideband backscatter techniques;universal backscatter readers;wired Universal Backscatter Readers;Backscatter;Digital TV;Frequency modulation;Poles and towers;Radio frequency;Ultra wideband technology;Ambient Backscatter Communication;Energy Harvesting;Sensor Node;Ultra-Wideband}, 
doi={10.1109/INFOCOM.2017.8057162}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057163, 
author={C. Wu and Y. Zhang and L. Zhang and B. Yang and X. Chen and W. Zhu and L. Qiu}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={ButterFly: Mobile collaborative rendering over GPU workload migration}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={The ever increasing of display resolution on mobile devices raises high demand for GPU rendering details. However, the challenge of poor hardware support but fine-grained rendering details often makes user unsatisfied especially in calling for high frame rate scenarios, e.g., game. To resolve such issue, we propose BUTTERFLY, a novel system which collaboratively utilizes mobile GPUs to process high-quality rendering details for on-the-go mobile users. In particular, ButterFly achieves two technical contributions for the collaborative design: (1) a mobile device can migrate GPU workloads in buffer queue to peers, and (2) the collaborative rendering mechanism benefits user high quality details while significant power saving performance. Both techniques are compatible with the OpenGL ES standards. Furthermore, a 40-person survey perceives that ButterFly can provide excellent user experience of both rendering details and frame rate over Wi-Fi network. In addition, our comprehensive trace-driven experiments on Android prototype reveal the benefits of Butterfly have more superior performance over state-of-the-art systems, which achieves more than 28.3% power saving.}, 
keywords={Android (operating system);graphics processing units;mobile computing;rendering (computer graphics);screens (display);Android prototype;BUTTERFLY;ButterFly;Butterfly;GPU rendering;GPU workload migration;display resolution;mobile GPU;mobile collaborative rendering;mobile device;Androids;Collaboration;Graphics processing units;Hardware;Humanoid robots;Mobile communication;Rendering (computer graphics)}, 
doi={10.1109/INFOCOM.2017.8057163}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057164, 
author={C. Pei and Z. Wang and Y. Zhao and Z. Wang and Y. Meng and D. Pei and Y. Peng and W. Tang and X. Qu}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Why it takes so long to connect to a WiFi access point}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Today's WiFi networks deliver a large fraction of traffic. However, the performance and quality of WiFi networks are still far from satisfactory. Among many popular quality metrics (throughput, latency), the probability of successfully connecting to WiFi APs and the time cost of the WiFi connection set-up process are the two of the most critical metrics that affect WiFi users' experience. To understand the WiFi connection set-up process in real-world settings, we carry out measurement studies on 5 million mobile users from 4 representative cities associating with 7 million APs in 0.4 billion WiFi sessions, collected from a mobile “WiFi Manager” App that tops the Android/iOS App market. To the best of our knowledge, we are the first to do such large scale study on: how large the WiFi connection set-up time cost is, what factors affect the WiFi connection set-up process, and what can be done to reduce the WiFi connection set-up time cost. Based on our data-driven measurement and analysis, we reveal the insights as follows: (1) Connection set-up failure and large connection set-up time cost are common in today's WiFi use. As large as 45% of the users suffer connection set-up failures, and 15% (5%) of them have large connection set-up time costs over 5 seconds (10 seconds). (2) Contrary to the state-of-the-art work, scan, one of the subphase of four phases in the connection set-up process, contributes the most (47%) to the overall connection set-up time cost. (3) Mobile device model and AP model can greatly help us to predict the connection set-up time cost if we can make good use of the hidden information. Based on the measurement analysis, we develop a machine learning based AP selection strategy that can significantly improve WiFi connection set-up performance, against the conventional strategy purely based on signal strength, by reducing the connection set-up failures from 33% to 3.6% and reducing 80% time costs of the conne- tion set-up processes by more than 10 times.}, 
keywords={learning (artificial intelligence);mobile radio;wireless LAN;AP model;WiFi access point;WiFi connection set-up process;WiFi connection set-up time cost;WiFi networks;WiFi users experience;connection set-up failures;machine learning based AP selection strategy;mobile WiFi Manager App;mobile device model;time 10.0 s;time 5.0 s;Authentication;IP networks;Measurement;Mobile communication;Mobile handsets;Switches;Wireless fidelity}, 
doi={10.1109/INFOCOM.2017.8057164}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057165, 
author={F. Ahmed and J. Erman and Z. Ge and A. X. Liu and J. Wang and H. Yan}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Monitoring quality-of-experience for operational cellular networks using machine-to-machine traffic}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={It is crucial for cellular data network operators to understand the service quality perceived by its customers. The state-of-art systems deployed in cellular networks mostly report service quality aggregated on cell site level, which is typically an aggregation of tens or hundreds of customers depending on the locations of the cell sites. In this paper, we propose to enhance the measurement of customer-perceived service quality by leveraging M2M devices as sensors in the field, which provide an unprecedented opportunity for cellular network operators to measure what end-users experience with better accuracy and coverage. Our approach is to identify a set of M2M devices which are stationary and communicate continuously over the cellular network over an indefinite period of time. We use these M2M devices to estimate the customer-perceived service quality during cell site outages. We implement our methodology as a system called M2MScan and evaluate M2MScan with both synthetic outages and real outages from a large-scale operational cellular network. To the best of our knowledge, this is the first work that employs M2M devices to measure the service quality perceived by customers in operational cellular networks at a large scale.}, 
keywords={cellular radio;customer services;machine-to-machine communication;quality of experience;quality of service;telecommunication traffic;M2M devices;M2MScan;cell site level;cell site outages;cellular data network operators;large-scale operational cellular network;machine-to-machine traffic;quality-of-experience;service quality;Cellular networks;Level measurement;Machine-to-machine communications;Monitoring;Object recognition;Performance evaluation;Sensors}, 
doi={10.1109/INFOCOM.2017.8057165}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057166, 
author={L. Xue and X. Ma and X. Luo and L. Yu and S. Wang and T. Chen}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Is what you measure what you expect? Factors affecting smartphone-based mobile network measurement}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Many apps have been developed to measure the performance of mobile networks. Unfortunately, their measurement results may not be what users expect, because the results could be biased by various factors and the apps' descriptions may confuse users. Although a few recent studies pointed out several factors, they missed other important factors and lacked of finegrained analysis on the factors and measurement apps. Moreover, none has studied whether or not the descriptions of such apps will mislead users. In this paper, we conduct the first systematic study of the factors that could bias the result from measurement apps and their descriptions. We identify new factors, revisit known factors, and propose a novel approach with new tools to discover these factors in proprietary apps. We also develop a new measurement app named MobiScope for demonstrating how to mitigate the negative effects of these factors. Furthermore, we construct enhanced descriptions for measurement apps to provide users more information about what is measured. The extensive experimental results illustrate the negative effects of various factors, the improvement in performance measurement brought by MobiScope, and the clarity of the enhanced descriptions.}, 
keywords={smart phones;MobiScope;measurement app;mobile network measurement;performance measurement;Androids;Delays;Humanoid robots;Mobile communication;Protocols;Runtime;Smart phones}, 
doi={10.1109/INFOCOM.2017.8057166}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057167, 
author={Z. Wang and Y. Zhang and Y. Li and Q. Wang and F. Xia}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Exploiting social influence for context-aware event recommendation in event-based social networks}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Event-based Social Networks (EBSNs) which bridge the gap between online and offline interactions among users have received increasing popularity. The unique cold-start nature makes event recommendation more challenging than traditional recommendation problems, since even for two events with the same content, they may not happen at the same time, the same location, or be organized by the same host. Existing event recommendation algorithms mainly exploit the basic context information (e.g., location, time and content), while the social influence of event hosts and group members have been ignored. In this paper, we propose a Social Information Augmented Recommender System (SIARS), which fully exploits the social influence of event hosts and group members together with basic context information for event recommendation. In particular, we combine the information of EBSNs and other social networks to characterize the social influence of event hosts, and take interactions between group members into consideration for event recommendation. In addition, we propose a new content-aware recommendation model using the topic model to find the most similar topic the event belongs to, and a new location-aware recommendation model integrating location popularity with location distribution for event recommendation. Extensive experiments on real-world datasets demonstrate that SIARS outperforms other recommendation algorithms.}, 
keywords={mobile computing;recommender systems;social networking (online);EBSN;SIARS;Social Information Augmented Recommender System;content-aware recommendation model;context information;context-aware event recommendation;event hosts;event recommendation algorithms;event-based social networks;group members;location-aware recommendation model;recommendation problems;social influence;social networks;Collaboration;Computer security;Conferences;Facebook;Recommender systems;Twitter}, 
doi={10.1109/INFOCOM.2017.8057167}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057168, 
author={B. Samanta and A. De and N. Ganguly}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={STRM: A sister tweet reinforcement process for modeling hashtag popularity}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={With social media platform such as Twitter becoming the de facto destination for users' views and opinions, it is of great importance to forecast an information outbreak. In Twitter, tweets are often annotated with hashtags to help its users to quickly extract their contents. The existing approaches for modeling the dynamics of tweet-messages are usually limited to individual or simple aggregates of tweets rather than the underlying hashtags. In this paper, we develop, STRM, a novel point process driven model that considers the effect of cross-tweet impact in hashtag popularity. STRM, by assuming hashtag to be a heterogeneous collection of tweet-chains. Through extensive experimentation, we find that our algorithm - STRM, shows consistent performance boosts with six diverse real datasets against several strong baselines. Moreover surprisingly, it also offers significant accuracy gains in popularity-prediction for individual tweets as compared with the existing paradigms.}, 
keywords={information retrieval;social networking (online);STRM process;Twitter;cross-tweet impact;hashtag popularity;information outbreak;popularity-prediction;sister tweet reinforcement process;social media platform;tweet-chains;tweet-messages;Computational modeling;Data models;History;Predictive models;Proposals;Tagging;Twitter}, 
doi={10.1109/INFOCOM.2017.8057168}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057169, 
author={X. Xu and C. H. Lee and D. Y. Eun}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Challenging the limits: Sampling online social networks with cost constraints}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Graph sampling techniques via random walk crawling have been popular for analyzing statistical characteristics of large online social networks due to simple implementation and provable guarantees on unbiased estimates. Despite the growing popularity, the `cost' of sampling and its true impact on the accuracy of estimates still have not been carefully studied. In addition, the random walk-based methods inherently suffer from the sluggish nature of random walks and the `slow-mixing' structure of social graphs, thereby leading to high correlation in the samples obtained. With these in mind, in this paper, we develop a mathematical framework such that the cost of sampling is properly taken into account, which in turn re-defines a widely used asymptotic variance into a cost-based asymptotic variance. Our new metric enables us to compare a class of sampling policies under the same cost constraint, integrating “random skipping” (bypassing nodes without sampling) into the random walk-based sampling. We obtain an optimal policy striking the right balance between sampling quality (less correlation) and sampling quantity (higher cost per sample), which greatly improves over the usual skip-free crawling-based samplers. We further extend our framework, enabling one to design more sophisticated sampling strategies with an array of control knobs, which all produce unbiased estimates under the same cost constraint.}, 
keywords={graph theory;information retrieval;network theory (graphs);random processes;sampling methods;social networking (online);cost constraint;cost-based asymptotic variance;graph sampling;online social networks sampling;provable guarantees;random skipping;random walk crawling;random walk-based sampling;sampling policies;sampling quality;sampling quantity;sampling strategies;simple implementation;skip-free crawling-bsaed samplers;slow-mixing structure;social graphs;statistical characteristics;unbiased estimates;Conferences;Correlation;Crawlers;Measurement;Sampling methods;Social network services;Web pages}, 
doi={10.1109/INFOCOM.2017.8057169}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057170, 
author={Q. Lin and L. Yang and Y. Liu}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={TagScreen: Synchronizing social televisions through hidden sound markers}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Millions of people nowadays share their television (TV) experience with other people through social media like Twitter or Facebook with mobile devices. It is generally believed that TV has been repurposed for social networks, called as social television. A key functionality of social television is that it allows the viewers to interchange their comments through mobile devices, thus creating the impression of watching TV like alongside a group of friends. To do so, mobile devices have to be aware of the current media context (identifier and progress) of what the TV is playing, known as synchronizing context from TVs to mobile devices. Unfortunately, most legacy systems are not able to track the playing progresses or need to upgrade TV devices. To address the issue, we design a purely software-based solution, called as TagScreen, which inserts a series of hidden sound markers into the audio of the content. TagScreen supports longrange or multipath-resistant synchronization at second-level, being independent of device diversity over severely frequency-selective acoustic channels. We implement TagScreen by using COTS TVs and mobile devices. The system has been extensively tested on 150 movies and 150 TV series across five different environments. Results show that TagScreen has a mean recognition accuracy of 98% up to 35m, and a mean tracking accuracy of 97%.}, 
keywords={mobile handsets;mobile television;social networking (online);COTS TVs;TV devices;TV series;TagScreen;device diversity;frequency-selective acoustic channels;hidden sound markers;mobile devices;multipath-resistant synchronization;social media;social networks;social television;synchronizing context;television experience;Frequency modulation;Media;Mobile handsets;Motion pictures;Social network services;Synchronization;TV}, 
doi={10.1109/INFOCOM.2017.8057170}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057171, 
author={Q. Liang and E. Modiano}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Coflow scheduling in input-queued switches: Optimal delay scaling and algorithms}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={A coflow is a collection of parallel flows belonging to the same job. It has the all-or-nothing property: a coflow is not complete until the completion of all its constituent flows. In this paper, we focus on optimizing coflow-level delay, i.e., the time to complete all the flows in a coflow, in the context of an N × N input-queued switch. In particular, we develop a throughput-optimal scheduling policy that achieves the best scaling of coflow-level delay as N → ∞. We first derive lower bounds on the coflow-level delay that can be achieved by any scheduling policy. It is observed that these lower bounds critically depend on the variability of flow sizes. Then we analyze the coflow-level performance of some existing coflow-agnostic scheduling policies and show that none of them achieves provably optimal performance with respect to coflow-level delay. Finally, we propose the Coflow-Aware Batching (CAB) policy which achieves the optimal scaling of coflow-level delay under some mild assumptions.}, 
keywords={computer networks;delays;optimisation;queueing theory;scheduling;telecommunication scheduling;CAB policy;Coflow-Aware Batching policy;Optimal delay scaling;coflow-agnostic scheduling policies;coflow-level delay;coflow-level performance;throughput-optimal scheduling policy;Delays;Optimal scheduling;Ports (Computers);Processor scheduling;Scheduling}, 
doi={10.1109/INFOCOM.2017.8057171}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057172, 
author={W. Wang and S. Ma and B. Li and B. Li}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Coflex: Navigating the fairness-efficiency tradeoff for coflow scheduling}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Fair and efficient coflow scheduling improves application-level networking performance in today's datacenters. Ideally, a coflow scheduler should provide isolation guarantees on the minimum coflow progress to achieve predictable networking performance. Network operators, on the other hand, strive to decrease the average coflow completion time (CCT). Unfortunately, optimal isolation guarantees and minimum average CCT are conflicting objectives and cannot be achieved at the same time. Existing coflow schedulers either optimize isolation guarantees at the expense of long CCTs (e.g., HUG [1]), or decrease the average CCT without performance isolation (e.g., Varys and Aalo [2], [3]). The lack of a smooth tradeoff in between poses a dilemma between low efficiency and no performance isolation. To bridge this gap, we develop a new coflow scheduler, Coflex, to navigate this tradeoff. Coflex allows network operators to specify the desired level of isolation guarantee using a tunable fairness knob, while at the same time decreasing the average CCT. Both our real-world deployments and trace-driven simulations have shown that Coflex offers a smooth tradeoff between fairness and efficiency. At an appropriate tradeoff level, Coflex outperforms fair schedulers by 2 × in minimizing the average CCT.}, 
keywords={computer centres;computer networks;telecommunication scheduling;Coflex;application-level networking performance;average coflow completion time;coflow scheduler;coflow scheduling;fairness-efficiency tradeoff;minimum average CCT;minimum coflow progress;network operators;optimal isolation guarantees;performance isolation;predictable networking performance;Bandwidth;Conferences;Fabrics;Navigation;Production;Resource management;Silicon}, 
doi={10.1109/INFOCOM.2017.8057172}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057173, 
author={C. H. Wang and S. T. Maguluri and T. Javidi}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Heavy traffic queue length behavior in switches with reconfiguration delay}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Optical switches have been drawing attention due to their large data bandwidth and low power consumption. However, scheduling policies need to account for the schedule reconfiguration delay of optical switches to achieve good performance. The Adaptive MaxWeight policy achieves optimal throughput for switches with nonzero reconfiguration delay, and has been shown in simulation to have good delay performance. In this paper, we analyze the queue length behavior of a switch with nonzero reconfiguration delay operating under the Adaptive MaxWeight. We first show that the Adaptive MaxWeight policy exhibits a weak state space collapse behavior in steady-state, which could be viewed as an inheritance of the MaxWeight policy in a switch with zero reconfiguration delay. We then use the weak state space collapse result to obtain a steady state delay bound under the Adaptive MaxWeight algorithm in heavy traffic by applying a recently developed drift technique. The resulting delay bound is dependent on the expected schedule duration. We then derive the relation between the expected schedule duration and the steady state queue length through drift analysis, and obtain asymptotically tight queue length bounds in the heavy traffic regime.}, 
keywords={delays;optical switches;queueing theory;telecommunication scheduling;telecommunication traffic;Adaptive MaxWeight algorithm;Adaptive MaxWeight policy;asymptotically tight queue length;drift analysis;expected schedule duration;heavy traffic regime;nonzero reconfiguration delay;optical switches;queue length behavior;schedule reconfiguration delay;scheduling policies;steady state delay;steady state queue length;weak state space collapse behavior;weak state space collapse result;Delays;Optical switches;Ports (Computers);Queueing analysis;Schedules;Steady-state;Throughput}, 
doi={10.1109/INFOCOM.2017.8057173}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057174, 
author={S. Yang and B. Lin and P. Tune and J. J. Xu}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={A simple re-sequencing load-balanced switch based on analytical packet reordering bounds}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Chang et al. proposed the load-balanced switch in their seminal work [1], which has received wide attention due to its inherent scalability properties in both size and speed. These scalability properties continue to be of significant interest due to the relentless exponential growth in Internet traffic. The main drawback of the load-balanced switch is that packets can depart out-of-order from the switch, which can significantly degrade network performance by negatively interacting with TCP congestion control. Hence, a large body of subsequent work has proposed a variety of modifications for ensuring packet ordering, but all the proposed approaches tend to increase packet delay significantly in comparison to the basic load-balanced switch. In this paper, we show that the amount of packet reordering that can occur with the load-balanced switch is actually quite limited, which means that packet reordering can simply be rectified by employing reordering buffers at the switch outputs. In particular, we formally bound the worst-case amount of time that a packet has to wait in these output reordering buffers before it is guaranteed to be ready for in-order departure with high probability, and we prove that this bound is linear with respect to the switch size. This linear bound is significant because previous approaches can add quadratic or cubic delays to the load-balanced switch. In addition, we use a hash-grouping method that further reduces resequencing delays significantly. Although simple and intuitive, our experimental results show that our output packet reordering approach substantially outperforms existing load-balanced switch architectures.}, 
keywords={Internet;packet switching;probability;resource allocation;telecommunication congestion control;telecommunication traffic;transport protocols;Internet traffic;TCP congestion control;analytical packet;analytical packet reordering bounds;basic load-balanced switch;hash-grouping method;load-balanced switch architectures;simple resequencing load-balanced switch;switch size;Computer architecture;Delays;Optical switches;Out of order;Ports (Computers);Semantics}, 
doi={10.1109/INFOCOM.2017.8057174}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057175, 
author={J. Lin and M. Li and D. Yang and G. Xue and J. Tang}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Sybil-proof incentive mechanisms for crowdsensing}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={The rapid growth of sensor-embedded smartphones has led to a new data sensing and collecting paradigm, known as crowdsensing. Many auction-based incentive mechanisms have been proposed to stimulate smartphone users to participate in crowdsensing. However, none of them have taken into consideration the Sybil attack where a user illegitimately pretends multiple identities to gain benefits. This attack may undermine existing inventive mechanisms. To deter the Sybil attack, we design Sybil-proof auction-based incentive mechanisms for crowdsensing in this paper. We investigate both the single-minded and multi-minded cases and propose SPIM-S and SPIM-M, respectively. SPIM-S achieves computational efficiency, individual rationality, truthfulness, and Sybil-proofness. SPIM-M achieves individual rationality, truthfulness, and Sybil-proofness. We evaluate the performance and validate the desired properties of SPIM-S and SPIM-M through extensive simulations.}, 
keywords={incentive schemes;mobile computing;smart phones;SPIM-M;SPIM-S;Sybil attack;Sybil-proof auction-based incentive mechanisms;Sybil-proof incentive mechanisms;Sybil-proofness;crowdsensing;data collecting paradigm;data sensing paradigm;inventive mechanisms;sensor-embedded smart phones;smartphone users;Computational modeling;Conferences;Cost function;Electronic mail;Sensors;Smart phones}, 
doi={10.1109/INFOCOM.2017.8057175}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057176, 
author={C. Liu and S. Wang and L. Ma and X. Cheng and R. Bie and J. Yu}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Mechanism design games for thwarting malicious behavior in crowdsourcing applications}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Crowdsourcing applications are vulnerable to malicious behaviors, posing serious threats to their adoption and large deployment. Based on the notion that the requestor (i.e., the crowdsourcer) can block malicious behaviors via leveraging the market power through task allocation and pricing, we propose two novel frameworks based on the mechanism design game theory (i.e., the reverse game theory). To the best of our knowledge, we are the first to exploit the market power and to apply the mechanism design game theory in thwarting malicious behaviors in crowdsourcing. The first proposed framework is built on a requestor-dominant mechanism design game (Rd-MDG), where the game rule is determined solely by the requestor. The second proposed framework is based on the worker-assisted mechanism design game (WaMDG), where the worker (i.e., the contributor) can assist the requestor to determine the game rules by offering advices. These two frameworks have the following salient features: i) neither of them requires the workers to reveal their private information; ii) the game rules of each framework are designed to be able to force the workers to calculate their best strategies based on their actual private information; iii) our theoretical analysis shows that equilibriums exist for both frameworks; and iv) our extensive simulation results demonstrate that these two frameworks can thwart malicious behaviors by driving the workers with a higher attack intent into obtaining lower utilities.}, 
keywords={crowdsourcing;data privacy;game theory;security of data;Rd-MDG;WaMDG;crowdsourcing applications;game rule;malicious behavior;private information;requestor-dominant mechanism design game;reverse game theory;worker-assisted mechanism design game;Conferences;Crowdsourcing;Electronic mail;Game theory;Games;Resource management}, 
doi={10.1109/INFOCOM.2017.8057176}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057177, 
author={F. Qiu and Z. He and L. Kong and F. Wu}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={MAGIK: An efficient key extraction mechanism based on dynamic geomagnetic field}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Secret key establishment is a fundamental requirement for private communication between two wireless entities. An intriguing solution is to extract secret keys from the inherent randomness shared between them. Although several works have been done to extract secret keys from different kinds of mediums (e.g., RSSI, CSI, CIR), the efficiency and security problems are not fully solved. In this paper, we consider the problem of secret key establishment for wireless devices, and propose MAGIK, a secure and efficient scheme based on dynamic geoMAGnetic field in Indoor environment for Key establishment. We carefully study the feasibility of utilizing indoor geomagnetic field for key extraction through extensive measurements. Our results demonstrate that geomagnetic field has several dynamic properties, including space-varying, time-varying, sensitive to measurement device, and correlative between two observed points in proximity. We also optimize the key extraction process and present two rotation-angle-based quantification methods, which can achieve faster key generation rates and lower bit mismatching ratios. Besides, we build a prototype on commodity mobile devices, and evaluate its performance by conducting real-word experiments in indoor scenarios. The experiment results confirm that our system is efficient, in terms of key extraction rate, and robust in secret key establishment without requiring additional overhead on mobile devices.}, 
keywords={private key cryptography;quantum cryptography;telecommunication security;wireless channels;MAGIK;dynamic geomagnetic field;indoor geomagnetic field;key extraction mechanism;key extraction rate;key generation rates;secret key establishment;secret keys;secure scheme;Communication system security;Extraterrestrial measurements;Magnetometers;Mobile handsets;Security;Wireless communication;Wireless sensor networks}, 
doi={10.1109/INFOCOM.2017.8057177}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057178, 
author={B. Wang and W. Song and W. Lou and Y. T. Hou}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Privacy-preserving pattern matching over encrypted genetic data in cloud computing}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Personalized medicine performs diagnoses and treatments according to the DNA information of the patients. The new paradigm will change the health care model in the future. A doctor will perform the DNA sequence matching instead of the regular clinical laboratory tests to diagnose and medicate the diseases. Additionally, with the help of the affordable personal genomics services such as 23andMe, personalized medicine will be applied to a great population. Cloud computing will be the perfect computing model as the volume of the DNA data and the computation over it are often immense. However, due to the sensitivity, the DNA data should be encrypted before being outsourced into the cloud. In this paper, we start from a practical system model of the personalize medicine and present a solution for the secure DNA sequence matching problem in cloud computing. Comparing with the existing solutions, our scheme protects the DNA data privacy as well as the search pattern to provide a better privacy guarantee. We have proved that our scheme is secure under the well-defined cryptographic assumption, i.e., the sub-group decision assumption over a bilinear group. Unlike the existing interactive schemes, our scheme requires only one round of communication, which is critical in practical application scenarios. We also carry out a simulation study using the real-world DNA data to evaluate the performance of our scheme. The simulation results show that the computation overhead for real world problems is practical, and the communication cost is small. Furthermore, our scheme is not limited to the genome matching problem but it applies to general privacy preserving pattern matching problems which is widely used in real world.}, 
keywords={DNA;biology computing;cloud computing;cryptography;data privacy;diseases;genetics;genomics;health care;pattern matching;DNA data;DNA data privacy;DNA information;cloud computing;encrypted genetic data;genome matching problem;health care model;personal genomics services;personalized medicine;privacy guarantee;privacy-preserving pattern matching;regular clinical laboratory tests;secure DNA sequence matching problem;Cloud computing;Computational modeling;DNA;Encryption;Testing}, 
doi={10.1109/INFOCOM.2017.8057178}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057179, 
author={Q. Yin and J. Kaur and F. D. Smith}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={TCP Rapid: From theory to practice}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Delay and rate-based alternatives to TCP congestion-control have been around for nearly three decades and have seen a recent surge in interest. However, such designs have faced significant resistance in being deployed on a wide-scale across the Internet - this has been mostly due to serious concerns about noise in delay measurements, pacing inter-packet gaps, and/or required changes to the standard TCP stack/headers. With the advent of high-speed networking, some of these concerns become even more significant. In this paper, we consider Rapid, a recent proposal for ultra-high speed congestion control, which perhaps stretches each of these challenges to the greatest extent. Rapid adopts a framework of continuous fine-scale bandwidth probing, which requires a potentially different and finely-controlled gap for every packet, high-precision timestamping of received packets, and reliance on fine-scale changes in inter-packet gaps. While simulation-based evaluations of Rapid show that it has outstanding performance gains along several important dimensions, these will not translate to the real-world unless the above challenges are addressed. We design a Linux implementation of Rapid after carefully considering each of these challenges. Our evaluations on a 10Gbps testbed confirm that the implementation can indeed achieve the claimed performance gains, and that it would not have been possible unless each of the above challenges was addressed.}, 
keywords={Internet;Linux;telecommunication congestion control;transport protocols;Internet;Linux;TCP Rapid;TCP congestion-control;bit rate 10 Gbit/s;continuous fine-scale bandwidth probing;delay measurements;fine-scale changes;finely-controlled gap;greatest extent;high-precision timestamping;high-speed networking;interpacket gaps;required changes;standard TCP stack/headers;ultrahigh speed congestion control;Bandwidth;Delays;Loss measurement;Noise measurement;Performance gain;Probes;Protocols}, 
doi={10.1109/INFOCOM.2017.8057179}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057180, 
author={T. Bonald and J. Roberts and C. Vitale}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Convergence to multi-resource fairness under end-to-end window control}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={The paper relates to multi-resource sharing between flows with heterogeneous requirements as arises in networks with wireless links or software routers implementing network function virtualization. Bottleneck max fairness (BMF) is a sharing objective in this context with good performance. The paper shows that BMF results when local fairness is imposed at each resource while flow rates are controlled by an end-to-end window. We analytically prove convergence to BMF under a fluid model when flows share a network limited to 2 resources while numerical results confirm BMF convergence for larger networks. Simulation results illustrate the impact of packetized transmission.}, 
keywords={Internet;radio links;telecommunication network routing;virtualisation;bottleneck max fairness;end-to-end window control;multiresource fairness;multiresource sharing;network function virtualization;software routers;wireless links;Bit rate;Convergence;Mathematical model;Resource management;Wireless communication}, 
doi={10.1109/INFOCOM.2017.8057180}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057181, 
author={D. Shan and F. Ren}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Improving ECN marking scheme with micro-burst traffic in data center networks}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={In data centers, micro-burst is a common traffic pattern. The packet dropping caused by it usually leads to serious performance degradations. Therefore, much attention has been paid to avoiding buffer overflow caused by micro-burst traffic. In particular, ECN is widely used in data centers to keep persistent queue occupancy low, so that enough buffer space can be available as headroom to absorb micro-burst traffic. However, we find that instantaneous-queue-length-based ECN may cause problems in another direction - buffer underflow. Specifically, current ECN marking scheme in data centers is easy to trigger spurious congestion signals, which may result in overreaction of senders and queue length oscillations in switches. Since ECN threshold is low, the buffer may underflow and link capacity is not fully used. In this paper, we reveal this problem by experiments and simulations. Besides, we theoretically deduce the amplitude of queue length oscillations. The analysis result shows that overreaction of senders is caused by ECN mis-marking. Therefore, we propose Combined Enqueue and Dequeue Marking (CEDM), which can mark packets more accurately. Through simulations, we show that CEDM can greatly reduce throughput loss and improve flow completion time.}, 
keywords={computer centres;computer networks;queueing theory;telecommunication congestion control;telecommunication switching;telecommunication traffic;transport protocols;CEDM scheme;ECN marking scheme;ECN mis-marking;ECN threshold;buffer avoidance;buffer space;buffer underflow;combined enqueue and dequeue marking scheme;common traffic pattern;data center networks;instantaneous-queue-length;low persistent queue occupancy;microburst traffic;queue length oscillations;Conferences;Delays;Integrated circuits;Oscillators;Proposals;Throughput;ECN marking;Interrupt Coalescing;Large Segment Offload;Micro-burst traffic}, 
doi={10.1109/INFOCOM.2017.8057181}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057182, 
author={J. Zhao and J. Liu and H. Wang and C. Xu}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Multipath TCP for datacenters: From energy efficiency perspective}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Nearly 50% of the energy overhead in today's datacenters comes from host-to-host data transfers, which largely depend on the transport layer performance. Multipath TCP (MPTCP) has recently been suggested as a promising transport protocol to improve datacenter network throughput, yet it also increases the host CPU power consumption. It remains unclear whether datacenters can indeed benefit from using MPTCP from the perspective of energy efficiency. By analyzing the performance of MPTCP, we show that (1) despite consuming higher host CPU power than TCP, MPTCP can largely reduce the long flow completion time and thus save the aggregated energy; (2) link-sharing subflows in MPTCP not only has negative impact on both throughput-sensitive long flows and latency-sensitive short flows, but also noticeably increases the host CPU power, especially for short flows. We present MPTCP-D, an energy-efficient variant of multipath TCP for datacenters. MPTCP-D incorporates a novel congestion control algorithm that can provide energy efficiency by minimizing the flow completion time, and an extra subflow elimination mechanism that can preclude link-sharing subflows from increasing the host CPU power. We implement MPTCP-D in the Linux kernel and study its performance by experiments on Amazon EC2. Our results show that, without degrading the performance of the long flow throughput and short flow completion time, MPTCP-D reduces the long flow energy consumption by up to 72% compared to DCTCP for data transfers, and reduces the short flow power consumption by up to 46% compared to MPTCP with linksharing subflows.}, 
keywords={Linux;computer centres;computer networks;energy conservation;power consumption;telecommunication congestion control;telecommunication power management;transport protocols;Linux kernel;MPTCP-D;aggregated energy;datacenter network throughput;energy efficiency perspective;energy overhead;energy-efficient variant;host-to-host data transfers;latency-sensitive short flows;link-sharing subflows;long flow completion time;long flow energy consumption;long flow throughput;multipath TCP;short flow completion time;short flow power consumption;throughput-sensitive long flows;transport layer performance;transport protocol;Data transfer;Energy consumption;Interference;Power demand;Servers;Throughput;Transport protocols}, 
doi={10.1109/INFOCOM.2017.8057182}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057183, 
author={J. Palacios and P. Casari and J. Widmer}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={JADE: Zero-knowledge device localization and environment mapping for millimeter wave systems}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Device localization is a highly important functionality for a range of applications. It is particularly beneficial in mmWave networks, where it can be used to reduce the beam training overhead and anticipate handovers between access points. In this paper, we present JADE, an algorithm that estimates the location of a mobile user in an indoor space without any knowledge about the surrounding environment (floor plan, location of walls and presence of reflective surfaces) or about the location and number of access points available therein. JADE leverages the beam procedure used in pre-standard and commercial mmWave equipment to estimate the angle-of-arrival of multipath components of the signal sent by visible access points. This information is then employed to localize the mobile user, estimate the position of access points and finally form a map of the environment. No radar-like ranging operations are required for this. Our results demonstrate that JADE can localize a user with sub-meter accuracy in the broad majority of the cases, and that it even outperforms localization algorithms that require full knowledge of the environment and access point positions.}, 
keywords={array signal processing;direction-of-arrival estimation;indoor radio;mobility management (mobile radio);multi-access systems;multipath channels;JADE;Zero-knowledge device localization;angle-of-arrival;beam procedure;beam training;environment mapping;indoor space;joint anchor and device location estimation;millimeter wave systems;mmWave networks;mobile user;multipath components;radar-like ranging operations;visible access points;Algorithm design and analysis;Estimation;Mobile communication;Phased arrays;Signal processing algorithms;Simultaneous localization and mapping;Millimeter wave;indoor;localization;mobility;simulation;virtual anchors}, 
doi={10.1109/INFOCOM.2017.8057183}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057184, 
author={Z. Zhao and J. Wang and X. Zhao and C. Peng and Q. Guo and B. Wu}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={NaviLight: Indoor localization and navigation under arbitrary lights}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Thanks to the highly-dense lighting infrastructure in public areas, visible light emerges as a promising means to indoor localization and navigation. State-of-the-art techniques generally require customized hardware (sensing boards), and mainly work with one single light source (e.g., customized LEDs). This greatly limits their application scope. In this paper, we propose NaviLight, a generic indoor localization and navigation framework based on existing lighting infrastructure with any unmodified light sources (e.g., LED, fluorescent, and incandescent lights). NaviLight simply adopts commercial off-the-shelf mobile phones as receivers, and light intensity values as location signatures. Unlike existing WiFi systems, a single light intensity value is not discriminative enough over space though the light intensity field does vary, which makes our design more challenging. We thus propose a LightPrint as a location signature using a vector of multiple light intensity values obtained during user's walks. Such LightPrints are created by leveraging any user movement (of varying distance and direction) in order to minimize user efforts. A set of techniques are proposed to achieve quick LightPrint matching, which includes a coarse-grained classification and a fine-grained matching over dynamic time warping. We have implemented NaviLight to provide real-time service on Android phones in three typical indoor environments, covering a total area size over 1000m2. Our experiments show that NaviLight can achieve sub-meter localization accuracy to meet practical engineering requirements.}, 
keywords={indoor communication;indoor radio;light sources;lighting;navigation;smart phones;wireless LAN;LED;LightPrint matching;NaviLight;WiFi systems;generic indoor localization;highly-dense lighting infrastructure;incandescent lights;indoor environments;light intensity field;light intensity value;light source;lighting infrastructure;location signature;mobile phones;multiple light intensity;navigation framework;sensing boards;size 1000.0 m;visible light;High intensity discharge lamps;Indoor environments;Light emitting diodes;Light sources;Navigation;Sensors}, 
doi={10.1109/INFOCOM.2017.8057184}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057185, 
author={X. Chen and C. Ma and M. Allegue and X. Liu}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Taming the inconsistency of Wi-Fi fingerprints for device-free passive indoor localization}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Device-free Passive (DfP) indoor localization releases the users from the burden of wearing sensors or carrying smartphones. Instead of locating devices, DfP technology directly locates human bodies. This promising technology upgrades and even redefines many services, such as intruder alarm, fire rescue, fall detection, baby monitoring, etc. Using Wi-Fi based fingerprints, DfP approaches can achieve a nearly perfect accuracy with a resolution less than one meter. However, Wi-Fi localization profiles may easily drift with a minor environment change, resulting in an inconsistency between fingerprints and new profiles. This inconsistency issue could lead to large errors, and may quickly ruin the whole system. To address this issue, we propose a approach named AutoFi to automatically calibrate the localization profiles in an unsupervised manner. AutoFi embraces a new technique that online estimates and cancels profile contaminants introduced by environment changes. It applies an autoencoder to preserve critical features of fingerprints, and reproduces them later in new localization profiles. Experiment results demonstrate that AutoFi indeed rescues the Wi-Fi fingerprints from variations in the surrounding. The localization accuracy is improved from 18.8% (before auto-calibration) to 84.9% (after auto-calibration).}, 
keywords={fingerprint identification;indoor radio;telecommunication security;wireless LAN;AutoFi;DfP technology;Wi-Fi based fingerprints;Wi-Fi fingerprints;Wi-Fi localization profiles;baby monitoring;device-free passive indoor localization;direct human bodies localization;environment change;fire rescue;inconsistency issue;intruder alarm;localization accuracy;profile contaminants;Antenna arrays;Array signal processing;Databases;OFDM;Signal resolution;Wireless communication;Wireless fidelity}, 
doi={10.1109/INFOCOM.2017.8057185}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057186, 
author={R. Gao and B. Zhou and F. Ye and Y. Wang}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Knitter: Fast, resilient single-user indoor floor plan construction}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Lacking of floor plans is a fundamental obstacle to ubiquitous indoor location-based services. Recent work have made significant progress to accuracy, but they largely rely on slow crowdsensing that may take weeks or even months to collect enough data. In this paper, we propose Knitter that can generate accurate floor maps by a single random user's one hour data collection efforts. Knitter extracts high quality floor layout information from single images, calibrates user trajectories and filters outliers. It uses a multi-hypothesis map fusion framework that updates landmark positions/orientations and accessible areas incrementally according to evidences from each measurement. Our experiments on 3 different large buildings and 30+ users show that Knitter produces correct map topology, and 90-percentile landmark location and orientation errors of 3 ~ 5m and 4 ~ 6°, comparable to the state-of-the-art at more than 20× speed up: data collection can finish in about one hour even by a novice user trained just a few minutes.}, 
keywords={calibration;construction;floors;planning;Knitter;buildings;correct map topology;crowdsensing;data collection;filters outliers;floor maps;floor plans;high quality floor layout information;multihypothesis map fusion framework;single-user indoor floor plan construction;size 5.0 m;time 1.0 hour;ubiquitous indoor location-based services;Calibration;Cleaning;Data collection;Geometry;Gyroscopes;Layout;Trajectory}, 
doi={10.1109/INFOCOM.2017.8057186}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057187, 
author={R. Naves and H. Khalifé and G. Jakllari and V. Conan and A. L. Beylot}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={A framework for evaluating physical-layer network coding gains in multi-hop wireless networks}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={We investigate the potential gains of Physical-Layer Network Coding (PLNC) in multi-hop wireless networks. Physical-Layer Network Coding was first introduced as a solution to increase the throughput of a two-way relay channel communication. Unlike most wireless communications techniques which try to avoid collisions, PLNC allows two simultaneous transmissions to a common receiver. Such transmitted messages are summed at signal level and then decoded at packet level. In basic topologies, Physical-Layer Network Coding has been shown to significantly enhance the throughput performance compared to classical communications. However, the impact of PLNC in large multi-hop networks remains an open question. We therefore exploit Linear Programming to evaluate the impact of this paradigm in large realistic radio deployments. Our numerical results show that PLNC can increase the throughput in large multi-hop topologies by 30%. Such gains set theoretical benchmarks for designing new access methods and routing protocols to efficiently exploit the Physical-Layer Network Coding concept.}, 
keywords={linear programming;network coding;relay networks (telecommunication);routing protocols;telecommunication network topology;wireless channels;PLNC;Physical-Layer Network Coding concept;large realistic radio deployments;linear programming;multihop topologies;multihop wireless networks;routing protocols;throughput performance enhancement;two-way relay channel communication;wireless communications techniques;Computational modeling;Network coding;Network topology;Spread spectrum communication;Throughput;Topology;Wireless communication}, 
doi={10.1109/INFOCOM.2017.8057187}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057188, 
author={A. Zhou and X. Zhang and H. Ma}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Beam-forecast: Facilitating mobile 60 GHz networks via model-driven beam steering}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Low robustness under mobility is the Achilles' heel of the emerging 60 GHz networking technology. Instead of using omni-directional antennas as in existing Wi-Fi/cellular networks, 60 GHz radios communicate via highly-directional links formed by phased-array beam-forming, so as to upgrade wireless link throughput to multi-Gbps. However, user motion causes misalignment between the Tx's and Rx's beam directions, and often leads to link outage. Legacy 60 GHz protocols realign the beams by scanning alternative Tx/Rx beams. But unfortunately this tedious process can easily overwhelm the useful channel time, leaving the Tx/Rx in misalignment most of the time during mobility. In this paper, we propose Beam-forecast, a novel model-driven beam steering approach that can sustain high performance for mobile 60 GHz links. Beam-forecast is built on the observation that 60 GHz channel profiles at nearby locations are highly-correlated. By exploiting this correlation, Beam-forecast can reconstruct the channel profile as the Tx/Rx moves, without explicit channel scanning. In this way, it can predict new optimal beams and realign links for mobile users with minimal overhead. We evaluate Beam-forecast using a reconfigurable 60 GHz testbed along with a trace-driven simulator. Our experiments demonstrate multi-fold throughput gain compared with state-of-the-art under various practical scenarios.}, 
keywords={array signal processing;beam steering;cellular radio;directive antennas;mobile radio;protocols;radio links;wireless LAN;Beam-forecast;Wi-Fi;alternative Tx/Rx beams;beam directions;beam steering approach;cellular networks;channel profiles;channel scanning;frequency 60.0 GHz;highly-directional links;legacy 60 GHz protocols;mobile 60 GHz networks;mobile links;model-driven beam steering;networking technology;omnidirectional antennas;optimal beams;phased-array beam-forming;wireless link;Antenna measurements;Array signal processing;Beam steering;Correlation;Horn antennas;Mobile communication;Wireless communication}, 
doi={10.1109/INFOCOM.2017.8057188}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057189, 
author={Q. Chen and H. Gao and Y. Li and S. Cheng and J. Li}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Edge-based beaconing schedule in duty-cycled multihop wireless networks}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Beaconing is a fundamental networking service where each node broadcasts a packet to all its neighbors locally. Unfortunately, the problem Minimum Latency Beaconing Schedule (MLBS) in duty-cycled scenarios is not well studied. Existing works always have rigid assumption that each node is only active once per working cycle. Aiming at making the work more practical and general, MLBS problem in duty-cycled network where each node is allowed to active multiple times in each working cycle (MLBSDCA for short) is investigated in this paper. Firstly, a modified first-fit coloring based algorithm is proposed for MLBSDCA under protocol interference model. After that, a (ρ + 1)2* |W|-approximation algorithm is proposed to further reduce the beaconing latency, where ρ denotes the interference radius, and |W| is the maximum number of active time slots per working cycle. When ρ and |W| is equal to 1, the approximation ratio is only 4, which is better than the one (i.e., 10) in existing works. Furthermore, two approximation algorithms for MLBSDCA under physical interference model are also investigated. The theoretical analysis and experimental results demonstrate the efficiency of the proposed algorithms in term of latency.}, 
keywords={approximation theory;graph colouring;protocols;radiofrequency interference;telecommunication scheduling;wireless sensor networks;MLBSDCA;Minimum Latency Beaconing Schedule;active multiple times;active time slots;duty-cycled network;duty-cycled scenarios;edge-based beaconing schedule;first-fit coloring based algorithm;fundamental networking service;multihop wireless networks;physical interference model;protocol interference model;Algorithm design and analysis;Approximation algorithms;Interference;Protocols;Schedules;Scheduling;Scheduling algorithms}, 
doi={10.1109/INFOCOM.2017.8057189}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057190, 
author={S. Saha and M. C. Chan}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Design and application of a many-to-one communication protocol}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={In this paper, we address the fundamental problem of improving the performance of many-to-one and many-to-many communications. Our approach is Time Division Multiple Access (TDMA) based but addresses the limitations of existing TDMA implementations in a novel way. In a nutshell, we combine packets from many senders into a single large packet transmission by exploiting capture effect achieved through fine grained power control at the level of segments within a single packet. We applied our technique to the design of a one-hop, many-to-one communication protocol, SyncMerge, and a multi-hop, many-to-many communication protocol, ByteCast. Our evaluation shows that SyncMerge is able to achieve 2 to 15 times improvement over traditional many-to-one communication schemes. In addition, ByteCast is able to disseminate 1 byte of data from every node to all other nodes in about 600 ms with 99.5% reliability on a 90 node testbed. Compared to the state-of-the-art protocols such as LWB and Chaos, ByteCast is able to reduce the radio-on time by up to 90% while achieving similar reliability.}, 
keywords={power control;telecommunication network topology;time division multiple access;ByteCast protocol;SyncMerge protocol;TDMA implementations;Time Division Multiple Access;communication schemes;fine grained power control;many-to-many communication;many-to-one communication protocol;memory size 1.0 Byte;single large packet transmission;state-of-the-art protocols;Chaos;Conferences;Power control;Protocols;Reliability;Synchronization;Time division multiple access}, 
doi={10.1109/INFOCOM.2017.8057190}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057191, 
author={H. Dai and B. Liu and H. Yuan and P. Crowley and J. Lu}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Analysis of tandem PIT and CS with non-zero download delay}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Collapsed forwarding has long been used in cache systems to reduce the load on servers by aggregating requests for the same content. Named Data Networking (NDN) as a future Internet architecture incorporates this technique through a data structure called Pending Interest Table (PIT). The request aggregation feature suggests that PIT can be viewed as a nonreset time-to-live (TTL) based cache. The Content Store (CS) is a content cache placed in front of the PIT on the NDN forwarding path, so they make up a tandem cache network. To investigate the metrics of interest in this network, like the hit probability for the PIT and the CS, the expected PIT size, non-zero download delay (non-ZDD) should be taken into consideration. Caching policies usually assume zero download delay (ZDD), i.e., request and object arrive simultaneously, and numerous analytical methods have been proposed to study the ZDD caching policies. In this paper, after dissecting the LRU policy, we for the first time propose two LRU variants considering non-ZDD by defining separate operations for the request and object arrivals. When CS adopts the proposed LRU variants, the analysis of the CS-PIT network can still take advantage of the existing models, so the metrics of interest can be computed. Especially, the distribution for the “inter-miss” time of this network can be derived, which has not been achieved by prior works. Finally, the analytical results are verified through simulations.}, 
keywords={Internet;cache storage;data structures;CS-PIT network;Content Store;LRU policy;LRU variants;NDN forwarding path;Named Data Networking;Pending Interest Table;TTL based cache;ZDD caching policies;cache systems;content cache;data structure;future Internet architecture;inter-miss time;nonZDD;nonreset time-to-live cache;nonzero download delay;request aggregation feature;tandem cache network;zero download delay;Analytical models;Computational modeling;Conferences;Data structures;Delays;Random variables}, 
doi={10.1109/INFOCOM.2017.8057191}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057192, 
author={G. Neglia and D. Carra and P. Michiardi}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Cache policies for linear utility maximization}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Cache policies to minimize the content retrieval cost have been studied through competitive analysis when the miss costs are additive and the sequence of content requests is arbitrary. More recently, a cache utility maximization problem has been introduced, where contents have stationary popularities and utilities are strictly concave in the hit rates. This paper bridges the two formulations, considering linear costs and content popularities. We show that minimizing the retrieval cost corresponds to solving an online knapsack problem, and we propose new dynamic policies inspired by simulated annealing, including DynqLRU, a variant of qLRU. For such policies we prove asymptotic convergence to the optimum under the characteristic time approximation. In a real scenario, popularities vary over time and their estimation is very difficult. DynqLRU does not require popularity estimation, and our realistic, trace-driven evaluation shows that it significantly outperforms state-of-the-art policies, with up to 45% cost reduction.}, 
keywords={cache storage;information retrieval;knapsack problems;simulated annealing;DynqLRU;cache policies;cache utility maximization problem;content requests;content retrieval cost;cost reduction;linear costs;linear utility maximization;miss costs;online knapsack problem;retrieval cost;simulated annealing;trace-driven evaluation;Approximation algorithms;Conferences;Detectors;Estimation;Heuristic algorithms;Simulated annealing;Space exploration}, 
doi={10.1109/INFOCOM.2017.8057192}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057193, 
author={M. Zhang and V. Lehman and L. Wang}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Scalable name-based data synchronization for named data networking}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={In Named Data Networking (NDN), data synchronization plays an important role similar to transport protocols in IP. Many distributed applications, including pub-sub applications such as news and weather services, require a synchronization protocol where each consumer can subscribe to a different subset of a producer's data streams. However, existing Sync protocols support only full-data synchronization, which is a special case of this problem. We propose PSync to efficiently address different types of data synchronization. Names are used in PSync messages to carry producers' latest namespace information and each consumer's subscription information, which allows producers to maintain a single state for all consumers and enables consumers to synchronize with any producer that replicates the same data. We have implemented PSync in the NDN codebase and used it to develop a prototype pub-sub module for building management. Our experimental results show that PSync scales well as the number of consumers, subscriptions, and data streams increases and it outperforms the state-of-the-art Sync protocol in supporting full-data synchronization.}, 
keywords={Internet;synchronisation;telecommunication traffic;transport protocols;NDN codebase;PSync messages;Sync protocols support;data streams increases;full-data synchronization;named data networking;scalable name-based data synchronization;state-of-the-art Sync protocol;synchronization protocol;transport protocols;Arrays;Buildings;Conferences;Distributed databases;Protocols;Synchronization}, 
doi={10.1109/INFOCOM.2017.8057193}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057194, 
author={J. Choi and S. Moon and J. Woo and K. Son and J. Shin and Y. Yi}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Rumor source detection under querying with untruthful answers}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Social networks are the major routes for most individuals to exchange their opinions about new products, social trends and political issues via their interactions. It is often of significant importance to figure out who initially diffuses the information, i.e., finding a rumor source or a trend setter. It is known that such a task is highly challenging and the source detection probability cannot be beyond 31% for regular trees, if we just estimate the source from a given diffusion snapshot. In practice, finding the source often entails the process of querying that asks “Are you the rumor source?” or “Who tells you the rumor?” that would increase the chance of detecting the source. In this paper, we consider two kinds of querying: (a) simple batch querying and (b) interactive querying with direction under the assumption that queriees can be untruthful with some probability. We propose estimation algorithms for those queries, and quantify their detection performance and the amount of extra budget due to untruthfulness, analytically showing that querying significantly improves the detection performance. We perform extensive simulations to validate our theoretical findings over synthetic and real-world social network topologies.}, 
keywords={probability;query processing;social networking (online);social sciences computing;detection performance;diffusion snapshot;political issues;rumor source detection;simple batch querying;social network topologies;social networks;social trends;source detection probability;untruthful answers;Computational modeling;Conferences;Internet;Maximum likelihood estimation;Network topology;Topology}, 
doi={10.1109/INFOCOM.2017.8057194}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057195, 
author={S. Achleitner and T. L. Porta and P. McDaniel and S. V. Krishnamurthy and A. Poylisher and C. Serban}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Stealth migration: Hiding virtual machines on the network}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Live virtual machine (VM) migration is commonly used for enabling dynamic resource or fault management, or for load balancing in datacenters or cloud platforms. A service hosted by a VM may also be migrated to prevent its visibility to an external adversary who may seek to disrupt its operation by launching a DDoS attack against it. We first show that current systems cannot adequately hide a VM migration from an external adversary. The key reason for this is that a migration typically manifests a traffic pattern with distinguishable statistical properties. We introduce two new attacks that can allow an adversary to effectively track a migration in progress, by leveraging observations of these properties. As our primary contribution, we design and implement a stealth migration framework that causes migration traffic to be indistinguishable from regular Internet traffic, with a negligible latency overhead of approximately 0.37 seconds, on average.}, 
keywords={cloud computing;computer centres;computer network security;resource allocation;telecommunication traffic;virtual machines;DDoS attack;cloud platforms;datacenters;dynamic resource;external adversary;fault management;latency overhead;live VM migration;live virtual machine migration;load balancing;migration traffic;statistical properties;stealth migration framework;traffic pattern;Cloud computing;Conferences;Hardware;Timing;Virtual machine monitors;Virtual machining}, 
doi={10.1109/INFOCOM.2017.8057195}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057196, 
author={Y. Xiao and M. Krunz}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={QoE and power efficiency tradeoff for fog computing networks with fog node cooperation}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={This paper studies the workload offloading problem for fog computing networks in which a set of fog nodes can offload part or all the workload originally targeted to the cloud data centers to further improve the quality-of-experience (QoE) of users. We investigate two performance metrics for fog computing networks: users' QoE and fog nodes' power efficiency. We observe a fundamental tradeoff between these two metrics for fog computing networks. We then consider cooperative fog computing networks in which multiple fog nodes can help each other to jointly offload workload from cloud data centers. We propose a novel cooperation strategy referred to as offload forwarding, in which each fog node, instead of always relying on cloud data centers to process its unprocessed workload, can also forward part or all of its unprocessed workload to its neighboring fog nodes to further improve the QoE of its users. A distributed optimization algorithm based on distributed alternating direction method of multipliers (ADMM) via variable splitting is proposed to achieve the optimal workload allocation solution that maximizes users' QoE under the given power efficiency. We consider a fog computing platform that is supported by a wireless infrastructure as a case study to verify the performance of our proposed framework. Numerical results show that our proposed approach significantly improves the performance of fog computing networks.}, 
keywords={cloud computing;evolutionary computation;resource allocation;ADMM;cloud data centers;distributed alternating direction method of multipliers;distributed optimization algorithm;fog computing networks;fog computing platform;fog node cooperation;power efficiency;quality of experience;user QoE;variable splitting;Cloud computing;Edge computing;Measurement;Power demand;Resource management;ADMM;Fog computing;offload forwarding;power efficiency;response-time analysis}, 
doi={10.1109/INFOCOM.2017.8057196}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057197, 
author={R. D. Yates and M. Tavan and Y. Hu and D. Raychaudhuri}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Timely cloud gaming}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={This work introduces a new model for cloud gaming systems aimed at optimizing the timeliness of video frames based on an age of information (AoI) metric. Mobile clients submit actions through an access network to a game server. The game server generates video frames at a constant frame rate. At the mobile device, the display of these frames represent game status updates. We develop a Markov model to characterize the frame delivery process in low-latency edge cloud gaming systems. Based on this model, we derive a simple formula for the average status age of a tightly synchronized low-latency mobile gaming system in which the inter-frame period is a significant contributor to the system latency. We validate the model by ns-3 simulation of a low-latency edge cloud gaming system. Our evaluation scenarios included single-player games as well as multi-player games in which the game processing was conducted by a combination of a centralized game server and edge cloud renderers.}, 
keywords={Markov processes;cloud computing;computer games;rendering (computer graphics);Markov model;centralized game server;cloud gaming systems;edge cloud renderers;frame delivery process;game processing;game status updates;inter-frame period;low-latency edge cloud gaming system;low-latency mobile gaming system;multiplayer games;single-player games;timely cloud gaming;video frames;Cloud gaming;Delays;Mobile communication;Mobile computing;Servers;Streaming media}, 
doi={10.1109/INFOCOM.2017.8057197}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057198, 
author={X. Wang and X. Chen and W. Wu}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Towards truthful auction mechanisms for task assignment in mobile device clouds}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Despite the increased capabilities of mobile devices, resource-demanded mobile applications still transcend what can be accomplished on a single device. As such, mobile device cloud (MDC), an environment that enables computation-intensive tasks to be performed among a set of nearby mobile devices, offers a promising architecture to support real-time mobile applications. To stimulate mobile devices to execute tasks for others, it is essential to design an incentive mechanism that appropriately charges the owners of the tasks, acted as the buyers, and rewards the mobile devices, acted as the sellers. In this paper, we propose two truthful auction mechanisms for two different task models, heterogeneous and homogeneous task models, which assume the different and the same resource requirements of the tasks, respectively. Specifically, for heterogeneous task model, we propose an efficient heuristic winning bids determination algorithm to allocate the tasks, and decide the payment of each seller for its winning bids. For homogeneous task model, we design an optimal winning bid determination algorithm, and propose a Vickrey-Clarke-Groves (VCG) based auction mechanism to determine the payment of each bid. Both theoretical analysis and simulations show that the proposed auction mechanisms achieve several desirable properties such as individual rationality, truthfulness and computational efficiency.}, 
keywords={cloud computing;mobile computing;resource allocation;VCG based auction mechanism;Vickrey-Clarke-Groves based auction mechanism;computation-intensive tasks;heterogeneous task models;homogeneous task models;mobile device cloud;mobile devices;real-time mobile applications;task assignment;truthful auction mechanisms;winning bid determination algorithm;Batteries;Cloud computing;Computational modeling;Mobile communication;Mobile handsets;Resource management}, 
doi={10.1109/INFOCOM.2017.8057198}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057199, 
author={C. Hu and A. Alhothaily and A. Alrawais and X. Cheng and C. Sturtivant and H. Liu}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={A secure and verifiable outsourcing scheme for matrix inverse computation}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Matrix inverse computation is one of the most fundamental mathematical problems in large-scale data analytics and computing. It is often too expensive to be solved in resource-constrained devices such as sensors. Outsourcing the computation task to a cloud server or a fog server is a potential approach as the server is able to perform large-scale scientific computations on behalf of resource-constrained users with special software. However, outsourcing brings in new security concerns and challenges such as data privacy violations and result invalidation. In this paper, we propose a secure and verifiable outsourcing scheme to compute the matrix inverse in a server. In our scheme, the client generates two secret key sets based on two chaotic systems, which are utilized to create two sparse matrices whose permuted versions are used for matrix encryption and decryption to protect input and output privacy. The server computes the inverse over the ciphertext matrix and returns the result to the client who can verify the validity of the inverse. We analyze the proposed scheme in terms of correctness, security, verifiability, and attack resistance, and compare its performance (computation, storage, and communication overheads) with those of the state-of-the-art. Our theoretical results and comparison study demonstrate that the proposed scheme provides a secure and efficient outsourcing mechanism for matrix inverse computation.}, 
keywords={cryptography;data privacy;matrix inversion;ciphertext matrix;cloud server;computation task;fog server;large-scale data analytics;large-scale scientific computations;matrix decryption;matrix encryption;matrix inverse computation;resource-constrained devices;resource-constrained users;secure outsourcing mechanism;secure outsourcing scheme;verifiable outsourcing scheme;Computational modeling;Encryption;Iterative methods;Outsourcing;Servers;Sparse matrices;Matrix inversion;chaotic systems;cloud/fog computing;data privacy;secure outsourcing;verification}, 
doi={10.1109/INFOCOM.2017.8057199}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057200, 
author={X. Feng and Z. Zheng and D. Cansever and A. Swami and P. Mohapatra}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={A signaling game model for moving target defense}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Incentive-driven advanced attacks have become a major concern to cyber-security. Traditional defense techniques that adopt a passive and static approach by assuming a fixed attack type are insufficient in the face of highly adaptive and stealthy attacks. In particular, a passive defense approach often creates information asymmetry where the attacker knows more about the defender. To this end, moving target defense (MTD) has emerged as a promising way to reverse this information asymmetry. The main idea of MTD is to (continuously) change certain aspects of the system under control to increase the attacker's uncertainty, which in turn increases attack cost/complexity and reduces the chance of a successful exploit in a given amount of time. In this paper, we go one step beyond and show that MTD can be further improved when combined with information disclosure. In particular, we consider that the defender adopts a MTD strategy to protect a critical resource across a network of nodes, and propose a Bayesian Stackelberg game model with the defender as the leader and the attacker as the follower. After fully characterizing the defender's optimal migration strategies, we show that the defender can design a signaling scheme to exploit the uncertainty created by MTD to further affect the attacker's behavior for its own advantage. We obtain conditions under which signaling is useful, and show that strategic information disclosure can be a promising way to further reverse the information asymmetry and achieve more efficient active defense.}, 
keywords={Bayes methods;game theory;security of data;Bayesian Stackelberg game model;MTD strategy;attack cost-complexity;cyber-security;efficient active defense;fixed attack type;highly adaptive attacks;incentive-driven advanced attacks;information asymmetry;moving target defense;passive defense approach;signaling game model;signaling scheme;static approach;stealthy attacks;strategic information disclosure;traditional defense techniques;Bayes methods;Computational modeling;Conferences;Face;Games;Servers;Uncertainty}, 
doi={10.1109/INFOCOM.2017.8057200}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057201, 
author={J. Chen and S. Yao and Q. Yuan and R. Du and G. Xue}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Checks and balances: A tripartite public key infrastructure for secure web-based connections}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Recent real-world attacks against Certification Authorities (CAs) and fraudulently issued certificates arouse the public to rethink the security of public key infrastructure for web-based connections. To distribute the trust of CAs, notaries, as an independent party, are introduced to record certificates, and a client can request an audit proof of certificates from notaries directly. However, there are two challenges. On one hand, existing works consider the security of notaries insufficiently. Due to lack of systematic mutual verification, notaries might bring safety bottlenecks. On the other hand, the service of these works is not sustainable, when any party leaks its private key or fails. In this paper, we propose a Tripartite Public Key Infrastructure (TriPKI), using Certificates Authorities, Integrity Log Servers, and Domain Name Servers, to provide a basis for establishing secure SSL/TLS connections. Specifically, we apply checks-and balances among those three parties in the structure to make them verify mutually, which avoids any single party compromise. Furthermore, we design a collaborative certificate management scheme to provide sustainable services. The security analysis and experiment results demonstrate that our scheme is suitable for practical usage with moderate overhead.}, 
keywords={Internet;authorisation;certification;public key cryptography;CAs;Certificates Authorities;Certification Authorities;balances;checks;collaborative certificate management scheme;fraudulently issued certificates;independent party;notaries;party leaks;private key;record certificates;secure SSL/TLS;secure Web-based connections;security analysis;single party compromise;systematic mutual verification;tripartite public key infrastructure;Authentication;Collaboration;Conferences;Electronic mail;Public key;Servers;DNS-based;Mutual Verification;Public Key Infrastructure}, 
doi={10.1109/INFOCOM.2017.8057201}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057202, 
author={A. Klein and H. Shulman and M. Waidner}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Internet-wide study of DNS cache injections}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={DNS caches are an extremely important tool, providing services for DNS as well as for a multitude of applications, systems and security mechanisms, such as anti-spam defences, routing security (e.g., RPKI), firewalls. Subverting the security of DNS is detrimental to the stability and security of the clients and services, and can facilitate attacks, circumventing even cryptographic mechanisms. We study the caching component of DNS resolution platforms in diverse networks in the Internet, and evaluate injection vulnerabilities allowing cache poisoning attacks. Our evaluation includes networks of leading Internet Service Providers and enterprises, and professionally managed open DNS resolvers. We test injection vulnerabilities against known payloads as well as a new class of indirect attacks that we define in this work. Our Internet evaluation indicates that more than 92% of the Internet's DNS resolution platforms are vulnerable to records injection and can be persistently poisoned.}, 
keywords={Internet;cache storage;computer network security;DNS cache injections;DNS resolution platforms;Internet evaluation;Internet service providers;antispam defences;cache poisoning attacks;caching component;cryptographic mechanisms;firewalls;indirect attacks;injection vulnerabilities;open DNS resolvers;routing security;security mechanisms;Computer crime;Electronic mail;IP networks;Internet;Payloads;Servers}, 
doi={10.1109/INFOCOM.2017.8057202}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057203, 
author={Y. Cheng and H. Jiang and F. Wang and Y. Hua and D. Feng}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={BlitzG: Exploiting high-bandwidth networks for fast graph processing}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Nowadays, high-bandwidth networks are easily accessible in data centers. However, existing distributed graph-processing frameworks fail to efficiently utilize the additional bandwidth capacity in these networks for higher performance, due to their inefficient computation and communication models, leading to very long waiting times experienced by users for the graph-computing results. The root cause lies in the fact that the computation and communication models of these frameworks generate, send and receive messages so slowly that only a small fraction of the available network bandwidth is utilized. In this paper, we propose a high-performance distributed graph-processing framework, called BlitzG, to address this problem. This framework fully exploits the available network bandwidth capacity for fast graph processing. Our approach aims at significant reduction in (i) the computation workload of each vertex for fast message generation by using a new slimmed-down vertex-centric computation model and (ii) the average message overhead for fast message delivery by designing a lightweight message-centric communication model. Evaluation on a 40Gbps Ethernet, driven by real-world graph datasets, shows that BlitzG outperforms the state-of-the-art distributed graph-processing frameworks by up to 27x with an average of 20.7x.}, 
keywords={computer centres;distributed processing;graph theory;local area networks;network theory (graphs);BlitzG;Ethernet;bit rate 40 Gbit/s;communication models;computation workload;data centers;fast graph processing;fast message delivery;fast message generation;high-bandwidth networks;high-performance distributed graph-processing framework;lightweight message-centric communication model;long waiting times;network bandwidth capacity;real-world graph datasets;slimmed-down vertex-centric computation model;vertex-centric computation model;Bandwidth;Computational modeling;Conferences;Kernel;Message systems;Receivers;Scalability}, 
doi={10.1109/INFOCOM.2017.8057203}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057204, 
author={T. Shu and C. Q. Wu}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Performance optimization of Hadoop workflows in public clouds through adaptive task partitioning}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Cloud computing provides a cost-effective computing platform for big data workflows where moldable parallel computing models such as MapReduce are widely applied to meet stringent performance requirements. The granularity of task partitioning in each moldable job has a significant impact on workflow completion time and financial cost. We investigate the properties of moldable jobs and design a big-data workflow mapping model, based on which, we formulate a workflow mapping problem to minimize workflow makespan under a budget constraint in public clouds. We show this problem to be strongly NP-complete and design i) a fully polynomial-time approximation scheme (FPTAS) for a special case with a pipeline-structured workflow executed on virtual machines in a single class, and ii) a heuristic for a generalized problem with an arbitrary directed acyclic graph-structured workflow executed on virtual machines in multiple classes. The performance superiority of the proposed solution is illustrated by extensive simulation-based results in Hadoop/YARN in comparison with existing workflow mapping models and algorithms.}, 
keywords={Big Data;cloud computing;computational complexity;directed graphs;optimisation;parallel processing;scheduling;Big Data workflow;Hadoop workflows;Hadoop/YARN;NP-complete design;adaptive task partitioning;arbitrary directed acyclic graph-structured workflow;big-data workflow mapping model;cloud computing;cost-effective computing platform;financial cost;fully polynomial-time approximation scheme;generalized problem;moldable parallel computing models;performance optimization;performance superiority;public clouds;task partitioning;virtual machines;workflow completion time;workflow mapping models;workflow mapping problem;Big Data;Cloud computing;Computational modeling;Optimization;Program processors;Scheduling}, 
doi={10.1109/INFOCOM.2017.8057204}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057205, 
author={X. Mei and X. Chu and H. Liu and Y. W. Leung and Z. Li}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Energy efficient real-time task scheduling on CPU-GPU hybrid clusters}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Conserving the energy consumption of large data centers is of critical significance, where a few percent in consumption reduction translates into millions-dollar savings. This work studies energy conservation on emerging CPU-GPU hybrid clusters through dynamic voltage and frequency scaling (DVFS). We aim at minimizing the total energy consumption of processing a sequence of real-time tasks under deadline constraints. We compute the appropriate voltage/frequency setting for each task through mathematical optimization, and assign multiple tasks to the cluster with heuristic scheduling algorithms. In performance evaluation driven by real-world power measurement traces, our scheduling algorithm shows comparable energy savings to the theoretical upper bound. With a GPU scaling interval where analytically at most 38% of energy can be saved, we record 30-36% of energy savings. Our results are applicable to energy management on modern heterogeneous clusters. In particular, our model stresses the nonlinear relationship between task execution time and processor speed for GPU-accelerated applications, for more accurately capturing real-world GPU energy consumption.}, 
keywords={graphics processing units;multiprocessing systems;performance evaluation;power aware computing;processor scheduling;CPU-GPU hybrid clusters;GPU energy consumption;GPU scaling interval;GPU-accelerated applications;data centers;dynamic voltage and frequency scaling;energy efficient real-time task scheduling;heuristic scheduling algorithms;real-world power measurement traces;task execution time;total energy consumption reduction;upper bound;Energy consumption;Graphics processing units;Power demand;Runtime;Scheduling algorithms;Servers}, 
doi={10.1109/INFOCOM.2017.8057205}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057206, 
author={D. Cheng and Y. Chen and X. Zhou and D. Gmach and D. Milojicic}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Adaptive scheduling of parallel jobs in spark streaming}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Streaming data analytics has become increasingly vital in many applications such as dynamic content delivery (e.g., advertisements), Twitter sentiment analysis, and security event processing (e.g., intrusion detection systems, and spam filters). Emerging stream processing systems, such as Spark Streaming, treat the continuous stream as a series of micro-batches of data and continuously process these micro-batch jobs. Such micro-batch based stream processing provides several advantages over traditional stream processing systems, which process streaming data one record at a time, including fast recovery from failures, better load balancing and scalability. However, efficient scheduling of micro-batch jobs to achieve high throughput and low latency is very challenging due to the complex data dependency and dynamism inherent in streaming workloads. In this paper, we propose A-scheduler, an adaptive scheduling approach that dynamically schedules parallel micro-batch jobs in Spark Streaming and automatically adjusts scheduling parameters to improve performance and resource efficiency. Specifically, A-scheduler dynamically schedules multiple jobs concurrently using different policies based on their data dependencies and automatically adjusts the level of job parallelism and resource shares among jobs based on workload properties. We implemented A-scheduler and evaluated it with a real-time security event processing workload. Our experimental results show that A-scheduler can reduce end-to-end latency by 42% and improve workload throughput and energy efficiency by 21% and 13%, respectively, compared to the default Spark Streaming scheduler.}, 
keywords={adaptive scheduling;data analysis;dynamic scheduling;parallel processing;resource allocation;scheduling;security of data;A-scheduler;Spark Streaming scheduler;adaptive scheduling approach;data analytics;data dependencies;data dependency;dynamic content delivery;intrusion detection systems;job parallelism;microbatch based stream processing;microbatch jobs;parallel jobs;real-time security event processing workload;resource shares;streaming data;traditional stream processing systems;Parallel processing;Real-time systems;Resource management;Schedules;Sparks;Throughput}, 
doi={10.1109/INFOCOM.2017.8057206}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057207, 
author={W. Wang and Y. Chen and L. Yang and Q. Zhang}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Detecting on-body devices through creeping wave propagation}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={The ability to detect which wearables and smartphones are on the same body has the potential to support a wealth of applications, including user authentication, automatic data synchronization, and personalized profile loading. This paper brings this feature to commercial off-the-shelf (COTS) wearables and smartphones, by creating a virtual “on-body detection sensor” based on devices' inherent wireless capabilities. We investigate using the peculiar propagation characteristics of creeping waves to discern on-body wearables. To this end, we decompose signals into multiple independent components to exploit the variation features of creeping waves. We implement our system on COTS wearables and a smartphone. Extensive experiments are conducted in a lab, apartments, malls, and outdoor areas, involving 12 volunteer subjects of different age groups, to demonstrate the robustness of our system. Results show that our system can identify on-body devices at 92.3% average true positive rate and 5% average false positive rate.}, 
keywords={body sensor networks;electromagnetic wave propagation;object detection;smart phones;COTS wearables;automatic data synchronization;commercial off-the-shelf wearables;creeping wave propagation;on-body devices;on-body wearables;personalized profile loading;smartphone;smartphones;user authentication;virtual on-body detection sensor;Biomedical monitoring;Dynamics;Feature extraction;Radio propagation;Smart phones;Wireless communication;Wireless sensor networks;Creeping waves;on-body detection;wearables}, 
doi={10.1109/INFOCOM.2017.8057207}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057208, 
author={X. Guo and J. Liu and Y. Chen}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={FitCoach: Virtual fitness coach empowered by wearable mobile devices}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Acknowledging the powerful sensors on wearables and smartphones enabling various applications to improve users' life styles and qualities (e.g., sleep monitoring and running rhythm tracking), this paper takes one step forward developing FitCoach, a virtual fitness coach leveraging users' wearable mobile devices (including wrist-worn wearables and arm-mounted smartphones) to assess dynamic postures (movement patterns & positions) in workouts. FitCoach aims to help the user to achieve effective workout and prevent injury by dynamically depicting the short-term and long-term picture of a user's workout based on various sensors in wearable mobile devices. In particular, FitCoach recognizes different types of exercises and interprets fine-grained fitness data (i.e., motion strength and speed) to an easy-to-understand exercise review score, which provides a comprehensive workout performance evaluation and recommendation. FitCoach has the ability to align the sensor readings from wearable devices to the human coordinate system, ensuring the accuracy and robustness of the system. Extensive experiments with over 5000 repetitions of 12 types of exercises involve 12 participants doing both anaerobic and aerobic exercises in indoors as well as outdoors. Our results demonstrate that FitCoach can provide meaningful review and recommendations to users by accurately measure their workout performance and achieve 93% accuracy for workout analysis.}, 
keywords={accelerometers;assisted living;biomechanics;body sensor networks;medical computing;mobile computing;patient rehabilitation;sleep;smart phones;FitCoach;arm-mounted smart phones;comprehensive workout performance evaluation;dynamic postures;exercise review score;fine-grained fitness data;movement pattern-and-positions;running rhythm tracking;sensor readings;sleep monitoring;smartphones;virtual fitness coach;wearable devices;wearable mobile devices;workout analysis;wrist-worn wearables;Biomedical monitoring;Monitoring;Performance evaluation;Smart phones;Wearable sensors}, 
doi={10.1109/INFOCOM.2017.8057208}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057209, 
author={L. Xie and X. Dong and W. Wang and D. Huang}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Meta-activity recognition: A wearable approach for logic cognition-based activity sensing}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Activity sensing has become a key technology for many ubiquitous applications, such as exercise monitoring and elder care. Most traditional approaches track the human motions and perform activity recognition based on the waveform matching schemes in the raw data representation level. In regard to the complex activities with relatively large moving range, they usually fail to accurately recognize these activities, due to the inherent variations in human activities. In this paper, we propose a wearable approach for logic cognition-based activity sensing scheme in the logical representation level, by leveraging the meta-activity recognition. Our solution extracts the angle profiles from the raw inertial measurements, to depict the angle variation of limb movement in regard to the consistent body coordinate system. It further extracts the meta-activity profiles to depict the sequence of small-range activity units in the complex activity. By leveraging the least edit distance-based matching scheme, our solution is able to accurately perform the activity sensing. Based on the logic cognition-based activity sensing, our solution achieves lightweight-training recognition, which requires a small quantity of training samples to build the templates, and user-independent recognition, which requires no training from the specific user. The experiment results in real settings shows that our meta-activity recognition achieves an average accuracy of 92% for user-independent activity sensing.}, 
keywords={cognition;feature extraction;geriatrics;medical signal processing;patient monitoring;ubiquitous computing;wearable computers;complex activity;elder care;exercise monitoring;human activities;human motions;least edit distance-based matching scheme;limb movement;logic cognition-based activity sensing;logical representation level;meta-activity profiles;meta-activity recognition;raw data representation level;small-range activity units;ubiquitous applications;user-independent activity sensing;waveform matching;wearable approach;Activity recognition;Coordinate measuring machines;Motion measurement;Optical wavelength conversion;Training;Transforms}, 
doi={10.1109/INFOCOM.2017.8057209}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057210, 
author={X. Liang and T. Yun and R. Peterson and D. Kotz}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={LightTouch: Securely connecting wearables to ambient displays with user intent}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Wearables are small and have limited user interfaces, so they often wirelessly interface with a personal smartphone/computer to relay information from the wearable for display or other interactions. In this paper, we envision a new method, LightTouch, by which a wearable can establish a secure connection to an ambient display, such as a television or a computer monitor, while ensuring the user's intention to connect to the display. LightTouch uses standard RF methods (like Bluetooth) for communicating the data to display, securely bootstrapped via the visible-light communication (the brightness channel) from the display to the low-cost, low-power, ambient light sensor of a wearable. A screen `touch' gesture is adopted by users to ensure that the modulation of screen brightness can be securely captured by the ambient light sensor with minimized noise. Wireless coordination with the processor driving the display establishes a shared secret based on the brightness channel information. We further propose novel onscreen localization and correlation algorithms to improve security and reliability. Through experiments and a preliminary user study we demonstrate that LightTouch is compatible with current display and wearable designs, is easy to use (about 6 seconds to connect), is reliable (up to 98% success connection ratio), and is secure against attacks.}, 
keywords={interactive devices;mobile handsets;security of data;smart phones;user interfaces;LightTouch;ambient display;ambient light sensor;brightness channel information;connection ratio;personal computer;personal smartphone;preliminary user study;screen brightness;screen touch gesture;standard RF methods;user intent;user interfaces;visible-light communication;wearable designs;wireless coordination;Biomedical monitoring;Brightness;Correlation;Light sources;Monitoring;Radio frequency;Security}, 
doi={10.1109/INFOCOM.2017.8057210}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057211, 
author={Z. Li and K. G. Shin and L. Zhen}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={When and how much to neutralize interference?}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Interference management (IM) is essential to wireless communication networks, but interference suppression, a key component of IM, is known to degrade users' achievable spectral efficiency (SE). It is thus important to select an appropriate IM method with optimal operating parameters according to diverse network deployments, transmit power differences of various communication equipments, and dynamically changing channel conditions so as to balance the benefits brought by and the cost of IM. Interference neutralization (IN) has recently been receiving considerable attention, with which a duplicate of interference of the same strength and opposite phase w.r.t. the original interfering signal is generated to neutralize the disturbance at the intended receiver. However, to the best of our knowledge, all existing IN schemes assume that interference is completely neutralized without accounting for their power consumption. To remedy this deficiency, we propose a novel scheme, called dynamic interference neutralization (DIN). By intelligently determining the appropriate portion of interference to be neutralized, we balance the transmitter's power used for IN and the desired signal's transmission. We then present a new way to adaptively select one of DIN and other IM methods by taking into account the cost of multiple IM methods and their benefits. Our analysis has shown that DIN can include complete IN and non-interference management (non-IM) as special cases. The proposed strategy is shown via simulation to be able to make better use of the transmit power than existing IM methods, hence enhancing users' SE.}, 
keywords={interference suppression;radio networks;radiofrequency interference;DIN;channel conditions;communication equipments;diverse network deployments;dynamic interference neutralization;intended receiver;interference management;interference suppression;multiple IM methods;nonIM;noninterference management;opposite phase w.r.t;optimal operating parameters;original interfering signal;power consumption;transmit power differences;transmitter;wireless communication networks;Array signal processing;Conferences;Interference;Macrocell networks;Receivers;Relays;Transmitters}, 
doi={10.1109/INFOCOM.2017.8057211}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057212, 
author={K. C. Hsu and K. C. J. Lin and H. Y. Wei}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Inter-client interference cancellation for full-duplex networks}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Recent studies have experimentally shown the gains of full-duplex radios. However, due to its relatively higher cost and complexity, we can envision a more practical step in the network evolution is to have a full-duplex access point (AP) but keep the clients half-duplex. Unfortunately, the full-duplex gains can hardly be extracted in practice as the uplink transmission from a half-duplex client introduces inter-client interference to another downlink client. To address this issue, we present the design and implementation of IC2 (Inter-Client Interference Cancellation), the first physical layer solution that exploits the AP's full-duplex capability to actively cancel the interference at the downlink client. Such active cancellation not only improves the achievable capacity, but also better tolerates imperfect user pairing, simplifying the MAC design as a result. We build a prototype of IC2 on USRP-N200 and evaluate its performance via both testbed experiments and large-scale trace-driven simulations. The results show that, without IC2, about 60% of client pairs produce no gain from full-duplex transmissions, while, with IC2 the median throughput gain over conventional half-duplex networks can be 1.65× even when clients are simply paired randomly.}, 
keywords={access protocols;interference suppression;radio networks;radiofrequency interference;software radio;wireless LAN;AP's full-duplex capability;IC2;Inter-Client Interference Cancellation;Inter-client interference cancellation;active cancellation;client pairs;downlink client;full-duplex access point;full-duplex networks;full-duplex radios;full-duplex transmissions;Downlink;Hardware;Interference cancellation;Relays;Throughput;Uplink}, 
doi={10.1109/INFOCOM.2017.8057212}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057213, 
author={T. Vermeulen and M. Laghate and G. Hattab and D. Cabric and S. Pollin}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Towards instantaneous collision and interference detection using in-band full duplex}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Wireless devices are ubiquitous nowadays and, since most of them use the same unlicensed frequency bands, the high number of packet losses due to interference and collisions degrade performance. Reliability, energy consumption, and latency are key challenges for future dense networks. Allowing the transmitter to take action, i.e., vacating the channel, as soon as a collision or interference is detected is crucial in improving these metrics. In-band full duplex radios enable the transmitter to simultaneously transmit packets and sense the spectrum for collisions and interference. This paper studies two important questions regarding transmitter-based collision and interference detection: (1) from an overall system perspective, does such detection outperform receiver-based detection and (2) which test statistic is the most accurate and sensitive at detecting collisions and interference. First, NS-3 simulations are used to show that transmitter-based detection reduces the energy consumption while improving the throughput in a typical star topology network. Next, we present a measurement-based study of four different techniques for transmitter-based collision and interference detection. In particular, we compare the energy detector with three goodness-of-fit tests in terms of probability of detection and false alarm. Our analysis shows that transmitter-based detection can detect between 80% to 100% of the collisions and interference occurring at the receiver, depending on the distance between the transmitter and the receiver. Of those detectable by the transmitter, our measurement results show that goodness-of-fit tests can detect nearly 100% of the collisions and have at least 10 dB better sensitivity as compared to the commonly proposed energy detection test. In general, the proposed techniques can detect interfering signals that are up to 25 dB below the remaining self-interference power.}, 
keywords={energy consumption;interference suppression;probability;radio receivers;radio transmitters;radiofrequency interference;signal detection;telecommunication network reliability;telecommunication network topology;wireless channels;In-band full duplex radios;NS-3 simulations;collision;energy consumption reduction;energy detection test;false alarm;goodness-of-fit tests;instantaneous collision;interference detection;interfering signal detection;latency;probability of detection;receiver;remaining self-interference power;star topology network;transmitter-based collision;unlicensed frequency bands;Energy consumption;Interference;Radio transmitters;Receivers;Reliability;Silicon}, 
doi={10.1109/INFOCOM.2017.8057213}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057214, 
author={G. Naik and J. Liu and J. M. J. Park}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Coexistence of Dedicated Short Range Communications (DSRC) and Wi-Fi: Implications to Wi-Fi performance}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={The 5.9 GHz band is being actively explored for possible spectrum sharing opportunities between Dedicated Short Range Communications (DSRC) and IEEE 802.11ac networks in order to address the increasing demand for bandwidth-intensive Wi-Fi applications. In this paper, we study the implications of this spectrum sharing to the performance of Wi-Fi systems. Through experiments performed on our testbed, we first investigate band sharing options available for Wi-Fi devices. Using experimental results, we show the need for using conservative Wi-Fi transmission parameters to enable harmonious coexistence between DSRC and Wi-Fi. Moreover, we show that under the current 802.11ac standard, certain channelization options, particularly the high bandwidth ones, cannot be used by Wi-Fi devices without causing interference to the DSRC nodes. Under these constraints, we propose a Real-time Channelization Algorithm (RCA) for Wi-Fi Access Points (APs) operating in the shared spectrum. Evaluation of the proposed algorithm using a prototype implementation on commodity hardware as well as via simulations show that informed channelization decisions can significantly increase Wi-Fi throughput compared to static channelization schemes.}, 
keywords={radio spectrum management;real-time systems;wireless LAN;DSRC;IEEE 802.11ac networks;Wi-Fi Access Points;Wi-Fi devices;Wi-Fi performance;Wi-Fi systems;band sharing options;bandwidth-intensive Wi-Fi applications;conservative Wi-Fi transmission parameters;dedicated short range communications;frequency 5.9 GHz;real-time channelization algorithms;shared spectrum;spectrum sharing;Bandwidth;Hardware;IEEE 802.11 Standard;Performance evaluation;Transmitters;Wireless fidelity}, 
doi={10.1109/INFOCOM.2017.8057214}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057215, 
author={R. Ben Basat and G. Einziger and R. Friedman and Y. Kassner}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Randomized admission policy for efficient top-k and frequency estimation}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Network management protocols often require timely and meaningful insight about per flow network traffic. This paper introduces Randomized Admission Policy (RAP) - a novel algorithm for the frequency and top-k estimation problems, which are fundamental in network monitoring. We demonstrate space reductions compared to the alternatives by a factor of up to 32 on real packet traces and up to 128 on heavy-tailed workloads. For top-k identification, RAP exhibits memory savings by a factor of between 4 and 64 depending on the workloads' skewness. These empirical results are backed by formal analysis, indicating the asymptotic space improvement of our probabilistic admission approach. Additionally, we present d-Way RAP, a hardware friendly variant of RAP that empirically maintains its space and accuracy benefits.}, 
keywords={frequency estimation;protocols;telecommunication network management;telecommunication traffic;Randomized Admission Policy;Randomized admission policy;flow network traffic;frequency estimation;network management protocols;top-k estimation problems;Algorithm design and analysis;Estimation;Frequency estimation;Monitoring;Probabilistic logic;Radiation detectors;Random access memory}, 
doi={10.1109/INFOCOM.2017.8057215}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057216, 
author={R. Ben Basat and G. Einziger and R. Friedman and Y. Kassner}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Optimal elephant flow detection}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Monitoring the traffic volumes of elephant flows, including the total byte count per flow, is a fundamental capability for online network measurements. We present an asymptotically optimal algorithm for solving this problem in terms of both space and time complexity. This improves on previous approaches, which can only count the number of packets in constant time. We evaluate our work on real packet traces, demonstrating an up to X2.5 speedup compared to the best alternative.}, 
keywords={computational complexity;optimisation;telecommunication network management;telecommunication traffic;asymptotically optimal algorithm;byte count;constant time;fundamental capability;online network measurements;optimal elephant flow detection;space complexity;time complexity;traffic volumes;Data structures;Maintenance engineering;Monitoring;Radiation detectors;Real-time systems;Runtime;Software algorithms}, 
doi={10.1109/INFOCOM.2017.8057216}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057217, 
author={G. Xie and K. Xie and J. Huang and X. Wang and Y. Chen and J. Wen}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Fast low-rank matrix approximation with locality sensitive hashing for quick anomaly detection}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Detecting anomalous traffic is a critical task for advanced Internet management. The traditional approaches based on Principal Component Analysis (PCA) are effective only when the corruption is caused by small additive i.i.d. Gaussian noise. The recent Direct Robust Matrix Factorization (DRMF) is proven to be more robust and accurate in anomaly detection, but it incurs a high computation cost due to its need of singular value decomposition (SVD) for low-rank matrix approximation and the iterative use of SVD execution to find the final solution. To enable the anomaly detection for large traffic matrix with the use of DRMF, we formulate the low-rank matrix approximation problem as a problem of searching for the subspace to project the traffic matrix with the minimum error. We propose a novel approach, LSH-subspace, for fast low-rank matrix approximation. To facilitate the matrix partition for the quick search of the subspace, we propose several novel techniques: a multi-layer locality sensitive hashing (LSH) table to reorder the OD pairs based on LSH function, a partition principle to guide the partition to minimize the projection error, and a lightweight algorithm to exploit the sparsity of the outlier matrix to update the LSH table at low overhead. Our extensive simulations based on real trace data demonstrate that our LSH-subspace is 3 times faster than DRMF with high anomaly detection accuracy.}, 
keywords={Internet;approximation theory;matrix decomposition;security of data;singular value decomposition;DRMF;LSH-subspace;SVD execution;anomalous traffic detection;direct robust matrix factorization;fast low-rank matrix approximation;high anomaly detection accuracy;low-rank matrix approximation problem;matrix partition;multilayer locality sensitive hashing;multilayer locality sensitive hashing table;outlier matrix;partition principle;quick anomaly detection;singular value decomposition;traffic matrix;Anomaly detection;Approximation algorithms;Matrix decomposition;Noise measurement;Principal component analysis;Robustness;Sparse matrices;Anomaly Detection;Low-Rank Matrix Approximation}, 
doi={10.1109/INFOCOM.2017.8057217}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057218, 
author={K. Xie and C. Peng and X. Wang and G. Xie and J. Wen}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Accurate recovery of internet traffic data under dynamic measurements}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={The inference of the network traffic matrix from partial measurement data becomes increasingly critical for various network engineering tasks, such as capacity planning, load balancing, path setup, network provisioning, anomaly detection, and failure recovery. The recent study shows it is promising to more accurately interpolate the missing data with a three-dimensional tensor as compared to interpolation methods based on two-dimensional matrix. Despite the potential, it is difficult to form a tensor with measurements taken at varying rate in a practical network. To address the issues, we propose Reshape-Align scheme to form the regular tensor with data from dynamic measurements, and introduce user-domain and temporal-domain factor matrices which takes full advantage of features from both domains to translate the matrix completion problem to the tensor completion problem based on CP decomposition for more accurate missing data recovery. Our performance results demonstrate that our Reshape-Align scheme can achieve significantly better performance in terms of two metrics: error ratio and mean absolute error (MAE).}, 
keywords={Internet;interpolation;matrix algebra;telecommunication traffic;tensors;Reshape-Align scheme;anomaly detection;capacity planning;data recovery;dynamic measurements;failure recovery;internet traffic data;interpolation methods;load balancing;matrix completion problem;mean absolute error;network engineering tasks;network provisioning;network traffic matrix;partial measurement data;path setup;practical network;regular tensor;temporal-domain factor matrices;tensor completion problem;three-dimensional tensor;two-dimensional matrix;user-domain;Computational modeling;Correlation;Indexes;Interpolation;Matrix decomposition;Monitoring;Tensile stress;Internet traffic data recovery;Matrix completion;Tensor completion}, 
doi={10.1109/INFOCOM.2017.8057218}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057219, 
author={F. Dang and P. Zhou and Z. Li and E. Zhai and A. Mohaisen and Q. Wen and M. Li}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Large-scale invisible attack on AFC systems with NFC-equipped smartphones}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Automated Fare Collection (AFC) systems have been globally deployed for decades, particularly in public transportation. Although the transaction messages of AFC systems are mostly transferred in plaintext, which is obviously insecure, system operators do not need to pay much attention to this issue, since the AFC network is well isolated from public network (e.g., the Internet). Nevertheless, in recent years, the advent of Near Field Communication (NFC)-equipped smartphones has bridged the gap between the AFC network and the Internet through Host-based Card Emulation (HCE). Motivated by this fact, we design and practice a novel paradigm of attack on modern distance-based pricing AFC systems, enabling users to pay much less than actually required. Our constructed attack has two important properties: 1) it is invisible to AFC system operators because the attack never causes any inconsistency in the backend database of the operators; and 2) it can be scalable to large number of users (e.g., 10,000) by maintaining a moderate-sized AFC card pool (e.g., containing 150 cards). Based upon this constructed attack, we developed an HCE app, named LessPay. Our real-world experiments on LessPay demonstrate not only the feasibility of our attack (with 97.6% success rate), but also its low-overhead in terms of bandwidth and computation.}, 
keywords={Internet;mobile computing;near-field communication;security of data;smart cards;smart phones;AFC network;HCE;LessPay;NFC-equipped smartphones;Near Field Communication;automated fare collection systems;backend database;distance-based pricing AFC systems;host-based card emulation;large-scale invisible attack;moderate-sized AFC card pool;public network;public transportation;transaction messages;Authentication;Conferences;Protocols;Smart phones;Urban areas;Web servers}, 
doi={10.1109/INFOCOM.2017.8057219}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057220, 
author={Y. Chen and J. Sun and X. Jin and T. Li and R. Zhang and Y. Zhang}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Your face your heart: Secure mobile face authentication with photoplethysmograms}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Face authentication emerges as a powerful method for preventing unauthorized access to mobile devices. It is, however, vulnerable to photo-based forgery attacks (PFA) and videobased forgery attacks (VFA), in which the adversary exploits a photo or video containing the user's frontal face. Effective defenses against PFA and VFA often rely on liveness detection, which seeks to find a live indicator that the submitted face photo or video of the legitimate user is indeed captured in real time. In this paper, we propose FaceHeart, a novel and practical face authentication system for mobile devices. FaceHeart simultaneously takes a face video with the front camera and a fingertip video with the rear camera on COTS mobile devices. It then achieves liveness detection by comparing the two photoplethysmograms independently extracted from the face and fingertip videos, which should be highly consistent if the two videos are for the same live person and taken at the same time. As photoplethysmograms are closely tied to human cardiac activity and almost impossible to forge or control, FaceHeart is strongly resilient to PFA and VFA. Extensive user experiments on Samsung Galaxy S5 have confirmed the high efficacy and efficiency of FaceHeart.}, 
keywords={face recognition;feature extraction;mobile computing;security of data;smart phones;video cameras;video signal processing;COTS mobile devices;FaceHeart;PFA;Samsung Galaxy S5;VFA;extensive user experiments;face authentication system;face photo;face video;fingertip video;fingertip videos;legitimate user;live indicator;live person;liveness detection;mobile face authentication security;photo-based forgery attack;photoplethysmograms;rear camera;unauthorized access;videobased forgery attacks;Authentication;Cameras;Face;Feature extraction;Mobile communication;Mobile handsets;Streaming media}, 
doi={10.1109/INFOCOM.2017.8057220}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057221, 
author={H. Fu and Z. Zheng and S. Bose and M. Bishop and P. Mohapatra}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={LeakSemantic: Identifying abnormal sensitive network transmissions in mobile applications}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Mobile applications (apps) often transmit sensitive data through network with various intentions. Some transmissions are needed to fulfill the app's functionalities. However, transmissions with malicious receivers may lead to privacy leakage and tend to behave stealthily to evade detection. The problem is twofold: how does one unveil sensitive transmissions in mobile apps, and given a sensitive transmission, how does one determine if it is legitimate? In this paper, we propose LeakSemantic, a framework that can automatically locate abnormal sensitive network transmissions from mobile apps. LeakSemantic consists of a hybrid program analysis component and a machine learning component. Our program analysis component combines static analysis and dynamic analysis to precisely identify sensitive transmissions. Compared to existing taint analysis approaches, LeakSemantic achieves better accuracy with fewer false positives and is able to collect runtime data such as network traffic for each transmission. Based on features derived from the runtime data, machine learning classifiers are built to further differentiate between the legal and illegal disclosures. Experiments show that LeakSemantic achieves 91% accuracy on 2279 sensitive connections from 1404 apps.}, 
keywords={data privacy;learning (artificial intelligence);mobile computing;pattern classification;program diagnostics;security of data;system monitoring;LeakSemantic;abnormal sensitive network transmission identification;dynamic analysis;hybrid program analysis component;illegal disclosures;legal disclosures;machine learning classifiers;mobile applications;network traffic;runtime data;sensitive data transmission;Analytical models;Androids;Conferences;Humanoid robots;Mobile communication;Privacy;Runtime}, 
doi={10.1109/INFOCOM.2017.8057221}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057222, 
author={Y. Yang and J. Sun}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Energy-efficient W-layer for behavior-based implicit authentication on mobile devices}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Motivated by the great potential of implicit and seamless user authentication, we attempt to build an efficient middle layer running on mobile devices to support implicit authentication (IA) systems with adaptive sampling. Various activities, such as user location, application usage, user motion, and battery usage have been popular choices to generate behaviors, the soft biometrics, for implicit authentication. Unlike password-based or hard biometric-based authentication, implicit authentication does not require explicit user action or expensive hardware. However, user behaviors can change unpredictably which renders it more challenging to develop systems that depend on them. Various machine learning algorithms have been used to address this challenge. The expensive training process is usually outsourced to the remote server but this can potentially increase the chance of data leakage. In addition, mobile devices may not always have reliable network connections to send real-time data to the server for training. Motivated by these limitations, we propose a W-layer, an overlay that provides an energy-efficient solution for real-time implicit authentication on mobile devices. The size of the data the system needs to collect at different times depends on the legitimacy of the user. This in turn affects how the sampling rate is adjusted which can reduce energy consumption. To evaluate our method, we conducted several experiments on both synthetic and real datasets. The average accuracy of identifying legitimate users is 96.73% using the synthetic dataset and 96.70% using the real dataset. Furthermore, we tested the power consumption on a low-end Nexus S smartphone to obtain a more pessimistic result. We found that our method consumed 14.5% of the device's total battery usage. The power consumption performance is expected to improve significantly on high-end mobile devices.}, 
keywords={biometrics (access control);energy conservation;learning (artificial intelligence);message authentication;mobile computing;mobile handsets;power aware computing;smart phones;Nexus S smartphone;battery usage;energy consumption reduction;energy-efficient W-layer;energy-efficient solution;high-end mobile devices;implicit authentication systems;implicit user authentication;legitimate users;power consumption;real-time implicit authentication;seamless user authentication;user action;user behaviors;user location;user motion;Authentication;Biometrics (access control);Mobile handsets;Real-time systems;Sensors;Servers;Training}, 
doi={10.1109/INFOCOM.2017.8057222}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057223, 
author={R. Vaze}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Online knapsack problem and budgeted truthful bipartite matching}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Two related online problems: knapsack and truthful bipartite matching are considered. For these two problems, the common theme is how to `match' an arriving left vertex in an online fashion with any of the available right vertices, if at all, so as to maximize the sum of the value of the matched edges, subject to satisfying a sum-weight constraint on the matched left vertices. Assuming that the left vertices arrive in an uniformly random order (secretary model), two almost similar algorithms are proposed for the two problems, that are 2e competitive and 24 competitive, respectively. The proposed online bipartite matching algorithm is also shown to be truthful: there is no incentive for any left vertex to misreport its bid/weight. Direct applications of these problems include job allocation with load balancing, generalized adwords, crowdsourcing auctions, and matching wireless users to cooperative relays in device-to-device communication enabled cellular network.}, 
keywords={combinatorial mathematics;knapsack problems;bipartite matching algorithm;cooperative relays;crowdsourcing auctions;device-to-device communication enabled cellular network;generalized adwords;job allocation;left vertex;load balancing;online knapsack problem;sum-weight constraint;truthful bipartite matching;wireless users;Conferences;Crowdsourcing;Device-to-device communication;Impedance matching;Load management;Resource management;Wireless communication}, 
doi={10.1109/INFOCOM.2017.8057223}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057224, 
author={J. Zhang and E. Modiano}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Robust routing in interdependent networks}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={We consider a model of two interdependent networks, where every node in one network depends on one or more supply nodes in the other network and a node fails if it loses all of its supply nodes. We develop algorithms to compute the failure probability of a path, and obtain the most reliable path between a pair of nodes in a network, under the condition that each supply node fails independently with a given probability. Our work generalizes the classical shared risk group model, by considering multiple risks associated with a node and letting a node fail if all the risks occur. Moreover, we study the diverse routing problem by considering two paths between a pair of nodes. We define two paths to be d-failure resilient if at least one path survives after removing d or fewer supply nodes, which generalizes the concept of disjoint paths in a single network, and risk-disjoint paths in a classical shared risk group model. We compute the probability that both paths fail, and develop algorithms to compute the most reliable pair of paths.}, 
keywords={probability;telecommunication network reliability;telecommunication network routing;diverse routing problem;failure probability;interdependent networks;risk-disjoint paths;shared risk group model;supply node;Algorithm design and analysis;Approximation algorithms;Computational modeling;Computer network reliability;Robustness;Routing}, 
doi={10.1109/INFOCOM.2017.8057224}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057225, 
author={J. Yu and X. Ning and Y. Sun and S. Wang and Y. Wang}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Constructing a self-stabilizing CDS with bounded diameter in wireless networks under SINR}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={As a virtual backbone structure, connected dominating sets (CDSs) play an important role in topology control for wireless networks. In this paper, we develop a distributed self-stabilizing CDS construction algorithm under the SINR model (also known as the physical interference model), a more practical yet more challenging interference model for distributed algorithm design. Specifically, we propose a randomized distributed algorithm that can construct a CDS in O (log n) timeslots with a high probability, where n is the total number of nodes in the network. The constructed CDS achieves constant approximation in both density and diameter. To the best of our knowledge, this is the first known asymptotically optimal self-stabilizing result in terms of both density and diameter for distributed CDS construction under the practical SINR model.}, 
keywords={approximation theory;distributed algorithms;probability;radiofrequency interference;set theory;telecommunication network topology;wireless sensor networks;asymptotically optimal self-stabilizing result;bounded diameter;distributed algorithm design;distributed self-stabilizing CDS construction algorithm;physical interference model;practical SINR model;randomized distributed algorithm;topology control;virtual backbone structure;wireless networks;Algorithm design and analysis;Approximation algorithms;Interference;Signal to noise ratio;Wireless networks;Wireless sensor networks}, 
doi={10.1109/INFOCOM.2017.8057225}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057226, 
author={W. Li and J. Zhang and Y. Zhao}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Conflict graph embedding for wireless network optimization}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={With the dense deployment of wireless infrastructure such as radio towers and WiFi access points, wireless network optimization becomes very important for improving the network capacity and enhancing the communication quality of wireless links. Most optimization algorithms rely on the conflict graph to describe the interference situation. However, building a conflict graph requires exhaustive measurements of the whole network and the existing estimation approaches are static and inaccurate. In this paper, we propose a conflict graph embedding approach to assess network interference situations by representing the wireless nodes with low-dimensional vectors while preserving their conflict relationships. Specifically, our approach introduces a sliding-window based partial measurement method to sample the interference graph in the network, then adopts a learning algorithm to obtain the vector representation of the nodes, and then infers the interference situations by exploring the feature vectors. The proposed approach has been proved to be low measurement overhead, low computational cost, and self-adaptive, which is suitable for large-scale dynamic wireless networks. We illustrate that conflict graph embedding can be used for interference-aware wireless network optimizations. We conduct extensive experiments based on real wireless network datasets, which show the efficiency of the proposed approach.}, 
keywords={graph theory;optimisation;radio networks;radiofrequency interference;WiFi access points;conflict graph embedding;interference situation;interference-aware wireless network optimizations;large-scale dynamic wireless networks;network capacity;optimization algorithms;sliding-window based partial measurement method;wireless links;wireless network datasets;wireless network optimization;Extraterrestrial measurements;Interference;Optimization;Time measurement;Wireless networks}, 
doi={10.1109/INFOCOM.2017.8057226}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057227, 
author={S. C. Lin and I. F. Akyildiz}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Dynamic base station formation for solving NLOS problem in 5G millimeter-wave communication}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Millimeter-wave communication is one of the enabling technologies to meet high data-rate requirements of 5G wireless systems. Millimeter-wave systems due large available bandwidth enable gigabit-per-second data rates for line-of-sight (LOS) transmissions in short distances. However, for non-line-of-sight (NLOS) transmissions, millimeter-wave systems suffers performance degradation because the received signal strengths at user equipments (UEs) are not satisfactory. In this paper, the NLOS problem in millimeter-wave systems is treated from SoftAir (a wireless software-defined networking architecture) perspective. In particular, a so-called dynamic base station (BS) formation is introduced, which adaptively coordinates BSs and their multiple antennas to always satisfy UEs' quality-of-service (QoS) requirements in NLOS cases. First, the architecture for software-defined millimeter-wave system is introduced, where remote radio heads (RRHs) coordination is explained and millimeter-wave channel model between RRHs and UEs is analyzed. A ubiquitous millimeter-wave coverage problem is formulated, which jointly optimizes RRH-UE associations and beamforming weights of RRHs to maximize the UE sum-rate while guaranteeing QoS and system-level constraints. After proving the np-hardness of the coverage optimization problem with non-convex constraints, an iterative algorithm is developed for dynamic BS formation that achieves ubiquitous coverage with high data rates in LOS and NLOS cases. Through successive convex approximations, the proposed dynamic BS formation algorithm transforms the original mixed-integer nonlinear programming into a mixed-integer second-order cone programming, which is efficiently solved by convex tools. Simulations validate the efficacy of our solution that completely solves NLOS problem by facilitating ubiquitous coverage in 5G millimeter-wave systems.}, 
keywords={5G mobile communication;RSSI;approximation theory;array signal processing;concave programming;convex programming;integer programming;iterative methods;quality of service;software defined networking;5G millimeter-wave communication;5G wireless systems;NLOS cases;NLOS problem;SoftAir;beamforming weights;convex approximations;coverage optimization problem;dynamic base station formation;iterative algorithm;line-of-sight transmission;millimeter-wave channel model;mixed-integer nonlinear programming;mixed-integer second-order cone programming;multiple antennas;nonconvex constraints;nonline-of-sight transmissions;quality-of-service;received signal strengths;remote radio heads;software-defined millimeter-wave system;system-level constraints;ubiquitous millimeter-wave coverage problem;user equipments;wireless software-defined networking architecture;5G mobile communication;Antennas;Array signal processing;Computer architecture;Millimeter wave communication;Nonlinear optics}, 
doi={10.1109/INFOCOM.2017.8057227}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057228, 
author={S. Borst and A. Ö. Kaya and D. Calin and H. Viswanathan}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Dynamic path selection in 5G multi-RAT wireless networks}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Emerging 5G networks will not only offer higher link rates, but also integrate a variety of Radio Access Technologies (RATs) in order to provide ultra-reliable broadband access to a wide range of applications with high throughput and low latency requirements. SDN-enabled dynamic path selection is of critical importance in exploiting the collective transmission resources in such heterogeneous multi-RAT environments and delivering excellent user performance. In the present paper we propose the `best-rate' path selection algorithm for multi-RAT networks with various types of traffic flows. The best-rate algorithm accounts for the radio conditions and performance requirements of individual flows as well as the load conditions at the various access points. We analytically establish that the rates received by the various flows under the best-rate path selection, in conjunction with local fair resource sharing at the individual access points, are close to globally Proportional Fair. Detailed simulation experiments demonstrate that the best-rate algorithm achieves significant gains in terms of user-perceived throughput performance over various baseline policies.}, 
keywords={5G mobile communication;radio access networks;resource allocation;software defined networking;5G multi-RAT wireless networks;Radio Access Technologies;SDN-enabled dynamic path selection;best-rate algorithm accounts;best-rate path selection algorithm;collective transmission resources;heterogeneous multiRAT environments;individual access points;local fair resource sharing;radio conditions;traffic flows;ultra-reliable broadband access;Heuristic algorithms;Load management;Resource management;Sociology;Statistics;Streaming media;Throughput}, 
doi={10.1109/INFOCOM.2017.8057228}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057229, 
author={J. J. Kuo and S. H. Shen and H. Y. Kang and D. N. Yang and M. J. Tsai and W. T. Chen}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Service chain embedding with maximum flow in software defined network and application to the next-generation cellular network architecture}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={With software-defined network (SDN) and network function virtualization (NFV) techniques, we can embed the service chain consisting of a sequence of virtualized network functions (VNFs), i.e., we can determine the flow path and deploy the VNFs contained in the service chain at any place on the path. In the literature, the methods of service chain embedding bound the number of VNFs at a node, whereas the link capacities are disregarded and the amount of flows is not considered, which could cause serious congestion. In addition, according to our experiment, the process overhead on a computation node is linear to the total amount of flows processed. In this paper, we propose a method of service chain embedding to maximize the total amount of flows while bounding the process overhead of the flows on a node by its computation capability and the total amount of flows on an link by its bandwidth capacity. To our knowledge, our method is the first approximation algorithm of service chain embedding with considering flow in the literature. Simulations show our algorithm has good performance in terms of the total amount of flows.}, 
keywords={cellular radio;embedded systems;mobile computing;software defined networking;virtualisation;SDN;VNF;flow path;next-generation cellular network architecture;service chain embedding;software defined network;virtualized network functions;Approximation algorithms;Bandwidth;Encapsulation;Network function virtualization;Routing;Servers;Software}, 
doi={10.1109/INFOCOM.2017.8057229}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057230, 
author={V. Sciancalepore and K. Samdanis and X. Costa-Perez and D. Bega and M. Gramaglia and A. Banchs}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Mobile traffic forecasting for maximizing 5G network slicing resource utilization}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={The emerging network slicing paradigm for 5G provides new business opportunities by enabling multi-tenancy support. At the same time, new technical challenges are introduced, as novel resource allocation algorithms are required to accommodate different business models. In particular, infrastructure providers need to implement radically new admission control policies to decide on network slices requests depending on their Service Level Agreements (SLA). When implementing such admission control policies, infrastructure providers may apply forecasting techniques in order to adjust the allocated slice resources so as to optimize the network utilization while meeting network slices' SLAs. This paper focuses on the design of three key network slicing building blocks responsible for (i) traffic analysis and prediction per network slice, (ii) admission control decisions for network slice requests, and (iii) adaptive correction of the forecasted load based on measured deviations. Our results show very substantial potential gains in terms of system utilization as well as a trade-off between conservative forecasting configurations versus more aggressive ones (higher gains, SLA risk).}, 
keywords={5G mobile communication;contracts;quality of service;resource allocation;telecommunication congestion control;telecommunication traffic;5G network slicing resource utilization;SLA;admission control decisions;admission control policies;allocated slice resources;business opportunities;different business models;emerging network slicing paradigm;forecasted load;infrastructure providers;mobile traffic forecasting;multitenancy support;network slice requests;network utilization;resource allocation algorithms;resource utilization;service level agreements;slicing building blocks;trade-off between conservative forecasting configurations;traffic analysis;3GPP;5G mobile communication;Admission control;Forecasting;Prediction algorithms;Resource management;Training}, 
doi={10.1109/INFOCOM.2017.8057230}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057231, 
author={S. Pradhan and L. Qiu and A. Parate and K. H. Kim}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={Understanding and managing notifications}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={In today's always-connected world, we receive a large number of notifications on our mobile devices. These notifications cause interruptions, stress, and even impact users' lifestyle. To understand how users respond to notifications, we develop an application that monitors various features (e.g., importance) of the notifications, users' actions, and the level of users' engagement with the notifications. We recruit 30 users to use the application and monitor over 30 days, and subsequently find that 20% to 50% of the notifications generally get ignored by the users. In addition, we also solicit explicit feedback about the importance of notifications from 12 users over 14 days and identify the relation between perceived importance and users' engagement level. Based on this study, we identify the key characteristics of notifications and users' engagement, which is further substantiated by an onfine survey of 400+ users. In addition, we develop a notification manager that includes a machine learning based prediction model and that shows only the important notifications and delays the unimportant notifications. Our experimental results show that our notification manager automatically assesses the importance of notifications with more than 87% accuracy. We believe this work is a promising step toward intelligent personal assistant that manages notifications.}, 
keywords={learning (artificial intelligence);mobile computing;user interfaces;always-connected world;important notifications;machine learning based prediction model;mobile devices;notification management;notification manager;notification understanding;user engagement;Conferences;Delays;Electronic mail;Mobile communication;Monitoring;Operating systems;Vibrations}, 
doi={10.1109/INFOCOM.2017.8057231}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057232, 
author={Y. Chen and X. Jin and J. Sun and R. Zhang and Y. Zhang}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={POWERFUL: Mobile app fingerprinting via power analysis}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Which apps a mobile user has and how they are used can disclose significant private information about the user. In this paper, we present the design and evaluation of POWERFUL, a new attack which can fingerprint sensitive mobile apps (or infer sensitive app usage) by analyzing the power consumption profiles on Android devices. POWERFUL works on the observation that distinct apps and their different usage patterns all lead to distinguishable power consumption profiles. Since the power profiles on Android devices require no permission to access, POWERFUL is very difficult to detect and can pose a serious threat against user privacy. Extensive experiments involving popular and sensitive apps in Google Play Store show that POWERFUL can identify the app used at any particular time with accuracy up to 92.9%, demonstrating the feasibility of POWERFUL.}, 
keywords={data privacy;mobile computing;power aware computing;security of data;smart phones;Android devices;Google Play Store;POWERFUL;mobile user;power analysis;power consumption profiles;power profiles;sensitive app usage;sensitive mobile app fingerprinting;usage patterns;user privacy;Androids;Feature extraction;Google;Humanoid robots;Mobile communication;Mobile handsets;Power demand}, 
doi={10.1109/INFOCOM.2017.8057232}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057233, 
author={Z. Li and M. Li and P. Mohapatra and J. Han and S. Chen}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={iType: Using eye gaze to enhance typing privacy}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={This paper presents iType, a system that uses eye gaze for typing private information on commodity mobile platforms. The design combats three primary challenges: 1) relatively low accuracy of mobile gaze tracking; 2) difficulties in correcting input errors due to lacking the comparison with the true text-entry value; and 3) device motions and other noises that may interfere gaze tracking accuracy and thus the iType performance. We devise a set of effective techniques, including leveraging a collective behavior of the gaze tracking results, unique correlation of the typing error spatial distributions, and motion sensor hints from mobile devices, to address above challenges. A set of enhancement techniques are applied to further improve iType's robustness and reliability. We consolidate above designs and implement iType on iOS platform. Evaluations show that iType achieves high keystroke detection accuracy for the secure typing within a reasonable short latency.}, 
keywords={data privacy;eye;gaze tracking;mobile computing;commodity mobile platforms;enhancement techniques;eye gaze;gaze tracking accuracy;high keystroke detection accuracy;iOS platform;iType performance;input errors;leveraging a collective behavior;mobile devices;mobile gaze tracking;motion sensor hints;private information;reliability;secure typing;text-entry value;typing error spatial distributions;typing privacy;Cameras;Engines;Gaze tracking;Keyboards;Mobile communication;Mobile handsets;Privacy}, 
doi={10.1109/INFOCOM.2017.8057233}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8057234, 
author={P. Y. Cao and G. Li and A. C. Champion and D. Xuan and S. Romig and W. Zhao}, 
booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
title={On human mobility predictability via WLAN logs}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={In this research, we conduct a comprehensive measurement study on the predictability of human mobility with respect to demographic differences. We leverage an extensive WLAN dataset collected on a large university campus. Specifically, our dataset includes over 41 million WLAN entries gathered from over 5,000 students (with demographic information) during a four-month period in 2015. We observed surprising patterns on large increases of long-term mobility entropy by age, and the impact of academic majors on students long-term mobility entropy. The distribution of long-term entropy follows a bimodal distribution, which is different from previous studies. We also find that the predictability of students' short-term (daily or weekly) mobility varies on different days of the week and with student gender. Because of the large campus size, our results can mimic people's mobility patterns in metropolitan areas. We also anticipate that our results will provide insight that guides academic administrators' decisions regarding facilities planning, emergency management, etc. on campus.}, 
keywords={Internet;demography;educational institutions;entropy;mobile computing;social sciences computing;wireless LAN;WLAN dataset;WLAN logs;academic majors;bimodal distribution;campus size;comprehensive measurement study;demographic differences;demographic information;human mobility predictability;metropolitan area;student gender;student long-term mobility entropy;university campus;Buildings;Conferences;Entropy;Global Positioning System;Mobile communication;Mobile handsets;Wireless LAN}, 
doi={10.1109/INFOCOM.2017.8057234}, 
ISSN={}, 
month={May},}
