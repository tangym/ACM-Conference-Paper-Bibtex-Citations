@inproceedings{Vijaykumar:2015:CCB:2749469.2750399,
 abstract = {Modern Graphics Processing Units (GPUs) are well provisioned to support the concurrent execution of thousands of threads. Unfortunately, different bottlenecks during execution and heterogeneous application requirements create imbalances in utilization of resources in the cores. For example, when a GPU is bottlenecked by the available off-chip memory bandwidth, its computational resources are often overwhelmingly idle, waiting for data from memory to arrive. This paper introduces the Core-Assisted Bottleneck Acceleration (CABA) framework that employs idle on-chip resources to alleviate different bottlenecks in GPU execution. CABA provides flexible mechanisms to automatically generate "assist warps" that execute on GPU cores to perform specific tasks that can improve GPU performance and efficiency. CABA enables the use of idle computational units and pipelines to alleviate the memory bandwidth bottleneck, e.g., by using assist warps to perform data compression to transfer less data from memory. Conversely, the same framework can be employed to handle cases where the GPU is bottlenecked by the available computational units, in which case the memory pipelines are idle and can be used by CABA to speed up computation, e.g., by performing memoization using assist warps. We provide a comprehensive design and evaluation of CABA to perform effective and flexible data compression in the GPU memory hierarchy to alleviate the memory bandwidth bottleneck. Our extensive evaluations show that CABA, when used to implement data compression, provides an average performance improvement of 41.7% (as high as 2.6X) across a variety of memory-bandwidth-sensitive GPGPU applications.},
 acmid = {2750399},
 address = {New York, NY, USA},
 author = {Vijaykumar, Nandita and Pekhimenko, Gennady and Jog, Adwait and Bhowmick, Abhishek and Ausavarungnirun, Rachata and Das, Chita and Kandemir, Mahmut and Mowry, Todd C. and Mutlu, Onur},
 booktitle = {Proceedings of the 42Nd Annual International Symposium on Computer Architecture},
 doi = {10.1145/2749469.2750399},
 isbn = {978-1-4503-3402-0},
 link = {http://doi.acm.org/10.1145/2749469.2750399},
 location = {Portland, Oregon},
 numpages = {13},
 pages = {41--53},
 publisher = {ACM},
 series = {ISCA '15},
 title = {A Case for Core-assisted Bottleneck Acceleration in GPUs: Enabling Flexible Data Compression with Assist Warps},
 year = {2015}
}


@article{Jun:2015:BAB:2872887.2750412,
 abstract = {Complex data queries, because of their need for random accesses, have proven to be slow unless all the data can be accommodated in DRAM. There are many domains, such as genomics, geological data and daily twitter feeds where the datasets of interest are 5TB to 20 TB. For such a dataset, one would need a cluster with 100 servers, each with 128GB to 256GBs of DRAM, to accommodate all the data in DRAM. On the other hand, such datasets could be stored easily in the flash memory of a rack-sized cluster. Flash storage has much better random access performance than hard disks, which makes it desirable for analytics workloads. In this paper we present BlueDBM, a new system architecture which has flash-based storage with in-store processing capability and a low-latency high-throughput inter-controller network. We show that BlueDBM outperforms a flash-based system without these features by a factor of 10 for some important applications. While the performance of a ram-cloud system falls sharply even if only 5%~10% of the references are to the secondary storage, this sharp performance degradation is not an issue in BlueDBM. BlueDBM presents an attractive point in the cost-performance trade-off for Big Data analytics.},
 acmid = {2750412},
 address = {New York, NY, USA},
 author = {Jun, Sang-Woo and Liu, Ming and Lee, Sungjin and Hicks, Jamey and Ankcorn, John and King, Myron and Xu, Shuotao and Arvind},
 doi = {10.1145/2872887.2750412},
 issn = {0163-5964},
 issue_date = {June 2015},
 journal = {SIGARCH Comput. Archit. News},
 link = {http://doi.acm.org/10.1145/2872887.2750412},
 month = {jun},
 number = {3},
 numpages = {13},
 pages = {1--13},
 publisher = {ACM},
 title = {BlueDBM: An Appliance for Big Data Analytics},
 volume = {43},
 year = {2015}
}


@article{Paul:2015:HBC:2872887.2750404,
 abstract = {In this paper, we address the problem of efficiently managing the relative power demands of a high-performance GPU and its memory subsystem. We develop a management approach that dynamically tunes the hardware operating configurations to maintain balance between the power dissipated in compute versus memory access across GPGPU application phases. Our goal is to reduce power with minimal performance degradation. Accordingly, we construct predictors that assess the online sensitivity of applications to three hardware tunables---compute frequency, number of active compute units, and memory bandwidth. Using these sensitivity predictors, we propose a two-level coordinated power management scheme, Harmonia, which coordinates the hardware power states of the GPU and the memory system. Through hardware measurements on a commodity GPU, we evaluate Harmonia against a state-of-the-practice commodity GPU power management scheme, as well as an oracle scheme. Results show that Harmonia improves measured energy-delay squared (ED2) by up to 36% (12% on average) with negligible performance loss across representative GPGPU workloads, and on an average is within 3% of the oracle scheme.},
 acmid = {2750404},
 address = {New York, NY, USA},
 author = {Paul, Indrani and Huang, Wei and Arora, Manish and Yalamanchili, Sudhakar},
 doi = {10.1145/2872887.2750404},
 issn = {0163-5964},
 issue_date = {June 2015},
 journal = {SIGARCH Comput. Archit. News},
 link = {http://doi.acm.org/10.1145/2872887.2750404},
 month = {jun},
 number = {3},
 numpages = {12},
 pages = {54--65},
 publisher = {ACM},
 title = {Harmonia: Balancing Compute and Memory Power in High-performance GPUs},
 volume = {43},
 year = {2015}
}


@article{Lee:2015:FAT:2872887.2750383,
 abstract = {This paper introduces a tagless cache architecture for large in-package DRAM caches. The conventional die-stacked DRAM cache has both a TLB and a cache tag array, which are responsible for virtual-to-physical and physical-to-cache address translation, respectively. We propose to align the granularity of caching with OS page size and take a unified approach to address translation and cache tag management. To this end, we introduce cache-map TLB (cTLB), which stores virtual-to-cache, instead of virtual-to-physical, address mappings. At a TLB miss, the TLB miss handler allocates the requested block into the cache if it is not cached yet, and updates both the page table and cTLB with the virtual-to-cache address mapping. Assuming the availability of large in-package DRAM caches, this ensures that an access to the memory region within the TLB reach always hits in the cache with low hit latency since a TLB access immediately returns the exact location of the requested block in the cache, hence saving a tag-checking operation. The remaining cache space is used as victim cache for memory pages that are recently evicted from cTLB. By completely eliminating data structures for cache tag management, from either on-die SRAM or in-package DRAM, the proposed DRAM cache achieves best scalability and hit latency, while maintaining high hit rate of a fully associative cache. Our evaluation with 3D Through-Silicon Via (TSV)-based in-package DRAM demonstrates that the proposed cache improves the IPC and energy efficiency by 30.9% and 39.5%, respectively, compared to the baseline with no DRAM cache. These numbers translate to 4.3% and 23.8% improvements over an impractical SRAM-tag cache requiring megabytes of on-die SRAM storage, due to low hit latency and zero energy waste for cache tags.},
 acmid = {2750383},
 address = {New York, NY, USA},
 author = {Lee, Yongjun and Kim, Jongwon and Jang, Hakbeom and Yang, Hyunggyun and Kim, Jangwoo and Jeong, Jinkyu and Lee, Jae W.},
 doi = {10.1145/2872887.2750383},
 issn = {0163-5964},
 issue_date = {June 2015},
 journal = {SIGARCH Comput. Archit. News},
 link = {http://doi.acm.org/10.1145/2872887.2750383},
 month = {jun},
 number = {3},
 numpages = {12},
 pages = {211--222},
 publisher = {ACM},
 title = {A Fully Associative, Tagless DRAM Cache},
 volume = {43},
 year = {2015}
}


@proceedings{Yew:2014:2665671,
 abstract = {
                  An abstract is not available.
              },
 address = {Piscataway, NJ, USA},
 isbn = {978-1-4799-4394-4},
 location = {Minneapolis, Minnesota, USA},
 publisher = {IEEE Press},
 title = {ISCA '14: Proceeding of the 41st Annual International Symposium on Computer Architecuture},
 year = {2014}
}


@inproceedings{Li:2015:RWS:2749469.2750406,
 abstract = {Modern computers are built with increasingly complex software stack crossing multiple layers (i.e., worlds), where cross-world call has been a necessity for various important purposes like security, reliability, and reduced complexity. Unfortunately, there is currently limited cross-world call support (e.g., syscall, vmcall), and thus other calls need to be emulated by detouring multiple times to the privileged software layer (i.e., OS kernel and hypervisor). This causes not only significant performance degradation, but also unnecessary implementation complexity. This paper argues that it is time to rethink the design of traditional cross-world call mechanisms by reviewing existing systems built upon hypervisors. Following the design philosophy of separating authentication from authorization, this paper advocates decoupling of the authorization on whether a world call is permitted (by software) from unforgeable identification of calling peers (by hardware). This results in a flexible cross-world call scheme (namely CrossOver) that allows secure, efficient and flexible cross-world calls across multiple layers not only within the same address space, but also across multiple address spaces. We demonstrate that CrossOver can be approximated by using existing hardware mechanism (namely VMFUNC) and a trivial modification of the VMFUNC mechanism can provide a full support of CrossOver. To show its usefulness, we have conducted case studies by using several recent systems such as Proxos, Hyper-Shell, Tahoma and ShadowContext. Performance measurements using full-system emulation and a real processor with VMFUNC shows that CrossOver significantly boosts the performance of the mentioned systems.},
 acmid = {2750406},
 address = {New York, NY, USA},
 author = {Li, Wenhao and Xia, Yubin and Chen, Haibo and Zang, Binyu and Guan, Haibing},
 booktitle = {Proceedings of the 42Nd Annual International Symposium on Computer Architecture},
 doi = {10.1145/2749469.2750406},
 isbn = {978-1-4503-3402-0},
 link = {http://doi.acm.org/10.1145/2749469.2750406},
 location = {Portland, Oregon},
 numpages = {13},
 pages = {375--387},
 publisher = {ACM},
 series = {ISCA '15},
 title = {Reducing World Switches in Virtualized Environment with Flexible Cross-world Calls},
 year = {2015}
}


@inproceedings{Lustig:2015:ADA:2749469.2750378,
 abstract = {Architectural heterogeneity is increasing: numerous products and studies have proven the benefits of combining cores and accelerators with varying ISAs into a single system. However, an underappreciated barrier to unlocking the full potential of heterogeneity is the need to specify and to reconcile differences in memory consistency models across layers of the hardware-software stack and among on-chip components. This paper presents ArMOR, a framework for specifying, comparing, and translating between memory consistency models. ArMOR defines MOSTs, an architecture-independent and precise format for specifying the semantics of memory ordering requirements such as preserved program order or explicit fences. MOSTs allow any two consistency models to be directly and algorithmically compared, and they help avoid many of the pitfalls of traditional consistency model analysis. As a case study, we use ArMOR to automatically generate translation modules called shims that dynamically translate code compiled for one memory model to execute on hardware implementing a different model.},
 acmid = {2750378},
 address = {New York, NY, USA},
 author = {Lustig, Daniel and Trippel, Caroline and Pellauer, Michael and Martonosi, Margaret},
 booktitle = {Proceedings of the 42Nd Annual International Symposium on Computer Architecture},
 doi = {10.1145/2749469.2750378},
 isbn = {978-1-4503-3402-0},
 link = {http://doi.acm.org/10.1145/2749469.2750378},
 location = {Portland, Oregon},
 numpages = {13},
 pages = {388--400},
 publisher = {ACM},
 series = {ISCA '15},
 title = {ArMOR: Defending Against Memory Consistency Model Mismatches in Heterogeneous Architectures},
 year = {2015}
}


@proceedings{Marr:2015:2749469,
 abstract = {Welcome to the 42nd International Symposium on Computer Architecture (ISCA), in Portland, Oregon, June 13-17, 2015, at the Oregon Convention Center. ISCA has a long history of leadership as the top conference in the field of computer architecture. ISCA's success is because of all of you, our participants, organizers, and supporters. Thank you for coming, participating, and supporting this conference. As in previous ISCA conferences, expect to participate in technical presentations, workshops, tutorials, and networking opportunities of the highest caliber with colleagues from all over the world!},
 address = {New York, NY, USA},
 isbn = {978-1-4503-3402-0},
 location = {Portland, Oregon},
 publisher = {ACM},
 title = {ISCA '15: Proceedings of the 42Nd Annual International Symposium on Computer Architecture},
 year = {2015}
}


@inproceedings{Lopes:2015:SRI:2749469.2750391,
 abstract = {Microprocessor manufacturers typically keep old instruction sets in modern processors to ensure backward compatibility with legacy software. The introduction of newer extensions to the ISA increases the design complexity of microprocessor front-ends, exacerbates the consumption of precious on-chip resources (e.g., silicon area and energy), and demands more efforts for hardware verification and debugging. We analyzed several x86 applications and operating systems deployed between 1995 and 2012 and observed that many instructions stop being used over time, and more than 500 instructions were never used in these applications. We also investigate the impact of including these unused instructions in the design of the x86 decoders and propose SHRINK, a mechanism to remove old instructions without breaking backward compatibility with legacy code. SHRINK allows us to remove 40% of the instructions from the x86 ISA and improve the critical path, area, and power consumption of the instruction decoder, respectively, by 23%, 48%, and 49%, on average.},
 acmid = {2750391},
 address = {New York, NY, USA},
 author = {Lopes, Bruno Cardoso and Auler, Rafael and Ramos, Luiz and Borin, Edson and Azevedo, Rodolfo},
 booktitle = {Proceedings of the 42Nd Annual International Symposium on Computer Architecture},
 doi = {10.1145/2749469.2750391},
 isbn = {978-1-4503-3402-0},
 link = {http://doi.acm.org/10.1145/2749469.2750391},
 location = {Portland, Oregon},
 numpages = {12},
 pages = {311--322},
 publisher = {ACM},
 series = {ISCA '15},
 title = {SHRINK: Reducing the ISA Complexity via Instruction Recycling},
 year = {2015}
}


@inproceedings{Komuravelli:2015:SYS:2749469.2750374,
 abstract = {Heterogeneous systems employ specialization for energy efficiency. Since data movement is expected to be a dominant consumer of energy, these systems employ specialized memories (e.g., scratchpads and FIFOs) for better efficiency for targeted data. These memory structures, however, tend to exist in local address spaces, incurring significant performance and energy penalties due to inefficient data movement between the global and private spaces. We propose an efficient heterogeneous memory system where specialized memory components are tightly coupled in a unified and coherent address space. This paper applies these ideas to a system with CPUs and GPUs with scratchpads and caches. We introduce a new memory organization, stash, that combines the benefits of caches and scratchpads without incurring their downsides. Like a scratchpad, the stash is directly addressed (without tags and TLB accesses) and provides compact storage. Like a cache, the stash is globally addressable and visible, providing implicit data movement and increased data reuse. We show that the stash provides better performance and energy than a cache and a scratchpad, while enabling new use cases for heterogeneous systems. For 4 microbenchmarks, which exploit new use cases (e.g., reuse across GPU compute kernels), compared to scratchpads and caches, the stash reduces execution cycles by an average of 27% and 13% respectively and energy by an average of 53% and 35%. For 7 current GPU applications, which are not designed to exploit the new features of the stash, compared to scratchpads and caches, the stash reduces cycles by 10% and 12% on average (max 22% and 31%) respectively, and energy by 16% and 32% on average (max 30% and 51%).},
 acmid = {2750374},
 address = {New York, NY, USA},
 author = {Komuravelli, Rakesh and Sinclair, Matthew D. and Alsop, Johnathan and Huzaifa, Muhammad and Kotsifakou, Maria and Srivastava, Prakalp and Adve, Sarita V. and Adve, Vikram S.},
 booktitle = {Proceedings of the 42Nd Annual International Symposium on Computer Architecture},
 doi = {10.1145/2749469.2750374},
 isbn = {978-1-4503-3402-0},
 link = {http://doi.acm.org/10.1145/2749469.2750374},
 location = {Portland, Oregon},
 numpages = {13},
 pages = {707--719},
 publisher = {ACM},
 series = {ISCA '15},
 title = {Stash: Have Your Scratchpad and Cache It Too},
 year = {2015}
}


@inproceedings{Chou:2015:BTM:2749469.2750387,
 abstract = {Die stacking memory technology can enable gigascale DRAM caches that can operate at 4x-8x higher bandwidth than commodity DRAM. Such caches can improve system performance by servicing data at a faster rate when the requested data is found in the cache, potentially increasing the memory bandwidth of the system by 4x-8x. Unfortunately, a DRAM cache uses the available memory bandwidth not only for data transfer on cache hits, but also for other secondary operations such as cache miss detection, fill on cache miss, and writeback lookup and content update on dirty evictions from the last-level on-chip cache. Ideally, we want the bandwidth consumed for such secondary operations to be negligible, and have almost all the bandwidth be available for transfer of useful data from the DRAM cache to the processor. We evaluate a 1GB DRAM cache, architected as Alloy Cache, and show that even the most bandwidth-efficient proposal for DRAM cache consumes 3.8x bandwidth compared to an idealized DRAM cache that does not consume any bandwidth for secondary operations. We also show that redesigning the DRAM cache to minimize the bandwidth consumed by secondary operations can potentially improve system performance by 22%. To that end, this paper proposes Bandwidth Efficient ARchitecture (BEAR) for DRAM caches. BEAR integrates three components, one each for reducing the bandwidth consumed by miss detection, miss fill, and writeback probes. BEAR reduces the bandwidth consumption of DRAM cache by 32%, which reduces cache hit latency by 24% and increases overall system performance by 10%. BEAR, with negligible overhead, outperforms an idealized SRAM Tag-Store design that incurs an unacceptable overhead of 64 megabytes, as well as Sector Cache designs that incur an SRAM storage overhead of 6 megabytes.},
 acmid = {2750387},
 address = {New York, NY, USA},
 author = {Chou, Chiachen and Jaleel, Aamer and Qureshi, Moinuddin K.},
 booktitle = {Proceedings of the 42Nd Annual International Symposium on Computer Architecture},
 doi = {10.1145/2749469.2750387},
 isbn = {978-1-4503-3402-0},
 link = {http://doi.acm.org/10.1145/2749469.2750387},
 location = {Portland, Oregon},
 numpages = {13},
 pages = {198--210},
 publisher = {ACM},
 series = {ISCA '15},
 title = {BEAR: Techniques for Mitigating Bandwidth Bloat in Gigascale DRAM Caches},
 year = {2015}
}


@inproceedings{Seshadri:2015:POE:2749469.2750379,
 abstract = {Many recent works propose mechanisms demonstrating the potential advantages of managing memory at a fine (e.g., cache line) granularity---e.g., fine-grained deduplication and fine-grained memory protection. Unfortunately, existing virtual memory systems track memory at a larger granularity (e.g., 4 KB pages), inhibiting efficient implementation of such techniques. Simply reducing the page size results in an unacceptable increase in page table overhead and TLB pressure. We propose a new virtual memory framework that enables efficient implementation of a variety of fine-grained memory management techniques. In our framework, each virtual page can be mapped to a structure called a page overlay, in addition to a regular physical page. An overlay contains a subset of cache lines from the virtual page. Cache lines that are present in the overlay are accessed from there and all other cache lines are accessed from the regular physical page. Our page-overlay framework enables cache-line-granularity memory management without significantly altering the existing virtual memory framework or introducing high overheads. We show that our framework can enable simple and efficient implementations of seven memory management techniques, each of which has a wide variety of applications. We quantitatively evaluate the potential benefits of two of these techniques: overlay-on-write and sparse-data-structure computation. Our evaluations show that overlay-on-write, when applied to fork, can improve performance by 15% and reduce memory capacity requirements by 53% on average compared to traditional copy-on-write. For sparse data computation, our framework can outperform a state-of-the-art software-based sparse representation on a number of real-world sparse matrices. Our framework is general, powerful, and effective in enabling fine-grained memory management at low cost.},
 acmid = {2750379},
 address = {New York, NY, USA},
 author = {Seshadri, Vivek and Pekhimenko, Gennady and Ruwase, Olatunji and Mutlu, Onur and Gibbons, Phillip B. and Kozuch, Michael A. and Mowry, Todd C. and Chilimbi, Trishul},
 booktitle = {Proceedings of the 42Nd Annual International Symposium on Computer Architecture},
 doi = {10.1145/2749469.2750379},
 isbn = {978-1-4503-3402-0},
 link = {http://doi.acm.org/10.1145/2749469.2750379},
 location = {Portland, Oregon},
 numpages = {13},
 pages = {79--91},
 publisher = {ACM},
 series = {ISCA '15},
 title = {Page Overlays: An Enhanced Virtual Memory Framework to Enable Fine-grained Memory Management},
 year = {2015}
}


@article{Hauswald:2015:DTD:2872887.2749472,
 abstract = {As applications such as Apple Siri, Google Now, Microsoft Cortana, and Amazon Echo continue to gain traction, web-service companies are adopting large deep neural networks (DNN) for machine learning challenges such as image processing, speech recognition, natural language processing, among others. A number of open questions arise as to the design of a server platform specialized for DNN and how modern warehouse scale computers (WSCs) should be outfitted to provide DNN as a service for these applications. In this paper, we present DjiNN, an open infrastructure for DNN as a service in WSCs, and Tonic Suite, a suite of 7 end-to-end applications that span image, speech, and language processing. We use DjiNN to design a high throughput DNN system based on massive GPU server designs and provide insights as to the varying characteristics across applications. After studying the throughput, bandwidth, and power properties of DjiNN and Tonic Suite, we investigate several design points for future WSC architectures. We investigate the total cost of ownership implications of having a WSC with a disaggregated GPU pool versus a WSC composed of homogeneous integrated GPU servers. We improve DNN throughput by over 120x for all but one application (40x for Facial Recognition) on an NVIDIA K40 GPU. On a GPU server composed of 8 NVIDIA K40s, we achieve near-linear scaling (around 1000x throughput improvement) for 3 of the 7 applications. Through our analysis, we also find that GPU-enabled WSCs improve total cost of ownership over CPU-only designs by 4-20x, depending on the composition of the workload},
 acmid = {2749472},
 address = {New York, NY, USA},
 author = {Hauswald, Johann and Kang, Yiping and Laurenzano, Michael A. and Chen, Quan and Li, Cheng and Mudge, Trevor and Dreslinski, Ronald G. and Mars, Jason and Tang, Lingjia},
 doi = {10.1145/2872887.2749472},
 issn = {0163-5964},
 issue_date = {June 2015},
 journal = {SIGARCH Comput. Archit. News},
 link = {http://doi.acm.org/10.1145/2872887.2749472},
 month = {jun},
 number = {3},
 numpages = {14},
 pages = {27--40},
 publisher = {ACM},
 title = {DjiNN and Tonic: DNN As a Service and Its Implications for Future Warehouse Scale Computers},
 volume = {43},
 year = {2015}
}


@article{Kumar:2015:FDT:2872887.2750421,
 abstract = {Chip designers have shown increasing interest in integrating specialized fixed-function coprocessors into multicore designs to improve energy efficiency. Recent work in academia [11, 37] and industry [16] has sought to enable more fine-grain offloading at the granularity of functions and loops. The sequential program now needs to migrate across the chip utilizing the appropriate accelerator for each program region. As the execution migrates, it has become increasingly challenging to retain the temporal and spatial locality of the original program as well as manage the data sharing. We show that with the increasing energy cost of wires and caches relative to compute operations, it is imperative to optimize data movement to retain the energy benefits of accelerators. We develop FUSION, a lightweight coherent cache hierarchy for accelerators and study the tradeoffs compared to a scratchpad based architecture. We find that coherency, both between the accelerators and with the CPU, can help minimize data movement and save energy. FUSION leverages temporal coherence [32] to optimize data movement within the accelerator tile. The accelerator tile includes small per-accelerator L0 caches to minimize hit energy and a per-tile shared cache to improve localized-sharing between accelerators and minimize data exchanges with the host LLC. We find that overall FUSION improves performance by 4.3× compared to an oracle DMA that pushes data into the scratchpad. In workloads with inter-accelerator sharing we save up to 10x the dynamic energy of the cache hierarchy by minimizing the host-accelerator data ping-ponging.},
 acmid = {2750421},
 address = {New York, NY, USA},
 author = {Kumar, Snehasish and Shriraman, Arrvindh and Vedula, Naveen},
 doi = {10.1145/2872887.2750421},
 issn = {0163-5964},
 issue_date = {June 2015},
 journal = {SIGARCH Comput. Archit. News},
 link = {http://doi.acm.org/10.1145/2872887.2750421},
 month = {jun},
 number = {3},
 numpages = {13},
 pages = {733--745},
 publisher = {ACM},
 title = {Fusion: Design Tradeoffs in Coherent Cache Hierarchies for Accelerators},
 volume = {43},
 year = {2015}
}


@inproceedings{Asilioglu:2015:LS:2749469.2750409,
 abstract = {LaZy Superscalar is a processor architecture which delays the execution of fetched instructions until their results are needed by other instructions. This approach eliminates dead instructions and provides the necessary means to fuse dependent instructions across multiple control dependencies by explicitly tracking control and data dependencies through a matrix based scheduler. We present this novel redesign of scheduling, recovery and commit mechanisms and evaluate the performance of the proposed architecture. Our simulations using Spec 2006 benchmark suite indicate that LaZy Superscalar can achieve significant speed-ups while providing respectable power savings compared to a conventional superscalar processor.},
 acmid = {2750409},
 address = {New York, NY, USA},
 author = {A\c{s}\il\io\u{g}lu, G\"{o}rkem and Jin, Zhaoxiang and K\"{o}ksal, Murat and Javeri, Omkar and \"{O}nder, Soner},
 booktitle = {Proceedings of the 42Nd Annual International Symposium on Computer Architecture},
 doi = {10.1145/2749469.2750409},
 isbn = {978-1-4503-3402-0},
 link = {http://doi.acm.org/10.1145/2749469.2750409},
 location = {Portland, Oregon},
 numpages = {12},
 pages = {260--271},
 publisher = {ACM},
 series = {ISCA '15},
 title = {LaZy Superscalar},
 year = {2015}
}


@inproceedings{Ahn:2015:SPA:2749469.2750386,
 abstract = {The explosion of digital data and the ever-growing need for fast data analysis have made in-memory big-data processing in computer systems increasingly important. In particular, large-scale graph processing is gaining attention due to its broad applicability from social science to machine learning. However, scalable hardware design that can efficiently process large graphs in main memory is still an open problem. Ideally, cost-effective and scalable graph processing systems can be realized by building a system whose performance increases proportionally with the sizes of graphs that can be stored in the system, which is extremely challenging in conventional systems due to severe memory bandwidth limitations. In this work, we argue that the conventional concept of processing-in-memory (PIM) can be a viable solution to achieve such an objective. The key modern enabler for PIM is the recent advancement of the 3D integration technology that facilitates stacking logic and memory dies in a single package, which was not available when the PIM concept was originally examined. In order to take advantage of such a new technology to enable memory-capacity-proportional performance, we design a programmable PIM accelerator for large-scale graph processing called Tesseract. Tesseract is composed of (1) a new hardware architecture that fully utilizes the available memory bandwidth, (2) an efficient method of communication between different memory partitions, and (3) a programming interface that reflects and exploits the unique hardware design. It also includes two hardware prefetchers specialized for memory access patterns of graph processing, which operate based on the hints provided by our programming model. Our comprehensive evaluations using five state-of-the-art graph processing workloads with large real-world graphs show that the proposed architecture improves average system performance by a factor of ten and achieves 87% average energy reduction over conventional systems.},
 acmid = {2750386},
 address = {New York, NY, USA},
 author = {Ahn, Junwhan and Hong, Sungpack and Yoo, Sungjoo and Mutlu, Onur and Choi, Kiyoung},
 booktitle = {Proceedings of the 42Nd Annual International Symposium on Computer Architecture},
 doi = {10.1145/2749469.2750386},
 isbn = {978-1-4503-3402-0},
 link = {http://doi.acm.org/10.1145/2749469.2750386},
 location = {Portland, Oregon},
 numpages = {13},
 pages = {105--117},
 publisher = {ACM},
 series = {ISCA '15},
 title = {A Scalable Processing-in-memory Accelerator for Parallel Graph Processing},
 year = {2015}
}


@inproceedings{Peled:2015:SLC:2749469.2749473,
 abstract = {Most modern memory prefetchers rely on spatio-temporal locality to predict the memory addresses likely to be accessed by a program in the near future. Emerging workloads, however, make increasing use of irregular data structures, and thus exhibit a lower degree of spatial locality. This makes them less amenable to spatio-temporal prefetchers. In this paper, we introduce the concept of Semantic Locality, which uses inherent program semantics to characterize access relations. We show how, in principle, semantic locality can capture the relationship between data elements in a manner agnostic to the actual data layout, and we argue that semantic locality transcends spatio-temporal concerns. We further introduce the context-based memory prefetcher, which approximates semantic locality using reinforcement learning. The prefetcher identifies access patterns by applying reinforcement learning methods over machine and code attributes, that provide hints on memory access semantics. We test our prefetcher on a variety of benchmarks that employ both regular and irregular patterns. For the SPEC 2006 suite, it delivers speedups as high as 2.8X (20% on average) over a baseline with no prefetching, and outperforms leading spatio-temporal prefetchers. Finally, we show that the context-based prefetcher makes it possible for naive, pointer-based implementations of irregular algorithms to achieve performance comparable to that of spatially optimized code.},
 acmid = {2749473},
 address = {New York, NY, USA},
 author = {Peled, Leeor and Mannor, Shie and Weiser, Uri and Etsion, Yoav},
 booktitle = {Proceedings of the 42Nd Annual International Symposium on Computer Architecture},
 doi = {10.1145/2749469.2749473},
 isbn = {978-1-4503-3402-0},
 link = {http://doi.acm.org/10.1145/2749469.2749473},
 location = {Portland, Oregon},
 numpages = {13},
 pages = {285--297},
 publisher = {ACM},
 series = {ISCA '15},
 title = {Semantic Locality and Context-based Prefetching Using Reinforcement Learning},
 year = {2015}
}


@article{Yang:2015:CPM:2872887.2750401,
 abstract = {Developers and architects spend a lot of time trying to understand and eliminate performance problems. Unfortunately, the root causes of many problems occur at a fine granularity that existing continuous profiling and direct measurement approaches cannot observe. This paper presents the design and implementation of Shim, a continuous profiler that samples at resolutions as fine as 15 cycles; three to five orders of magnitude finer than current continuous profilers. Shim's fine-grain measurements reveal new behaviors, such as variations in instructions per cycle (IPC) within the execution of a single function. A Shim observer thread executes and samples autonomously on unutilized hardware. To sample, it reads hardware performance counters and memory locations that store software state. Shim improves its accuracy by automatically detecting and discarding samples affected by measurement skew. We measure Shim's observer effects and show how to analyze them. When on a separate core, Shim can continuously observe one software signal with a 2% overhead at a ~1200 cycle resolution. At an overhead of 61%, Shim samples one software signal on the same core with SMT at a ~15 cycle resolution. Modest hardware changes could significantly reduce overheads and add greater analytical capability to Shim. We vary prefetching and DVFS policies in case studies that show the diagnostic power of fine-grain IPC and memory bandwidth results. By repurposing existing hardware, we deliver a practical tool for fine-grain performance microscopy for developers and architects.},
 acmid = {2750401},
 address = {New York, NY, USA},
 author = {Yang, Xi and Blackburn, Stephen M. and McKinley, Kathryn S.},
 doi = {10.1145/2872887.2750401},
 issn = {0163-5964},
 issue_date = {June 2015},
 journal = {SIGARCH Comput. Archit. News},
 link = {http://doi.acm.org/10.1145/2872887.2750401},
 month = {jun},
 number = {3},
 numpages = {15},
 pages = {170--184},
 publisher = {ACM},
 title = {Computer Performance Microscopy with Shim},
 volume = {43},
 year = {2015}
}


@article{Segulja:2015:CLR:2872887.2750395,
 abstract = {Data races make parallel programs hard to understand. Precise race detection that stops an execution on first occurrence of a race addresses this problem, but it comes with significant overhead. In this work, we exploit the insight that precisely detecting only write-after-write (WAW) and read-after-write (RAW) races suffices to provide cleaner semantics for racy programs. We demonstrate that stopping an execution only when these races occur ensures that synchronization-free-regions appear to be executed in isolation and that their writes appear atomic. Additionally, the undetected racy executions can be given certain deterministic guarantees with efficient mechanisms. We present Clean, a system that precisely detects WAW and RAW races and deterministically orders synchronization. We demonstrate that the combination of these two relatively inexpensive mechanisms provides cleaner semantics for racy programs. We evaluate both software-only and hardware-supported Clean. The software-only Clean runs all Pthread benchmarks from the SPLASH-2 and PARSEC suites with an average 7.8x slowdown. The overhead of precise WAW and RAW detection (5.8x) constitutes the majority of this slowdown. Simple hardware extensions reduce the slowdown of Clean's race detection to on average 10.4% and never more than 46.7%.},
 acmid = {2750395},
 address = {New York, NY, USA},
 author = {Segulja, Cedomir and Abdelrahman, Tarek S.},
 doi = {10.1145/2872887.2750395},
 issn = {0163-5964},
 issue_date = {June 2015},
 journal = {SIGARCH Comput. Archit. News},
 link = {http://doi.acm.org/10.1145/2872887.2750395},
 month = {jun},
 number = {3},
 numpages = {13},
 pages = {401--413},
 publisher = {ACM},
 title = {Clean: A Race Detector with Cleaner Semantics},
 volume = {43},
 year = {2015}
}


@article{Choi:2015:MCR:2872887.2750402,
 abstract = {Several previous works have changed DRAM bank structure to reduce memory access latency and have shown performance improvement. However, changes in the area-optimized DRAM bank can incur large area-overhead. To solve this problem, we propose Multiple Clone Row DRAM (MCR-DRAM), which uses existing DRAM bank structure without any modification. Our key idea is Multiple Clone Row (MCR), in which multiple rows are simultaneously turned on or off to consist of a logically single row. MCR provides two advantages which enable our low-latency mechanisms (Early-Access, Early-Precharge and Fast-Refresh). First, MCR increases the speed of the sensing process by increasing the number of sensed-cells. Thus, it enables a READ/WRITE command to an MCR to be issued earlier than possible for a normal row (Early-Access). Second, DRAM cells in an MCR exhibit more frequent refreshes without additional REFRESH commands, thereby reducing the amount of charge leakage during the refresh interval for the identical cell. The reduced amount of charge leakage enables a PRECHARGE command to be served before the activated-cells are fully restored (Early-Precharge) and a REFRESH operation to be completed before the refreshed-cells are fully restored (Fast-Refresh). Even though MCR-DRAM sacrifices memory capacity for low-latency, it can be dynamically reconfigured from low-latency to full-capacity DRAM. MCR-DRAM improves both performance and energy efficiency for both single-core and multi-core systems.},
 acmid = {2750402},
 address = {New York, NY, USA},
 author = {Choi, Jungwhan and Shin, Wongyu and Jang, Jaemin and Suh, Jinwoong and Kwon, Yongkee and Moon, Youngsuk and Kim, Lee-Sup},
 doi = {10.1145/2872887.2750402},
 issn = {0163-5964},
 issue_date = {June 2015},
 journal = {SIGARCH Comput. Archit. News},
 link = {http://doi.acm.org/10.1145/2872887.2750402},
 month = {jun},
 number = {3},
 numpages = {12},
 pages = {223--234},
 publisher = {ACM},
 title = {Multiple Clone Row DRAM: A Low Latency and Area Optimized DRAM},
 volume = {43},
 year = {2015}
}


@inproceedings{Nowatzki:2015:EPH:2749469.2750380,
 abstract = {General purpose processors (GPPs), from small inorder designs to many-issue out-of-order, incur large power overheads which must be addressed for future technology generations. Major sources of overhead include structures which dynamically extract the data-dependence graph or maintain precise state. Considering irregular workloads, current specialization approaches either heavily curtail performance, or provide simply too little benefit. Interestingly, well known explicit-dataflow architectures eliminate these overheads by directly executing the data-dependence graph and eschewing instruction-precise recoverability. However, even after decades of research, dataflow architectures have yet to come into prominence as a solution. We attribute this to a lack of effective control speculation and the latency overhead of explicit communication, which is crippling for certain codes. This paper makes the observation that if both out-of-order and explicit-dataflow were available in one processor, many types of GPP cores can benefit from dynamically switching during certain phases of an application's lifetime. Analysis reveals that an ideal explicit-dataflow engine could be profitable for more than half of instructions, providing significant performance and energy improvements. The challenge is to achieve these benefits without introducing excess hardware complexity. To this end, we propose the Specialization Engine for Explicit-Dataflow (SEED). Integrated with an inorder core, we see 1.67× performance and 1.65× energy benefits, with an Out-Of-Order (OOO) dual-issue core we see 1.33× and 1.70×, and with a quad-issue OOO, 1.14× and 1.54×.},
 acmid = {2750380},
 address = {New York, NY, USA},
 author = {Nowatzki, Tony and Gangadhar, Vinay and Sankaralingam, Karthikeyan},
 booktitle = {Proceedings of the 42Nd Annual International Symposium on Computer Architecture},
 doi = {10.1145/2749469.2750380},
 isbn = {978-1-4503-3402-0},
 link = {http://doi.acm.org/10.1145/2749469.2750380},
 location = {Portland, Oregon},
 numpages = {13},
 pages = {298--310},
 publisher = {ACM},
 series = {ISCA '15},
 title = {Exploring the Potential of Heterogeneous Von Neumann/Dataflow Execution Models},
 year = {2015}
}


@article{Rogers:2015:VWS:2872887.2750410,
 abstract = {This paper studies the effect of warp sizing and scheduling on performance and efficiency in GPUs. We propose Variable Warp Sizing (VWS) which improves the performance of divergent applications by using a small base warp size in the presence of control flow and memory divergence. When appropriate, our proposed technique groups sets of these smaller warps together by ganging their execution in the warp scheduler, improving performance and energy efficiency for regular applications. Warp ganging is necessary to prevent performance degradation on regular workloads due to memory convergence slip, which results from the inability of smaller warps to exploit the same intra-warp memory locality as larger warps. This paper explores the effect of warp sizing on control flow divergence, memory divergence, and locality. For an estimated 5% area cost, our ganged scheduling microarchitecture results in a simulated 35% performance improvement on divergent workloads by allowing smaller groups of threads to proceed independently, and eliminates the performance degradation due to memory convergence slip that is observed when convergent applications are executed with smaller warp sizes.},
 acmid = {2750410},
 address = {New York, NY, USA},
 author = {Rogers, Timothy G. and Johnson, Daniel R. and O'Connor, Mike and Keckler, Stephen W.},
 doi = {10.1145/2872887.2750410},
 issn = {0163-5964},
 issue_date = {June 2015},
 journal = {SIGARCH Comput. Archit. News},
 link = {http://doi.acm.org/10.1145/2872887.2750410},
 month = {jun},
 number = {3},
 numpages = {13},
 pages = {489--501},
 publisher = {ACM},
 title = {A Variable Warp Size Architecture},
 volume = {43},
 year = {2015}
}


@article{McFarlin:2015:BVD:2872887.2750400,
 abstract = {While control speculation is highly effective for generating good schedules in out-of-order processors, it is less effective for in-order processors because compilers have trouble scheduling in the presence of unbiased branches, even when those branches are highly predictable. In this paper, we demonstrate a novel architectural branch decomposition that separates the prediction and deconvergence point of a branch from its resolution, which enables the compiler to profitably schedule across predictable, but unbiased branches. We show that the hardware support for this branch architecture is a trivial extension of existing systems and describe a simple code transformation for exploiting this architectural support. As architectural changes are required, this technique is most compelling for a dynamic binary translation-based system like Project Denver. We evaluate the performance improvements enabled by this transformation for several in-order configurations across the SPEC 2006 benchmark suites. We show that our technique produces a Geomean speedup of 11% for SPEC 2006 Integer, with speedups as large as 35%. As floating point benchmarks contain fewer unbiased, but predictable branches, our Geomean speedup on SPEC 2006 FP is 7%, with a maximum speedup of 26%.},
 acmid = {2750400},
 address = {New York, NY, USA},
 author = {McFarlin, Daniel S. and Zilles, Craig},
 doi = {10.1145/2872887.2750400},
 issn = {0163-5964},
 issue_date = {June 2015},
 journal = {SIGARCH Comput. Archit. News},
 link = {http://doi.acm.org/10.1145/2872887.2750400},
 month = {jun},
 number = {3},
 numpages = {13},
 pages = {323--335},
 publisher = {ACM},
 title = {Branch Vanguard: Decomposing Branch Functionality into Prediction and Resolution Instructions},
 volume = {43},
 year = {2015}
}


@article{Das:2015:SRW:2872887.2750398,
 abstract = {Wire energy has become the major contributor to energy in large lower level caches. While wire energy is related to wire latency its costs are exposed differently in the memory hierarchy. We propose Sub-Level Insertion Policy (SLIP), a cache management policy which improves cache energy consumption by increasing the number of accesses from energy efficient locations while simultaneously decreasing intra-level data movement. In SLIP, each cache level is partitioned into several cache sublevels of differing sizes. Then, the recent reuse distance distribution of a line is used to choose an energy-optimized insertion and movement policy for the line. The policy choice is made by a hardware unit that predicts the number of accesses and inter-level movements. Using a full-system simulation including OS interactions and hardware overheads, we show that SLIP saves 35% energy at the L2 and 22% energy at the L3 level and performs 0.75% better than a regular cache hierarchy in a single core system. When configured to include a bypassing policy, SLIP reduces traffic to DRAM by 2.2%. This is achieved at the cost of storing 12b metadata per cache line (2.3% overhead), a 6b policy in the PTE, and 32b distribution metadata for each page in the DRAM (a overhead of 0.1%). Using SLIP in a multiprogrammed system saves 47% LLC energy, and reduces traffic to DRAM by 5.5%.},
 acmid = {2750398},
 address = {New York, NY, USA},
 author = {Das, Subhasis and Aamodt, Tor M. and Dally, William J.},
 doi = {10.1145/2872887.2750398},
 issn = {0163-5964},
 issue_date = {June 2015},
 journal = {SIGARCH Comput. Archit. News},
 link = {http://doi.acm.org/10.1145/2872887.2750398},
 month = {jun},
 number = {3},
 numpages = {13},
 pages = {349--361},
 publisher = {ACM},
 title = {SLIP: Reducing Wire Energy in the Memory Hierarchy},
 volume = {43},
 year = {2015}
}


@inproceedings{Perais:2015:CSS:2749469.2749470,
 abstract = {To maximize performance, out-of-order execution processors sometimes issue instructions without having the guarantee that operands will be available in time; e.g. loads are typically assumed to hit in the L1 cache and dependent instructions are issued accordingly. This form of speculation -- that we refer to as speculative scheduling -- has been used for two decades in real processors, but has received little attention from the research community. In particular, as pipeline depth grows, and the distance between the Issue and the Execute stages increases, it becomes critical to issue instructions dependent on variable-latency instructions as soon as possible rather than wait for the actual cycle at which the result becomes available. Unfortunately, due to the uncertain nature of speculative scheduling, the scheduler may wrongly issue an instruction that will not have its source(s) available on the bypass network when it reaches the Execute stage. In that event, the instruction is canceled and replayed, potentially impairing performance and increasing energy consumption. In this work, we do not present a new replay mechanism. Rather, we focus on ways to reduce the number of replays that are agnostic of the replay scheme. First, we propose an easily implementable, low-cost solution to reduce the number of replays caused by L1 bank conflicts. Schedule shifting always assumes that, given a dual-load issue capacity, the second load issued in a given cycle will be delayed because of a bank conflict. Its dependents are thus always issued with the corresponding delay. Second, we also improve on existing L1 hit/miss prediction schemes by taking into account instruction criticality. That is, for some criterion of criticality and for loads whose hit/miss behavior is hard to predict, we show that it is more cost-effective to stall dependents if the load is not predicted critical. In total, in our experiments assuming a 4-cycle issue-to-execute delay, we found that the majority of instruction replays caused by L1 data cache banks conflicts -- 78.0% -- and L1 hit mispredictions -- 96.5% -- can be avoided, thus leading to a 3.4% performance gain and a 13.4% decrease in the number of issued instructions, over a baseline pipeline with speculative scheduling.},
 acmid = {2749470},
 address = {New York, NY, USA},
 author = {Perais, Arthur and Seznec, Andr{\'e} and Michaud, Pierre and Sembrant, Andreas and Hagersten, Erik},
 booktitle = {Proceedings of the 42Nd Annual International Symposium on Computer Architecture},
 doi = {10.1145/2749469.2749470},
 isbn = {978-1-4503-3402-0},
 link = {http://doi.acm.org/10.1145/2749469.2749470},
 location = {Portland, Oregon},
 numpages = {13},
 pages = {247--259},
 publisher = {ACM},
 series = {ISCA '15},
 title = {Cost-effective Speculative Scheduling in High Performance Processors},
 year = {2015}
}


@inproceedings{Skach:2015:TTS:2749469.2749474,
 abstract = {Datacenters, or warehouse scale computers, are rapidly increasing in size and power consumption. However, this growth comes at the cost of an increasing thermal load that must be removed to prevent overheating and server failure. In this paper, we propose to use phase changing materials (PCM) to shape the thermal load of a datacenter, absorbing and releasing heat when it is advantageous to do so. We present and validate a methodology to study the impact of PCM on a datacenter, and evaluate two important opportunities for cost savings. We find that in a datacenter with full cooling system subscription, PCM can reduce the necessary cooling system size by up to 12% without impacting peak throughput, or increase the number of servers by up to 14.6% without increasing the cooling load. In a thermally constrained setting, PCM can increase peak throughput up to 69% while delaying the onset of thermal limits by over 3 hours.},
 acmid = {2749474},
 address = {New York, NY, USA},
 author = {Skach, Matt and Arora, Manish and Hsu, Chang-Hong and Li, Qi and Tullsen, Dean and Tang, Lingjia and Mars, Jason},
 booktitle = {Proceedings of the 42Nd Annual International Symposium on Computer Architecture},
 doi = {10.1145/2749469.2749474},
 isbn = {978-1-4503-3402-0},
 link = {http://doi.acm.org/10.1145/2749469.2749474},
 location = {Portland, Oregon},
 numpages = {11},
 pages = {439--449},
 publisher = {ACM},
 series = {ISCA '15},
 title = {Thermal Time Shifting: Leveraging Phase Change Materials to Reduce Cooling Costs in Warehouse-scale Computers},
 year = {2015}
}


@inproceedings{Lo:2015:HIR:2749469.2749475,
 abstract = {User-facing, latency-sensitive services, such as websearch, underutilize their computing resources during daily periods of low traffic. Reusing those resources for other tasks is rarely done in production services since the contention for shared resources can cause latency spikes that violate the service-level objectives of latency-sensitive tasks. The resulting under-utilization hurts both the affordability and energy-efficiency of large-scale datacenters. With technology scaling slowing down, it becomes important to address this opportunity. We present Heracles, a feedback-based controller that enables the safe colocation of best-effort tasks alongside a latency-critical service. Heracles dynamically manages multiple hardware and software isolation mechanisms, such as CPU, memory, and network isolation, to ensure that the latency-sensitive job meets latency targets while maximizing the resources given to best-effort tasks. We evaluate Heracles using production latency-critical and batch workloads from Google and demonstrate average server utilizations of 90% without latency violations across all the load and colocation scenarios that we evaluated.},
 acmid = {2749475},
 address = {New York, NY, USA},
 author = {Lo, David and Cheng, Liqun and Govindaraju, Rama and Ranganathan, Parthasarathy and Kozyrakis, Christos},
 booktitle = {Proceedings of the 42Nd Annual International Symposium on Computer Architecture},
 doi = {10.1145/2749469.2749475},
 isbn = {978-1-4503-3402-0},
 link = {http://doi.acm.org/10.1145/2749469.2749475},
 location = {Portland, Oregon},
 numpages = {13},
 pages = {450--462},
 publisher = {ACM},
 series = {ISCA '15},
 title = {Heracles: Improving Resource Efficiency at Scale},
 year = {2015}
}


@inproceedings{Ho:2015:EEM:2749469.2750390,
 abstract = {This paper identifies a new opportunity for improving the efficiency of a processor core: memory access phases of programs. These are dynamic regions of programs where most of the instructions are devoted to memory access or address computation. These occur naturally in programs because of workload properties, or when employing an in-core accelerator, we get induced phases where the code execution on the core is access code. We observe such code requires an OOO core's dataflow and dynamism to run fast and does not execute well on an in-order processor. However, an OOO core consumes much power, effectively increasing energy consumption and reducing the energy efficiency of in-core accelerators. We develop an execution model called memory access dataflow (MAD) that encodes dataflow computation, event-condition-action rules, and explicit actions. Using it we build a specialized engine that provides an OOO core's performance but at a fraction of the power. Such an engine can serve as a general way for any accelerator to execute its respective induced phase, thus providing a common interface and implementation for current and future accelerators. We have designed and implemented MAD in RTL, and we demonstrate its generality and flexibility by integration with four diverse accelerators (SSE, DySER, NPU, and C-Cores). Our quantitative results show, relative to in-order, 2-wide OOO, and 4-wide OOO, MAD provides 2.4×, 1.4× and equivalent performance respectively. It provides 0.8×, 0.6× and 0.4× lower energy.},
 acmid = {2750390},
 address = {New York, NY, USA},
 author = {Ho, Chen-Han and Kim, Sung Jin and Sankaralingam, Karthikeyan},
 booktitle = {Proceedings of the 42Nd Annual International Symposium on Computer Architecture},
 doi = {10.1145/2749469.2750390},
 isbn = {978-1-4503-3402-0},
 link = {http://doi.acm.org/10.1145/2749469.2750390},
 location = {Portland, Oregon},
 numpages = {13},
 pages = {118--130},
 publisher = {ACM},
 series = {ISCA '15},
 title = {Efficient Execution of Memory Access Phases Using Dataflow Specialization},
 year = {2015}
}


@inproceedings{Carlson:2015:LSC:2749469.2750407,
 abstract = {Driven by the motivation to expose instruction-level parallelism (ILP), microprocessor cores have evolved from simple, in-order pipelines into complex, superscalar out-of-order designs. By extracting ILP, these processors also enable parallel cache and memory operations as a useful side-effect. Today, however, the growing off-chip memory wall and complex cache hierarchies of many-core processors make cache and memory accesses ever more costly. This increases the importance of extracting memory hierarchy parallelism (MHP), while reducing the net impact of more general, yet complex and power-hungry ILP-extraction techniques. In addition, for multi-core processors operating in power- and energy-constrained environments, energy-efficiency has largely replaced single-thread performance as the primary concern. Based on this observation, we propose a core microarchitecture that is aimed squarely at generating parallel accesses to the memory hierarchy while maximizing energy efficiency. The Load Slice Core extends the efficient in-order, stall-on-use core with a second in-order pipeline that enables memory accesses and address-generating instructions to bypass stalled instructions in the main pipeline. Backward program slices containing address-generating instructions leading up to loads and stores are extracted automatically by the hardware, using a novel iterative algorithm that requires no software support or recompilation. On average, the Load Slice Core improves performance over a baseline in-order processor by 53% with overheads of only 15% in area and 22% in power, leading to an increase in energy efficiency (MIPS/Watt) over in-order and out-of-order designs by 43% and over 4.7×, respectively. In addition, for a power- and area-constrained many-core design, the Load Slice Core outperforms both in-order and out-of-order designs, achieving a 53% and 95% higher performance, respectively, thus providing an alternative direction for future many-core processors.},
 acmid = {2750407},
 address = {New York, NY, USA},
 author = {Carlson, Trevor E. and Heirman, Wim and Allam, Osman and Kaxiras, Stefanos and Eeckhout, Lieven},
 booktitle = {Proceedings of the 42Nd Annual International Symposium on Computer Architecture},
 doi = {10.1145/2749469.2750407},
 isbn = {978-1-4503-3402-0},
 link = {http://doi.acm.org/10.1145/2749469.2750407},
 location = {Portland, Oregon},
 numpages = {13},
 pages = {272--284},
 publisher = {ACM},
 series = {ISCA '15},
 title = {The Load Slice Core Microarchitecture},
 year = {2015}
}


@article{Du:2015:SSV:2872887.2750389,
 abstract = {In recent years, neural network accelerators have been shown to achieve both high energy efficiency and high performance for a broad application scope within the important category of recognition and mining applications. Still, both the energy efficiency and performance of such accelerators remain limited by memory accesses. In this paper, we focus on image applications, arguably the most important category among recognition and mining applications. The neural networks which are state-of-the-art for these applications are Convolutional Neural Networks (CNN), and they have an important property: weights are shared among many neurons, considerably reducing the neural network memory footprint. This property allows to entirely map a CNN within an SRAM, eliminating all DRAM accesses for weights. By further hoisting this accelerator next to the image sensor, it is possible to eliminate all remaining DRAM accesses, i.e., for inputs and outputs. In this paper, we propose such a CNN accelerator, placed next to a CMOS or CCD sensor. The absence of DRAM accesses combined with a careful exploitation of the specific data access patterns within CNNs allows us to design an accelerator which is 60× more energy efficient than the previous state-of-the-art neural network accelerator. We present a full design down to the layout at 65 nm, with a modest footprint of 4.86mm2 and consuming only 320mW, but still about 30× faster than high-end GPUs.},
 acmid = {2750389},
 address = {New York, NY, USA},
 author = {Du, Zidong and Fasthuber, Robert and Chen, Tianshi and Ienne, Paolo and Li, Ling and Luo, Tao and Feng, Xiaobing and Chen, Yunji and Temam, Olivier},
 doi = {10.1145/2872887.2750389},
 issn = {0163-5964},
 issue_date = {June 2015},
 journal = {SIGARCH Comput. Archit. News},
 link = {http://doi.acm.org/10.1145/2872887.2750389},
 month = {jun},
 number = {3},
 numpages = {13},
 pages = {92--104},
 publisher = {ACM},
 title = {ShiDianNao: Shifting Vision Processing Closer to the Sensor},
 volume = {43},
 year = {2015}
}


@article{Stephenson:2015:FSP:2872887.2750375,
 abstract = {To aid application characterization and architecture design space exploration, researchers and engineers have developed a wide range of tools for CPUs, including simulators, profilers, and binary instrumentation tools. With the advent of GPU computing, GPU manufacturers have developed similar tools leveraging hardware profiling and debugging hooks. To date, these tools are largely limited by the fixed menu of options provided by the tool developer and do not offer the user the flexibility to observe or act on events not in the menu. This paper presents SASSI (NVIDIA assembly code "SASS" Instrumentor), a low-level assembly-language instrumentation tool for GPUs. Like CPU binary instrumentation tools, SASSI allows a user to specify instructions at which to inject user-provided instrumentation code. These facilities allow strategic placement of counters and code into GPU assembly code to collect user-directed, fine-grained statistics at hardware speeds. SASSI instrumentation is inherently parallel, leveraging the concurrency of the underlying hardware. In addition to the details of SASSI, this paper provides four case studies that show how SASSI can be used to characterize applications and explore the architecture design space along the dimensions of instruction control flow, memory systems, value similarity, and resilience.},
 acmid = {2750375},
 address = {New York, NY, USA},
 author = {Stephenson, Mark and Sastry Hari, Siva Kumar and Lee, Yunsup and Ebrahimi, Eiman and Johnson, Daniel R. and Nellans, David and O'Connor, Mike and Keckler, Stephen W.},
 doi = {10.1145/2872887.2750375},
 issn = {0163-5964},
 issue_date = {June 2015},
 journal = {SIGARCH Comput. Archit. News},
 link = {http://doi.acm.org/10.1145/2872887.2750375},
 month = {jun},
 number = {3},
 numpages = {13},
 pages = {185--197},
 publisher = {ACM},
 title = {Flexible Software Profiling of GPU Architectures},
 volume = {43},
 year = {2015}
}


@inproceedings{Pannuto:2015:MUP:2749469.2750376,
 abstract = {As we show in this paper, I/O has become the limiting factor in scaling down size and power toward the goal of invisible computing. Achieving this goal will require composing optimized and specialized---yet reusable---components with an interconnect that permits tiny, ultra-low power systems. In contrast to today's interconnects which are limited by power-hungry pull-ups or high-overhead chip-select lines, our approach provides a superset of common bus features but at lower power, with fixed area and pin count, using fully synthesizable logic, and with surprisingly low protocol overhead. We present MBus, a new 4-pin, 22.6 pJ/bit/chip chip-to-chip interconnect made of two "shoot-through" rings. MBus facilitates ultra-low power system operation by implementing automatic power-gating of each chip in the system, easing the integration of active, inactive, and activating circuits on a single die. In addition, we introduce a new bus primitive: power oblivious communication, which guarantees message reception regardless of the recipient's power state when a message is sent. This disentangles power management from communication, greatly simplifying the creation of viable, modular, and heterogeneous systems that operate on the order of nanowatts. To evaluate the viability, power, performance, overhead, and scalability of our design, we build both hardware and software implementations of MBus and show its seamless operation across two FPGAs and twelve custom chips from three different semiconductor processes. A three-chip, 2.2mm3 MBus system draws 8nW of total system standby power and uses only 22.6 pJ/bit/chip for communication. This is the lowest power for any system bus with MBus's feature set.},
 acmid = {2750376},
 address = {New York, NY, USA},
 author = {Pannuto, Pat and Lee, Yoonmyung and Kuo, Ye-Sheng and Foo, ZhiYoong and Kempke, Benjamin and Kim, Gyouho and Dreslinski, Ronald G. and Blaauw, David and Dutta, Prabal},
 booktitle = {Proceedings of the 42Nd Annual International Symposium on Computer Architecture},
 doi = {10.1145/2749469.2750376},
 isbn = {978-1-4503-3402-0},
 link = {http://doi.acm.org/10.1145/2749469.2750376},
 location = {Portland, Oregon},
 numpages = {13},
 pages = {629--641},
 publisher = {ACM},
 series = {ISCA '15},
 title = {MBus: An Ultra-low Power Interconnect Bus for Next Generation Nanopower Systems},
 year = {2015}
}


@article{Nachiappan:2015:VVI:2872887.2750382,
 abstract = {Energy-efficient user-interactive and display-oriented applications on handhelds rely heavily on multiple accelerators (termed IP cores) to meet their periodic frame processing needs. Further, these platforms are starting to host multiple applications concurrently on the multiple CPU cores. Unfortunately, today's hardware exposes an interface that forces the host software (Android drivers) to treat each IP core as an isolated device. Consequently, the host CPU has to get involved in the (i) processing of each frame, (ii) scheduling them to ensure timely progress through the IP cores to meet their QoS needs, and (iii) explicitly having to move data from one IP core to the next, with main memory serving as the common staging area. We show in this paper through measurements on a Nexus 7 platform that the frequent invocation of the CPU for processing these frames and the involvement of main memory as a data flow conduit, are serious limitations. Instead, we propose a novel IP virtualization framework (VIP), involving three key ideas that allow several IPs to be chained together and made to appear to the software as a single device. First, chaining of IPs avoids data transfer through the memory system, enhancing the throughput of flows through the IPs. Second, by using a burst-mode, the CPU can initiate the processing of several frames through the virtual IP chain, without getting involved (and interrupted) for each frame, thereby allowing better energy saving and utilization opportunities. Removing the CPU from this loop, requires alternate orchestration of frame flows to ensure QoS guarantees for each frame of each application. Our third enhancement in VIP creates several virtual paths, one for each flow, through these IP chains with the hardware scheduling the frames to enforce QoS guarantees despite any contention for resources along the way. Our experimental evaluations demonstrate the effectiveness of VIP on energy consumption and QoS for multiple applications.},
 acmid = {2750382},
 address = {New York, NY, USA},
 author = {Nachiappan, Nachiappan Chidambaram and Zhang, Haibo and Ryoo, Jihyun and Soundararajan, Niranjan and Sivasubramaniam, Anand and Kandemir, Mahmut T. and Iyer, Ravi and Das, Chita R.},
 doi = {10.1145/2872887.2750382},
 issn = {0163-5964},
 issue_date = {June 2015},
 journal = {SIGARCH Comput. Archit. News},
 link = {http://doi.acm.org/10.1145/2872887.2750382},
 month = {jun},
 number = {3},
 numpages = {13},
 pages = {655--667},
 publisher = {ACM},
 title = {VIP: Virtualizing IP Chains on Handheld Platforms},
 volume = {43},
 year = {2015}
}


@inproceedings{Kanev:2015:PWC:2749469.2750392,
 abstract = {With the increasing prevalence of warehouse-scale (WSC) and cloud computing, understanding the interactions of server applications with the underlying microarchitecture becomes ever more important in order to extract maximum performance out of server hardware. To aid such understanding, this paper presents a detailed microarchitectural analysis of live datacenter jobs, measured on more than 20,000 Google machines over a three year period, and comprising thousands of different applications. We first find that WSC workloads are extremely diverse, breeding the need for architectures that can tolerate application variability without performance loss. However, some patterns emerge, offering opportunities for co-optimization of hardware and software. For example, we identify common building blocks in the lower levels of the software stack. This "datacenter tax" can comprise nearly 30% of cycles across jobs running in the fleet, which makes its constituents prime candidates for hardware specialization in future server systems-on-chips. We also uncover opportunities for classic microarchitectural optimizations for server processors, especially in the cache hierarchy. Typical workloads place significant stress on instruction caches and prefer memory latency over bandwidth. They also stall cores often, but compute heavily in bursts. These observations motivate several interesting directions for future warehouse-scale computers.},
 acmid = {2750392},
 address = {New York, NY, USA},
 author = {Kanev, Svilen and Darago, Juan Pablo and Hazelwood, Kim and Ranganathan, Parthasarathy and Moseley, Tipp and Wei, Gu-Yeon and Brooks, David},
 booktitle = {Proceedings of the 42Nd Annual International Symposium on Computer Architecture},
 doi = {10.1145/2749469.2750392},
 isbn = {978-1-4503-3402-0},
 link = {http://doi.acm.org/10.1145/2749469.2750392},
 location = {Portland, Oregon},
 numpages = {12},
 pages = {158--169},
 publisher = {ACM},
 series = {ISCA '15},
 title = {Profiling a Warehouse-scale Computer},
 year = {2015}
}


@article{Karakostas:2015:RMM:2872887.2749471,
 abstract = {Page-based virtual memory improves programmer productivity, security, and memory utilization, but incurs performance overheads due to costly page table walks after TLB misses. This overhead can reach 50% for modern workloads that access increasingly vast memory with stagnating TLB sizes. To reduce the overhead of virtual memory, this paper proposes Redundant Memory Mappings (RMM), which leverage ranges of pages and provides an efficient, alternative representation of many virtual-to-physical mappings. We define a range be a subset of process's pages that are virtually and physically contiguous. RMM translates each range with a single range table entry, enabling a modest number of entries to translate most of the process's address space. RMM operates in parallel with standard paging and uses a software range table and hardware range TLB with arbitrarily large reach. We modify the operating system to automatically detect ranges and to increase their likelihood with eager page allocation. RMM is thus transparent to applications. We prototype RMM software in Linux and emulate the hardware. RMM performs substantially better than paging alone and huge pages, and improves a wider variety of workloads than direct segments (one range per program), reducing the overhead of virtual memory to less than 1% on average.},
 acmid = {2749471},
 address = {New York, NY, USA},
 author = {Karakostas, Vasileios and Gandhi, Jayneel and Ayar, Furkan and Cristal, Adri\'{a}n and Hill, Mark D. and McKinley, Kathryn S. and Nemirovsky, Mario and Swift, Michael M. and \"{U}nsal, Osman},
 doi = {10.1145/2872887.2749471},
 issn = {0163-5964},
 issue_date = {June 2015},
 journal = {SIGARCH Comput. Archit. News},
 link = {http://doi.acm.org/10.1145/2872887.2749471},
 month = {jun},
 number = {3},
 numpages = {13},
 pages = {66--78},
 publisher = {ACM},
 title = {Redundant Memory Mappings for Fast Access to Large Memories},
 volume = {43},
 year = {2015}
}


@article{Liang:2015:MMS:2872887.2750396,
 abstract = {While numerous hardware synchronization mechanisms have been proposed, they either no longer function or suffer great performance loss when their hardware resources are exceeded, or they add significant complexity and cost to handle such resource overflows. Additionally, prior hardware synchronization proposals focus on one type (barrier or lock) of synchronization, so several mechanisms are likely to be needed to support real applications, many of which use locks, barriers, and/or condition variables. This paper proposes MiSAR, a minimalistic synchronization accelerator (MSA) that supports all three commonly used types of synchronization (locks, barriers, and condition variables), and a novel overflow management unit (OMU) that dynamically manages its (very) limited hardware synchronization resources. The OMU allows safe and efficient dynamic transitions between using hardware (MSA) and software synchronization implementations. This allows the MSA's resources to be used only for currently-active synchronization operations, providing significant performance benefits even when the number of synchronization variables used in the program is much larger than the MSA's resources. Because it allows a safe transition between hardware and software synchronization, the OMU also facilitates thread suspend/resume, migration, and other thread-management activities. Finally, the MSA/OMU combination decouples the instruction set support (how the program invokes hardware-supported synchronization) from the actual implementation of the accelerator, allowing different accelerators (or even wholesale removal of the accelerator) in the future without changes to OMU-compatible application or system code. We show that, even with only 2 MSA entries in each tile, the MSA/OMU combination on average performs within 3% of ideal (zero-latency) synchronization, and achieves a speedup of 1.43X over the software (pthreads) implementation.},
 acmid = {2750396},
 address = {New York, NY, USA},
 author = {Liang, Ching-Kai and Prvulovic, Milos},
 doi = {10.1145/2872887.2750396},
 issn = {0163-5964},
 issue_date = {June 2015},
 journal = {SIGARCH Comput. Archit. News},
 link = {http://doi.acm.org/10.1145/2872887.2750396},
 month = {jun},
 number = {3},
 numpages = {13},
 pages = {414--426},
 publisher = {ACM},
 title = {MiSAR: Minimalistic Synchronization Accelerator with Resource Overflow Management},
 volume = {43},
 year = {2015}
}


@inproceedings{Huang:2015:UAT:2749469.2750420,
 abstract = {Applications can map data on SSDs into virtual memory to transparently scale beyond DRAM capacity, permitting them to leverage high SSD capacities with few code changes. Obtaining good performance for memory-mapped SSD content, however, is hard because the virtual memory layer, the file system and the flash translation layer (FTL) perform address translations, sanity and permission checks independently from each other. We introduce FlashMap, an SSD interface that is optimized for memory-mapped SSD-files. FlashMap combines all the address translations into page tables that are used to index files and also to store the FTL-level mappings without altering the guarantees of the file system or the FTL. It uses the state in the OS memory manager and the page tables to perform sanity and permission checks respectively. By combining these layers, FlashMap reduces critical-path latency and improves DRAM caching efficiency. We find that this increases performance for applications by up to 3.32x compared to state-of-the-art SSD file-mapping mechanisms. Additionally, latency of SSD accesses reduces by up to 53.2%.},
 acmid = {2750420},
 address = {New York, NY, USA},
 author = {Huang, Jian and Badam, Anirudh and Qureshi, Moinuddin K. and Schwan, Karsten},
 booktitle = {Proceedings of the 42Nd Annual International Symposium on Computer Architecture},
 doi = {10.1145/2749469.2750420},
 isbn = {978-1-4503-3402-0},
 link = {http://doi.acm.org/10.1145/2749469.2750420},
 location = {Portland, Oregon},
 numpages = {12},
 pages = {580--591},
 publisher = {ACM},
 series = {ISCA '15},
 title = {Unified Address Translation for Memory-mapped SSDs with FlashMap},
 year = {2015}
}


@inproceedings{Callan:2015:FFA:2749469.2750394,
 abstract = {While all computation generates electromagnetic (EM) side-channel signals, some of the strongest and farthest-propagating signals are created when an existing strong periodic signal (e.g. a clock signal) becomes stronger or weaker (amplitude-modulated) depending on processor or memory activity. However, modern systems create emanations at thousands of different frequencies, so it is a difficult, error-prone, and time-consuming task to find those few emanations that are AM-modulated by processor/memory activity. This paper presents a methodology for rapidly finding such activity-modulated signals. This method creates recognizable spectral patterns generated by specially designed microbenchmarks and then processes the recorded spectra to identify signals that exhibit amplitude-modulation behavior. We apply this method to several computer systems and find several such modulated signals. To illustrate how our methodology can benefit side-channel security research and practice, we also identify the physical mechanisms behind those signals, and find that the strongest signals are created by voltage regulators, memory refreshes, and DRAM clocks. Our results indicate that each signal may carry unique information about system activity, potentially enhancing an attacker's capability to extract sensitive information. We also confirm that our methodology correctly separates emanated signals that are affected by specific processor or memory activities from those that are not.},
 acmid = {2750394},
 address = {New York, NY, USA},
 author = {Callan, Robert and Zaji\'{c}, Alenka and Prvulovic, Milos},
 booktitle = {Proceedings of the 42Nd Annual International Symposium on Computer Architecture},
 doi = {10.1145/2749469.2750394},
 isbn = {978-1-4503-3402-0},
 link = {http://doi.acm.org/10.1145/2749469.2750394},
 location = {Portland, Oregon},
 numpages = {12},
 pages = {592--603},
 publisher = {ACM},
 series = {ISCA '15},
 title = {FASE: Finding Amplitude-modulated Side-channel Emanations},
 year = {2015}
}


@article{Akin:2015:DRM:2872887.2750397,
 abstract = {In this paper we focus on common data reorganization operations such as shuffle, pack/unpack, swap, transpose, and layout transformations. Although these operations simply relocate the data in the memory, they are costly on conventional systems mainly due to inefficient access patterns, limited data reuse and roundtrip data traversal throughout the memory hierarchy. This paper presents a two pronged approach for efficient data reorganization, which combines (i) a proposed DRAM-aware reshape accelerator integrated within 3D-stacked DRAM, and (ii) a mathematical framework that is used to represent and optimize the reorganization operations. We evaluate our proposed system through two major use cases. First, we demonstrate the reshape accelerator in performing a physical address remapping via data layout transform to utilize the internal parallelism/locality of the 3D-stacked DRAM structure more efficiently for general purpose workloads. Then, we focus on offloading and accelerating commonly used data reorganization routines selected from the Intel Math Kernel Library package. We evaluate the energy and performance benefits of our approach by comparing it against existing optimized implementations on state-of-the-art GPUs and CPUs. For the various test cases, in-memory data reorganization provides orders of magnitude performance and energy efficiency improvements via low overhead hardware.},
 acmid = {2750397},
 address = {New York, NY, USA},
 author = {Akin, Berkin and Franchetti, Franz and Hoe, James C.},
 doi = {10.1145/2872887.2750397},
 issn = {0163-5964},
 issue_date = {June 2015},
 journal = {SIGARCH Comput. Archit. News},
 link = {http://doi.acm.org/10.1145/2872887.2750397},
 month = {jun},
 number = {3},
 numpages = {13},
 pages = {131--143},
 publisher = {ACM},
 title = {Data Reorganization in Memory Using 3D-stacked DRAM},
 volume = {43},
 year = {2015}
}


@article{Zhang:2015:CAS:2872887.2750422,
 abstract = {Cloud customers need guarantees regarding the security of their virtual machines (VMs), operating within an Infrastructure as a Service (IaaS) cloud system. This is complicated by the customer not knowing where his VM is executing, and on the semantic gap between what the customer wants to know versus what can be measured in the cloud. We present an architecture for monitoring a VM's security health, with the ability to attest this to the customer in an unforgeable manner. We show a concrete implementation of property-based attestation and a full prototype based on the OpenStack open source cloud software.},
 acmid = {2750422},
 address = {New York, NY, USA},
 author = {Zhang, Tianwei and Lee, Ruby B.},
 doi = {10.1145/2872887.2750422},
 issn = {0163-5964},
 issue_date = {June 2015},
 journal = {SIGARCH Comput. Archit. News},
 link = {http://doi.acm.org/10.1145/2872887.2750422},
 month = {jun},
 number = {3},
 numpages = {13},
 pages = {362--374},
 publisher = {ACM},
 title = {CloudMonatt: An Architecture for Security Health Monitoring and Attestation of Virtual Machines in Cloud Computing},
 volume = {43},
 year = {2015}
}


@inproceedings{Bhati:2015:FAE:2749469.2750408,
 abstract = {DRAM cells require periodic refreshing to preserve data. In JEDEC DDRx devices, a refresh operation is performed via an auto-refresh command, which refreshes multiple rows in multiple banks simultaneously. The internal implementation of auto-refresh is completely opaque outside the DRAM --- all the memory controller can do is to instruct the DRAM to refresh itself --- the DRAM handles all else, in particular determining which rows in which banks are to be refreshed. This is in conflict with a large body of research on reducing the refresh overhead, in which the memory controller needs fine-grained control over which regions of the memory are refreshed. For example, prior works exploit the fact that a subset of DRAM rows can be refreshed at a slower rate than other rows due to access rate or retention period variations. However, such row-granularity approaches cannot use the standard auto-refresh command, which refreshes an entire batch of rows at once and does not permit skipping of rows. Consequently, prior schemes are forced to use explicit sequences of activate (ACT) and precharge (PRE) operations to mimic row-level refreshing. The drawback is that, compared to using JEDEC's auto-refresh mechanism, using explicit ACT and PRE commands is inefficient, both in terms of performance and power. In this paper, we show that even when skipping a high percentage of refresh operations, existing row-granurality refresh techniques are mostly ineffective due to the inherent efficiency disparity between ACT/PRE and the JEDEC auto-refresh mechanism. We propose a modification to the DRAM that extends its existing control-register access protocol to include the DRAM's internal refresh counter. We also introduce a new "dummy refresh" command that skips refresh operations and simply increments the internal counter. We show that these modifications allow a memory controller to reduce as many refreshes as in prior work, while achieving significant energy and performance advantages by using auto-refresh most of the time.},
 acmid = {2750408},
 address = {New York, NY, USA},
 author = {Bhati, Ishwar and Chishti, Zeshan and Lu, Shih-Lien and Jacob, Bruce},
 booktitle = {Proceedings of the 42Nd Annual International Symposium on Computer Architecture},
 doi = {10.1145/2749469.2750408},
 isbn = {978-1-4503-3402-0},
 link = {http://doi.acm.org/10.1145/2749469.2750408},
 location = {Portland, Oregon},
 numpages = {12},
 pages = {235--246},
 publisher = {ACM},
 series = {ISCA '15},
 title = {Flexible Auto-refresh: Enabling Scalable and Energy-efficient DRAM Refresh Reductions},
 year = {2015}
}


@inproceedings{Palframan:2015:CCP:2749469.2750377,
 abstract = {Protecting main memories from soft errors typically requires special dual-inline memory modules (DIMMs) which incorporate at least one extra chip per rank to store error-correcting codes (ECC). This increases the cost of the DIMM as well as its power consumption. To avoid these costs, some proposals have suggested protecting non-ECC DIMMs by allocating a portion of memory space to store ECC metadata. However, such proposals can significantly shrink the available memory space while degrading performance due to extra memory accesses. In this work, we propose a technique called COP which uses block-level compression to make room for ECC check bits in DRAM. Because a compressed block with check bits is the same size as an uncompressed block, no extra memory accesses are required and the memory space is not reduced. Unlike other approaches that require explicit compression-tracking metadata, COP employs a novel mechanism that relies on ECC to detect compressed data. Our results show that COP can reduce the DRAM soft error rate by 93% with no storage overhead and negligible impact on performance. We also propose a technique using COP to protect both compressible and incompressible data with minimal storage and performance overheads.},
 acmid = {2750377},
 address = {New York, NY, USA},
 author = {Palframan, David J. and Kim, Nam Sung and Lipasti, Mikko H.},
 booktitle = {Proceedings of the 42Nd Annual International Symposium on Computer Architecture},
 doi = {10.1145/2749469.2750377},
 isbn = {978-1-4503-3402-0},
 link = {http://doi.acm.org/10.1145/2749469.2750377},
 location = {Portland, Oregon},
 numpages = {12},
 pages = {682--693},
 publisher = {ACM},
 series = {ISCA '15},
 title = {COP: To Compress and Protect Main Memory},
 year = {2015}
}


@inproceedings{Zhang:2015:HPT:2749469.2750388,
 abstract = {Racetrack memory is an emerging non-volatile memory based on spintronic domain wall technology. It can achieve ultra-high storage density. Also, its read/write speed is comparable to that of SRAM. Due to the tape-like structure of its storage cell, a "shift" operation is introduced to access racetrack memory. Thus, prior research mainly focused on minimizing shift latency/energy of racetrack memory while leveraging its ultra-high storage density. Yet the reliability issue of a shift operation, however, is not well addressed. In fact, racetrack memory suffers from unsuccessful shift due to domain misalignment. Such a problem is called "position error" in this work. It can significantly reduce mean-time-to-failure (MTTF) of racetrack memory to an intolerable level. Even worse, conventional error correction codes (ECCs), which are designed for "bit errors", cannot protect racetrack memory from the position errors. In this work, we investigate the position error model of a shift operation and categorize position errors into two types: "stop-in-middle" error and "out-of-step" error. To eliminate the stop-in-middle error, we propose a technique called sub-threshold shift (STS) to perform a more reliable shift in two stages. To detect and recover the out-of-step error, a protection mechanism called position error correction code (p-ECC) is proposed. We first describe how to design a p-ECC for different protection strength and analyze corresponding design overhead. Then, we further propose how to reduce area cost of p-ECC by leveraging the "overhead region" in a racetrack memory stripe. With these protection mechanisms, we introduce a position-error-aware shift architecture. Experimental results demonstrate that, after using our techniques, the overall MTTF of racetrack memory is improved from 1.33μs to more than 69 years, with only 0:2% performance degradation. Trade-off among reliability, area, performance, and energy is also explored with comprehensive discussion.},
 acmid = {2750388},
 address = {New York, NY, USA},
 author = {Zhang, Chao and Sun, Guangyu and Zhang, Xian and Zhang, Weiqi and Zhao, Weisheng and Wang, Tao and Liang, Yun and Liu, Yongpan and Wang, Yu and Shu, Jiwu},
 booktitle = {Proceedings of the 42Nd Annual International Symposium on Computer Architecture},
 doi = {10.1145/2749469.2750388},
 isbn = {978-1-4503-3402-0},
 link = {http://doi.acm.org/10.1145/2749469.2750388},
 location = {Portland, Oregon},
 numpages = {13},
 pages = {694--706},
 publisher = {ACM},
 series = {ISCA '15},
 title = {Hi-fi Playback: Tolerating Position Errors in Shift Operations of Racetrack Memory},
 year = {2015}
}


@inproceedings{Lee:2015:CCW:2749469.2750418,
 abstract = {The ubiquity of graphics processing unit (GPU) architectures has made them efficient alternatives to chip-multiprocessors for parallel workloads. GPUs achieve superior performance by making use of massive multi-threading and fast context-switching to hide pipeline stalls and memory access latency. However, recent characterization results have shown that general purpose GPU (GPGPU) applications commonly encounter long stall latencies that cannot be easily hidden with the large number of concurrent threads/warps. This results in varying execution time disparity between different parallel warps, hurting the overall performance of GPUs -- the warp criticality problem. To tackle the warp criticality problem, we propose a coordinated solution, criticality-aware warp acceleration (CAWA), that efficiently manages compute and memory resources to accelerate the critical warp execution. Specifically, we design (1) an instruction-based and stall-based criticality predictor to identify the critical warp in a thread-block, (2) a criticality-aware warp scheduler that preferentially allocates more time resources to the critical warp, and (3) a criticality-aware cache reuse predictor that assists critical warp acceleration by retaining latency-critical and useful cache blocks in the L1 data cache. CAWA targets to remove the significant execution time disparity in order to improve resource utilization for GPGPU workloads. Our evaluation results show that, under the proposed coordinated scheduler and cache prioritization management scheme, the performance of the GPGPU workloads can be improved by 23% while other state-of-the-art schedulers, GTO and 2-level schedulers, improve performance by 16% and -2% respectively.},
 acmid = {2750418},
 address = {New York, NY, USA},
 author = {Lee, Shin-Ying and Arunkumar, Akhil and Wu, Carole-Jean},
 booktitle = {Proceedings of the 42Nd Annual International Symposium on Computer Architecture},
 doi = {10.1145/2749469.2750418},
 isbn = {978-1-4503-3402-0},
 link = {http://doi.acm.org/10.1145/2749469.2750418},
 location = {Portland, Oregon},
 numpages = {13},
 pages = {515--527},
 publisher = {ACM},
 series = {ISCA '15},
 title = {CAWA: Coordinated Warp Scheduling and Cache Prioritization for Critical Warp Acceleration of GPGPU Workloads},
 year = {2015}
}


@article{Wang:2015:DTB:2872887.2750393,
 abstract = {GPUs have been proven effective for structured applications that map well to the rigid 1D-3D grid of threads in modern bulk synchronous parallel (BSP) programming languages. However, less success has been encountered in mapping data intensive irregular applications such as graph analytics, relational databases, and machine learning. Recently introduced nested device-side kernel launching functionality in the GPU is a step in the right direction, but still falls short of being able to effectively harness the GPUs performance potential. We propose a new mechanism called Dynamic Thread Block Launch (DTBL) to extend the current bulk synchronous parallel model underlying the current GPU execution model by supporting dynamic spawning of lightweight thread blocks. This mechanism supports the nested launching of thread blocks rather than kernels to execute dynamically occurring parallel work elements. This paper describes the execution model of DTBL, device-runtime support, and microarchitecture extensions to track and execute dynamically spawned thread blocks. Experiments with a set of irregular data intensive CUDA applications executing on a cycle-level simulator show that DTBL achieves average 1.21x speedup over the original flat implementation and average 1.40x over the implementation with device-side kernel launches using CUDA Dynamic Parallelism.},
 acmid = {2750393},
 address = {New York, NY, USA},
 author = {Wang, Jin and Rubin, Norm and Sidelnik, Albert and Yalamanchili, Sudhakar},
 doi = {10.1145/2872887.2750393},
 issn = {0163-5964},
 issue_date = {June 2015},
 journal = {SIGARCH Comput. Archit. News},
 link = {http://doi.acm.org/10.1145/2872887.2750393},
 month = {jun},
 number = {3},
 numpages = {13},
 pages = {528--540},
 publisher = {ACM},
 title = {Dynamic Thread Block Launch: A Lightweight Execution Mechanism to Support Irregular Applications on GPUs},
 volume = {43},
 year = {2015}
}


@article{Lee:2015:WEP:2872887.2750417,
 abstract = {This paper presents Warped-Compression, a warp-level register compression scheme for reducing GPU power consumption. This work is motivated by the observation that the register values of threads within the same warp are similar, namely the arithmetic differences between two successive thread registers is small. Removing data redundancy of register values through register compression reduces the effective register width, thereby enabling power reduction opportunities. GPU register files are huge as they are necessary to keep concurrent execution contexts and to enable fast context switching. As a result register file consumes a large fraction of the total GPU chip power. GPU design trends show that the register file size will continue to increase to enable even more thread level parallelism. To reduce register file data redundancy warped-compression uses low-cost and implementation-efficient base-delta-immediate (BDI) compression scheme, that takes advantage of banked register file organization used in GPUs. Since threads within a warp write values with strong similarity, BDI can quickly compress and decompress by selecting either a single register, or one of the register banks, as the primary base and then computing delta values of all the other registers, or banks. Warped-compression can be used to reduce both dynamic and leakage power. By compressing register values, each warp-level register access activates fewer register banks, which leads to reduction in dynamic power. When fewer banks are used to store the register content, leakage power can be reduced by power gating the unused banks. Evaluation results show that register compression saves 25% of the total register file power consumption.},
 acmid = {2750417},
 address = {New York, NY, USA},
 author = {Lee, Sangpil and Kim, Keunsoo and Koo, Gunjae and Jeon, Hyeran and Ro, Won Woo and Annavaram, Murali},
 doi = {10.1145/2872887.2750417},
 issn = {0163-5964},
 issue_date = {June 2015},
 journal = {SIGARCH Comput. Archit. News},
 link = {http://doi.acm.org/10.1145/2872887.2750417},
 month = {jun},
 number = {3},
 numpages = {13},
 pages = {502--514},
 publisher = {ACM},
 title = {Warped-compression: Enabling Power Efficient GPUs Through Register Compression},
 volume = {43},
 year = {2015}
}


@article{Daglis:2015:MNI:2872887.2750415,
 abstract = {Datacenter operators rely on low-cost, high-density technologies to maximize throughput for data-intensive services with tight tail latencies. In-memory rack-scale computing is emerging as a promising paradigm in scale-out datacenters capitalizing on commodity SoCs, low-latency and high-bandwidth communication fabrics and a remote memory access model to enable aggregation of a rack's memory for critical data-intensive applications such as graph processing or key-value stores. Low latency and high bandwidth not only dictate eliminating communication bottlenecks in the software protocols and off-chip fabrics but also a careful on-chip integration of network interfaces. The latter is a key challenge especially in architectures with RDMA-inspired one-sided operations that aim to achieve low latency and high bandwidth through on-chip Network Interface (NI) support. This paper proposes and evaluates network interface architectures for tiled manycore SoCs for in-memory rack-scale computing. Our results indicate that a careful splitting of NI functionality per chip tile and at the chip's edge along a NOC dimension enables a rack-scale architecture to optimize for both latency and bandwidth. Our best manycore NI architecture achieves latencies within 3% of an idealized hardware NUMA and efficiently uses the full bisection bandwidth of the NOC, without changing the on-chip coherence protocol or the core's microarchitecture.},
 acmid = {2750415},
 address = {New York, NY, USA},
 author = {Daglis, Alexandros and Novakovi\'{c}, Stanko and Bugnion, Edouard and Falsafi, Babak and Grot, Boris},
 doi = {10.1145/2872887.2750415},
 issn = {0163-5964},
 issue_date = {June 2015},
 journal = {SIGARCH Comput. Archit. News},
 link = {http://doi.acm.org/10.1145/2872887.2750415},
 month = {jun},
 number = {3},
 numpages = {13},
 pages = {567--579},
 publisher = {ACM},
 title = {Manycore Network Interfaces for In-memory Rack-scale Computing},
 volume = {43},
 year = {2015}
}


@article{Khudia:2015:ROQ:2872887.2750371,
 abstract = {Approximate computing can be employed for an emerging class of applications from various domains such as multimedia, machine learning and computer vision. The approximated output of such applications, even though not 100% numerically correct, is often either useful or the difference is unnoticeable to the end user. This opens up a new design dimension to trade off application performance and energy consumption with output correctness. However, a largely unaddressed challenge is quality control: how to ensure the user experience meets a prescribed level of quality. Current approaches either do not monitor output quality or use sampling approaches to check a small subset of the output assuming that it is representative. While these approaches have been shown to produce average errors that are acceptable, they often miss large errors without any means to take corrective actions. To overcome this challenge, we propose Rumba for online detection and correction of large approximation errors in an approximate accelerator-based computing environment. Rumba employs continuous lightweight checks in the accelerator to detect large approximation errors and then fixes these errors by exact re-computation on the host processor. Rumba employs computationally inexpensive output error prediction models for efficient detection. Computing patterns amenable for approximation (e.g., map and stencil) are usually data parallel in nature and Rumba exploits this property for selective correction. Overall, Rumba is able to achieve 2.1x reduction in output error for an unchecked approximation accelerator while maintaining the accelerator performance gains at the cost of reducing the energy savings from 3.2x to 2.2x for a set of applications from different approximate computing domains.},
 acmid = {2750371},
 address = {New York, NY, USA},
 author = {Khudia, Daya S. and Zamirai, Babak and Samadi, Mehrzad and Mahlke, Scott},
 doi = {10.1145/2872887.2750371},
 issn = {0163-5964},
 issue_date = {June 2015},
 journal = {SIGARCH Comput. Archit. News},
 link = {http://doi.acm.org/10.1145/2872887.2750371},
 month = {jun},
 number = {3},
 numpages = {13},
 pages = {554--566},
 publisher = {ACM},
 title = {Rumba: An Online Quality Management System for Approximate Computing},
 volume = {43},
 year = {2015}
}


@proceedings{Min:2016:3001136,
 abstract = {The ISCA conference is the premier forum for presenting new contributions in computer architecture.},
 address = {Piscataway, NJ, USA},
 isbn = {978-1-4673-8947-1},
 location = {Seoul, Republic of Korea},
 note = {IEEE Computer Society Order No.: E5861},
 publisher = {IEEE Press},
 title = {ISCA '16: Proceedings of the 43rd International Symposium on Computer Architecture},
 year = {2016}
}


@inproceedings{Ahn:2015:PIL:2749469.2750385,
 abstract = {Processing-in-memory (PIM) is rapidly rising as a viable solution for the memory wall crisis, rebounding from its unsuccessful attempts in 1990s due to practicality concerns, which are alleviated with recent advances in 3D stacking technologies. However, it is still challenging to integrate the PIM architectures with existing systems in a seamless manner due to two common characteristics: unconventional programming models for in-memory computation units and lack of ability to utilize large on-chip caches. In this paper, we propose a new PIM architecture that (1) does not change the existing sequential programming models and (2) automatically decides whether to execute PIM operations in memory or processors depending on the locality of data. The key idea is to implement simple in-memory computation using compute-capable memory commands and use specialized instructions, which we call PIM-enabled instructions, to invoke in-memory computation. This allows PIM operations to be interoperable with existing programming models, cache coherence protocols, and virtual memory mechanisms with no modification. In addition, we introduce a simple hardware structure that monitors the locality of data accessed by a PIM-enabled instruction at runtime to adaptively execute the instruction at the host processor (instead of in memory) when the instruction can benefit from large on-chip caches. Consequently, our architecture provides the illusion that PIM operations are executed as if they were host processor instructions. We provide a case study of how ten emerging data-intensive workloads can benefit from our new PIM abstraction and its hardware implementation. Evaluations show that our architecture significantly improves system performance and, more importantly, combines the best parts of conventional and PIM architectures by adapting to data locality of applications.},
 acmid = {2750385},
 address = {New York, NY, USA},
 author = {Ahn, Junwhan and Yoo, Sungjoo and Mutlu, Onur and Choi, Kiyoung},
 booktitle = {Proceedings of the 42Nd Annual International Symposium on Computer Architecture},
 doi = {10.1145/2749469.2750385},
 isbn = {978-1-4503-3402-0},
 link = {http://doi.acm.org/10.1145/2749469.2750385},
 location = {Portland, Oregon},
 numpages = {13},
 pages = {336--348},
 publisher = {ACM},
 series = {ISCA '15},
 title = {PIM-enabled Instructions: A Low-overhead, Locality-aware Processing-in-memory Architecture},
 year = {2015}
}


@article{Nakaike:2015:QCH:2872887.2750403,
 abstract = {Transactional Memory (TM) is a new programming paradigm for both simple concurrent programming and high concurrent performance. Hardware Transactional Memory (HTM) is hardware support for TM-based programming. It has lower overhead than software transactional memory (STM), which is a software-based implementation of TM. There are now four commercial systems, IBM Blue Gene/Q, IBM zEnterprise EC12, Intel Core, and IBM POWER8, offering HTM. Our work is the first to compare the performance of these four HTM systems. We measured the STAMP benchmarks, the most widely used TM benchmarks. We also evaluated the specific features of each HTM system. Our experimental results show that: (1) there is no single HTM system that is more scalable than the others in all of the benchmarks, (2) there are measurable performance differences among the HTM systems in some benchmarks, and (3) each HTM system has its own implementation characteristics that limit its scalability.},
 acmid = {2750403},
 address = {New York, NY, USA},
 author = {Nakaike, Takuya and Odaira, Rei and Gaudet, Matthew and Michael, Maged M. and Tomari, Hisanobu},
 doi = {10.1145/2872887.2750403},
 issn = {0163-5964},
 issue_date = {June 2015},
 journal = {SIGARCH Comput. Archit. News},
 link = {http://doi.acm.org/10.1145/2872887.2750403},
 month = {jun},
 number = {3},
 numpages = {14},
 pages = {144--157},
 publisher = {ACM},
 title = {Quantitative Comparison of Hardware Transactional Memory for Blue Gene/Q, zEnterprise EC12, Intel Core, and POWER8},
 volume = {43},
 year = {2015}
}


@inproceedings{Li:2015:AAB:2749469.2750416,
 abstract = {Distributed in-memory key-value stores (KVSs), such as memcached, have become a critical data serving layer in modern Internet-oriented datacenter infrastructure. Their performance and efficiency directly affect the QoS of web services and the efficiency of datacenters. Traditionally, these systems have had significant overheads from inefficient network processing, OS kernel involvement, and concurrency control. Two recent research thrusts have focused upon improving key-value performance. Hardware-centric research has started to explore specialized platforms including FPGAs for KVSs; results demonstrated an order of magnitude increase in throughput and energy efficiency over stock memcached. Software-centric research revisited the KVS application to address fundamental software bottlenecks and to exploit the full potential of modern commodity hardware; these efforts too showed orders of magnitude improvement over stock memcached. We aim at architecting high performance and efficient KVS platforms, and start with a rigorous architectural characterization across system stacks over a collection of representative KVS implementations. Our detailed full-system characterization not only identifies the critical hardware/software ingredients for high-performance KVS systems, but also leads to guided optimizations atop a recent design to achieve a record-setting throughput of 120 million requests per second (MRPS) on a single commodity server. Our implementation delivers 9.2X the performance (RPS) and 2.8X the system energy efficiency (RPS/watt) of the best-published FPGA-based claims. We craft a set of design principles for future platform architectures, and via detailed simulations demonstrate the capability of achieving a billion RPS with a single server constructed following our principles.},
 acmid = {2750416},
 address = {New York, NY, USA},
 author = {Li, Sheng and Lim, Hyeontaek and Lee, Victor W. and Ahn, Jung Ho and Kalia, Anuj and Kaminsky, Michael and Andersen, David G. and Seongil, O. and Lee, Sukhan and Dubey, Pradeep},
 booktitle = {Proceedings of the 42Nd Annual International Symposium on Computer Architecture},
 doi = {10.1145/2749469.2750416},
 isbn = {978-1-4503-3402-0},
 link = {http://doi.acm.org/10.1145/2749469.2750416},
 location = {Portland, Oregon},
 numpages = {13},
 pages = {476--488},
 publisher = {ACM},
 series = {ISCA '15},
 title = {Architecting to Achieve a Billion Requests Per Second Throughput on a Single Key-value Store Server Platform},
 year = {2015}
}


@article{Liu:2015:DDS:2872887.2750414,
 abstract = {Spatial architectures are more efficient than traditional Out-of-Order (OOO) processors for computationally intensive programs. However, spatial architectures require mapping a program, either statically or dynamically, onto the spatial fabric. Static methods can generate efficient mappings, but they cannot adapt to changing workloads and are not compatible across hardware generations. Current dynamic methods are adaptive and compatible, but do not optimize as well due to their limited use of speculation and small mapping scopes. To overcome the limitations of existing dynamic mapping methods for spatial architectures, while minimizing the inefficiencies inherent in OOO superscalar processors, this paper presents DynaSpAM (Dynamic Spatial Architecture Mapping), a framework that tightly couples a spatial fabric with an OOO pipeline. DynaSpAM coaxes the OOO processor into producing an optimized mapping with a simple modification to the processor's scheduler. The insight behind DynaSpAM is that today's powerful OOO processors do for themselves most of the work necessary to produce a highly optimized mapping for a spatial architecture, including aggressively speculating control and memory dependences, and scheduling instructions using a large window. Evaluation of DynaSpAM shows a geomean speedup of 1.42x for 11 benchmarks from the Rodinia benchmark suite with a geomean 23.9% reduction in energy consumption compared to an 8-issue OOO pipeline.},
 acmid = {2750414},
 address = {New York, NY, USA},
 author = {Liu, Feng and Ahn, Heejin and Beard, Stephen R. and Oh, Taewook and August, David I.},
 doi = {10.1145/2872887.2750414},
 issn = {0163-5964},
 issue_date = {June 2015},
 journal = {SIGARCH Comput. Archit. News},
 link = {http://doi.acm.org/10.1145/2872887.2750414},
 month = {jun},
 number = {3},
 numpages = {13},
 pages = {541--553},
 publisher = {ACM},
 title = {DynaSpAM: Dynamic Spatial Architecture Mapping Using out of Order Instruction Schedules},
 volume = {43},
 year = {2015}
}


@article{Li:2015:TSI:2872887.2750381,
 abstract = {Recent years have seen an explosion of data volumes from a myriad of distributed sources such as ubiquitous cameras and various sensors. The challenges of analyzing these geographically dispersed datasets are increasing due to the significant data movement overhead, time-consuming data aggregation, and escalating energy needs. Rather than constantly move a tremendous amount of raw data to remote warehouse-scale computing systems for processing, it would be beneficial to leverage in-situ server systems (InS) to pre-process data, i.e., bringing computation to where the data is located. This paper takes the first step towards designing server clusters for data processing in the field. We investigate two representative in-situ computing applications, where data is normally generated from environmentally sensitive areas or remote places that lack established utility infrastructure. These very special operating environments of in-situ servers urge us to explore standalone (i.e., off-grid) systems that offer the opportunity to benefit from local, self-generated energy sources. In this work we implement a heavily instrumented proof-of-concept prototype called InSURE: in-situ server systems using renewable energy. We develop a novel energy buffering mechanism and a unique joint spatio-temporal power management strategy to coordinate standalone power supplies and in-situ servers. We present detailed deployment experiences to quantify how our design fits with in-situ processing in the real world. Overall, InSURE yields 20%~60% improvements over a state-of-the-art baseline. It maintains impressive control effectiveness in under-provisioned environment and can economically scale along with the data processing needs. The proposed design is well complementary to today's grid-connected cloud data centers and provides competitive cost-effectiveness.},
 acmid = {2750381},
 address = {New York, NY, USA},
 author = {Li, Chao and Hu, Yang and Liu, Longjun and Gu, Juncheng and Song, Mingcong and Liang, Xiaoyao and Yuan, Jingling and Li, Tao},
 doi = {10.1145/2872887.2750381},
 issn = {0163-5964},
 issue_date = {June 2015},
 journal = {SIGARCH Comput. Archit. News},
 link = {http://doi.acm.org/10.1145/2872887.2750381},
 month = {jun},
 number = {3},
 numpages = {13},
 pages = {14--26},
 publisher = {ACM},
 title = {Towards Sustainable In-situ Server Systems in the Big Data Era},
 volume = {43},
 year = {2015}
}


@inproceedings{Rahmati:2015:PCD:2749469.2750419,
 abstract = {Approximate computing research seeks to trade-off the accuracy of computation for increases in performance or reductions in power consumption. The observation driving approximate computing is that many applications tolerate small amounts of error which allows for an opportunistic relaxation of guard bands (e.g., clock rate and voltage). Besides affecting performance and power, reducing guard bands exposes analog properties of traditionally digital components. For DRAM, one analog property exposed by approximation is the variability of memory cell decay times. In this paper, we show how the differing cell decay times of approximate DRAM creates an error pattern that serves as a system identifying fingerprint. To validate this observation, we build an approximate memory platform and perform experiments that show that the fingerprint due to approximation is device dependent and resilient to changes in environment and level of approximation. To identify a DRAM chip given an approximate output, we develop a distance metric that yields a two-orders-of-magnitude difference in the distance between approximate results produced by the same DRAM chip and those produced by other DRAM chips. We use these results to create a mathematical model of approximate DRAM that we leverage to explore the end-to-end deanonymizing effects of approximate memory using a commodity system running an image manipulation program. The results from our experiment show that given less than 100 approximate outputs, the fingerprint for an approximate DRAM begins to converge to a single, machine identifying fingerprint.},
 acmid = {2750419},
 address = {New York, NY, USA},
 author = {Rahmati, Amir and Hicks, Matthew and Holcomb, Daniel E. and Fu, Kevin},
 booktitle = {Proceedings of the 42Nd Annual International Symposium on Computer Architecture},
 doi = {10.1145/2749469.2750419},
 isbn = {978-1-4503-3402-0},
 link = {http://doi.acm.org/10.1145/2749469.2750419},
 location = {Portland, Oregon},
 numpages = {12},
 pages = {604--615},
 publisher = {ACM},
 series = {ISCA '15},
 title = {Probable Cause: The Deanonymizing Effects of Approximate DRAM},
 year = {2015}
}


@inproceedings{Yu:2015:PDP:2749469.2750413,
 abstract = {Oblivious RAM (ORAM) is an established technique to hide the access pattern to an untrusted storage system. With ORAM, a curious adversary cannot tell what address the user is accessing when observing the bits moving between the user and the storage system. All existing ORAM schemes achieve obliviousness by adding redundancy to the storage system, i.e., each access is turned into multiple random accesses. Such redundancy incurs a large performance overhead. Although traditional data prefetching techniques successfully hide memory latency in DRAM based systems, it turns out that they do not work well for ORAM because ORAM does not have enough memory bandwidth available for issuing prefetch requests. In this paper, we exploit ORAM locality by taking advantage of the ORAM internal structures. While it might seem apparent that obliviousness and locality are two contradictory concepts, we challenge this intuition by exploiting data locality in ORAM without sacrificing security. In particular, we propose a dynamic ORAM prefetching technique called PrORAM (Dynamic Prefetcher for ORAM) and comprehensively explore its design space. PrORAM detects data locality in programs at runtime, and exploits the locality without leaking any information on the access pattern. Our simulation results show that with PrORAM, the performance of ORAM can be significantly improved. PrORAM achieves an average performance gain of 20% over the baseline ORAM for memory intensive benchmarks among Splash2 and 5.5% for SPEC06 workloads. The performance gain for YCSB and TPCC in DBMS benchmarks is 23.6% and 5% respectively. On average, PrORAM offers twice the performance gain than that offered by a static super block scheme.},
 acmid = {2750413},
 address = {New York, NY, USA},
 author = {Yu, Xiangyao and Haider, Syed Kamran and Ren, Ling and Fletcher, Christopher and Kwon, Albert and van Dijk, Marten and Devadas, Srinivas},
 booktitle = {Proceedings of the 42Nd Annual International Symposium on Computer Architecture},
 doi = {10.1145/2749469.2750413},
 isbn = {978-1-4503-3402-0},
 link = {http://doi.acm.org/10.1145/2749469.2750413},
 location = {Portland, Oregon},
 numpages = {13},
 pages = {616--628},
 publisher = {ACM},
 series = {ISCA '15},
 title = {PrORAM: Dynamic Prefetcher for Oblivious RAM},
 year = {2015}
}


@inproceedings{Alvarez:2015:CPT:2749469.2750411,
 abstract = {The increasing number of cores in manycore architectures causes important power and scalability problems in the memory subsystem. One solution is to introduce scratchpad memories alongside the cache hierarchy, forming a hybrid memory system. Scratchpad memories are more power-efficient than caches and they do not generate coherence traffic, but they suffer from poor programmability. A good way to hide the programmability difficulties to the programmer is to give the compiler the responsibility of generating code to manage the scratchpad memories. Unfortunately, compilers do not succeed in generating this code in the presence of random memory accesses with unknown aliasing hazards. This paper proposes a coherence protocol for the hybrid memory system that allows the compiler to always generate code to manage the scratchpad memories. In coordination with the compiler, memory accesses that may access stale copies of data are identified and diverted to the valid copy of the data. The proposal allows the architecture to be exposed to the programmer as a shared memory manycore, maintaining the programming simplicity of shared memory models and preserving backwards compatibility. In a 64-core manycore, the coherence protocol adds overheads of 4% in performance, 8% in network traffic and 9% in energy consumption to enable the usage of the hybrid memory system that, compared to a cache-based system, achieves a speedup of 1.14x and reduces on-chip network traffic and energy consumption by 29% and 17%, respectively.},
 acmid = {2750411},
 address = {New York, NY, USA},
 author = {Alvarez, Lluc and Vilanova, Llu\'{\i}s and Moreto, Miquel and Casas, Marc and Gonz\`{a}lez, Marc and Martorell, Xavier and Navarro, Nacho and Ayguad{\'e}, Eduard and Valero, Mateo},
 booktitle = {Proceedings of the 42Nd Annual International Symposium on Computer Architecture},
 doi = {10.1145/2749469.2750411},
 isbn = {978-1-4503-3402-0},
 link = {http://doi.acm.org/10.1145/2749469.2750411},
 location = {Portland, Oregon},
 numpages = {13},
 pages = {720--732},
 publisher = {ACM},
 series = {ISCA '15},
 title = {Coherence Protocol for Transparent Management of Scratchpad Memories in Shared Memory Manycore Architectures},
 year = {2015}
}


@article{Ros:2015:CES:2872887.2750405,
 abstract = {Cache coherence protocols based on self-invalidation allow a simpler design compared to traditional invalidation-based protocols, by relying on data-race-free (DRF) semantics and applying self-invalidation on racy synchronization points exposed to the hardware. Their simplicity lies in the absence of invalidation traffic, which eliminates the need to track readers in a directory, and reduces the number of transient protocol states. With the addition of self-downgrade these protocols can become effectively directory-free. While this works well for race-free data, unfortunately, lack of explicit invalidations compromises the effectiveness of any synchronization that relies on races. This includes any form of spin waiting, which is employed for signaling, locking, and barrier primitives. In this work we propose a new solution for spin-waiting in these protocols, the callback mechanism, that is simpler and more efficient than explicit invalidation. Callbacks are set by reads involved in spin waiting, and are satisfied by writes (that can even precede these reads). To implement callbacks we use a small (just a few entries) directory-cache structure that is intended to service only these "spin-waiting" races. This directory structure is self-contained and is not backed up in any way. Entries are created on demand and can be evicted without the need to preserve their information. Our evaluation shows a significant improvement both over explicit invalidation and over exponential back-off, the state-of-the-art mechanism for self-invalidation protocols to avoid spinning in the shared cache.},
 acmid = {2750405},
 address = {New York, NY, USA},
 author = {Ros, Alberto and Kaxiras, Stefanos},
 doi = {10.1145/2872887.2750405},
 issn = {0163-5964},
 issue_date = {June 2015},
 journal = {SIGARCH Comput. Archit. News},
 link = {http://doi.acm.org/10.1145/2872887.2750405},
 month = {jun},
 number = {3},
 numpages = {12},
 pages = {427--438},
 publisher = {ACM},
 title = {Callback: Efficient Synchronization Without Invalidation with a Directory Just for Spin-waiting},
 volume = {43},
 year = {2015}
}


@article{Nitin:2015:FVS:2872887.2750372,
 abstract = {Soft error susceptibility is a growing concern with continued CMOS scaling. Previous work explores full- and partial-redundancy schemes in hardware and software for soft-fault tolerance. However, full-redundancy schemes incur high performance and energy overheads whereas partial-redundancy schemes achieve low coverage. An initial study, called Perturbation Based Fault Screening (PBFS), explores exploiting value locality to provide hints of soft faults whenever a value falls outside its neighborhood. PBFS employs bit-mask filters to capture value neighborhoods. However, PBFS achieves low coverage; straightforwardly improving the coverage results in high false-positive rates, and performance and energy overheads. We propose FaultHound, a value-locality-based soft-fault tolerance scheme, which employs five mechanisms to address PBFS's limitations: (1) a scheme to cluster the filters via an inverted organization of the filter tables to reinforce learning and reduce the false-positive rates; (2) a learning scheme for ignoring the delinquent bit positions that raise repeated false alarms, to reduce further the false-positive rate; (3) a light-weight predecessor replay scheme instead of a full rollback to reduce the performance and energy penalty of the remaining false positives; (4) a simple scheme to distinguish rename faults, which require rollback instead of replay for recovery, from false positives to avoid unnecessary rollback penalty; and (5) a detection scheme, which avoids rollback, for the load-store queue which is not covered by our replay. Using simulations, we show that while PBFS achieves either low coverage (30%), or high false-positive rates (8%) with high performance overheads (97%), FaultHound achieves higher coverage (75%) and lower false-positive rates (3%) with lower performance and energy overheads (10% and 25%).},
 acmid = {2750372},
 address = {New York, NY, USA},
 author = {Nitin and Pomeranz, Irith and Vijaykumar, T. N.},
 doi = {10.1145/2872887.2750372},
 issn = {0163-5964},
 issue_date = {June 2015},
 journal = {SIGARCH Comput. Archit. News},
 link = {http://doi.acm.org/10.1145/2872887.2750372},
 month = {jun},
 number = {3},
 numpages = {14},
 pages = {668--681},
 publisher = {ACM},
 title = {FaultHound: Value-locality-based Soft-fault Tolerance},
 volume = {43},
 year = {2015}
}


@inproceedings{Liu:2015:HDM:2749469.2750384,
 abstract = {Today, an increasing number of applications and services are being hosted by large-scale data centers. The massive and irregular load surges challenge data center power infrastructures. As a result, power mismatching between supply and demand has emerged as a crucial issue in modern data centers which are either under-provisioned or powered by intermittent power sources. Recent proposals have employed energy storage devices such as the uninterruptible power supply (UPS) systems to address this issue. However, current approaches lack the capacity of efficiently handling the irregular and unpredictable power mismatches. In this paper, we propose Hybrid Energy Buffering (HEB), the first heterogeneous and adaptive strategy that incorporates super-capacitors (SCs) into existing data centers to dynamically deal with power mismatches. Our techniques exploit diverse energy absorbing characteristics and intelligent load assignment policies to provide efficiency- and scenario- aware power mismatch management. More attractively, our management schemes make the costly energy storage devices more affordable and economical for datacenter-scale usage. We evaluate the HEB design with a real system prototype. Compared with a homogenous battery energy buffering system, HEB could improve energy efficiency by 39.7%, extend UPS lifetime by 4.7X, reduce system downtime by 41% and improve renewable energy utilization by 81.2%. Our TCO analysis shows that HEB manifests high ROI and is able to gain more than 1.9X peak shaving benefit during an 8-years period. It allows datacenters to adapt to various power supply anomalies, thereby improving operational efficiency, resiliency and economy.},
 acmid = {2750384},
 address = {New York, NY, USA},
 author = {Liu, Longjun and Li, Chao and Sun, Hongbin and Hu, Yang and Gu, Juncheng and Li, Tao and Xin, Jingmin and Zheng, Nanning},
 booktitle = {Proceedings of the 42Nd Annual International Symposium on Computer Architecture},
 doi = {10.1145/2749469.2750384},
 isbn = {978-1-4503-3402-0},
 keyword = {datacenter, energy storage, power management},
 link = {http://doi.acm.org/10.1145/2749469.2750384},
 location = {Portland, Oregon},
 numpages = {13},
 pages = {463--475},
 publisher = {ACM},
 series = {ISCA '15},
 title = {HEB: Deploying and Managing Hybrid Energy Buffers for Improving Datacenter Efficiency and Economy},
 year = {2015}
}


@article{Chadha:2015:AAP:2872887.2750373,
 abstract = {Asynchronous or event-driven programming is now being used to develop a wide range of systems, including mobile and Web 2.0 applications, Internet-of-Things, and even distributed servers. We observe that these programs perform poorly on conventional processor architectures that are heavily optimized for the characteristics of synchronous programs. Execution characteristics of asynchronous programs significantly differ from synchronous programs as they interleave short events from varied tasks in a fine-grained manner. This paper proposes the Event Sneak Peek (ESP) architecture to mitigate microarchitectural bottlenecks in asynchronous programs. ESP exploits the fact that events are posted to an event queue before they get executed. By exposing this event queue to the processor, ESP gains knowledge of the future events. Instead of stalling on long latency cache misses, ESP jumps ahead to pre-execute future events and gathers useful information that later help initiate accurate instruction and data prefetches and correct branch mispredictions. We demonstrate that ESP improves the performance of popular asynchronous Web 2.0 applications including Amazon, Google maps, and Facebook, by an average of 16%.},
 acmid = {2750373},
 address = {New York, NY, USA},
 author = {Chadha, Gaurav and Mahlke, Scott and Narayanasamy, Satish},
 doi = {10.1145/2872887.2750373},
 issn = {0163-5964},
 issue_date = {June 2015},
 journal = {SIGARCH Comput. Archit. News},
 link = {http://doi.acm.org/10.1145/2872887.2750373},
 month = {jun},
 number = {3},
 numpages = {13},
 pages = {642--654},
 publisher = {ACM},
 title = {Accelerating Asynchronous Programs Through Event Sneak Peek},
 volume = {43},
 year = {2015}
}


