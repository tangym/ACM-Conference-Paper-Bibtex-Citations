@inproceedings{Herrero:2010:ECC:1815961.1816018,
 abstract = {Next generation tiled microarchitectures are going to be limited by off-chip misses and by on-chip network usage. Furthermore, these platforms will run an heterogeneous mix of applications with very different memory needs, leading to significant optimization opportunities. Existing adaptive memory hierarchies use either centralized structures that limit the scalability or software based resource allocation that increases programming complexity. We propose Elastic Cooperative Caching, a dynamic and scalable memory hierarchy that adapts automatically and autonomously to application behavior for each node. Our configuration uses elastic shared/private caches with fully autonomous and distributed repartitioning units for better scalability. Furthermore, we have extended our elastic configuration with an Adaptive Spilling mechanism to use the shared cache space only when it can produce a performance improvement. Elastic caches allow both the creation of big local private caches for threads with high reuse of private data and the creation of big shared spaces from unused caches. Local data allocation in private regions allows to reduce network usage and efficient cache partitioning allows to reduce off-chip misses. The proposed scheme outperforms previous proposals by a minimum of 12% (on average across the benchmarks) and reduces the number of offchip misses by 16%. Plus, the dynamic and autonomous management of cache resources avoids the reallocation of cache blocks without reuse which results in an increase in energy efficiency of 24%.},
 acmid = {1816018},
 address = {New York, NY, USA},
 author = {Herrero, Enric and Gonz\'{a}lez, Jos{\'e} and Canal, Ramon},
 booktitle = {Proceedings of the 37th Annual International Symposium on Computer Architecture},
 doi = {10.1145/1815961.1816018},
 isbn = {978-1-4503-0053-7},
 keyword = {chip multiprocessors, elastic cooperative caching, memory hierarchy, tiled microarchitectures},
 link = {http://doi.acm.org/10.1145/1815961.1816018},
 location = {Saint-Malo, France},
 numpages = {10},
 pages = {419--428},
 publisher = {ACM},
 series = {ISCA '10},
 title = {Elastic Cooperative Caching: An Autonomous Dynamically Adaptive Memory Hierarchy for Chip Multiprocessors},
 year = {2010}
}


@article{Soundararajan:2010:IMO:1816038.1816003,
 abstract = {Virtualization has the potential to dramatically reduce the total cost of ownership of datacenters and increase the flexibility of deployments for general-purpose workloads. If present trends continue, the datacenter of the future will be largely virtualized. The base platform in such a datacenter will consist of physical hosts that run hypervisors, and workloads will run within virtual machines on these platforms. From a system management perspective, the virtualized environment enables a number of new workflows in the datacenter. These workflows involve operations on the physical hosts themselves, such as upgrading the hypervisor, as well as operations on the virtual machines, such as reconfiguration or reverting from snapshots. While traditional datacenter design has focused on the cost vs. capability tradeoffs for the end-user applications running in the datacenter, we argue that the management workload from these workflows must be factored into the design of the virtualized datacenter. In this paper, we examine data from real-world virtualized deployments to characterize common management workflows and assess their impact on resource usage in the datacenter. We show that while many end-user applications are fairly light on I/O requirements, the management workload has considerable network and disk I/O requirements. We show that the management workload scales with the increasing compute power in the datacenter. Finally, we discuss the implications of this management workload for the datacenter.},
 acmid = {1816003},
 address = {New York, NY, USA},
 author = {Soundararajan, Vijayaraghavan and Anderson, Jennifer M.},
 doi = {10.1145/1816038.1816003},
 issn = {0163-5964},
 issue_date = {June 2010},
 journal = {SIGARCH Comput. Archit. News},
 keyword = {cloud computing, datacenter management, management workload, virtual machine management},
 link = {http://doi.acm.org/10.1145/1816038.1816003},
 month = {jun},
 number = {3},
 numpages = {12},
 pages = {326--337},
 publisher = {ACM},
 title = {The Impact of Management Operations on the Virtualized Datacenter},
 volume = {38},
 year = {2010}
}


@inproceedings{Wilkerson:2010:RCP:1815961.1815973,
 abstract = {Technology advancements have enabled the integration of large on-die embedded DRAM (eDRAM) caches. eDRAM is significantly denser than traditional SRAMs, but must be periodically refreshed to retain data. Like SRAM, eDRAM is susceptible to device variations, which play a role in determining refresh time for eDRAM cells. Refresh power potentially represents a large fraction of overall system power, particularly during low-power states when the CPU is idle. Future designs need to reduce cache power without incurring the high cost of flushing cache data when entering low-power states. In this paper, we show the significant impact of variations on refresh time and cache power consumption for large eDRAM caches. We propose Hi-ECC, a technique that incorporates multi-bit error-correcting codes to significantly reduce refresh rate. Multi-bit error-correcting codes usually have a complex decoder design and high storage cost. Hi-ECC avoids the decoder complexity by using strong ECC codes to identify and disable sections of the cache with multi-bit failures, while providing efficient single-bit error correction for the common case. Hi-ECC includes additional optimizations that allow us to amortize the storage cost of the code over large data words, providing the benefit of multi-bit correction at same storage cost as a single-bit error-correcting (SECDED) code (2% overhead). Our proposal achieves a 93% reduction in refresh power vs. a baseline eDRAM cache without error correcting capability, and a 66% reduction in refresh power vs. a system using SECDED codes.},
 acmid = {1815973},
 address = {New York, NY, USA},
 author = {Wilkerson, Chris and Alameldeen, Alaa R. and Chishti, Zeshan and Wu, Wei and Somasekhar, Dinesh and Lu, Shih-lien},
 booktitle = {Proceedings of the 37th Annual International Symposium on Computer Architecture},
 doi = {10.1145/1815961.1815973},
 isbn = {978-1-4503-0053-7},
 keyword = {dram, ecc, edram, idle power, idle states, multi-bit ecc, refresh power, vccmin},
 link = {http://doi.acm.org/10.1145/1815961.1815973},
 location = {Saint-Malo, France},
 numpages = {11},
 pages = {83--93},
 publisher = {ACM},
 series = {ISCA '10},
 title = {Reducing Cache Power with Low-cost, Multi-bit Error-correcting Codes},
 year = {2010}
}


@article{Tan:2010:CFF:1816038.1815999,
 abstract = {Given the multicore microprocessor revolution, we argue that the architecture research community needs a dramatic increase in simulation capacity. We believe FPGA Architecture Model Execution (FAME) simulators can increase the number of useful architecture research experiments per day by two orders of magnitude over Software Architecture Model Execution (SAME) simulators. To clear up misconceptions about FPGA-based simulation methodologies, we propose a FAME taxonomy to distinguish the costperformance of variations on these ideas. We demonstrate our simulation speedup claim with a case study wherein we employ a prototype FAME simulator, RAMP Gold, to research the interaction between hardware partitioning mechanisms and operating system scheduling policy. The study demonstrates FAME's capabilities: we run a modern parallel benchmark suite on a research operating system, simulate 64-core target architectures with multi-level memory hierarchy timing models, and add experimental hardware mechanisms to the target machine. The simulation speedup achieved by our adoption of FAME-250Ã—-enables experiments with more realistic time scales and data set sizes thanare possible with SAME.},
 acmid = {1815999},
 address = {New York, NY, USA},
 author = {Tan, Zhangxi and Waterman, Andrew and Cook, Henry and Bird, Sarah and Asanovi\'{c}, Krste and Patterson, David},
 doi = {10.1145/1816038.1815999},
 issn = {0163-5964},
 issue_date = {June 2010},
 journal = {SIGARCH Comput. Archit. News},
 keyword = {fpga, microprocessors, simulation},
 link = {http://doi.acm.org/10.1145/1816038.1815999},
 month = {jun},
 number = {3},
 numpages = {12},
 pages = {290--301},
 publisher = {ACM},
 title = {A Case for FAME: FPGA Architecture Model Execution},
 volume = {38},
 year = {2010}
}


@proceedings{Varbanescu:2010:2185870,
 abstract = {
                  An abstract is not available.
              },
 address = {Berlin, Heidelberg},
 editor = {Varbanescu, Ana Lucia and Molnos, Anca and Nieuwpoort, Rob},
 isbn = {978-3-642-24321-9},
 issn = {0302-9743},
 location = {Saint-Malo, France},
 publisher = {Springer-Verlag},
 title = {ISCA'10: Proceedings of the 2010 International Conference on Computer Architecture},
 year = {2012}
}


@article{Lee:2010:DGV:1816038.1816021,
 abstract = {Recent advances in computing have led to an explosion in the amount of data being generated. Processing the ever-growing data in a timely manner has made throughput computing an important aspect for emerging applications. Our analysis of a set of important throughput computing kernels shows that there is an ample amount of parallelism in these kernels which makes them suitable for today's multi-core CPUs and GPUs. In the past few years there have been many studies claiming GPUs deliver substantial speedups (between 10X and 1000X) over multi-core CPUs on these kernels. To understand where such large performance difference comes from, we perform a rigorous performance analysis and find that after applying optimizations appropriate for both CPUs and GPUs the performance gap between an Nvidia GTX280 processor and the Intel Core i7-960 processor narrows to only 2.5x on average. In this paper, we discuss optimization techniques for both CPU and GPU, analyze what architecture features contributed to performance differences between the two architectures, and recommend a set of architectural features which provide significant improvement in architectural efficiency for throughput kernels.},
 acmid = {1816021},
 address = {New York, NY, USA},
 author = {Lee, Victor W. and Kim, Changkyu and Chhugani, Jatin and Deisher, Michael and Kim, Daehyun and Nguyen, Anthony D. and Satish, Nadathur and Smelyanskiy, Mikhail and Chennupaty, Srinivas and Hammarlund, Per and Singhal, Ronak and Dubey, Pradeep},
 doi = {10.1145/1816038.1816021},
 issn = {0163-5964},
 issue_date = {June 2010},
 journal = {SIGARCH Comput. Archit. News},
 keyword = {cpu architecture, gpu architecture, performance analysis, performance measurement, software optimization, throughput computing},
 link = {http://doi.acm.org/10.1145/1816038.1816021},
 month = {jun},
 number = {3},
 numpages = {10},
 pages = {451--460},
 publisher = {ACM},
 title = {Debunking the 100X GPU vs. CPU Myth: An Evaluation of Throughput Computing on CPU and GPU},
 volume = {38},
 year = {2010}
}


@inproceedings{Kelm:2010:CHM:1815961.1816019,
 abstract = {Two broad classes of memory models are available today: models with hardware cache coherence, used in conventional chip multiprocessors, and models that rely upon software to manage coherence, found in compute accelerators. In some systems, both types of models are supported using disjoint address spaces and/or physical memories. In this paper we present Cohesion, a hybrid memory model that enables fine-grained temporal reassignment of data between hardware-managed and software-managed coherence domains, allowing a system to support both. Cohesion can be used to dynamically adapt to the sharing needs of both applications and runtimes. Cohesion requires neither copy operations nor multiple address spaces. Cohesion offers the benefits of reduced message traffic and on-die directory overhead when software-managed coherence can be used and the advantages of hardware coherence for cases in which software-managed coherence is impractical. We demonstrate our protocol using a hierarchical, cached 1024-core processor with a single address space that supports both software-enforced coherence and a directory-based hardware coherence protocol. Relative to an optimistic, hardware-coherent baseline, a realizable Cohesion design achieves competitive performance with a 2Ã— reduction in message traffic, 2.1Ã— reduction in directory utilization, and greater robustness to on-die directory capacity.},
 acmid = {1816019},
 address = {New York, NY, USA},
 author = {Kelm, John H. and Johnson, Daniel R. and Tuohy, William and Lumetta, Steven S. and Patel, Sanjay J.},
 booktitle = {Proceedings of the 37th Annual International Symposium on Computer Architecture},
 doi = {10.1145/1815961.1816019},
 isbn = {978-1-4503-0053-7},
 keyword = {accelerator, cache coherence, computer architecture},
 link = {http://doi.acm.org/10.1145/1815961.1816019},
 location = {Saint-Malo, France},
 numpages = {12},
 pages = {429--440},
 publisher = {ACM},
 series = {ISCA '10},
 title = {Cohesion: A Hybrid Memory Model for Accelerators},
 year = {2010}
}


@inproceedings{Irwin:2010:SCM:1815961.1815990,
 abstract = {As we transition from clock-frequency performance scaling to performance scaling with multicores, the pressure on the memory hierarchy is increasing dramatically. Many different on-chip cache topologies have been proposed/implemented; effective management of these shared caches is crucial to multicore performance. This talk will begin with a description of a cache miss classification scheme for multicores (compulsory, inter-core misses, intra-core misses) that gives insight into the interactions between memory transactions of the different cores on a chip sharing a cache. Ways to improve the on-chip cache performance with architectural enhancements, compiler enhancements, and runtime system enhancements will then be discussed. If the application thread mapping and the on-chip topology is static (i.e., does not change during runtime), then compiler enhancements that support cache topology aware code optimization can be used to significantly improve an application's performance. Results from such an augmented compiler, where the topology is exposed to the compiler and where the compiler also does thread-to-core mapping assignments, will be presented. If the application thread mapping or the on-chip topology is dynamic, then other alternatives exist. For example, a thread scheduler, or allocator, can make decisions about moving threads to different cores during runtime in the hopes of improving overall cache performance. Initial experiments with the REEact system being developed by researchers at Penn State-UPittsburgh-UVirginia that "reacts" to hardware conditions (such as cache miss rates, hot-spots, etc.) by reallocating threads at runtime will be outlined. Finally, if the on-chip cache topology itself is dynamic (i.e., is designed to be reconfigurable at runtime), large performance benefits might be obtained. However, both hardware and software design challenges to realizing such a dynamic system abound. Some of these challenges will be briefly discussed.},
 acmid = {1815990},
 address = {New York, NY, USA},
 author = {Irwin, Mary Jane},
 booktitle = {Proceedings of the 37th Annual International Symposium on Computer Architecture},
 doi = {10.1145/1815961.1815990},
 isbn = {978-1-4503-0053-7},
 keyword = {caches, multicore},
 link = {http://doi.acm.org/10.1145/1815961.1815990},
 location = {Saint-Malo, France},
 numpages = {1},
 pages = {234--234},
 publisher = {ACM},
 series = {ISCA '10},
 title = {Shared Caches in Multicores: The Good, the Bad, and the Ugly},
 year = {2010}
}


@article{Schechter:2010:UEE:1816038.1815980,
 abstract = {As leakage and other charge storage limitations begin to impair the scalability of DRAM, non-volatile resistive memories are being developed as a potential replacement. Unfortunately, current error correction techniques are poorly suited to this emerging class of memory technologies. Unlike DRAM, PCM and other resistive memories have wear lifetimes, measured in writes, that are sufficiently short to make cell failures common during a system's lifetime. However, resistive memories are much less susceptible to transient faults than DRAM. The Hamming-based ECC codes used in DRAM are designed to handle transient faults with no effective lifetime limits, but ECC codes applied to resistive memories would wear out faster than the cells they are designed to repair. This paper evaluates Error-Correcting Pointers (ECP), a new approach to error correction optimized for memories in which errors are the result of permanent cell failures that occur, and are immediately detectable, at write time. ECP corrects errors by permanently encoding the locations of failed cells into a table and assigning cells to replace them. ECP provides longer lifetimes than previously proposed solutions with equivalent overhead. What's more, as the level of variance in cell lifetimes increases -- a likely consequence of further scalaing -- ECP's margin of improvement over existing schemes increases.},
 acmid = {1815980},
 address = {New York, NY, USA},
 author = {Schechter, Stuart and Loh, Gabriel H. and Strauss, Karin and Burger, Doug},
 doi = {10.1145/1816038.1815980},
 issn = {0163-5964},
 issue_date = {June 2010},
 journal = {SIGARCH Comput. Archit. News},
 keyword = {error correction, hard failures, memory, phase change memory, resistive memories},
 link = {http://doi.acm.org/10.1145/1816038.1815980},
 month = {jun},
 number = {3},
 numpages = {12},
 pages = {141--152},
 publisher = {ACM},
 title = {Use ECP, Not ECC, for Hard Failures in Resistive Memories},
 volume = {38},
 year = {2010}
}


@article{Guo:2010:RCA:1816038.1816012,
 abstract = {As CMOS scales beyond the 45nm technology node, leakage concerns are starting to limit microprocessor performance growth. To keep dynamic power constant across process generations, traditional MOSFET scaling theory prescribes reducing supply and threshold voltages in proportion to device dimensions, a practice that induces an exponential increase in subthreshold leakage. As a result, leakage power has become comparable to dynamic power in current-generation processes, and will soon exceed it in magnitude if voltages are scaled down any further. Beyond this inflection point, multicore processors will not be able to afford keeping more than a small fraction of all cores active at any given moment. Multicore scaling will soon hit a power wall. This paper presents resistive computation, a new technique that aims at avoiding the power wall by migrating most of the functionality of a modern microprocessor from CMOS to spin-torque transfer magnetoresistive RAM (STT-MRAM)---a CMOS-compatible, leakage-resistant, non-volatile resistive memory technology. By implementing much of the on-chip storage and combinational logic using leakage-resistant, scalable RAM blocks and lookup tables, and by carefully re-architecting the pipeline, an STT-MRAM based implementation of an eight-core Sun Niagara-like CMT processor reduces chip-wide power dissipation by 1.7Ã— and leakage power by 2.1Ã— at the 32nm technology node, while maintaining 93% of the system throughput of a CMOS-based design.},
 acmid = {1816012},
 address = {New York, NY, USA},
 author = {Guo, Xiaochen and Ipek, Engin and Soyata, Tolga},
 doi = {10.1145/1816038.1816012},
 issn = {0163-5964},
 issue_date = {June 2010},
 journal = {SIGARCH Comput. Archit. News},
 keyword = {STT-MRAM, power-efficiency},
 link = {http://doi.acm.org/10.1145/1816038.1816012},
 month = {jun},
 number = {3},
 numpages = {12},
 pages = {371--382},
 publisher = {ACM},
 title = {Resistive Computation: Avoiding the Power Wall with Low-leakage, STT-MRAM Based Computing},
 volume = {38},
 year = {2010}
}


@article{Barr:2010:TCS:1816038.1815970,
 abstract = {This paper explores the design space of MMU caches that accelerate virtual-to-physical address translation in processor architectures, such as x86-64, that use a radix tree page table. In particular, these caches accelerate the page table walk that occurs after a miss in the Translation Lookaside Buffer. This paper shows that the most effective MMU caches are translation caches, which store partial translations and allow the page walk hardware to skip one or more levels of the page table. In recent years, both AMD and Intel processors have implemented MMU caches. However, their implementations are quite different and represent distinct points in the design space. This paper introduces three new MMU cache structures that round out the design space and directly compares the effectiveness of all five organizations. This comparison shows that two of the newly introduced structures, both of which are translation cache variants, are better than existing structures in many situations. Finally, this paper contributes to the age-old discourse concerning the relative effectiveness of different page table organizations. Generally speaking, earlier studies concluded that organizations based on hashing, such as the inverted page table, outperformed organizations based upon radix trees for supporting large virtual address spaces. However, these studies did not take into account the possibility of caching page table entries from the higher levels of the radix tree. This paper shows that any of the five MMU cache structures will reduce radix tree page table DRAM accesses far below an inverted page table.},
 acmid = {1815970},
 address = {New York, NY, USA},
 author = {Barr, Thomas W. and Cox, Alan L. and Rixner, Scott},
 doi = {10.1145/1816038.1815970},
 issn = {0163-5964},
 issue_date = {June 2010},
 journal = {SIGARCH Comput. Archit. News},
 keyword = {memory management, page walk caching, tlb},
 link = {http://doi.acm.org/10.1145/1816038.1815970},
 month = {jun},
 number = {3},
 numpages = {12},
 pages = {48--59},
 publisher = {ACM},
 title = {Translation Caching: Skip, Don'T Walk (the Page Table)},
 volume = {38},
 year = {2010}
}


@article{Beamer:2010:RDM:1816038.1815978,
 abstract = {The performance of future manycore processors will only scale with the number of integrated cores if there is a corresponding increase in memory bandwidth. Projected scaling of electrical DRAM architectures appears unlikely to suffice, being constrained by processor and DRAM pin-bandwidth density and by total DRAM chip power, including off-chip signaling, cross-chip interconnect, and bank access energy. In this work, we redesign the DRAM main memory system using a proposed monolithically integrated silicon photonics technology and show that our photonically interconnected DRAM (PIDRAM) provides a promising solution to all of these issues. Photonics can provide high aggregate pin-bandwidth density through dense wavelength-division multiplexing. Photonic signaling provides energy-efficient communication, which we exploit to not only reduce chip-to-chip interconnect power but to also reduce cross-chip interconnect power by extending the photonic links deep into the actual PIDRAM chips. To complement these large improvements in interconnect bandwidth and power, we decrease the number of bits activated per bank to improve the energy efficiency of the PIDRAM banks themselves. Our most promising design point yields approximately a 10x power reduction for a single-chip PIDRAM channel with similar throughput and area as a projected future electrical-only DRAM. Finally, we propose optical power guiding as a new technique that allows a single PIDRAM chip design to be used efficiently in several multi-chip configurations that provide either increased aggregate capacity or bandwidth.},
 acmid = {1815978},
 address = {New York, NY, USA},
 author = {Beamer, Scott and Sun, Chen and Kwon, Yong-Jin and Joshi, Ajay and Batten, Christopher and Stojanovi\'{c}, Vladimir and Asanovi\'{c}, Krste},
 doi = {10.1145/1816038.1815978},
 issn = {0163-5964},
 issue_date = {June 2010},
 journal = {SIGARCH Comput. Archit. News},
 keyword = {dram architecture, energy-efficiency, silicon photonics},
 link = {http://doi.acm.org/10.1145/1816038.1815978},
 month = {jun},
 number = {3},
 numpages = {12},
 pages = {129--140},
 publisher = {ACM},
 title = {Re-architecting DRAM Memory Systems with Monolithically Integrated Silicon Photonics},
 volume = {38},
 year = {2010}
}


@article{Seong:2010:SRP:1816038.1816014,
 abstract = {Phase change memory (PCM) is an emerging memory technology for future computing systems. Compared to other non-volatile memory alternatives, PCM is more matured to production, and has a faster read latency and potentially higher storage density. The main roadblock precluding PCM from being used, in particular, in the main memory hierarchy, is its limited write endurance. To address this issue, recent studies proposed to either reduce PCM's write frequency or use wear-leveling to evenly distribute writes. Although these techniques can extend the lifetime of PCM, most of them will not prevent deliberately designed malicious codes from wearing it out quickly. Furthermore, all the prior techniques did not consider the circumstances of a compromised OS and its security implication to the overall PCM design. A compromised OS will allow adversaries to manipulate processes and exploit side channels to accelerate wear-out. In this paper, we argue that a PCM design not only has to consider normal wear-out under normal application behavior, most importantly, it must take the worst-case scenario into account with the presence of malicious exploits and a compromised OS to address the durability and security issues simultaneously. In this paper, we propose a novel, low-cost hardware mechanism called Security Refresh to avoid information leak by constantly migrating their physical locations inside the PCM, obfuscating the actual data placement from users and system software. It uses a dynamic randomized address mapping scheme that swaps data using random keys upon each refresh due. The hardware overhead is tiny without using any table. The best lifetime we can achieve under the worst-case malicious attack is more than six years. Also, our scheme incurs around 1% performance degradation for normal program operations.},
 acmid = {1816014},
 address = {New York, NY, USA},
 author = {Seong, Nak Hee and Woo, Dong Hyuk and Lee, Hsien-Hsin S.},
 doi = {10.1145/1816038.1816014},
 issn = {0163-5964},
 issue_date = {June 2010},
 journal = {SIGARCH Comput. Archit. News},
 keyword = {dynamic address remapping, phase change memory, security, wear leveling},
 link = {http://doi.acm.org/10.1145/1816038.1816014},
 month = {jun},
 number = {3},
 numpages = {12},
 pages = {383--394},
 publisher = {ACM},
 title = {Security Refresh: Prevent Malicious Wear-out and Increase Durability for Phase-change Memory with Dynamically Randomized Address Mapping},
 volume = {38},
 year = {2010}
}


@article{Xue:2010:IFO:1816038.1815975,
 abstract = {Continued device scaling enables microprocessors and other systems-on-chip (SoCs) to increase their performance, functionality, and hence, complexity. Simultaneously, relentless scaling, if uncompensated, degrades the performance and signal integrity of on-chip metal interconnects. These systems have therefore become increasingly communications-limited. The communications-centric nature of future high performance computing devices demands a fundamental change in intra- and inter-chip interconnect technologies. Optical interconnect is a promising long term solution. However, while significant progress in optical signaling has been made in recent years, networking issues for on-chip optical interconnect still require much investigation. Taking the underlying optical signaling systems as a drop-in replacement for conventional electrical signaling while maintaining conventional packet-switching architectures is unlikely to realize the full potential of optical interconnects. In this paper, we propose and study the design of a fully distributed interconnect architecture based on free-space optics. The architecture leverages a suite of newly-developed or emerging devices, circuits, and optics technologies. The interconnect avoids packet relay altogether, offers an ultra-low transmission latency and scalable bandwidth, and provides fresh opportunities for coherency substrate designs and optimizations.},
 acmid = {1815975},
 address = {New York, NY, USA},
 author = {Xue, Jing and Garg, Alok and Ciftcioglu, Berkehan and Hu, Jianyun and Wang, Shang and Savidis, Ioannis and Jain, Manish and Berman, Rebecca and Liu, Peng and Huang, Michael and Wu, Hui and Friedman, Eby and Wicks, Gary and Moore, Duncan},
 doi = {10.1145/1816038.1815975},
 issn = {0163-5964},
 issue_date = {June 2010},
 journal = {SIGARCH Comput. Archit. News},
 keyword = {3d, free-space optical interconnect, intra-chip},
 link = {http://doi.acm.org/10.1145/1816038.1815975},
 month = {jun},
 number = {3},
 numpages = {12},
 pages = {94--105},
 publisher = {ACM},
 title = {An Intra-chip Free-space Optical Interconnect},
 volume = {38},
 year = {2010}
}


@article{Stuecheli:2010:VWQ:1816038.1815972,
 abstract = {In computer architecture, caches have primarily been viewed as a means to hide memory latency from the CPU. Cache policies have focused on anticipating the CPU's data needs, and are mostly oblivious to the main memory. In this paper, we demonstrate that the era of many-core architectures has created new main memory bottlenecks, and mandates a new approach: coordination of cache policy with main memory characteristics. Using the cache for memory optimization purposes, we propose a Virtual Write Queue which dramatically expands the memory controller's visibility of processor behavior, at low implementation overhead. Through memory-centric modification of existing policies, such as scheduled writebacks, this paper demonstrates that performance limiting effects of highly-threaded architectures can be overcome. We show that through awareness of the physical main memory layout and by focusing on writes, both read and write average latency can be shortened, memory power reduced, and overall system performance improved. Through full-system cycle-accurate simulations of SPEC cpu2006, we demonstrate that the proposed Virtual Write Queue achieves an average 10.9% system-level throughput improvement on memory-intensive workloads, along with an overall reduction of 8.7% in memory power across the whole suite.},
 acmid = {1815972},
 address = {New York, NY, USA},
 author = {Stuecheli, Jeffrey and Kaseridis, Dimitris and Daly, David and Hunter, Hillery C. and John, Lizy K.},
 doi = {10.1145/1816038.1815972},
 issn = {0163-5964},
 issue_date = {June 2010},
 journal = {SIGARCH Comput. Archit. News},
 keyword = {cache-replacement, cmp many-core, ddr ddr2 ddr3, dram, dram-parameters, last-level-cache, memory-scheduling writeback, page-mode, write-queue, write-scheduling},
 link = {http://doi.acm.org/10.1145/1816038.1815972},
 month = {jun},
 number = {3},
 numpages = {11},
 pages = {72--82},
 publisher = {ACM},
 title = {The Virtual Write Queue: Coordinating DRAM and Last-level Cache Policies},
 volume = {38},
 year = {2010}
}


@article{Jaleel:2010:HPC:1816038.1815971,
 abstract = {Practical cache replacement policies attempt to emulate optimal replacement by predicting the re-reference interval of a cache block. The commonly used LRU replacement policy always predicts a near-immediate re-reference interval on cache hits and misses. Applications that exhibit a distant re-reference interval perform badly under LRU. Such applications usually have a working-set larger than the cache or have frequent bursts of references to non-temporal data (called scans). To improve the performance of such workloads, this paper proposes cache replacement using Re-reference Interval Prediction (RRIP). We propose Static RRIP (SRRIP) that is scan-resistant and Dynamic RRIP (DRRIP) that is both scan-resistant and thrash-resistant. Both RRIP policies require only 2-bits per cache block and easily integrate into existing LRU approximations found in modern processors. Our evaluations using PC games, multimedia, server and SPEC CPU2006 workloads on a single-core processor with a 2MB last-level cache (LLC) show that both SRRIP and DRRIP outperform LRU replacement on the throughput metric by an average of 4% and 10% respectively. Our evaluations with over 1000 multi-programmed workloads on a 4-core CMP with an 8MB shared LLC show that SRRIP and DRRIP outperform LRU replacement on the throughput metric by an average of 7% and 9% respectively. We also show that RRIP outperforms LFU, the state-of the art scan-resistant replacement algorithm to-date. For the cache configurations under study, RRIP requires 2X less hardware than LRU and 2.5X less hardware than LFU.},
 acmid = {1815971},
 address = {New York, NY, USA},
 author = {Jaleel, Aamer and Theobald, Kevin B. and Steely,Jr., Simon C. and Emer, Joel},
 doi = {10.1145/1816038.1815971},
 issn = {0163-5964},
 issue_date = {June 2010},
 journal = {SIGARCH Comput. Archit. News},
 keyword = {replacement, scan resistance, shared cache, thrashing},
 link = {http://doi.acm.org/10.1145/1816038.1815971},
 month = {jun},
 number = {3},
 numpages = {12},
 pages = {60--71},
 publisher = {ACM},
 title = {High Performance Cache Replacement Using Re-reference Interval Prediction (RRIP)},
 volume = {38},
 year = {2010}
}


@article{Chakradhar:2010:DCC:1816038.1815993,
 abstract = {Convolutional neural networks (CNN) applications range from recognition and reasoning (such as handwriting recognition, facial expression recognition and video surveillance) to intelligent text applications such as semantic text analysis and natural language processing applications. Two key observations drive the design of a new architecture for CNN. First, CNN workloads exhibit a widely varying mix of three types of parallelism: parallelism within a convolution operation, intra-output parallelism where multiple input sources (features) are combined to create a single output, and inter-output parallelism where multiple, independent outputs (features) are computed simultaneously. Workloads differ significantly across different CNN applications, and across different layers of a CNN. Second, the number of processing elements in an architecture continues to scale (as per Moore's law) much faster than off-chip memory bandwidth (or pin-count) of chips. Based on these two observations, we show that for a given number of processing elements and off-chip memory bandwidth, a new CNN hardware architecture that dynamically configures the hardware on-the-fly to match the specific mix of parallelism in a given workload gives the best throughput performance. Our CNN compiler automatically translates high abstraction network specification into a parallel microprogram (a sequence of low-level VLIW instructions) that is mapped, scheduled and executed by the coprocessor. Compared to a 2.3 GHz quad-core, dual socket Intel Xeon, 1.35 GHz C870 GPU, and a 200 MHz FPGA implementation, our 120 MHz dynamically configurable architecture is 4x to 8x faster. This is the first CNN architecture to achieve real-time video stream processing (25 to 30 frames per second) on a wide range of object detection and recognition tasks.},
 acmid = {1815993},
 address = {New York, NY, USA},
 author = {Chakradhar, Srimat and Sankaradas, Murugan and Jakkula, Venkata and Cadambi, Srihari},
 doi = {10.1145/1816038.1815993},
 issn = {0163-5964},
 issue_date = {June 2010},
 journal = {SIGARCH Comput. Archit. News},
 keyword = {convolutional neural networks, dynamic reconfiguration, parallel computer architecture},
 link = {http://doi.acm.org/10.1145/1816038.1815993},
 month = {jun},
 number = {3},
 numpages = {11},
 pages = {247--257},
 publisher = {ACM},
 title = {A Dynamically Configurable Coprocessor for Convolutional Neural Networks},
 volume = {38},
 year = {2010}
}


@article{Hong:2010:IGP:1816038.1815998,
 abstract = {GPU architectures are increasingly important in the multi-core era due to their high number of parallel processors. Performance optimization for multi-core processors has been a challenge for programmers. Furthermore, optimizing for power consumption is even more difficult. Unfortunately, as a result of the high number of processors, the power consumption of many-core processors such as GPUs has increased significantly. Hence, in this paper, we propose an integrated power and performance (IPP) prediction model for a GPU architecture to predict the optimal number of active processors for a given application. The basic intuition is that when an application reaches the peak memory bandwidth, using more cores does not result in performance improvement. We develop an empirical power model for the GPU. Unlike most previous models, which require measured execution times, hardware performance counters, or architectural simulations, IPP predicts execution times to calculate dynamic power events. We then use the outcome of IPP to control the number of running cores. We also model the increases in power consumption that resulted from the increases in temperature. With the predicted optimal number of active cores, we show that we can save up to 22.09%of runtime GPU energy consumption and on average 10.99% of that for the five memory bandwidth-limited benchmarks.},
 acmid = {1815998},
 address = {New York, NY, USA},
 author = {Hong, Sunpyo and Kim, Hyesoon},
 doi = {10.1145/1816038.1815998},
 issn = {0163-5964},
 issue_date = {June 2010},
 journal = {SIGARCH Comput. Archit. News},
 keyword = {CUDA, GPU architecture, analytical model, energy, performance, power estimation},
 link = {http://doi.acm.org/10.1145/1816038.1815998},
 month = {jun},
 number = {3},
 numpages = {10},
 pages = {280--289},
 publisher = {ACM},
 title = {An Integrated GPU Power and Performance Model},
 volume = {38},
 year = {2010}
}


@inproceedings{Shriraman:2010:SLA:1815961.1816016,
 abstract = {Light-weight, flexible access control, which allows software to regulate reads and writes to any granularity of memory region, can help improve the reliability of today's multi-module multi-programmer applications, as well as the efficiency of software debugging tools. Unfortunately, access control in today's processors is tied to support for virtual memory, making its use both heavy weight and coarse grain. In this paper, we propose Sentry, an auxiliary level of virtual memory tagging that is entirely subordinate to existing virtual memory-based protection mechanisms and can be manipulated at the user level. We implement these tags in a complexity-effective manner using an M-cache (metadata cache) structure that only intervenes on L1 misses, thereby minimizing changes to the processor core. Existing cache coherence states are repurposed to implicitly validate permissions for L1 hits. Sentry achieves its goal of flexible and light-weight access control without disrupting existing inter-application protection, sidestepping the challenges associated with adding a new protection framework to an existing operating system. We illustrate the benefits of our design point using 1) an Apache-based web server that uses the M-cache to enforce protection boundaries among its modules and 2) a watchpoint-based tool to demonstrate low-overhead debugging. Protection is achieved with very few changes to the source code, no changes to the programming model, minimal modifications to the operating system, and with low overhead incurred only when accessing memory regions for which the additional level of access control is enabled.},
 acmid = {1816016},
 address = {New York, NY, USA},
 author = {Shriraman, Arrvindh and Dwarkadas, Sandhya},
 booktitle = {Proceedings of the 37th Annual International Symposium on Computer Architecture},
 doi = {10.1145/1815961.1816016},
 isbn = {978-1-4503-0053-7},
 keyword = {access control, cache coherence, memory protection, multiprocessors, protection domains, safety, sentry},
 link = {http://doi.acm.org/10.1145/1815961.1816016},
 location = {Saint-Malo, France},
 numpages = {12},
 pages = {407--418},
 publisher = {ACM},
 series = {ISCA '10},
 title = {Sentry: Light-weight Auxiliary Memory Access Control},
 year = {2010}
}


@article{JanapaReddi:2010:WSU:1816038.1816002,
 abstract = {The commoditization of hardware, data center economies of scale, and Internet-scale workload growth all demand greater power efficiency to sustain scalability. Traditional enterprise workloads, which are typically memory and I/O bound, have been well served by chip multiprocessors com- prising of small, power-efficient cores. Recent advances in mobile computing have led to modern small cores capable of delivering even better power efficiency. While these cores can deliver performance-per-Watt efficiency for data center workloads, small cores impact application quality-of-service robustness, and flexibility, as these workloads increasingly invoke computationally intensive kernels. These challenges constitute the price of efficiency. We quantify efficiency for an industry-strength online web search engine in production at both the microarchitecture- and system-level, evaluating search on server and mobile-class architectures using Xeon and Atom processors.},
 acmid = {1816002},
 address = {New York, NY, USA},
 author = {Janapa Reddi, Vijay and Lee, Benjamin C. and Chilimbi, Trishul and Vaid, Kushagra},
 doi = {10.1145/1816038.1816002},
 issn = {0163-5964},
 issue_date = {June 2010},
 journal = {SIGARCH Comput. Archit. News},
 keyword = {bing, energy efficiency, mobile cores, web search},
 link = {http://doi.acm.org/10.1145/1816038.1816002},
 month = {jun},
 number = {3},
 numpages = {12},
 pages = {314--325},
 publisher = {ACM},
 title = {Web Search Using Mobile Cores: Quantifying and Mitigating the Price of Efficiency},
 volume = {38},
 year = {2010}
}


@inproceedings{Chen:2010:LPP:1815961.1815985,
 abstract = {Debugging parallel program is a well-known difficult problem. A promising method to facilitate debugging parallel program is using hardware support to achieve deterministic replay. A hardware-assisted deterministic replay scheme should have a small log size, as well as low design cost, to be feasible for adopting by industrial processors. To achieve the goals, we propose a novel and succinct hardware-assisted deterministic replay scheme named LReplay. The key innovation of LReplay is that instead of recording the logical time orders between instructions or instruction blocks as previous investigations, LReplay is built upon recording the pending period information [6]. According to the experimental results on Godson-3, the overall log size of LReplay is about 0.55B/K-Inst (byte per k-instruction) for sequential consistency, and 0.85B/K-Inst for Godson-3 consistency. The log size is smaller in an order of magnitude than state-of-art deterministic replay schemes incuring no performance loss. Furthermore, LReplay only consumes about $1.3%$ area of Godson-3, since it requires only trivial modifications to the existing components of Godson-3. The above features of LReplay demonstrate the potential of integrating hardware-assisted deterministic replay into future industrial processors.},
 acmid = {1815985},
 address = {New York, NY, USA},
 author = {Chen, Yunji and Hu, Weiwu and Chen, Tianshi and Wu, Ruiyang},
 booktitle = {Proceedings of the 37th Annual International Symposium on Computer Architecture},
 doi = {10.1145/1815961.1815985},
 isbn = {978-1-4503-0053-7},
 keyword = {deterministic replay, dfd, global clock, multi-core processor, pending period, physical time order},
 link = {http://doi.acm.org/10.1145/1815961.1815985},
 location = {Saint-Malo, France},
 numpages = {11},
 pages = {187--197},
 publisher = {ACM},
 series = {ISCA '10},
 title = {LReplay: A Pending Period Based Deterministic Replay Scheme},
 year = {2010}
}


@inproceedings{Lucia:2010:CES:1815961.1815987,
 abstract = {We argue in this paper that concurrency errors should be treated as exceptions, i.e., have fail-stop behavior and precise semantics. We propose an exception model based on conflict of synchronization free regions, which precisely detects a broad class of data-races. We show that our exceptions provide enough guarantees to simplify high-level programming language semantics and debugging, but are significantly cheaper to enforce than traditional data-race detection. To make the performance cost of enforcement negligible, we propose architecture support for accurately detecting and precisely delivering these exceptions. We evaluate the suitability of our model as well as the behavior of our architectural mechanisms using the PARSEC benchmark suite and commercial applications. Our results show that the exception model largely reflects how programmers are already writing code and that the main memory, traffic and performance overheads of the enforcement mechanisms we propose are very low.},
 acmid = {1815987},
 address = {New York, NY, USA},
 author = {Lucia, Brandon and Ceze, Luis and Strauss, Karin and Qadeer, Shaz and Boehm, Hans-J.},
 booktitle = {Proceedings of the 37th Annual International Symposium on Computer Architecture},
 doi = {10.1145/1815961.1815987},
 isbn = {978-1-4503-0053-7},
 keyword = {bug detection, data-races, memory consistency models, multicores, threads},
 link = {http://doi.acm.org/10.1145/1815961.1815987},
 location = {Saint-Malo, France},
 numpages = {12},
 pages = {210--221},
 publisher = {ACM},
 series = {ISCA '10},
 title = {Conflict Exceptions: Simplifying Concurrent Language Semantics with Precise Hardware Exceptions for Data-races},
 year = {2010}
}


@inproceedings{Huang:2010:IOM:1815961.1816015,
 abstract = {This paper proposes a unified off-chip memory integrity protection scheme, named IVEC. Today, a system needs two independent mechanisms in order to protect the memory integrity from both physical attacks and random errors. Integrity verification schemes detect malicious tampering of memory while error correcting codes (ECC) detect and correct random errors. IVEC enables both detection of malicious attacks for security and correction of random errors for reliability at the same time by extending the integrity verification techniques. Analytical and experimental studies show that IVEC can correct single-bit errors and even multi-bit errors from one DRAM chip within a cache block read without any additional ECC bits, when the integrity verification is also required for security, effectively removing the memory and bandwidth overheads (12.5%) of typical ECC schemes. Alternatively, with parity bits, IVEC can provide even stronger error correction capabilities comparable to the traditional chip-kill correct, still with less overheads. For both cases, IVEC can use standard non-ECC DIMMs.},
 acmid = {1816015},
 address = {New York, NY, USA},
 author = {Huang, Ruirui and Suh, G. Edward},
 booktitle = {Proceedings of the 37th Annual International Symposium on Computer Architecture},
 doi = {10.1145/1815961.1816015},
 isbn = {978-1-4503-0053-7},
 keyword = {error correction, error detection, fault tolerance, memory systems, reliability, security},
 link = {http://doi.acm.org/10.1145/1815961.1816015},
 location = {Saint-Malo, France},
 numpages = {12},
 pages = {395--406},
 publisher = {ACM},
 series = {ISCA '10},
 title = {IVEC: Off-chip Memory Integrity Protection for Both Security and Reliability},
 year = {2010}
}


@article{Keller:2010:NVC:1816038.1816010,
 abstract = {Cloud computing is a disruptive trend that is changing the way we use computers. The key underlying technology in cloud infrastructures is virtualization -- so much so that many consider virtualization to be one of the key features rather than simply an implementation detail. Unfortunately, the use of virtualization is the source of a significant security concern. Because multiple virtual machines run on the same server and since the virtualization layer plays a considerable role in the operation of a virtual machine, a malicious party has the opportunity to attack the virtualization layer. A successful attack would give the malicious party control over the all-powerful virtualization layer, potentially compromising the confidentiality and integrity of the software and data of any virtual machine. In this paper we propose removing the virtualization layer, while retaining the key features enabled by virtualization. Our NoHype architecture, named to indicate the removal of the hypervisor, addresses each of the key roles of the virtualization layer: arbitrating access to CPU, memory, and I/O devices, acting as a network device (e.g., Ethernet switch), and managing the starting and stopping of guest virtual machines. Additionally, we show that our NoHype architecture may indeed be "no hype" since nearly all of the needed features to realize the NoHype architecture are currently available as hardware extensions to processors and I/O devices.},
 acmid = {1816010},
 address = {New York, NY, USA},
 author = {Keller, Eric and Szefer, Jakub and Rexford, Jennifer and Lee, Ruby B.},
 doi = {10.1145/1816038.1816010},
 issn = {0163-5964},
 issue_date = {June 2010},
 journal = {SIGARCH Comput. Archit. News},
 keyword = {cloud computing, hypervisor, many-core, multi-core, security, system architecture, virtualization},
 link = {http://doi.acm.org/10.1145/1816038.1816010},
 month = {jun},
 number = {3},
 numpages = {12},
 pages = {350--361},
 publisher = {ACM},
 title = {NoHype: Virtualized Cloud Infrastructure Without the Virtualization},
 volume = {38},
 year = {2010}
}


@article{Ansari:2010:NES:1816038.1816024,
 abstract = {Aggressive technology scaling into the nanometer regime has led to a host of reliability challenges in the last several years. Unlike on-chip caches, which can be efficiently protected using conventional schemes, the general core area is less homogeneous and structured, making tolerating defects a much more challenging problem. Due to the lack of effective solutions, disabling non-functional cores is a common practice in industry to enhance manufacturing yield, which results in a significant reduction in system throughput. Although a faulty core cannot be trusted to correctly execute programs, we observe in this work that for most defects, when starting from a valid architectural state, execution traces on a defective core actually coarsely resemble those of fault-free executions. In light of this insight, we propose a robust and heterogeneous core coupling execution scheme, Necromancer, that exploits a functionally dead core to improve system throughput by supplying hints regarding high-level program behavior. We partition the cores in a conventional CMP system into multiple groups in which each group shares a lightweight core that can be substantially accelerated using these execution hints from a potentially dead core. To prevent this undead core from wandering too far from the correct path of execution, we dynamically resynchronize architectural state with the lightweight core. For a 4-core CMP system, on average, our approach enables the coupled core to achieve 78.5% of the performance of a fully functioning core. This defect tolerance and throughput enhancement comes at modest area and power overheads of 5.3% and 8.5%, respectively.},
 acmid = {1816024},
 address = {New York, NY, USA},
 author = {Ansari, Amin and Feng, Shuguang and Gupta, Shantanu and Mahlke, Scott},
 doi = {10.1145/1816038.1816024},
 issn = {0163-5964},
 issue_date = {June 2010},
 journal = {SIGARCH Comput. Archit. News},
 keyword = {execution abstraction, heterogeneous core coupling, manufacturing defects},
 link = {http://doi.acm.org/10.1145/1816038.1816024},
 month = {jun},
 number = {3},
 numpages = {12},
 pages = {473--484},
 publisher = {ACM},
 title = {Necromancer: Enhancing System Throughput by Animating Dead Cores},
 volume = {38},
 year = {2010}
}


@article{Qureshi:2010:MMS:1816038.1815981,
 abstract = {Phase Change Memory (PCM) is emerging as a scalable and power efficient technology to architect future main memory systems. The scalability of PCM is enhanced by the property that PCM devices can store multiple bits per cell. While such Multi-Level Cell (MLC) devices can offer high density, this benefit comes at the expense of increased read latency, which can cause significant performance degradation. This paper proposes Morphable Memory System (MMS), a robust architecture for efficiently incorporating MLC PCM devices in main memory. MMS is based on observation that memory requirement varies between workloads, and systems are typically over-provisioned in terms of memory capacity. So, during a phase of low memory usage, some of the MLC devices can be operated at fewer bits per cell to obtain lower latency. When the workload requires full memory capacity, these devices can be restored to high density MLC operation to have full main-memory capacity. We provide the runtime monitors, the hardware-OS interface, and the detailed mechanism for implementing MMS. Our evaluations on an 8-core 8GB MLC PCM-based system show that MMS provides, on average, low latency access for 95% of all memory requests, thereby improving overall system performance by 40%.},
 acmid = {1815981},
 address = {New York, NY, USA},
 author = {Qureshi, Moinuddin K. and Franceschini, Michele M. and Lastras-Monta\~{n}o, Luis A. and Karidis, John P.},
 doi = {10.1145/1816038.1815981},
 issn = {0163-5964},
 issue_date = {June 2010},
 journal = {SIGARCH Comput. Archit. News},
 keyword = {morphable memory, multi-level cell, phase change memory},
 link = {http://doi.acm.org/10.1145/1816038.1815981},
 month = {jun},
 number = {3},
 numpages = {10},
 pages = {153--162},
 publisher = {ACM},
 title = {Morphable Memory System: A Robust Architecture for Exploiting Multi-level Phase Change Memories},
 volume = {38},
 year = {2010}
}


@article{Abts:2010:EPD:1816038.1816004,
 abstract = {Numerous studies have shown that datacenter computers rarely operate at full utilization, leading to a number of proposals for creating servers that are energy proportional with respect to the computation that they are performing. In this paper, we show that as servers themselves become more energy proportional, the datacenter network can become a significant fraction (up to 50%) of cluster power. In this paper we propose several ways to design a high-performance datacenter network whose power consumption is more proportional to the amount of traffic it is moving -- that is, we propose energy proportional datacenter networks. We first show that a flattened butterfly topology itself is inherently more power efficient than the other commonly proposed topology for high-performance datacenter networks. We then exploit the characteristics of modern plesiochronous links to adjust their power and performance envelopes dynamically. Using a network simulator, driven by both synthetic workloads and production datacenter traces, we characterize and understand design tradeoffs, and demonstrate an 85% reduction in power --- which approaches the ideal energy-proportionality of the network. Our results also demonstrate two challenges for the designers of future network switches: 1) We show that there is a significant power advantage to having independent control of each unidirectional channel comprising a network link, since many traffic patterns show very asymmetric use, and 2) system designers should work to optimize the high-speed channel designs to be more energy efficient by choosing optimal data rate and equalization technology. Given these assumptions, we demonstrate that energy proportional datacenter communication is indeed possible.},
 acmid = {1816004},
 address = {New York, NY, USA},
 author = {Abts, Dennis and Marty, Michael R. and Wells, Philip M. and Klausler, Peter and Liu, Hong},
 doi = {10.1145/1816038.1816004},
 issn = {0163-5964},
 issue_date = {June 2010},
 journal = {SIGARCH Comput. Archit. News},
 keyword = {datacenter networks, interconnection networks, low-power networking},
 link = {http://doi.acm.org/10.1145/1816038.1816004},
 month = {jun},
 number = {3},
 numpages = {10},
 pages = {338--347},
 publisher = {ACM},
 title = {Energy Proportional Datacenter Networks},
 volume = {38},
 year = {2010}
}


@article{Thacker:2010:IFE:1816038.1816006,
 abstract = {During the last fifty years, the technology underlying computer systems has improved dramatically. As technology has evolved, designers have made a series of choices in the way it was applied in computers. In some cases, decisions that were made in the twentieth century make less sense in the twenty-first. Conversely, paths not taken might now be more attractive given the state of technology today, particularly in light of the limits the field is facing, such as the increasing gap between processor speed and storage access times and the difficulty of cooling today's computers. In this talk, I'll discuss some of these choices and suggest some possible changes that might make computing better in the twenty-first century.},
 acmid = {1816006},
 address = {New York, NY, USA},
 author = {Thacker, Charles P.},
 doi = {10.1145/1816038.1816006},
 issn = {0163-5964},
 issue_date = {June 2010},
 journal = {SIGARCH Comput. Archit. News},
 keyword = {turing award},
 link = {http://doi.acm.org/10.1145/1816038.1816006},
 month = {jun},
 number = {3},
 numpages = {1},
 pages = {348--348},
 publisher = {ACM},
 title = {Improving the Future by Examining the Past: ACM Turing Award Lecture},
 volume = {38},
 year = {2010}
}


@article{Das:2010:AEP:1816038.1815976,
 abstract = {Traditional Network-on-Chips (NoCs) employ simple arbitration strategies, such as round-robin or oldest-first, to decide which packets should be prioritized in the network. This is counter-intuitive since different packets can have very different effects on system performance due to, e.g., different level of memory-level parallelism (MLP) of applications. Certain packets may be performance-critical because they cause the processor to stall, whereas others may be delayed for a number of cycles with no effect on application-level performance as their latencies are hidden by other outstanding packets'latencies. In this paper, we define slack as a key measure that characterizes the relative importance of a packet. Specifically, the slack of a packet is the number of cycles the packet can be delayed in the network with no effect on execution time. This paper proposes new router prioritization policies that exploit the available slack of interfering packets in order to accelerate performance-critical packets and thus improve overall system performance. When two packets interfere with each other in a router, the packet with the lower slack value is prioritized. We describe mechanisms to estimate slack, prevent starvation, and combine slack-based prioritization with other recently proposed application-aware prioritization mechanisms. We evaluate slack-based prioritization policies on a 64-core CMP with an 8x8 mesh NoC using a suite of 35 diverse applications. For a representative set of case studies, our proposed policy increases average system throughput by 21.0% over the commonlyused round-robin policy. Averaged over 56 randomly-generated multiprogrammed workload mixes, the proposed policy improves system throughput by 10.3%, while also reducing application-level unfairness by 30.8%.},
 acmid = {1815976},
 address = {New York, NY, USA},
 author = {Das, Reetuparna and Mutlu, Onur and Moscibroda, Thomas and Das, Chita R.},
 doi = {10.1145/1816038.1815976},
 issn = {0163-5964},
 issue_date = {June 2010},
 journal = {SIGARCH Comput. Archit. News},
 keyword = {arbitration, memory systems, multi-core, on-chip networks, packet scheduling, prioritization},
 link = {http://doi.acm.org/10.1145/1816038.1815976},
 month = {jun},
 number = {3},
 numpages = {11},
 pages = {106--116},
 publisher = {ACM},
 title = {A{\'e}Rgia: Exploiting Packet Latency Slack in On-chip Networks},
 volume = {38},
 year = {2010}
}


@article{Watanabe:2010:WWD:1816038.1815965,
 abstract = {The recent paradigm shift to multi-core systems results in high system throughput within a specified power budget. However, future systems still require good single thread performance--no longer the predominant design priority--to mitigate sequential bottlenecks and/or to guarantee service-level agreements. Unfortunately, near saturation in voltage scaling necessitates a long-term alternative to dynamic voltage and frequency scaling. We propose an energy-proportional computing infrastructure, called WiDGET, that decouples thread context management from a sea of simple execution units (EUs). WiDGET's decoupled design provides flexibility to alter resource allocation for a particular power-performance target while turning off unallocated resources. In other words, WiDGET enables dynamic customization of different combinations of small and/or powerful cores on a single chip, consuming power in proportion to the delivered performance. Over all SPEC CPU2006 benchmarks, WiDGET provides average per-thread performance that is 26% better than a Xeon-like processor while using 8% less power. WiDGET can also scale down to a level comparable to an Atom-like processor, turning off resources to reduce average power by 58%. WiDGET achieves high power efficiency (BIPS3/W), exceeding Xeon-like and Atom-like processors by up to 2x and 21x, respectively.},
 acmid = {1815965},
 address = {New York, NY, USA},
 author = {Watanabe, Yasuko and Davis, John D. and Wood, David A.},
 doi = {10.1145/1816038.1815965},
 issn = {0163-5964},
 issue_date = {June 2010},
 journal = {SIGARCH Comput. Archit. News},
 keyword = {hardware, instruction steering, performance, power efficiency, power proportional computing},
 link = {http://doi.acm.org/10.1145/1816038.1815965},
 month = {jun},
 number = {3},
 numpages = {12},
 pages = {2--13},
 publisher = {ACM},
 title = {WiDGET: Wisconsin Decoupled Grid Execution Tiles},
 volume = {38},
 year = {2010}
}


@article{Lee:2010:TTD:1816038.1815996,
 abstract = {Extracting performance from modern parallel architectures requires that applications be divided into many different threads of execution. Unfortunately selecting the appropriate number of threads for an application is a daunting task. Having too many threads can quickly saturate shared resources, such as cache capacity or memory bandwidth, thus degrading performance. On the other hand, having too few threads makes inefficient use of the resources available. Beyond static resource assignment, the program inputs and dynamic system state (e.g., what other applications are executing in the system) can have a significant impact on the right number of threads to use for a particular application. To address this problem we present the Thread Tailor, a dynamic system that automatically adjusts the number of threads in an application to optimize system efficiency. The Thread Tailor leverages offline analysis to estimate what type of threads will exist at runtime and the communication patterns between them. Using this information Thread Tailor dynamically combines threads to better suit the needs of the target system. Thread Tailor adjusts not only to the architecture, but also other applications in the system, and this paper demonstrates that this type of adjustment can lead to significantly better use of thread-level parallelism in real-world architectures.},
 acmid = {1815996},
 address = {New York, NY, USA},
 author = {Lee, Janghaeng and Wu, Haicheng and Ravichandran, Madhumitha and Clark, Nathan},
 doi = {10.1145/1816038.1815996},
 issn = {0163-5964},
 issue_date = {June 2010},
 journal = {SIGARCH Comput. Archit. News},
 keyword = {dynamic compilation, managed parallelism, threading},
 link = {http://doi.acm.org/10.1145/1816038.1815996},
 month = {jun},
 number = {3},
 numpages = {10},
 pages = {270--279},
 publisher = {ACM},
 title = {Thread Tailor: Dynamically Weaving Threads Together for Efficient, Adaptive Parallel Applications},
 volume = {38},
 year = {2010}
}


@article{Lucia:2010:CAS:1816038.1815988,
 abstract = {In this paper, we propose ColorSafe, an architecture that detects and dynamically avoids single- and multi-variable atomicity violation bugs. The key idea is to group related data into colors and then monitor access interleavings in the "color space". This enables detection of atomicity violations involving any data of the same color. We leverage support for meta-data to maintain color information, and signatures to efficiently keep recent color access histories. ColorSafe dynamically avoids atomicity violations by inserting ephemeral transactions that prevent erroneous interleavings. ColorSafe has two modes of operation: (1)debugging mode makes detection more precise, producing fewer false positives and collecting more information; and, (2)deployment mode provides robust, efficient dynamic bug avoidance with less precise detection. This makes ColorSafe useful throughout the lifetime of programs, not just during development. Our results show that, in deployment mode, ColorSafe is able to successfully avoid the majority of multi-variable atomicity violations in bug kernels, as well as in large applications (Apache and MySQL). In debugging mode, ColorSafe detects bugs with few false positives.},
 acmid = {1815988},
 address = {New York, NY, USA},
 author = {Lucia, Brandon and Ceze, Luis and Strauss, Karin},
 doi = {10.1145/1816038.1815988},
 issn = {0163-5964},
 issue_date = {June 2010},
 journal = {SIGARCH Comput. Archit. News},
 keyword = {atomicity violations, bug avoidance, concurrency errors, data coloring, debugging, multi-variable},
 link = {http://doi.acm.org/10.1145/1816038.1815988},
 month = {jun},
 number = {3},
 numpages = {12},
 pages = {222--233},
 publisher = {ACM},
 title = {ColorSafe: Architectural Support for Debugging and Dynamically Avoiding Multi-variable Atomicity Violations},
 volume = {38},
 year = {2010}
}


@inproceedings{Meng:2010:DWS:1815961.1815992,
 abstract = {SIMD organizations amortize the area and power of fetch, decode, and issue logic across multiple processing units in order to maximize throughput for a given area and power budget. However, throughput is reduced when a set of threads operating in lockstep (a warp) are stalled due to long latency memory accesses. The resulting idle cycles are extremely costly. Multi-threading can hide latencies by interleaving the execution of multiple warps, but deep multi-threading using many warps dramatically increases the cost of the register files (multi-threading depth x SIMD width), and cache contention can make performance worse. Instead, intra-warp latency hiding should first be exploited. This allows threads that are ready but stalled by SIMD restrictions to use these idle cycles and reduces the need for multi-threading among warps. This paper introduces dynamic warp subdivision (DWS), which allows a single warp to occupy more than one slot in the scheduler without requiring extra register file space. Independent scheduling entities allow divergent branch paths to interleave their execution, and allow threads that hit to run ahead. The result is improved latency hiding and memory level parallelism (MLP). We evaluate the technique on a coherent cache hierarchy with private L1 caches and a shared L2 cache. With an area overhead of less than 1%, experiments with eight data-parallel benchmarks show our technique improves performance on average by 1.7X.},
 acmid = {1815992},
 address = {New York, NY, USA},
 author = {Meng, Jiayuan and Tarjan, David and Skadron, Kevin},
 booktitle = {Proceedings of the 37th Annual International Symposium on Computer Architecture},
 doi = {10.1145/1815961.1815992},
 isbn = {978-1-4503-0053-7},
 keyword = {branch divergence, cache, latency hiding, memory divergence, simd, warp},
 link = {http://doi.acm.org/10.1145/1815961.1815992},
 location = {Saint-Malo, France},
 numpages = {12},
 pages = {235--246},
 publisher = {ACM},
 series = {ISCA '10},
 title = {Dynamic Warp Subdivision for Integrated Branch and Memory Divergence Tolerance},
 year = {2010}
}


@article{Dally:2010:MNC:1816038.1815963,
 abstract = {The goal of computer architecture research is to move the needle, that is to affect the future of computing in a positive way. Publications, prototypes, and studies are all just different means to this common end. This talk will address how to move the needle in academic and industrial settings discussing what works and what doesn't. Our work is constrained by applications, technology, and commercial reality. The architecture funnel starts with many concepts that proceed through stages of evaluation and refinement. A relatively few successful concepts make it out the far side to deployment. Most concepts fail, and good researchers cut their losses early. The funnel has many years of latency and good researchers aim for results that are relevant beyond this latency. Academics are best at the early stages of the concept funnel -- where their long-term perspective and freedom from constraints are advantages. Industry excels at the later stages of the pipeline where resources and experience are well suited to refining ideas for deployment. Too often good concepts fall into a chasm between the two. Good partnerships are needed to bridge this chasm. This talk will give illustrate this exploration of architecture research with numerous examples of successes and failures. It will give recommended best practices for academic and industrial research. I will close with a glimpse of the future of architecture.},
 acmid = {1815963},
 address = {New York, NY, USA},
 author = {Dally, William J.},
 doi = {10.1145/1816038.1815963},
 issn = {0163-5964},
 issue_date = {June 2010},
 journal = {SIGARCH Comput. Archit. News},
 keyword = {research},
 link = {http://doi.acm.org/10.1145/1816038.1815963},
 month = {jun},
 number = {3},
 numpages = {1},
 pages = {1--1},
 publisher = {ACM},
 title = {Moving the Needle, Computer Architecture Research in Academe and Industry},
 volume = {38},
 year = {2010}
}


@inproceedings{Udipi:2010:RDD:1815961.1815983,
 abstract = {DRAM vendors have traditionally optimized the cost-per-bit metric, often making design decisions that incur energy penalties. A prime example is the overfetch feature in DRAM, where a single request activates thousands of bit-lines in many DRAM chips, only to return a single cache line to the CPU. The focus on cost-per-bit is questionable in modern-day servers where operating costs can easily exceed the purchase cost. Modern technology trends are also placing very different demands on the memory system: (i)queuing delays are a significant component of memory access time, (ii) there is a high energy premium for the level of reliability expected for business-critical computing, and (iii) the memory access stream emerging from multi-core systems exhibits limited locality. All of these trends necessitate an overhaul of DRAM architecture, even if it means a slight compromise in the cost-per-bit metric. This paper examines three primary innovations. The first is a modification to DRAM chip microarchitecture that re tains the traditional DDRx SDRAMinterface. Selective Bit-line Activation (SBA) waits for both RAS (row address) and CAS (column address) signals to arrive before activating exactly those bitlines that provide the requested cache line. SBA reduces energy consumption while incurring slight area and performance penalties. The second innovation, Single Subarray Access (SSA), fundamentally re-organizes the layout of DRAM arrays and the mapping of data to these arrays so that an entire cache line is fetched from a single subarray. It requires a different interface to the memory controller, reduces dynamic and background energy (by about 6X), incurs a slight area penalty (4%), and can even lead to performance improvements (54% on average) by reducing queuing delays. The third innovation further penalizes the cost-per-bit metric by adding a checksum feature to each cache line. This checksum error-detection feature can then be used to build stronger RAID-like fault tolerance, including chipkill-level reliability. Such a technique is especially crucial for the SSA architecture where the entire cache line is localized to a single chip. This DRAM chip microarchitectural change leads to a dramatic reduction in the energy and storage overheads for reliability. The proposed architectures will also apply to other emerging memory technologies (such as resistive memories) and will be less disruptive to standards, interfaces, and the design flow if they can be incorporated into first-generation designs.},
 acmid = {1815983},
 address = {New York, NY, USA},
 author = {Udipi, Aniruddha N. and Muralimanohar, Naveen and Chatterjee, Niladrish and Balasubramonian, Rajeev and Davis, Al and Jouppi, Norman P.},
 booktitle = {Proceedings of the 37th Annual International Symposium on Computer Architecture},
 doi = {10.1145/1815961.1815983},
 isbn = {978-1-4503-0053-7},
 keyword = {chipkill, dram architecture, energy-efficiency, locality, subarrays},
 link = {http://doi.acm.org/10.1145/1815961.1815983},
 location = {Saint-Malo, France},
 numpages = {12},
 pages = {175--186},
 publisher = {ACM},
 series = {ISCA '10},
 title = {Rethinking DRAM Design and Organization for Energy-constrained Multi-cores},
 year = {2010}
}


@article{Azizi:2010:ETP:1816038.1815967,
 abstract = {Power consumption has become a major constraint in the design of processors today. To optimize a processor for energy-efficiency requires an examination of energy-performance trade-offs in all aspects of the processor design space, including both architectural and circuit design choices. In this paper, we apply an integrated architecture-circuit optimization framework to map out energy-performance trade-offs of several different high-level processor architectures. We show how the joint architecture-circuit space provides a trade-off range of approximately 6.5x in performance for 4x energy, and we identify the optimal architectures for different design objectives. We then show that many of the designs in this space come at very high marginal costs. Our results show that, for a large range of design objectives, voltage scaling is effective in efficiently trading off performance and energy, and that the choice of optimal architecture and circuits does not change much during voltage scaling. Finally, we show that with only two designs--a dual-issue in-order design and a dual-issue out-of-order design, both properly optimized-a large part of the energy-performance trade-off space can be covered within 3% of the optimal energy-efficiency.},
 acmid = {1815967},
 address = {New York, NY, USA},
 author = {Azizi, Omid and Mahesri, Aqeel and Lee, Benjamin C. and Patel, Sanjay J. and Horowitz, Mark},
 doi = {10.1145/1816038.1815967},
 issn = {0163-5964},
 issue_date = {June 2010},
 journal = {SIGARCH Comput. Archit. News},
 keyword = {co-optimization, design space exploration, design trade-offs, energy efficiency, microarchitecture, optimization},
 link = {http://doi.acm.org/10.1145/1816038.1815967},
 month = {jun},
 number = {3},
 numpages = {11},
 pages = {26--36},
 publisher = {ACM},
 title = {Energy-performance Tradeoffs in Processor Architecture and Circuit Design: A Marginal Cost Analysis},
 volume = {38},
 year = {2010}
}


@inproceedings{Pritchett:2010:SHE:1815961.1815982,
 abstract = {Emerging solid-state storage media can significantly improve storage performance and energy. However, the high cost-per-byte of solid-state media has hindered wide-spread adoption in servers. This paper proposes a new, cost-effective architecture - SieveStore - which enables the use of solid-state media to significantly filter access to storage ensembles. Our paper makes three key contributions. First, we make a case for highly-selective, storage-ensemble-level disk-block caching based on the highly-skewed block popularity distribution and based on the dynamic nature of the popular block set. Second, we identify the problem of allocation-writes and show that selective cache allocation to reduce allocation-writes - sieving - is fundamental to enable efficient ensemble-level disk-caching. Third, we propose two practical variants of SieveStore. Based on week-long block access traces from a storage ensemble of 13 servers, we find that the two components (sieving and ensemble-level caching) each contribute to SieveStore's cost-effectiveness. Compared to unsieved, ensemble-level disk-caches, SieveStore achieves significantly higher hit ratios (35%-50% more, on average) while using only 1/7th the number of SSD drives. Further, ensemble-level caching is strictly better in cost-performance compared to per-server caching.},
 acmid = {1815982},
 address = {New York, NY, USA},
 author = {Pritchett, Timothy and Thottethodi, Mithuna},
 booktitle = {Proceedings of the 37th Annual International Symposium on Computer Architecture},
 doi = {10.1145/1815961.1815982},
 isbn = {978-1-4503-0053-7},
 keyword = {disk cache, flash memory, selective allocation, solid state disks, storage, storage ensembles},
 link = {http://doi.acm.org/10.1145/1815961.1815982},
 location = {Saint-Malo, France},
 numpages = {12},
 pages = {163--174},
 publisher = {ACM},
 series = {ISCA '10},
 title = {SieveStore: A Highly-selective, Ensemble-level Disk Cache for Cost-performance},
 year = {2010}
}


@proceedings{Iyer:2011:2000064,
 abstract = {It is a great honor for me to introduce the program of the 38th Annual International Symposium on Computer Architecture. This symposium is the premier forum for new ideas and experimental results in the area of computer architecture. It has a long tradition of attracting the most impactful research results in this area, and this year is no exception. It has been a great pleasure to work with the very talented and professional team of colleagues that accepted to serve in the program committee for this year's edition. Their dedication and high quality work has been key to maintain the high standards of excellence of the conference. I want to thank all of them for their generous effort for reviewing papers and selecting the final program. I also want to thank all the authors who worked very hard to submit their papers to the conference. The high quality of many of the submitted papers made the selection task very intense and difficult. This year we received 208 papers and each of them went through a thorough review process. Each paper was reviewed by at least 4 members of the program committee and 1 external reviewer. This representedmore than 1000 reviews. As an indication of the professionalism of the PC members and external reviewers, I want to highlight that I received all the requested reviews in time, with no exception. After all reviews were submitted, authors were given the opportunity to see them and rebut any issue raised by the reviewers before the PC meeting. As a last step for preparation for the PC meeting, PC members were requested to read all reviews and rebuttals of their assigned papers and re-score them taking into account the opinions of the other reviewers and the response of the authors. The paper selection was done during the PC meeting that was held at the O'Hare airport in Chicago on February 19, 2011. All PC members but one participated in the meeting. The meeting lasted all day, starting at 8:00am and finishing at 7:00pm with just a very short break for lunch. In the meeting, we discussed every paper that any PC member felt we should discuss. At the end, we selected 40 papers for presentation and publication in the proceedings, which represents an acceptance rate of 19%.},
 address = {New York, NY, USA},
 isbn = {978-1-4503-0472-6},
 location = {San Jose, California, USA},
 note = {415114},
 publisher = {ACM},
 title = {ISCA '11: Proceedings of the 38th Annual International Symposium on Computer Architecture},
 year = {2011}
}


@article{Blake:2010:ETP:1816038.1816000,
 abstract = {As the effective limits of frequency and instruction level parallelism have been reached, the strategy of microprocessor vendors has changed to increase the number of processing cores on a single chip each generation. The implicit expectation is that software developers will write their applications with concurrency in mind to take advantage of this sudden change in direction. In this study we analyze whether software developers for laptop/desktop machines have followed the recent hardware trends by creating software for chip multi-processing. We conduct a study of a wide range of applications on Microsoft Windows 7 and Apple's OS X Snow Leopard, measuring Thread Level Parallelism on a high performance workstation and a low power desktop. In addition, we explore graphics processing units (GPUs) and their impact on chip multi-processing. We compare our findings to a study done 10 years ago which concluded that a second core was sufficient to improve system responsiveness. Our results on today's machines show that, 10 years later, surprisingly 2-3 cores are more than adequate for most applications and that the GPU often remains under-utilized. However, in some application specific domains an 8 core SMT system with a 240 core GPU can be effectively utilized. Overall these studies suggest that many-core architectures are not a natural fit for current desktop/laptop applications.},
 acmid = {1816000},
 address = {New York, NY, USA},
 author = {Blake, Geoffrey and Dreslinski, Ronald G. and Mudge, Trevor and Flautner, Kriszti\'{a}n},
 doi = {10.1145/1816038.1816000},
 issn = {0163-5964},
 issue_date = {June 2010},
 journal = {SIGARCH Comput. Archit. News},
 keyword = {benchmarking, desktop applications, multi-core, thread level parallelism},
 link = {http://doi.acm.org/10.1145/1816038.1816000},
 month = {jun},
 number = {3},
 numpages = {12},
 pages = {302--313},
 publisher = {ACM},
 title = {Evolution of Thread-level Parallelism in Desktop Applications},
 volume = {38},
 year = {2010}
}


@inproceedings{Temam:2010:RNN:1815961.1816008,
 abstract = {After the hype of the 1990s, where companies like Intel or Philips built commercial hardware systems based on neural networks, the approach quickly lost ground for multiple reasons: hardware neural networks were no match for software neural networks run on rapidly progressing general-purpose processors, their application scope was considered too limited, and even progress in machine-learning theory overshadowed neural networks. However, in the past few years, a remarkable convergence of trends and innovations is casting a new light on neural networks and could make them valuable components of future computing systems. Trends in technology call for architectures which can sustain a large number of defects, something neural networks are intrinsically capable of. Tends in applications, summarized in the recent RMS categorization, highlight a number of key algorithms which are eligible to neural networks implementations. At the same time, innovations in technology, such as the recent realization of a memristor, are creating the conditions for the efficient hardware implementation of neural networks. Innovations in machine learning, with the recent advent of Deep Networks, have revived interest in neural networks. Finally, recent findings in neurobiology carry even greater prospects, where detailed explanations of how complex functions, such as vision, can be implemented further open up the defect-tolerance and application potential of neural network architectures.},
 acmid = {1816008},
 address = {New York, NY, USA},
 author = {Temam, Olivier},
 booktitle = {Proceedings of the 37th Annual International Symposium on Computer Architecture},
 doi = {10.1145/1815961.1816008},
 isbn = {978-1-4503-0053-7},
 keyword = {neural networks},
 link = {http://doi.acm.org/10.1145/1815961.1816008},
 location = {Saint-Malo, France},
 numpages = {1},
 pages = {349--349},
 publisher = {ACM},
 series = {ISCA '10},
 title = {The Rebirth of Neural Networks},
 year = {2010}
}


@article{Eyerman:2010:MCS:1816038.1816011,
 abstract = {This paper presents a fundamental law for parallel performance: it shows that parallel performance is not only limited by sequential code (as suggested by Amdahl's law) but is also fundamentally limited by synchronization through critical sections. Extending Amdahl's software model to include critical sections, we derive the surprising result that the impact of critical sections on parallel performance can be modeled as a completely sequential part and a completely parallel part. The sequential part is determined by the probability for entering a critical section and the contention probability (i.e., multiple threads wanting to enter the same critical section). This fundamental result reveals at least three important insights for multicore design. (i) Asymmetric multicore processors deliver less performance benefits relative to symmetric processors than suggested by Amdahl's law, and in some cases even worse performance. (ii) Amdahl's law suggests many tiny cores for optimum performance in asymmetric processors, however, we find that fewer but larger small cores can yield substantially better performance. (iii) Executing critical sections on the big core can yield substantial speedups, however, performance is sensitive to the accuracy of the critical section contention predictor.},
 acmid = {1816011},
 address = {New York, NY, USA},
 author = {Eyerman, Stijn and Eeckhout, Lieven},
 doi = {10.1145/1816038.1816011},
 issn = {0163-5964},
 issue_date = {June 2010},
 journal = {SIGARCH Comput. Archit. News},
 keyword = {Amdahl's law, analytical performance modeling, critical sections, synchronization},
 link = {http://doi.acm.org/10.1145/1816038.1816011},
 month = {jun},
 number = {3},
 numpages = {9},
 pages = {362--370},
 publisher = {ACM},
 title = {Modeling Critical Sections in Amdahl's Law and Its Implications for Multicore Design},
 volume = {38},
 year = {2010}
}


@article{Suleman:2010:DMM:1816038.1816020,
 abstract = {Previous research has shown that Staged Execution (SE), i.e., dividing a program into segments and executing each segment at the core that has the data and/or functionality to best run that segment, can improve performance and save power. However, SE's benefit is limited because most segments access inter-segment data, i.e., data generated by the previous segment. When consecutive segments run on different cores, accesses to inter-segment data incur cache misses, thereby reducing performance. This paper proposes Data Marshaling (DM), a new technique to eliminate cache misses to inter-segment data. DM uses profiling to identify instructions that generate inter-segment data, and adds only 96 bytes/core of storage overhead. We show that DM significantly improves the performance of two promising Staged Execution models, Accelerated Critical Sections and producer-consumer pipeline parallelism, on both homogeneous and heterogeneous multi-core systems. In both models, DM can achieve almost all of the potential of ideally eliminating cache misses to inter-segment data. DM's performance benefit increases with the number of cores.},
 acmid = {1816020},
 address = {New York, NY, USA},
 author = {Suleman, M. Aater and Mutlu, Onur and Joao, Jos{\'e} A. and Khubaib and Patt, Yale N.},
 doi = {10.1145/1816038.1816020},
 issn = {0163-5964},
 issue_date = {June 2010},
 journal = {SIGARCH Comput. Archit. News},
 keyword = {cmp, critical sections, pipelining, staged execution},
 link = {http://doi.acm.org/10.1145/1816038.1816020},
 month = {jun},
 number = {3},
 numpages = {10},
 pages = {441--450},
 publisher = {ACM},
 title = {Data Marshaling for Multi-core Architectures},
 volume = {38},
 year = {2010}
}


@inproceedings{Sridharan:2010:UHV:1815961.1816023,
 abstract = {Fault tolerance is now a primary design constraint for all major microprocessors. One step in determining a processor's compliance to its failure rate target is measuring the Architectural Vulnerability Factor (AVF) of each on-chip structure. The AVF of a hardware structure is the probability that a fault in the structure will affect the output of a program. While AVF generates meaningful insight into system behavior, it cannot quantify the vulnerability of an individual system component (hardware, user program, etc.), limiting the amount of insight that can be generated. To address this, prior work has introduced the Program Vulnerability Factor (PVF) to quantify the vulnerability of software. In this paper, we introduce and analyze the Hardware Vulnerability Factor (HVF) to quantify the vulnerability of hardware. HVF has three concrete benefits which we examine in this paper. First, HVF analysis can provide insight to hardware designers beyond that gained from AVF analysis alone. Second, separating AVF analysis into HVF and PVF steps can accelerate the AVF measurement process. Finally, HVF measurement enables runtime AVF estimation that combines compile-time PVF estimates with runtime HVF measurements. A key benefit of this technique is that it allows software developers to influence the runtime AVF estimates. We demonstrate that this technique can estimate AVF at runtime with an average absolute error of less than 3%.},
 acmid = {1816023},
 address = {New York, NY, USA},
 author = {Sridharan, Vilas and Kaeli, David R.},
 booktitle = {Proceedings of the 37th Annual International Symposium on Computer Architecture},
 doi = {10.1145/1815961.1816023},
 isbn = {978-1-4503-0053-7},
 keyword = {architectural vulnerability factor, fault tolerance, reliability},
 link = {http://doi.acm.org/10.1145/1815961.1816023},
 location = {Saint-Malo, France},
 numpages = {12},
 pages = {461--472},
 publisher = {ACM},
 series = {ISCA '10},
 title = {Using Hardware Vulnerability Factors to Enhance AVF Analysis},
 year = {2010}
}


@inproceedings{Yan:2010:LCC:1815961.1816025,
 abstract = {Process, Voltage, and Temperature (PVT) variations can significantly degrade the performance benefits expected from next nanoscale technology. The primary circuit implication of the PVT variations is the resultant timing emergencies. In a multi-core processor running multiple programs, variations create spatial and temporal unbalance across the processing cores. Most prior schemes are dedicated to tolerating PVT variations individually for a single core, but ignore the opportunity of leveraging the complementary effects between variations and the intrinsic variation unbalance among individual cores. We find that the notorious delay impacts from different variations are not necessary aggregated. Cores with mild variations can share the violent workload from cores suffering large variations. If operated correctly, variations on different cores can help mitigating each other and result in a variation-mild environment. In this paper, we propose Timing Emergency Aware Thread Migration (TEA-TM), a delay sensor-based scheme to reduce system timing emergencies under PVT variations. Fourier transform and frequency domain analysis are conducted to provide the insights and the potential of the PVT co-optimization scheme. Experimental results show on average TEA-TM can help save up to 24% throughput loss, at the same time improve the system fairness by 85%.},
 acmid = {1816025},
 address = {New York, NY, USA},
 author = {Yan, Guihai and Liang, Xiaoyao and Han, Yinhe and Li, Xiaowei},
 booktitle = {Proceedings of the 37th Annual International Symposium on Computer Architecture},
 doi = {10.1145/1815961.1816025},
 isbn = {978-1-4503-0053-7},
 keyword = {complimentary effects, delay sensor, pvt variations, thread migration, timing emergency},
 link = {http://doi.acm.org/10.1145/1815961.1816025},
 location = {Saint-Malo, France},
 numpages = {12},
 pages = {485--496},
 publisher = {ACM},
 series = {ISCA '10},
 title = {Leveraging the Core-level Complementary Effects of PVT Variations to Reduce Timing Emergencies in Multi-core Processors},
 year = {2010}
}


@inproceedings{deKruijf:2010:RAF:1815961.1816026,
 abstract = {As technology scales ever further, device unreliability is creating excessive complexity for hardware to maintain the illusion of perfect operation. In this paper, we consider whether exposing hardware fault information to software and allowing software to control fault recovery simplifies hardware design and helps technology scaling. The combination of emerging applications and emerging many-core architectures makes software recovery a viable alternative to hardware-based fault recovery. Emerging applications tend to have few I/O and memory side-effects, which limits the amount of information that needs checkpointing, and they allow discarding individual sub-computations with small qualitative impact. Software recovery can harness these properties in ways that hardware recovery cannot. We describe Relax, an architectural framework for software recovery of hardware faults. Relax includes three core components: (1) an ISA extension that allows software to mark regions of code for software recovery, (2) a hardware organization that simplifies reliability considerations and provides energy efficiency with hardware recovery support removed, and (3) software support for compilers and programmers to utilize the Relax ISA. Applying Relax to counter the effects of process variation, our results show a 20% energy efficiency improvement for PARSEC applications with only minimal source code changes and simpler hardware.},
 acmid = {1816026},
 address = {New York, NY, USA},
 author = {de Kruijf, Marc and Nomura, Shuou and Sankaralingam, Karthikeyan},
 booktitle = {Proceedings of the 37th Annual International Symposium on Computer Architecture},
 doi = {10.1145/1815961.1816026},
 isbn = {978-1-4503-0053-7},
 keyword = {reliability, software recovery},
 link = {http://doi.acm.org/10.1145/1815961.1816026},
 location = {Saint-Malo, France},
 numpages = {12},
 pages = {497--508},
 publisher = {ACM},
 series = {ISCA '10},
 title = {Relax: An Architectural Framework for Software Recovery of Hardware Faults},
 year = {2010}
}


@inproceedings{Hameed:2010:USI:1815961.1815968,
 abstract = {Due to their high volume, general-purpose processors, and now chip multiprocessors (CMPs), are much more cost effective than ASICs, but lag significantly in terms of performance and energy efficiency. This paper explores the sources of these performance and energy overheads in general-purpose processing systems by quantifying the overheads of a 720p HD H.264 encoder running on a general-purpose CMP system. It then explores methods to eliminate these overheads by transforming the CPU into a specialized system for H.264 encoding. We evaluate the gains from customizations useful to broad classes of algorithms, such as SIMD units, as well as those specific to particular computation, such as customized storage and functional units. The ASIC is 500x more energy efficient than our original four-processor CMP. Broadly applicable optimizations improve performance by 10x and energy by 7x. However, the very low energy costs of actual core ops (100s fJ in 90nm) mean that over 90% of the energy used in these solutions is still "overhead". Achieving ASIC-like performance and efficiency requires algorithm-specific optimizations. For each sub-algorithm of H.264, we create a large, specialized functional unit that is capable of executing 100s of operations per instruction. This improves performance and energy by an additional 25x and the final customized CMP matches an ASIC solution's performance within 3x of its energy and within comparable area.},
 acmid = {1815968},
 address = {New York, NY, USA},
 author = {Hameed, Rehan and Qadeer, Wajahat and Wachs, Megan and Azizi, Omid and Solomatnikov, Alex and Lee, Benjamin C. and Richardson, Stephen and Kozyrakis, Christos and Horowitz, Mark},
 booktitle = {Proceedings of the 37th Annual International Symposium on Computer Architecture},
 doi = {10.1145/1815961.1815968},
 isbn = {978-1-4503-0053-7},
 keyword = {ASIC, chip multiprocessor, customization, energy efficiency, h.264, high performance, tensilica},
 link = {http://doi.acm.org/10.1145/1815961.1815968},
 location = {Saint-Malo, France},
 numpages = {11},
 pages = {37--47},
 publisher = {ACM},
 series = {ISCA '10},
 title = {Understanding Sources of Inefficiency in General-purpose Chips},
 year = {2010}
}


@inproceedings{Gibson:2010:FSC:1815961.1815966,
 abstract = {Chip Multiprocessors (CMPs) are now commodity hardware, but commoditization of parallel software remains elusive. In the near term, the current trend of increased core-per-socket count will continue, despite a lack of parallel software to exercise the hardware. Future CMPs must deliver thread-level parallelism when software provides threads to run, but must also continue to deliver performance gains for single threads by exploiting instruction-level parallelism and memory-level parallelism. However, power limitations will prevent conventional cores from exploiting both simultaneously. This work presents the Forwardflow Architecture, which can scale its execution logic up to run single threads, or down to run multiple threads in a CMP. Forwardflow dynamically builds an explicit internal dataflow representation from a conventional instruction set architecture, using forward dependence pointers to guide instruction wakeup, selection, and issue. Forwardflow's backend is organized into discrete units that can be individually (de-)activated, allowing each core's performance to be scaled by system software at the architectural level. On single threads, Forwardflow core scaling yields a mean runtime reduction of 21% for a 37% increase in power consumption. For multithreaded workloads, a Forwardflow-based CMP allows system software to select the performance point that best matches available power.},
 acmid = {1815966},
 address = {New York, NY, USA},
 author = {Gibson, Dan and Wood, David A.},
 booktitle = {Proceedings of the 37th Annual International Symposium on Computer Architecture},
 doi = {10.1145/1815961.1815966},
 isbn = {978-1-4503-0053-7},
 keyword = {chip multiprocessor (cmp), power, scalable core},
 link = {http://doi.acm.org/10.1145/1815961.1815966},
 location = {Saint-Malo, France},
 numpages = {12},
 pages = {14--25},
 publisher = {ACM},
 series = {ISCA '10},
 title = {Forwardflow: A Scalable Core for Power-constrained CMPs},
 year = {2010}
}


@article{Koka:2010:SNA:1816038.1815977,
 abstract = {Scaling trends of logic, memories, and interconnect networks lead towards dense many-core chips. Unfortunately, process yields and reticle sizes limit the scalability of large single-chip systems. Multi-chip systems break free of these areal limits, but in turn require enormous chip-to-chip bandwidth. The "macrochip" concept presented here integrates multiple many-core processor chips in a single package with silicon-photonic interconnects. This design enables a multi-chip system to approach the performance of a single large die. In this paper we propose three silicon-photonic network designs that provide low-power, high-bandwidth inter-die communication: a static wavelength-routed point-to-point network, a "two-phase" arbitrated network, and a limited-connectivity point-to-point network. We also adapt two existing intra-chip silicon-photonic interconnects: a token-ring-based crossbar and a circuit-switched torus. We simulate a 64-die, 512-core cache-coherent macrochip using all of the above networks with synthetic kernels, and kernels from Splash-2 and PARSEC. We evaluate the networks on performance, optical power and complexity. Despite a narrow data-path width compared to the token-ring or torus, the point-to-point performs 3.3x and 3.9x better respectively. We show that the point-to-point is over 10x more power-efficient than the other networks. We also show that, contrary to electronic network designs, a point-to-point network has the lowest design complexity for an inter-chip silicon-photonic network.},
 acmid = {1815977},
 address = {New York, NY, USA},
 author = {Koka, Pranay and McCracken, Michael O. and Schwetman, Herb and Zheng, Xuezhe and Ho, Ron and Krishnamoorthy, Ashok V.},
 doi = {10.1145/1816038.1815977},
 issn = {0163-5964},
 issue_date = {June 2010},
 journal = {SIGARCH Comput. Archit. News},
 keyword = {interconnection networks, nanophotonics},
 link = {http://doi.acm.org/10.1145/1816038.1815977},
 month = {jun},
 number = {3},
 numpages = {12},
 pages = {117--128},
 publisher = {ACM},
 title = {Silicon-photonic Network Architectures for Scalable, Power-efficient Multi-chip Systems},
 volume = {38},
 year = {2010}
}


@article{Blundell:2010:RTR:1816038.1815995,
 abstract = {Over the past decade there has been a surge of academic and industrial interest in optimistic concurrency, i.e. the speculative parallel execution of code regions that have the semantics of isolation. This work analyzes scalability bottlenecks of workloads that use optimistic concurrency. We find that one common bottleneck is updates to auxiliary program data in otherwise non-conflicting operations, e.g. reference count updates and hashtable occupancy field increments. To eliminate the performance impact of conflicts on such auxiliary data, this work proposes RETCON, a hardware mechanism that tracks the relationship between input and output values symbolically and uses this symbolic information to transparently repair the output state of a transaction at commit. RETCON is inspired by instruction replay-based mechanisms but exploits simplifying properties of the nature of computations on auxiliary data to perform repair without replay. Our experiments show that RETCON provides significant speedups for workloads that exhibit conflicts on auxiliary data, including transforming a transactionalized version of the Python interpreter from a workload that exhibits no scaling to one that exhibits near-linear scaling on 32 cores.},
 acmid = {1815995},
 address = {New York, NY, USA},
 author = {Blundell, Colin and Raghavan, Arun and Martin, Milo M.K.},
 doi = {10.1145/1816038.1815995},
 issn = {0163-5964},
 issue_date = {June 2010},
 journal = {SIGARCH Comput. Archit. News},
 keyword = {parallel programming, transactional memory},
 link = {http://doi.acm.org/10.1145/1816038.1815995},
 month = {jun},
 number = {3},
 numpages = {12},
 pages = {258--269},
 publisher = {ACM},
 title = {RETCON: Transactional Repair Without Replay},
 volume = {38},
 year = {2010}
}


@proceedings{Seznec:2010:1815961,
 abstract = {On behalf of the Program Committee we are pleased to welcome you to the 2010 International Symposium on Computer Architecture held in Saint Malo, France. It has been a great honor for us to serve as PC chairs of the prestigious ISCA conference. The ISCA conference is well known in the community as the forerunner in Computer Architecture because of its unique ability to provide, under one roof, maximum exposure for the newest ideas in the field. It is the responsibility of this conference to shape the direction Computer Architecture will take in the future and to promote its enormous potential. Traditionally, the technical program of ISCA included some of the best research work in Computer Architecture, from the academia and industry. This year's ISCA program is as strong as ever, a testimonial to the continued vitality of Computer Architecture research. This year's program features papers on today's "hot" computer architecture topics, including energy efficiency designs, caches, memory subsystems, accelerator architecture, evaluation techniques, reliability and fault tolerance. This year we feature a unique QOB (Quantitative Out of the Box) session that emphasizes qualitative new innovative papers vs. thorough quantitative analysis papers. We hope you will enjoy and learn from the presentations and the papers. The conference papers selection is the fruit of a long and rigorous process. The conference is grateful to all of the authors for the time and effort invested in submitting their best work. 245 papers were submitted to the conference, out of which the Program Committee selected 44 papers to be presented in the ISCA 2010 conference (18% acceptance rate). We have been fortunate to recruit an outstanding Program Committee of 46 members that took the overwhelming task of selecting the best papers. Committee members contributed their time and expertise towards the goal of making this conference a success. We would like to deeply thank each and every Program Committee member. Only now we understand the enormous load they took on themselves: reviewing the submitted papers (we performed a total of 1200 reviews), interacting with other PC members and participating in the full day PC meeting marathon at the O'Hare Airport in Chicago (February 20th 2010).},
 address = {New York, NY, USA},
 isbn = {978-1-4503-0053-7},
 location = {Saint-Malo, France},
 note = {415106},
 publisher = {ACM},
 title = {ISCA '10: Proceedings of the 37th Annual International Symposium on Computer Architecture},
 year = {2010}
}


@article{Voskuilen:2010:TEA:1816038.1815986,
 abstract = {As chip multiprocessors emerge as the prevalent microprocessor architecture, support for debugging shared-memory parallel programs becomes important. A key difficulty is the programs' nondeterministic semantics due to which replay runs of a buggy program may not reproduce the bug. The non-determinism stems from memory races where accesses from two threads, at least one of which is a write, go to the same memory location. Previous hardware schemes for memory race recording log the predecessor-successor thread ordering at memory races and enforce the same orderings in the replay run to achieve deterministic replay. To reduce the log size, the schemes exploit transitivity in the orderings to avoid recording redundant orderings. To reduce the log size further while requiring minimal hardware, we propose Timetraveler which for the first time exploits acyclicity of races based on the key observation that an acyclic race need not be recorded even if the race is not covered already by transitivity. Timetraveler employs a novel and elegant mechanism called post-dating which both ensures that acyclic races, including those through the L2, are eventually ordered correctly, and identifies cyclic races. To address false cycles through the L2, Timetraveler employs another novel mechanism called time-delay buffer which delays the advancement of the L2 banks' timestamps and thereby reduces the false cycles. Using simulations, we show that Timetraveler reduces the log size for commercial workloads by 88% over the best previous approach while using only a 696-byte time-delay buffer.},
 acmid = {1815986},
 address = {New York, NY, USA},
 author = {Voskuilen, Gwendolyn and Ahmad, Faraz and Vijaykumar, T. N.},
 doi = {10.1145/1816038.1815986},
 issn = {0163-5964},
 issue_date = {June 2010},
 journal = {SIGARCH Comput. Archit. News},
 keyword = {debugging, determinism, race recording, replay},
 link = {http://doi.acm.org/10.1145/1816038.1815986},
 month = {jun},
 number = {3},
 numpages = {12},
 pages = {198--209},
 publisher = {ACM},
 title = {Timetraveler: Exploiting Acyclic Races for Optimizing Memory Race Recording},
 volume = {38},
 year = {2010}
}


