@inproceedings{Kumar:2010:ETM:1718487.1718526,
 abstract = {Two-sided markets arise when two different types of users may realize gains by interacting with one another through one or more platforms or mediators. We initiate a study of the evolution of such markets. We present an empirical analysis of the value accruing to members of each side of the market, based on the presence of the other side. We codify the range of value curves into a general theoretical model, characterize the equilibrium states of two-sided markets in our model, and prove that each platform will converge to one of these equilibria. We give some early experimental results of the stability of two-sided markets, and close with a theoretical treatment of the formation of different kinds of coalitions in such markets.},
 acmid = {1718526},
 address = {New York, NY, USA},
 author = {Kumar, Ravi and Lifshits, Yury and Tomkins, Andrew},
 booktitle = {Proceedings of the Third ACM International Conference on Web Search and Data Mining},
 doi = {10.1145/1718487.1718526},
 isbn = {978-1-60558-889-6},
 keyword = {coalitions, equilibrium, preferential attachment, two-sided markets},
 link = {http://doi.acm.org/10.1145/1718487.1718526},
 location = {New York, New York, USA},
 numpages = {10},
 pages = {311--320},
 publisher = {ACM},
 series = {WSDM '10},
 title = {Evolution of Two-sided Markets},
 year = {2010}
}


@inproceedings{Yu:2010:SLS:1718487.1718540,
 abstract = {In this paper, we study search bot traffic from search engine query logs at a large scale. Although bots that generate search traffic aggressively can be easily detected, a large number of distributed, low rate search bots are difficult to identify and are often associated with malicious attacks. We present SBotMiner, a system for automatically identifying stealthy, low-rate search bot traffic from query logs. Instead of detecting individual bots, our approach captures groups of distributed, coordinated search bots. Using sampled data from two different months, SBotMiner identifies over 123 million bot-related pageviews, accounting for 3.8% of total traffic. Our in-depth analysis shows that a large fraction of the identified bot traffic may be associated with various malicious activities such as phishing attacks or vulnerability exploits. This finding suggests that detecting search bot traffic holds great promise to detect and stop attacks early on.},
 acmid = {1718540},
 address = {New York, NY, USA},
 author = {Yu, Fang and Xie, Yinglian and Ke, Qifa},
 booktitle = {Proceedings of the Third ACM International Conference on Web Search and Data Mining},
 doi = {10.1145/1718487.1718540},
 isbn = {978-1-60558-889-6},
 keyword = {botnet detection, click fraud, search bot, search log analysis, web search},
 link = {http://doi.acm.org/10.1145/1718487.1718540},
 location = {New York, New York, USA},
 numpages = {10},
 pages = {421--430},
 publisher = {ACM},
 series = {WSDM '10},
 title = {SBotMiner: Large Scale Search Bot Detection},
 year = {2010}
}


@inproceedings{Schifanella:2010:FFS:1718487.1718521,
 abstract = {Web 2.0 applications have attracted a considerable amount of attention because their open-ended nature allows users to create lightweight semantic scaffolding to organize and share content. To date, the interplay of the social and semantic components of social media has been only partially explored. Here we focus on Flickr and Last.fm, two social media systems in which we can relate the tagging activity of the users with an explicit representation of their social network. We show that a substantial level of local lexical and topical alignment is observable among users who lie close to each other in the social network. We introduce a null model that preserves user activity while removing local correlations, allowing us to disentangle the actual local alignment between users from statistical effects due to the assortative mixing of user activity and centrality in the social network. This analysis suggests that users with similar topical interests are more likely to be friends, and therefore semantic similarity measures among users based solely on their annotation metadata should be predictive of social links. We test this hypothesis on the Last.fm data set, confirming that the social network constructed from semantic similarity captures actual friendship more accurately than Last.fm's suggestions based on listening patterns.},
 acmid = {1718521},
 address = {New York, NY, USA},
 author = {Schifanella, Rossano and Barrat, Alain and Cattuto, Ciro and Markines, Benjamin and Menczer, Filippo},
 booktitle = {Proceedings of the Third ACM International Conference on Web Search and Data Mining},
 doi = {10.1145/1718487.1718521},
 isbn = {978-1-60558-889-6},
 keyword = {collaborative tagging, folksonomies, lexical and topical alignment, link prediction, maximum information path, social media, social network, social semantic similarity, web 2.0},
 link = {http://doi.acm.org/10.1145/1718487.1718521},
 location = {New York, New York, USA},
 numpages = {10},
 pages = {271--280},
 publisher = {ACM},
 series = {WSDM '10},
 title = {Folks in Folksonomies: Social Link Prediction from Shared Metadata},
 year = {2010}
}


@inproceedings{Tyler:2010:LSQ:1718487.1718512,
 abstract = {Although Web search engines are targeted towards helping people find new information, people regularly use them to re-find Web pages they have seen before. Researchers have noted the existence of this phenomenon, but relatively little is understood about how re-finding behavior differs from the finding of new information. This paper dives deeply into the differences via analysis of three large-scale data sources: 1) query logs (queries, clicks, result impressions), 2) Web browsing logs (URL visits), and 3) a daily Web crawl (page content). It appears that people learn valuable information about the pages they find that helps them re-find what they are looking for later; compared to the initial finding query, re-finding queries are typically shorter, and rank the re-found URL higher. While many instances of re-finding probably serve as a type of bookmark for a known URL, others seem to represent the resumption of a previous task; results clicked at the end of a session are more likely than those at the beginning to be re-found during a later session, while re-finding is more likely to happen at the beginning of a session than at the end. Additionally, we observe differences in cross-session and intra-session re-finding that may indicate different types of re-finding tasks. Our findings suggest there is a rich opportunity for search engines to take advantage of re-finding behavior as a means to improve the search experience.},
 acmid = {1718512},
 address = {New York, NY, USA},
 author = {Tyler, Sarah K. and Teevan, Jaime},
 booktitle = {Proceedings of the Third ACM International Conference on Web Search and Data Mining},
 doi = {10.1145/1718487.1718512},
 isbn = {978-1-60558-889-6},
 keyword = {query log analysis, re-finding, web search},
 link = {http://doi.acm.org/10.1145/1718487.1718512},
 location = {New York, New York, USA},
 numpages = {10},
 pages = {191--200},
 publisher = {ACM},
 series = {WSDM '10},
 title = {Large Scale Query Log Analysis of Re-finding},
 year = {2010}
}


@inproceedings{DasSarma:2010:RMT:1718487.1718491,
 abstract = {We study the problem of designing a mechanism to rank items in forums by making use of the user reviews such as thumb and star ratings. We compare mechanisms where forum users rate individual posts and also mechanisms where the user is asked to perform a pairwise comparison and state which one is better. The main metric used to evaluate a mechanism is the ranking accuracy vs the cost of reviews, where the cost is measured as the average number of reviews used per post. We show that for many reasonable probability models, there is no thumb (or star) based ranking mechanism that can produce approximately accurate rankings with bounded number of reviews per item. On the other hand we provide a review mechanism based on pairwise comparisons which achieves approximate rankings with bounded cost. We have implemented a system, shoutvelocity, which is a twitter-like forum but items (i.e., tweets in Twitter) are rated by using comparisons. For each new item the user who posts the item is required to compare two previous entries. This ensures that over a sequence of n posts, we get at least n comparisons requiring one review per item on average. Our mechanism uses this sequence of comparisons to obtain a ranking estimate. It ensures that every item is reviewed at least once and winning entries are reviewed more often to obtain better estimates of top items.},
 acmid = {1718491},
 address = {New York, NY, USA},
 author = {Das Sarma, Anish and Das Sarma, Atish and Gollapudi, Sreenivas and Panigrahy, Rina},
 booktitle = {Proceedings of the Third ACM International Conference on Web Search and Data Mining},
 doi = {10.1145/1718487.1718491},
 isbn = {978-1-60558-889-6},
 keyword = {comparisons, ranking mechanisms, thumb-based ranking},
 link = {http://doi.acm.org/10.1145/1718487.1718491},
 location = {New York, New York, USA},
 numpages = {10},
 pages = {21--30},
 publisher = {ACM},
 series = {WSDM '10},
 title = {Ranking Mechanisms in Twitter-like Forums},
 year = {2010}
}


@inproceedings{Wang:2010:ISB:1718487.1718514,
 abstract = {This article describes an application of the partially observable Markov (POM) model to the analysis of a large scale commercial web search log. Mathematically, POM is a variant of the hidden Markov model in which all the hidden state transitions do not necessarily emit observable events. This property of POM is used to model, as the hidden process, a common search behavior that users would read and skip search results, leaving no observable user actions to record in the search logs. The Markov nature of the model further lends support to cope with the facts that a single observed sequence can be probabilistically associated with many hidden sequences that have variable lengths, and the search results can be read in various temporal orders that are not necessarily reflected in the observed sequence of user actions. To tackle the implementation challenges accompanying the flexibility and analytic powers of POM, we introduce segmental Viterbi algorithm based on segmental decoding and Viterbi training to train the POM model parameters and apply them to uncover hidden processes from the search logs. To validate the model, the latent variables modeling the browsing patterns on the search result page are compared with the experimental data of the eye tracking stu-dies. The close agreements suggest that the search logs do contain rich information of user behaviors in browsing the search result page even though they are not directly observable, and that using POM to understand these sophisticated search behaviors is a promising approach.},
 acmid = {1718514},
 address = {New York, NY, USA},
 author = {Wang, Kuansan and Gloy, Nikolas and Li, Xiaolong},
 booktitle = {Proceedings of the Third ACM International Conference on Web Search and Data Mining},
 doi = {10.1145/1718487.1718514},
 isbn = {978-1-60558-889-6},
 keyword = {eye tracking, partially observable markov model, search log mining, segmental viterbi algorithm, web search behaviors},
 link = {http://doi.acm.org/10.1145/1718487.1718514},
 location = {New York, New York, USA},
 numpages = {10},
 pages = {211--220},
 publisher = {ACM},
 series = {WSDM '10},
 title = {Inferring Search Behaviors Using Partially Observable Markov (POM) Model},
 year = {2010}
}


@inproceedings{Dupret:2010:MEI:1718487.1718510,
 abstract = {We propose a new model to interpret the clickthrough logs of a web search engine. This model is based on explicit assumptions on the user behavior. In particular, we draw conclusions on a document relevance by observing the user behavior after he examined the document and not based on whether a user clicks or not a document url. This results in a model based on intrinsic relevance, as opposed to perceived relevance. We use the model to predict document relevance and then use this as feature for a "Learning to Rank" machine learning algorithm. Comparing the ranking functions obtained by training the algorithm with and without the new feature we observe surprisingly good results. This is particularly notable given that the baseline we use is the heavily optimized ranking function of a leading commercial search engine. A deeper analysis shows that the new feature is particularly helpful for non navigational queries and queries with a large abandonment rate or a large average number of queries per session. This is important because these types of query is considered to be the most difficult to solve.},
 acmid = {1718510},
 address = {New York, NY, USA},
 author = {Dupret, Georges and Liao, Ciya},
 booktitle = {Proceedings of the Third ACM International Conference on Web Search and Data Mining},
 doi = {10.1145/1718487.1718510},
 isbn = {978-1-60558-889-6},
 keyword = {clickthrough data, search engines, user behavior model},
 link = {http://doi.acm.org/10.1145/1718487.1718510},
 location = {New York, New York, USA},
 numpages = {10},
 pages = {181--190},
 publisher = {ACM},
 series = {WSDM '10},
 title = {A Model to Estimate Intrinsic Document Relevance from the Clickthrough Logs of a Web Search Engine},
 year = {2010}
}


@inproceedings{Ravi:2010:AGB:1718487.1718530,
 abstract = {One of the most prevalent online advertising methods is textual advertising. To produce a textual ad, an advertiser must craft a short creative (the text of the ad) linking to a landing page, which describes the product or service being promoted. Furthermore, the advertiser must associate the creative to a set of manually chosen bid phrases representing those Web search queries that should trigger the ad. For efficiency, given a landing page, the bid phrases are often chosen first, and then for each bid phrase the creative is produced using a template. Nevertheless, an ad campaign (e.g., for a large retailer) might involve thousands of landing pages and tens or hundreds of thousands of bid phrases, hence the entire process is very laborious. Our study aims towards the automatic construction of online ad campaigns: given a landing page, we propose several algorithmic methods to generate bid phrases suitable for the given input. Such phrases must be both relevant (that is, reflect the content of the page) and well-formed (that is, likely to be used as queries to a Web search engine). To this end, we use a two phase approach. First, candidate bid phrases are generated by a number of methods, including a (mono-lingual) translation model capable of generating phrases contained within the text of the input as well as previously "unseen" phrases. Second, the candidates are ranked in a probabilistic framework using both the translation model, which favors relevant phrases, as well as a bid phrase language model, which favors well-formed phrases. Empirical evaluation based on a real-life corpus of advertiser-created landing pages and associated bid phrases confirms the value of our approach, which successfully re-generates many of the human-crafted bid phrases and performs significantly better than a pure text extraction method.},
 acmid = {1718530},
 address = {New York, NY, USA},
 author = {Ravi, Sujith and Broder, Andrei and Gabrilovich, Evgeniy and Josifovski, Vanja and Pandey, Sandeep and Pang, Bo},
 booktitle = {Proceedings of the Third ACM International Conference on Web Search and Data Mining},
 doi = {10.1145/1718487.1718530},
 isbn = {978-1-60558-889-6},
 keyword = {content match, sponsored search},
 link = {http://doi.acm.org/10.1145/1718487.1718530},
 location = {New York, New York, USA},
 numpages = {10},
 pages = {341--350},
 publisher = {ACM},
 series = {WSDM '10},
 title = {Automatic Generation of Bid Phrases for Online Advertising},
 year = {2010}
}


@inproceedings{Bian:2010:RQL:1718487.1718506,
 abstract = {Queries describe the users' search intent and therefore they play an essential role in the context of ranking for information retrieval and Web search. However, most of existing approaches for ranking do not explicitly take into consideration the fact that queries vary significantly along several dimensions and entail different treatments regarding the ranking models. In this paper, we propose to incorporate query difference into ranking by introducing query-dependent loss functions. In the context of Web search, query difference is usually represented as different query categories; and, queries are usually classified according to search intent such as navigational, informational and transactional queries. Based on the observation that such kind of query categorization has high correlation with the user's different expectation on the result accuracy on different rank positions, we develop position-sensitive query-dependent loss functions exploring such kind of query categorization. Beyond the simple learning method that builds ranking functions with pre-defined query categorization, we further propose a new method that learns both ranking functions and query categorization simultaneously. We apply the query-dependent loss functions to two particular ranking algorithms, RankNet and ListMLE. Experimental results demonstrate that query-dependent loss functions can be exploited to significantly improve the accuracy of learned ranking functions. We also show that the ranking function jointly learned with query categorization can achieve better performance than that learned with pre-defined query categorization. Finally, we provide analysis and conduct additional experiments to gain deeper understanding on the advantages of ranking with query-dependent loss functions over other query-dependent ranking approaches and query-independent approaches.},
 acmid = {1718506},
 address = {New York, NY, USA},
 author = {Bian, Jiang and Liu, Tie-Yan and Qin, Tao and Zha, Hongyuan},
 booktitle = {Proceedings of the Third ACM International Conference on Web Search and Data Mining},
 doi = {10.1145/1718487.1718506},
 isbn = {978-1-60558-889-6},
 keyword = {query difference, query-dependent loss function, ranking for web search},
 link = {http://doi.acm.org/10.1145/1718487.1718506},
 location = {New York, New York, USA},
 numpages = {10},
 pages = {141--150},
 publisher = {ACM},
 series = {WSDM '10},
 title = {Ranking with Query-dependent Loss for Web Search},
 year = {2010}
}


@inproceedings{Weng:2010:TFT:1718487.1718520,
 abstract = {This paper focuses on the problem of identifying influential users of micro-blogging services. Twitter, one of the most notable micro-blogging services, employs a social-networking model called "following", in which each user can choose who she wants to "follow" to receive tweets from without requiring the latter to give permission first. In a dataset prepared for this study, it is observed that (1) 72.4% of the users in Twitter follow more than 80% of their followers, and (2) 80.5% of the users have 80% of users they are following follow them back. Our study reveals that the presence of "reciprocity" can be explained by phenomenon of homophily. Based on this finding, TwitterRank, an extension of PageRank algorithm, is proposed to measure the influence of users in Twitter. TwitterRank measures the influence taking both the topical similarity between users and the link structure into account. Experimental results show that TwitterRank outperforms the one Twitter currently uses and other related algorithms, including the original PageRank and Topic-sensitive PageRank.},
 acmid = {1718520},
 address = {New York, NY, USA},
 author = {Weng, Jianshu and Lim, Ee-Peng and Jiang, Jing and He, Qi},
 booktitle = {Proceedings of the Third ACM International Conference on Web Search and Data Mining},
 doi = {10.1145/1718487.1718520},
 isbn = {978-1-60558-889-6},
 keyword = {influential, pagerank, twitter},
 link = {http://doi.acm.org/10.1145/1718487.1718520},
 location = {New York, New York, USA},
 numpages = {10},
 pages = {261--270},
 publisher = {ACM},
 series = {WSDM '10},
 title = {TwitterRank: Finding Topic-sensitive Influential Twitterers},
 year = {2010}
}


@inproceedings{DasSarma:2010:SDO:1718487.1718537,
 abstract = {We study the fundamental problem of computing distances between nodes in large graphs such as the web graph and social networks. Our objective is to be able to answer distance queries between pairs of nodes in real time. Since the standard shortest path algorithms are expensive, our approach moves the time-consuming shortest-path computation offline, and at query time only looks up precomputed values and performs simple and fast computations on these precomputed values. More specifically, during the offline phase we compute and store a small "sketch" for each node in the graph, and at query-time we look up the sketches of the source and destination nodes and perform a simple computation using these two sketches to estimate the distance.},
 acmid = {1718537},
 address = {New York, NY, USA},
 author = {Das Sarma, Atish and Gollapudi, Sreenivas and Najork, Marc and Panigrahy, Rina},
 booktitle = {Proceedings of the Third ACM International Conference on Web Search and Data Mining},
 doi = {10.1145/1718487.1718537},
 isbn = {978-1-60558-889-6},
 keyword = {algorithms, distance computation, embedding, shortest path, sketching},
 link = {http://doi.acm.org/10.1145/1718487.1718537},
 location = {New York, New York, USA},
 numpages = {10},
 pages = {401--410},
 publisher = {ACM},
 series = {WSDM '10},
 title = {A Sketch-based Distance Oracle for Web-scale Graphs},
 year = {2010}
}


@inproceedings{Goel:2010:ALT:1718487.1718513,
 abstract = {The success of "infinite-inventory" retailers such as Amazon.com and Netflix has been ascribed to a "long tail" phenomenon. To wit, while the majority of their inventory is not in high demand, in aggregate these "worst sellers," unavailable at limited-inventory competitors, generate a significant fraction of total revenue. The long tail phenomenon, however, is in principle consistent with two fundamentally different theories. The first, and more popular hypothesis, is that a majority of consumers consistently follow the crowds and only a minority have any interest in niche content; the second hypothesis is that everyone is a bit eccentric, consuming both popular and specialty products. Based on examining extensive data on user preferences for movies, music, Web search, and Web browsing, we find overwhelming support for the latter theory. However, the observed eccentricity is much less than what is predicted by a fully random model whereby every consumer makes his product choices independently and proportional to product popularity; so consumers do indeed exhibit at least some a priori propensity toward either the popular or the exotic. Our findings thus suggest an additional factor in the success of infinite-inventory retailers, namely, that tail availability may boost head sales by offering consumers the convenience of "one-stop shopping" for both their mainstream and niche interests. This hypothesis is further supported by our theoretical analysis that presents a simple model in which shared inventory stores, such as Amazon Marketplace, gain a clear advantage by satisfying tail demand, helping to explain the emergence and increasing popularity of such retail arrangements. Hence, we believe that the return-on-investment (ROI) of niche products goes beyond direct revenue, extending to second-order gains associated with increased consumer satisfaction and repeat patronage. More generally, our findings call into question the conventional wisdom that specialty products only appeal to a minority of consumers.},
 acmid = {1718513},
 address = {New York, NY, USA},
 author = {Goel, Sharad and Broder, Andrei and Gabrilovich, Evgeniy and Pang, Bo},
 booktitle = {Proceedings of the Third ACM International Conference on Web Search and Data Mining},
 doi = {10.1145/1718487.1718513},
 isbn = {978-1-60558-889-6},
 keyword = {infinite inventory, long tail},
 link = {http://doi.acm.org/10.1145/1718487.1718513},
 location = {New York, New York, USA},
 numpages = {10},
 pages = {201--210},
 publisher = {ACM},
 series = {WSDM '10},
 title = {Anatomy of the Long Tail: Ordinary People with Extraordinary Tastes},
 year = {2010}
}


@inproceedings{Hillard:2010:IAR:1718487.1718532,
 abstract = {We describe a machine learning approach for predicting sponsored search ad relevance. Our baseline model incorporates basic features of text overlap and we then extend the model to learn from past user clicks on advertisements. We present a novel approach using translation models to learn user click propensity from sparse click logs. Our relevance predictions are then applied to multiple sponsored search applications in both offline editorial evaluations and live online user tests. The predicted relevance score is used to improve the quality of the search page in three areas: filtering low quality ads, more accurate ranking for ads, and optimized page placement of ads to reduce prominent placement of low relevance ads. We show significant gains across all three tasks.},
 acmid = {1718532},
 address = {New York, NY, USA},
 author = {Hillard, Dustin and Schroedl, Stefan and Manavoglu, Eren and Raghavan, Hema and Leggetter, Chirs},
 booktitle = {Proceedings of the Third ACM International Conference on Web Search and Data Mining},
 doi = {10.1145/1718487.1718532},
 isbn = {978-1-60558-889-6},
 keyword = {advertising, clicks, relevance modeling, translation},
 link = {http://doi.acm.org/10.1145/1718487.1718532},
 location = {New York, New York, USA},
 numpages = {10},
 pages = {361--370},
 publisher = {ACM},
 series = {WSDM '10},
 title = {Improving Ad Relevance in Sponsored Search},
 year = {2010}
}


@inproceedings{Koppula:2010:LUP:1718487.1718535,
 abstract = {Presence of duplicate documents in the World Wide Web adversely affects crawling, indexing and relevance, which are the core building blocks of web search. In this paper, we present a set of techniques to mine rules from URLs and utilize these rules for de-duplication using just URL strings without fetching the content explicitly. Our technique is composed of mining the crawl logs and utilizing clusters of similar pages to extract transformation rules, which are used to normalize URLs belonging to each cluster. Preserving each mined rule for de-duplication is not efficient due to the large number of such rules. We present a machine learning technique to generalize the set of rules, which reduces the resource footprint to be usable at web-scale. The rule extraction techniques are robust against web-site specific URL conventions. We compare the precision and scalability of our approach with recent efforts in using URLs for de-duplication. Experimental results demonstrate that our approach achieves 2 times more reduction in duplicates with only half the rules compared to the most recent previous approach. Scalability of the framework is demonstrated by performing a large scale evaluation on a set of 3 Billion URLs, implemented using the MapReduce framework.},
 acmid = {1718535},
 address = {New York, NY, USA},
 author = {Koppula, Hema Swetha and Leela, Krishna P. and Agarwal, Amit and Chitrapura, Krishna Prasad and Garg, Sachin and Sasturkar, Amit},
 booktitle = {Proceedings of the Third ACM International Conference on Web Search and Data Mining},
 doi = {10.1145/1718487.1718535},
 isbn = {978-1-60558-889-6},
 keyword = {decision trees, generalization, mapreduce, page importance, search engines, site-specific delimiters, webpage de-duplication},
 link = {http://doi.acm.org/10.1145/1718487.1718535},
 location = {New York, New York, USA},
 numpages = {10},
 pages = {381--390},
 publisher = {ACM},
 series = {WSDM '10},
 title = {Learning URL Patterns for Webpage De-duplication},
 year = {2010}
}


@inproceedings{Zhu:2010:NCM:1718487.1718528,
 abstract = {Recent advances in click model have positioned it as an attractive method for representing user preferences in web search and online advertising. Yet, most of the existing works focus on training the click model for individual queries, and cannot accurately model the tail queries due to the lack of training data. Simultaneously, most of the existing works consider the query, url and position, neglecting some other important attributes in click log data, such as the local time. Obviously, the click through rate is different between daytime and midnight. In this paper, we propose a novel click model based on Bayesian network, which is capable of modeling the tail queries because it builds the click model on attribute values, with those values being shared across queries. We called our work General Click Model (GCM) as we found that most of the existing works can be special cases of GCM by assigning different parameters. Experimental results on a large-scale commercial advertisement dataset show that GCM can significantly and consistently lead to better results as compared to the state-of-the-art works.},
 acmid = {1718528},
 address = {New York, NY, USA},
 author = {Zhu, Zeyuan Allen and Chen, Weizhu and Minka, Tom and Zhu, Chenguang and Chen, Zheng},
 booktitle = {Proceedings of the Third ACM International Conference on Web Search and Data Mining},
 doi = {10.1145/1718487.1718528},
 isbn = {978-1-60558-889-6},
 keyword = {advertisement, attribute, bayesian, gaussian, search engine},
 link = {http://doi.acm.org/10.1145/1718487.1718528},
 location = {New York, New York, USA},
 numpages = {10},
 pages = {321--330},
 publisher = {ACM},
 series = {WSDM '10},
 title = {A Novel Click Model and Its Applications to Online Advertising},
 year = {2010}
}


@proceedings{Baeza-Yates:2009:1498759,
 abstract = {WSDM (pronounced "wisdom") is a young ACM conference intended to be the publication venue for research in the areas of Web search and data mining. Indeed, the importance of these topics and their pace of innovation prevent proper coverage by conferences of broader scope, as shown in the first edition held last February in Stanford, USA, which attracted more people than we expected. The need for this new conference was also recognized by the sponsorship of four ACM SIGs: SIGIR, SIGKDD, SIGMOD and SIGWEB, which have also allowed to have the content of the conference freely available in the Web for the first three years. This trend continued this year across the Atlantic, in Barcelona, one of the most visited European cities. Barcelona, the capital of the autonomous community of Catalonia, is well known for its history, architecture and culture. However, recently it has also developed a technological neighborhood, 22@, where the conference venue is located. Providing users with appropriate and relevant search results is a challenging task. Not only the interests and preferences of people change over time, but they also vary drastically from user to user; what is relevant and significant to a user might appear uninteresting and even offensive to another user. Further, Web pages change constantly and spamming becomes more dynamic and complex by the day. To cope with these challenges, search engines need to evolve to provide for personalized, context-sensitive, adaptive search, which uses information explicitly or implicitly offered by the user to improve results. Often, such systems will have to exploit classification techniques and to analyze data extracted from Web documents and/or from query logs to improve precision or to diversify search results. Plain and simple, search is hard! This time we received 170 papers from all around the world, and 29 of them were selected (17% acceptance ratio). The varied set of challenging problems in Web search and the key role that technologies like data mining and classification play in the solutions can be appreciated in the program of this second WSDM conference. It provides a fresh snapshot of the state-of-art in Web search; one that is broad, deep, and challenging to all those interested in research on this fascinating area of technology.},
 address = {New York, NY, USA},
 editor = {Baeza-Yates, Ricardo and Boldi, Paolo and Ribeiro-Neto, Berthier and Cambazoglu, B. Barla},
 isbn = {978-1-60558-390-7},
 location = {Barcelona, Spain},
 publisher = {ACM},
 title = {WSDM '09: Proceedings of the Second ACM International Conference on Web Search and Data Mining},
 year = {2009}
}


@inproceedings{Mislove:2010:YYK:1718487.1718519,
 abstract = {Online social networks are now a popular way for users to connect, express themselves, and share content. Users in today's online social networks often post a profile, consisting of attributes like geographic location, interests, and schools attended. Such profile information is used on the sites as a basis for grouping users, for sharing content, and for suggesting users who may benefit from interaction. However, in practice, not all users provide these attributes. In this paper, we ask the question: given attributes for some fraction of the users in an online social network, can we infer the attributes of the remaining users? In other words, can the attributes of users, in combination with the social network graph, be used to predict the attributes of another user in the network? To answer this question, we gather fine-grained data from two social networks and try to infer user profile attributes. We find that users with common attributes are more likely to be friends and often form dense communities, and we propose a method of inferring user attributes that is inspired by previous approaches to detecting communities in social networks. Our results show that certain user attributes can be inferred with high accuracy when given information on as little as 20% of the users.},
 acmid = {1718519},
 address = {New York, NY, USA},
 author = {Mislove, Alan and Viswanath, Bimal and Gummadi, Krishna P. and Druschel, Peter},
 booktitle = {Proceedings of the Third ACM International Conference on Web Search and Data Mining},
 doi = {10.1145/1718487.1718519},
 isbn = {978-1-60558-889-6},
 keyword = {communities, inferring attributes, social networks},
 link = {http://doi.acm.org/10.1145/1718487.1718519},
 location = {New York, New York, USA},
 numpages = {10},
 pages = {251--260},
 publisher = {ACM},
 series = {WSDM '10},
 title = {You Are Who You Know: Inferring User Profiles in Online Social Networks},
 year = {2010}
}


@inproceedings{Anagnostopoulos:2010:OFQ:1718487.1718508,
 abstract = {Query recommendation is an integral part of modern search engines. The goal of query recommendation is to facilitate users while searching for information. Query recommendation also allows users to explore concepts related to their information needs. In this paper, we present a formal treatment of the problem of query recommendation. In our framework we model the querying behavior of users by a probabilistic reformula- tion graph, or query-flow graph [Boldi et al. CIKM 2008]. A sequence of queries submitted by a user can be seen as a path on this graph. Assigning score values to queries allows us to define suitable utility functions and to consider the expected utility achieved by a reformulation path on the query-flow graph. Providing recommendations can be seen as adding shortcuts in the query-flow graph that "nudge" the reformulation paths of users, in such a way that users are more likely to follow paths with larger expected utility. We discuss in detail the most important questions that arise in the proposed framework. In particular, we provide examples of meaningful utility functions to optimize, we discuss how to estimate the effect of recommendations on the reformulation probabilities, we address the complexity of the optimization problems that we consider, we suggest efficient algorithmic solutions, and we validate our models and algorithms with extensive experimentation. Our techniques can be applied to other scenarios where user behavior can be modeled as a Markov process.},
 acmid = {1718508},
 address = {New York, NY, USA},
 author = {Anagnostopoulos, Aris and Becchetti, Luca and Castillo, Carlos and Gionis, Aristides},
 booktitle = {Proceedings of the Third ACM International Conference on Web Search and Data Mining},
 doi = {10.1145/1718487.1718508},
 isbn = {978-1-60558-889-6},
 keyword = {query reformulations, query suggestions},
 link = {http://doi.acm.org/10.1145/1718487.1718508},
 location = {New York, New York, USA},
 numpages = {10},
 pages = {161--170},
 publisher = {ACM},
 series = {WSDM '10},
 title = {An Optimization Framework for Query Recommendation},
 year = {2010}
}


@inproceedings{Xu:2010:IQT:1718487.1718509,
 abstract = {In information retrieval, relevance of documents with respect to queries is usually judged by humans, and used in evaluation and/or learning of ranking functions. Previous work has shown that certain level of noise in relevance judgments has little effect on evaluation, especially for comparison purposes. Recently learning to rank has become one of the major means to create ranking models in which the models are automatically learned from the data derived from a large number of relevance judgments. As far as we know, there was no previous work about quality of training data for learning to rank, and this paper tries to study the issue. Specifically, we address three problems. Firstly, we show that the quality of training data labeled by humans has critical impact on the performance of learning to rank algorithms. Secondly, we propose detecting relevance judgment errors using click-through data accumulated at a search engine. Two discriminative models, referred to as sequential dependency model and full dependency model, are proposed to make the detection. Both models consider the conditional dependency of relevance labels and thus are more powerful than the conditionally independent model previously proposed for other tasks. Finally, we verify that using training data in which the errors are detected and corrected by our method, we can improve the performance of learning to rank algorithms.},
 acmid = {1718509},
 address = {New York, NY, USA},
 author = {Xu, Jingfang and Chen, Chuanliang and Xu, Gu and Li, Hang and Abib, Elbio Renato Torres},
 booktitle = {Proceedings of the Third ACM International Conference on Web Search and Data Mining},
 doi = {10.1145/1718487.1718509},
 isbn = {978-1-60558-889-6},
 keyword = {judgment error correction, relevance label prediction, training data quality},
 link = {http://doi.acm.org/10.1145/1718487.1718509},
 location = {New York, New York, USA},
 numpages = {10},
 pages = {171--180},
 publisher = {ACM},
 series = {WSDM '10},
 title = {Improving Quality of Training Data for Learning to Rank Using Click-through Data},
 year = {2010}
}


@inproceedings{Sizov:2010:GLS:1718487.1718522,
 abstract = {We describe an approach for multi-modal characterization of social media by combining text features (e.g. tags as a prominent example of short, unstructured text labels) with spatial knowledge (e.g. geotags and coordinates of images and videos). Our model-based framework GeoFolk combines these two aspects in order to construct better algorithms for content management, retrieval, and sharing. The approach is based on multi-modal Bayesian models which allow us to integrate spatial semantics of social media in a well-formed, probabilistic manner. We systematically evaluate the solution on a subset of Flickr data, in characteristic scenarios of tag recommendation, content classification, and clustering. Experimental results show that our method outperforms baseline techniques that are based on one of the aspects alone. The approach described in this contribution can also be used in other domains such as Geoweb retrieval.},
 acmid = {1718522},
 address = {New York, NY, USA},
 author = {Sizov, Sergej},
 booktitle = {Proceedings of the Third ACM International Conference on Web Search and Data Mining},
 doi = {10.1145/1718487.1718522},
 isbn = {978-1-60558-889-6},
 keyword = {bayesian learning, geodata, mcmc, tagging, web2.0},
 link = {http://doi.acm.org/10.1145/1718487.1718522},
 location = {New York, New York, USA},
 numpages = {10},
 pages = {281--290},
 publisher = {ACM},
 series = {WSDM '10},
 title = {GeoFolk: Latent Spatial Semantics in Web 2.0 Social Media},
 year = {2010}
}


@inproceedings{Byers:2010:AWD:1718487.1718529,
 abstract = {Attributing a dollar value to a keyword is an essential part of running any profitable search engine advertising campaign. When an advertiser has complete control over the interaction with and monetization of each user arriving on a given keyword, the value of that term can be accurately tracked. However, in many instances, the advertiser may monetize arrivals indirectly through one or more third parties. In such cases, it is typical for the third party to provide only coarse-grained reporting: rather than report each monetization event, users are aggregated into larger channels and the third party reports aggregate information such as total daily revenue for each channel. Examples of third parties that use channels include Amazon and Google AdSense. In such scenarios, the number of channels is generally much smaller than the number of keywords whose value per click (VPC) we wish to learn. However, the advertiser has flexibility as to how to assign keywords to channels over time. We introduce the channelization problem: how do we adaptively assign keywords to channels over the course of multiple days to quickly obtain accurate VPC estimates of all keywords? We relate this problem to classical results in weighing design, devise new adaptive algorithms for this problem, and quantify the performance of these algorithms experimentally. Our results demonstrate that adaptive weighing designs that exploit statistics of term frequency, variability in VPCs across keywords, and flexible channel assignments over time provide the best estimators of keyword VPCs.},
 acmid = {1718529},
 address = {New York, NY, USA},
 author = {Byers, John W. and Mitzenmacher, Michael and Zervas, Georgios},
 booktitle = {Proceedings of the Third ACM International Conference on Web Search and Data Mining},
 doi = {10.1145/1718487.1718529},
 isbn = {978-1-60558-889-6},
 keyword = {design of experiments, least squares, regression, weighing designs},
 link = {http://doi.acm.org/10.1145/1718487.1718529},
 location = {New York, New York, USA},
 numpages = {10},
 pages = {331--340},
 publisher = {ACM},
 series = {WSDM '10},
 title = {Adaptive Weighing Designs for Keyword Value Computation},
 year = {2010}
}


@inproceedings{Dang:2010:QRU:1718487.1718493,
 abstract = {Query reformulation techniques based on query logs have been studied as a method of capturing user intent and improving retrieval effectiveness. The evaluation of these techniques has primarily, however, focused on proprietary query logs and selected samples of queries. In this paper, we suggest that anchor text, which is readily available, can be an effective substitute for a query log and study the effectiveness of a range of query reformulation techniques (including log-based stemming, substitution, and expansion) using standard TREC collections. Our results show that log-based query reformulation techniques are indeed effective with standard collections, but expansion is a much safer form of query modification than word substitution. We also show that using anchor text as a simulated query log is as least as effective as a real log for these techniques.},
 acmid = {1718493},
 address = {New York, NY, USA},
 author = {Dang, Van and Croft, Bruce W.},
 booktitle = {Proceedings of the Third ACM International Conference on Web Search and Data Mining},
 doi = {10.1145/1718487.1718493},
 isbn = {978-1-60558-889-6},
 keyword = {anchor log, anchor text, query expansion, query log, query reformulation, query substitution},
 link = {http://doi.acm.org/10.1145/1718487.1718493},
 location = {New York, New York, USA},
 numpages = {10},
 pages = {41--50},
 publisher = {ACM},
 series = {WSDM '10},
 title = {Query Reformulation Using Anchor Text},
 year = {2010}
}


@inproceedings{Zhou:2010:DCQ:1718487.1718503,
 abstract = {As the Web provides rich data embedded in the immense contents inside pages, we witness many ad-hoc efforts for exploiting fine granularity information across Web text, such as Web information extraction, typed-entity search, and question answering. To unify and generalize these efforts, this paper proposes a general search system--Data-oriented Content Query System(DoCQS)--to search directly into document contents for finding relevant values of desired data types. Motivated by the current limitations, we start by distilling the essential capabilities needed by such content querying. The capabilities call for a conceptually relational model, upon which we design a powerful Content Query Language (CQL). For efficient processing, we design novel index structures and query processing algorithms. We evaluate our proposal over two concrete domains of realistic Web corpora, demonstrating that our query language is rather flexible and expressive, and our query processing is efficient with reasonable index overhead.},
 acmid = {1718503},
 address = {New York, NY, USA},
 author = {Zhou, Mianwei and Cheng, Tao and Chang, Kevin Chen-Chuan},
 booktitle = {Proceedings of the Third ACM International Conference on Web Search and Data Mining},
 doi = {10.1145/1718487.1718503},
 isbn = {978-1-60558-889-6},
 keyword = {content query, content query language, contextual index, data oriented, inverted index, joint index},
 link = {http://doi.acm.org/10.1145/1718487.1718503},
 location = {New York, New York, USA},
 numpages = {10},
 pages = {121--130},
 publisher = {ACM},
 series = {WSDM '10},
 title = {Data-oriented Content Query System: Searching for Data into Text on the Web},
 year = {2010}
}


@inproceedings{Ganti:2010:PSF:1718487.1718496,
 abstract = {Query intent classification is crucial for web search and advertising. It is known to be challenging because web queries contain less than three words on average, and so provide little signal to base classification decisions on. At the same time, the vocabulary used in search queries is vast: thus, classifiers based on word-occurrence have to deal with a very sparse feature space, and often require large amounts of training data. Prior efforts to address the issue of feature sparseness augmented the feature space using features computed from the results obtained by issuing the query to be classified against a web search engine. However, these approaches induce high latency, making them unacceptable in practice. In this paper, we propose a new class of features that realizes the benefit of search-based features without high latency. These leverage co-occurrence between the query keywords and tags applied to documents in search results, resulting in a significant boost to web query classification accuracy. By pre-computing the tag incidence for a suitably chosen set of keyword-combinations, we are able to generate the features online with low latency and memory requirements. We evaluate the accuracy of our approach using a large corpus of real web queries in the context of commercial search.},
 acmid = {1718496},
 address = {New York, NY, USA},
 author = {Ganti, Venkatesh and K\"{o}nig, Arnd Christian and Li, Xiao},
 booktitle = {Proceedings of the Third ACM International Conference on Web Search and Data Mining},
 doi = {10.1145/1718487.1718496},
 isbn = {978-1-60558-889-6},
 keyword = {query classification, vertical search, web search},
 link = {http://doi.acm.org/10.1145/1718487.1718496},
 location = {New York, New York, USA},
 numpages = {10},
 pages = {61--70},
 publisher = {ACM},
 series = {WSDM '10},
 title = {Precomputing Search Features for Fast and Accurate Query Classification},
 year = {2010}
}


@proceedings{Davison:2010:1718487,
 abstract = {We welcome you to the Third ACM International Conference on Web Search and Data Mining (WSDM) held February 3--6, 2010 in New York City. As a premier conference in the field, WSDM 2010 provides a highly competitive forum for reporting the latest developments in the research and application of Web search and data mining. We are pleased to present the proceedings of the conference as its published record. WSDM (pronounced "wisdom") is a young conference for research in the areas of search and retrieval, Web mining, economics implications, and in depth analysis of accuracy and performance of search and mining systems. Although it is only in its third year, it has already witnessed significant growth. As evidence of that, WSDM 2010 received a record 290 submissions, representing a 70% increase compared to WSDM 2009. The conference accepted 45 papers (15.5%). The authors of submitted papers come from 35 countries and regions. Authors of accepted papers are from 11 countries.},
 address = {New York, NY, USA},
 isbn = {978-1-60558-889-6},
 location = {New York, New York, USA},
 note = {618103},
 publisher = {ACM},
 title = {WSDM '10: Proceedings of the Third ACM International Conference on Web Search and Data Mining},
 year = {2010}
}


@inproceedings{Wetzker:2010:ITY:1718487.1718497,
 abstract = {Collaborative tagging services (folksonomies) have been among the stars of the Web 2.0 era. They allow their users to label diverse resources with freely chosen keywords (tags). Our studies of two real-world folksonomies unveil that individual users develop highly personalized vocabularies of tags. While these meet individual needs and preferences, the considerable differences between personal tag vocabularies (personomies) impede services such as social search or customized tag recommendation. In this paper, we introduce a novel user-centric tag model that allows us to derive mappings between personal tag vocabularies and the corresponding folksonomies. Using these mappings, we can infer the meaning of user-assigned tags and can predict choices of tags a user may want to assign to new items. Furthermore, our translational approach helps in reducing common problems related to tag ambiguity, synonymous tags, or multilingualism. We evaluate the applicability of our method in tag recommendation and tag-based social search. Extensive experiments show that our translational model improves the prediction accuracy in both scenarios.},
 acmid = {1718497},
 address = {New York, NY, USA},
 author = {Wetzker, Robert and Zimmermann, Carsten and Bauckhage, Christian and Albayrak, Sahin},
 booktitle = {Proceedings of the Third ACM International Conference on Web Search and Data Mining},
 doi = {10.1145/1718487.1718497},
 isbn = {978-1-60558-889-6},
 keyword = {folksonomies, social search, tag recommendation, tagging, user modeling},
 link = {http://doi.acm.org/10.1145/1718487.1718497},
 location = {New York, New York, USA},
 numpages = {10},
 pages = {71--80},
 publisher = {ACM},
 series = {WSDM '10},
 title = {I Tag, You Tag: Translating Tags for Advanced User Models},
 year = {2010}
}


@inproceedings{Cambazoglu:2010:EEO:1718487.1718538,
 abstract = {Some commercial web search engines rely on sophisticated machine learning systems for ranking web documents. Due to very large collection sizes and tight constraints on query response times, online efficiency of these learning systems forms a bottleneck. An important problem in such systems is to speedup the ranking process without sacrificing much from the quality of results. In this paper, we propose optimization strategies that allow short-circuiting score computations in additive learning systems. The strategies are evaluated over a state-of-the-art machine learning system and a large, real-life query log, obtained from Yahoo!. By the proposed strategies, we are able to speedup the score computations by more than four times with almost no loss in result quality.},
 acmid = {1718538},
 address = {New York, NY, USA},
 author = {Cambazoglu, B. Barla and Zaragoza, Hugo and Chapelle, Olivier and Chen, Jiang and Liao, Ciya and Zheng, Zhaohui and Degenhardt, Jon},
 booktitle = {Proceedings of the Third ACM International Conference on Web Search and Data Mining},
 doi = {10.1145/1718487.1718538},
 isbn = {978-1-60558-889-6},
 keyword = {early exit, machine learning, optimization, web search},
 link = {http://doi.acm.org/10.1145/1718487.1718538},
 location = {New York, New York, USA},
 numpages = {10},
 pages = {411--420},
 publisher = {ACM},
 series = {WSDM '10},
 title = {Early Exit Optimizations for Additive Machine Learned Ranking Systems},
 year = {2010}
}


@inproceedings{Carlson:2010:CSL:1718487.1718501,
 abstract = {We consider the problem of semi-supervised learning to extract categories (e.g., academic fields, athletes) and relations (e.g., PlaysSport(athlete, sport)) from web pages, starting with a handful of labeled training examples of each category or relation, plus hundreds of millions of unlabeled web documents. Semi-supervised training using only a few labeled examples is typically unreliable because the learning task is underconstrained. This paper pursues the thesis that much greater accuracy can be achieved by further constraining the learning task, by coupling the semi-supervised training of many extractors for different categories and relations. We characterize several ways in which the training of category and relation extractors can be coupled, and present experimental results demonstrating significantly improved accuracy as a result.},
 acmid = {1718501},
 address = {New York, NY, USA},
 author = {Carlson, Andrew and Betteridge, Justin and Wang, Richard C. and Hruschka,Jr., Estevam R. and Mitchell, Tom M.},
 booktitle = {Proceedings of the Third ACM International Conference on Web Search and Data Mining},
 doi = {10.1145/1718487.1718501},
 isbn = {978-1-60558-889-6},
 keyword = {bootstrap learning, information extraction, semi-supervised learning, web mining},
 link = {http://doi.acm.org/10.1145/1718487.1718501},
 location = {New York, New York, USA},
 numpages = {10},
 pages = {101--110},
 publisher = {ACM},
 series = {WSDM '10},
 title = {Coupled Semi-supervised Learning for Information Extraction},
 year = {2010}
}


@inproceedings{Hassan:2010:BDU:1718487.1718515,
 abstract = {Web search engines are traditionally evaluated in terms of the relevance of web pages to individual queries. However, relevance of web pages does not tell the complete picture, since an individual query may represent only a piece of the user's information need and users may have different information needs underlying the same queries. In this work, we address the problem of predicting user search goal success by modeling user behavior. We show empirically that user behavior alone can give an accurate picture of the success of the user's web search goals, without considering the relevance of the documents displayed. In fact, our experiments show that models using user behavior are more predictive of goal success than those using document relevance. We build novel sequence models incorporating time distributions for this task and our experiments show that the sequence and time distribution models are more accurate than static models based on user behavior, or predictions based on document relevance.},
 acmid = {1718515},
 address = {New York, NY, USA},
 author = {Hassan, Ahmed and Jones, Rosie and Klinkner, Kristina Lisa},
 booktitle = {Proceedings of the Third ACM International Conference on Web Search and Data Mining},
 doi = {10.1145/1718487.1718515},
 isbn = {978-1-60558-889-6},
 keyword = {query log analysis, search engine evaluation, search sessions, user behavior models, user satisfaction},
 link = {http://doi.acm.org/10.1145/1718487.1718515},
 location = {New York, New York, USA},
 numpages = {10},
 pages = {221--230},
 publisher = {ACM},
 series = {WSDM '10},
 title = {Beyond DCG: User Behavior As a Predictor of a Successful Search},
 year = {2010}
}


@inproceedings{Goyal:2010:LIP:1718487.1718518,
 abstract = {Recently, there has been tremendous interest in the phenomenon of influence propagation in social networks. The studies in this area assume they have as input to their problems a social graph with edges labeled with probabilities of influence between users. However, the question of where these probabilities come from or how they can be computed from real social network data has been largely ignored until now. Thus it is interesting to ask whether from a social graph and a log of actions by its users, one can build models of influence. This is the main problem attacked in this paper. In addition to proposing models and algorithms for learning the model parameters and for testing the learned models to make predictions, we also develop techniques for predicting the time by which a user may be expected to perform an action. We validate our ideas and techniques using the Flickr data set consisting of a social graph with 1.3M nodes, 40M edges, and an action log consisting of 35M tuples referring to 300K distinct actions. Beyond showing that there is genuine influence happening in a real social network, we show that our techniques have excellent prediction performance.},
 acmid = {1718518},
 address = {New York, NY, USA},
 author = {Goyal, Amit and Bonchi, Francesco and Lakshmanan, Laks V.S.},
 booktitle = {Proceedings of the Third ACM International Conference on Web Search and Data Mining},
 doi = {10.1145/1718487.1718518},
 isbn = {978-1-60558-889-6},
 keyword = {influence, social networks, viral marketing},
 link = {http://doi.acm.org/10.1145/1718487.1718518},
 location = {New York, New York, USA},
 numpages = {10},
 pages = {241--250},
 publisher = {ACM},
 series = {WSDM '10},
 title = {Learning Influence Probabilities in Social Networks},
 year = {2010}
}


@inproceedings{Rendle:2010:PIT:1718487.1718498,
 abstract = {Tagging plays an important role in many recent websites. Recommender systems can help to suggest a user the tags he might want to use for tagging a specific item. Factorization models based on the Tucker Decomposition (TD) model have been shown to provide high quality tag recommendations outperforming other approaches like PageRank, FolkRank, collaborative filtering, etc. The problem with TD models is the cubic core tensor resulting in a cubic runtime in the factorization dimension for prediction and learning. In this paper, we present the factorization model PITF (Pairwise Interaction Tensor Factorization) which is a special case of the TD model with linear runtime both for learning and prediction. PITF explicitly models the pairwise interactions between users, items and tags. The model is learned with an adaption of the Bayesian personalized ranking (BPR) criterion which originally has been introduced for item recommendation. Empirically, we show on real world datasets that this model outperforms TD largely in runtime and even can achieve better prediction quality. Besides our lab experiments, PITF has also won the ECML/PKDD Discovery Challenge 2009 for graph-based tag recommendation.},
 acmid = {1718498},
 address = {New York, NY, USA},
 author = {Rendle, Steffen and Schmidt-Thieme, Lars},
 booktitle = {Proceedings of the Third ACM International Conference on Web Search and Data Mining},
 doi = {10.1145/1718487.1718498},
 isbn = {978-1-60558-889-6},
 keyword = {personalization, recommender systems, tag recommendation, tensor factorization},
 link = {http://doi.acm.org/10.1145/1718487.1718498},
 location = {New York, New York, USA},
 numpages = {10},
 pages = {81--90},
 publisher = {ACM},
 series = {WSDM '10},
 title = {Pairwise Interaction Tensor Factorization for Personalized Tag Recommendation},
 year = {2010}
}


@inproceedings{Dong:2010:TRR:1718487.1718490,
 abstract = {In web search, recency ranking refers to ranking documents by relevance which takes freshness into account. In this paper, we propose a retrieval system which automatically detects and responds to recency sensitive queries. The system detects recency sensitive queries using a high precision classifier. The system responds to recency sensitive queries by using a machine learned ranking model trained for such queries. We use multiple recency features to provide temporal evidence which effectively represents document recency. Furthermore, we propose several training methodologies important for training recency sensitive rankers. Finally, we develop new evaluation metrics for recency sensitive queries. Our experiments demonstrate the efficacy of the proposed approaches.},
 acmid = {1718490},
 address = {New York, NY, USA},
 author = {Dong, Anlei and Chang, Yi and Zheng, Zhaohui and Mishne, Gilad and Bai, Jing and Zhang, Ruiqiang and Buchner, Karolina and Liao, Ciya and Diaz, Fernando},
 booktitle = {Proceedings of the Third ACM International Conference on Web Search and Data Mining},
 doi = {10.1145/1718487.1718490},
 isbn = {978-1-60558-889-6},
 keyword = {recency ranking, recency sensitive query classification, temporal},
 link = {http://doi.acm.org/10.1145/1718487.1718490},
 location = {New York, New York, USA},
 numpages = {10},
 pages = {11--20},
 publisher = {ACM},
 series = {WSDM '10},
 title = {Towards Recency Ranking in Web Search},
 year = {2010}
}


@inproceedings{Kohlschutter:2010:BDU:1718487.1718542,
 abstract = {In addition to the actual content Web pages consist of navigational elements, templates, and advertisements. This boilerplate text typically is not related to the main content, may deteriorate search precision and thus needs to be detected properly. In this paper, we analyze a small set of shallow text features for classifying the individual text elements in a Web page. We compare the approach to complex, state-of-the-art techniques and show that competitive accuracy can be achieved, at almost no cost. Moreover, we derive a simple and plausible stochastic model for describing the boilerplate creation process. With the help of our model, we also quantify the impact of boilerplate removal to retrieval performance and show significant improvements over the baseline. Finally, we extend the principled approach by straight-forward heuristics, achieving a remarkable detection accuracy.},
 acmid = {1718542},
 address = {New York, NY, USA},
 author = {Kohlsch\"{u}tter, Christian and Fankhauser, Peter and Nejdl, Wolfgang},
 booktitle = {Proceedings of the Third ACM International Conference on Web Search and Data Mining},
 doi = {10.1145/1718487.1718542},
 isbn = {978-1-60558-889-6},
 keyword = {boilerplate removal, full-text extraction, template detection, text cleaning, web document modeling},
 link = {http://doi.acm.org/10.1145/1718487.1718542},
 location = {New York, New York, USA},
 numpages = {10},
 pages = {441--450},
 publisher = {ACM},
 series = {WSDM '10},
 title = {Boilerplate Detection Using Shallow Text Features},
 year = {2010}
}


@inproceedings{Galland:2010:CID:1718487.1718504,
 abstract = {We consider a set of views stating possibly conflicting facts. Negative facts in the views may come, e.g., from functional dependencies in the underlying database schema. We want to predict the truth values of the facts. Beyond simple methods such as voting (typically rather accurate), we explore techniques based on "corroboration", i.e., taking into account trust in the views. We introduce three fixpoint algorithms corresponding to different levels of complexity of an underlying probabilistic model. They all estimate both truth values of facts and trust in the views. We present experimental studies on synthetic and real-world data. This analysis illustrates how and in which context these methods improve corroboration results over baseline methods. We believe that corroboration can serve in a wide range of applications such as source selection in the semantic Web, data quality assessment or semantic annotation cleaning in social networks. This work sets the bases for a wide range of techniques for solving these more complex problems.},
 acmid = {1718504},
 address = {New York, NY, USA},
 author = {Galland, Alban and Abiteboul, Serge and Marian, Am{\'e}lie and Senellart, Pierre},
 booktitle = {Proceedings of the Third ACM International Conference on Web Search and Data Mining},
 doi = {10.1145/1718487.1718504},
 isbn = {978-1-60558-889-6},
 keyword = {confidence, contradiction, corroboration, fix-point, probabilistic model, view},
 link = {http://doi.acm.org/10.1145/1718487.1718504},
 location = {New York, New York, USA},
 numpages = {10},
 pages = {131--140},
 publisher = {ACM},
 series = {WSDM '10},
 title = {Corroborating Information from Disagreeing Views},
 year = {2010}
}


@inproceedings{Bendersky:2010:LCI:1718487.1718492,
 abstract = {Modeling query concepts through term dependencies has been shown to have a significant positive effect on retrieval performance, especially for tasks such as web search, where relevance at high ranks is particularly critical. Most previous work, however, treats all concepts as equally important, an assumption that often does not hold, especially for longer, more complex queries. In this paper, we show that one of the most effective existing term dependence models can be naturally extended by assigning weights to concepts. We demonstrate that the weighted dependence model can be trained using existing learning-to-rank techniques, even with a relatively small number of training queries. Our study compares the effectiveness of both endogenous (collection-based) and exogenous (based on external sources) features for determining concept importance. To test the weighted dependence model, we perform experiments on both publicly available TREC corpora and a proprietary web corpus. Our experimental results indicate that our model consistently and significantly outperforms both the standard bag-of-words model and the unweighted term dependence model, and that combining endogenous and exogenous features generally results in the best retrieval effectiveness.},
 acmid = {1718492},
 address = {New York, NY, USA},
 author = {Bendersky, Michael and Metzler, Donald and Croft, W. Bruce},
 booktitle = {Proceedings of the Third ACM International Conference on Web Search and Data Mining},
 doi = {10.1145/1718487.1718492},
 isbn = {978-1-60558-889-6},
 keyword = {query concept weighting, weighted dependence model},
 link = {http://doi.acm.org/10.1145/1718487.1718492},
 location = {New York, New York, USA},
 numpages = {10},
 pages = {31--40},
 publisher = {ACM},
 series = {WSDM '10},
 title = {Learning Concept Importance Using a Weighted Dependence Model},
 year = {2010}
}


@inproceedings{Elsas:2010:LTD:1718487.1718489,
 abstract = {Many web documents are dynamic, with content changing in varying amounts at varying frequencies. However, current document search algorithms have a static view of the document content, with only a single version of the document in the index at any point in time. In this paper, we present the first published analysis of using the temporal dynamics of document content to improve relevance ranking. We show that there is a strong relationship between the amount and frequency of content change and relevance. We develop a novel probabilistic document ranking algorithm that allows differential weighting of terms based on their temporal characteristics. By leveraging such content dynamics we show significant performance improvements for navigational queries.},
 acmid = {1718489},
 address = {New York, NY, USA},
 author = {Elsas, Jonathan L. and Dumais, Susan T.},
 booktitle = {Proceedings of the Third ACM International Conference on Web Search and Data Mining},
 doi = {10.1145/1718487.1718489},
 isbn = {978-1-60558-889-6},
 keyword = {temporal change, versioned documents, web search},
 link = {http://doi.acm.org/10.1145/1718487.1718489},
 location = {New York, New York, USA},
 numpages = {10},
 pages = {1--10},
 publisher = {ACM},
 series = {WSDM '10},
 title = {Leveraging Temporal Dynamics of Document Content in Relevance Ranking},
 year = {2010}
}


@inproceedings{Cheng:2010:PCP:1718487.1718531,
 abstract = {Sponsored search is a multi-billion dollar business that generates most of the revenue for search engines. Predicting the probability that users click on ads is crucial to sponsored search because the prediction is used to influence ranking, filtering, placement, and pricing of ads. Ad ranking, filtering and placement have a direct impact on the user experience, as users expect the most useful ads to rank high and be placed in a prominent position on the page. Pricing impacts the advertisers' return on their investment and revenue for the search engine. The objective of this paper is to present a framework for the personalization of click models in sponsored search. We develop user-specific and demographic-based features that reflect the click behavior of individuals and groups. The features are based on observations of search and click behaviors of a large number of users of a commercial search engine. We add these features to a baseline non-personalized click model and perform experiments on offline test sets derived from user logs as well as on live traffic. Our results demonstrate that the personalized models significantly improve the accuracy of click prediction.},
 acmid = {1718531},
 address = {New York, NY, USA},
 author = {Cheng, Haibin and Cant\'{u}-Paz, Erick},
 booktitle = {Proceedings of the Third ACM International Conference on Web Search and Data Mining},
 doi = {10.1145/1718487.1718531},
 isbn = {978-1-60558-889-6},
 keyword = {click feedback, click prediction, demographic, maximum entropy modeling, personalization, sponsored search, user profile},
 link = {http://doi.acm.org/10.1145/1718487.1718531},
 location = {New York, New York, USA},
 numpages = {10},
 pages = {351--360},
 publisher = {ACM},
 series = {WSDM '10},
 title = {Personalized Click Prediction in Sponsored Search},
 year = {2010}
}


@inproceedings{Taneva:2010:GRP:1718487.1718541,
 abstract = {Knowledge-sharing communities like Wikipedia and automated extraction methods like those of DBpedia enable the construction of large machine-processible knowledge bases with relational facts about entities. These endeavors lack multimodal data like photos and videos of people and places. While photos of famous entities are abundant on the Internet, they are much harder to retrieve for less popular entities such as notable computer scientists or regionally interesting churches. Querying the entity names in image search engines yields large candidate lists, but they often have low precision and unsatisfactory recall. Our goal is to populate a knowledge base with photos of named entities, with high precision, high recall, and diversity of photos for a given entity. We harness relational facts about entities for generating expanded queries to retrieve different candidate lists from image search engines. We use a weighted voting method to determine better rankings of an entity's photos. Appropriate weights are dependent on the type of entity (e.g., scientist vs. politician) and automatically computed from a small set of training entities. We also exploit visual similarity measures based on SIFT features, for higher diversity in the final rankings. Our experiments with photos of persons and landmarks show significant improvements of ranking measures like MAP and NDCG, and also for diversity-aware ranking.},
 acmid = {1718541},
 address = {New York, NY, USA},
 author = {Taneva, Bilyana and Kacimi, Mouna and Weikum, Gerhard},
 booktitle = {Proceedings of the Third ACM International Conference on Web Search and Data Mining},
 doi = {10.1145/1718487.1718541},
 isbn = {978-1-60558-889-6},
 keyword = {image gathering, knowledge base, query expansion, ranking},
 link = {http://doi.acm.org/10.1145/1718487.1718541},
 location = {New York, New York, USA},
 numpages = {10},
 pages = {431--440},
 publisher = {ACM},
 series = {WSDM '10},
 title = {Gathering and Ranking Photos of Named Entities with High Precision, High Recall, and Diversity},
 year = {2010}
}


@inproceedings{Becker:2010:LSM:1718487.1718524,
 abstract = {Social media sites (e.g., Flickr, YouTube, and Facebook) are a popular distribution outlet for users looking to share their experiences and interests on the Web. These sites host substantial amounts of user-contributed materials (e.g., photographs, videos, and textual content) for a wide variety of real-world events of different type and scale. By automatically identifying these events and their associated user-contributed social media documents, which is the focus of this paper, we can enable event browsing and search in state-of-the-art search engines. To address this problem, we exploit the rich "context" associated with social media content, including user-provided annotations (e.g., title, tags) and automatically generated information (e.g., content creation time). Using this rich context, which includes both textual and non-textual features, we can define appropriate document similarity metrics to enable online clustering of media to events. As a key contribution of this paper, we explore a variety of techniques for learning multi-feature similarity metrics for social media documents in a principled manner. We evaluate our techniques on large-scale, real-world datasets of event images from Flickr. Our evaluation results suggest that our approach identifies events, and their associated social media documents, more effectively than the state-of-the-art strategies on which we build.},
 acmid = {1718524},
 address = {New York, NY, USA},
 author = {Becker, Hila and Naaman, Mor and Gravano, Luis},
 booktitle = {Proceedings of the Third ACM International Conference on Web Search and Data Mining},
 doi = {10.1145/1718487.1718524},
 isbn = {978-1-60558-889-6},
 keyword = {event identification, similarity metric learning, social media},
 link = {http://doi.acm.org/10.1145/1718487.1718524},
 location = {New York, New York, USA},
 numpages = {10},
 pages = {291--300},
 publisher = {ACM},
 series = {WSDM '10},
 title = {Learning Similarity Metrics for Event Identification in Social Media},
 year = {2010}
}


@inproceedings{Moon:2010:IIR:1718487.1718507,
 abstract = {Ranking a set of retrieved documents according to their relevance to a given query has become a popular problem at the intersection of web search, machine learning, and information retrieval. Recent work on ranking focused on a number of different paradigms, namely, pointwise, pairwise, and list-wise approaches. Each of those paradigms focuses on a different aspect of the dataset while largely ignoring others. The current paper shows how a combination of them can lead to improved ranking performance and, moreover, how it can be implemented in log-linear time. The basic idea of the algorithm is to use isotonic regression with adaptive bandwidth selection per relevance grade. This results in an implicitly-defined loss function which can be minimized efficiently by a subgradient descent procedure. Experimental results show that the resulting algorithm is competitive on both commercial search engine data and publicly available LETOR data sets.},
 acmid = {1718507},
 address = {New York, NY, USA},
 author = {Moon, Taesup and Smola, Alex and Chang, Yi and Zheng, Zhaohui},
 booktitle = {Proceedings of the Third ACM International Conference on Web Search and Data Mining},
 doi = {10.1145/1718487.1718507},
 isbn = {978-1-60558-889-6},
 keyword = {isotonic regression, learning to rank, listwise constraints, pairwise constraints},
 link = {http://doi.acm.org/10.1145/1718487.1718507},
 location = {New York, New York, USA},
 numpages = {10},
 pages = {151--160},
 publisher = {ACM},
 series = {WSDM '10},
 title = {IntervalRank: Isotonic Regression with Listwise and Pairwise Constraints},
 year = {2010}
}


@inproceedings{Ferragina:2010:CTW:1718487.1718536,
 abstract = {Nowadays we know how to effectively compress most basic components of any modern search engine, such as, the graphs arising from the Web structure and/or its usage, the posting lists, and the dictionary of terms. But we are not aware of any study which has deeply addressed the issue of compressing the raw Web pages. Many Web applications use simple compression algorithms--- e.g. gzip, or word-based Move-to-Front or Huffman coders-and conclude that, even compressed, raw data take more space than Inverted Lists. In this paper we investigate two typical scenarios of use of data compression for large Web collections. In the first scenario, the compressed pages are stored on disk and we only need to support the fast scanning of large parts of the compressed collection (such as for map-reduce paradigms). In the second scenario, we consider the fast access to individual pages of the compressed collection that is distributed among the RAMs of many PCs (such as for search engines and miners). For the first scenario, we provide a thorough experimental comparison among state-of-the-art compressors thus indicating pros and cons of the available solutions. For the second scenario, we compare known compressed-storage solutions with the new algorithmic technology of compressed self-indexes [NM07]. Our results show that Web pages are more compressible than expected and, consequently, that some common beliefs in this area should be reconsidered. Our results are novel for the large spectrum of tested approaches and the size of datasets, and provide a threefold contribution: a non-trivial baseline for designing new compressed-storage solutions, a guide for software developers faced with Web-page storage, and a natural complement to the recent figures on InvertedList-compression achieved by [Yan et al, sigir 09 and www 09].},
 acmid = {1718536},
 address = {New York, NY, USA},
 author = {Ferragina, Paolo and Manzini, Giovanni},
 booktitle = {Proceedings of the Third ACM International Conference on Web Search and Data Mining},
 doi = {10.1145/1718487.1718536},
 isbn = {978-1-60558-889-6},
 keyword = {burrows-wheeler transform, compressed (self-)indexes, lossless data compression, text compression},
 link = {http://doi.acm.org/10.1145/1718487.1718536},
 location = {New York, New York, USA},
 numpages = {10},
 pages = {391--400},
 publisher = {ACM},
 series = {WSDM '10},
 title = {On Compressing the Textual Web},
 year = {2010}
}


@inproceedings{Mathioudakis:2010:EOI:1718487.1718525,
 abstract = {Activity in social media such as blogs, micro-blogs, social networks, etc is manifested via interaction that involves text, images, links and other information items. Naturally, some items attract more attention than others, expressed with large volumes of linking, commenting or tagging activity, to name a few examples. Moreover, high attention can be indicative of emerging events, breaking news or generally indicate information items of interest to a vast set of people. The numbers associated with digital social activity are astonishing: in excess of millions of blog posts, tweets and forums updates per day, millions of tags in photos, news articles or blogs. Being able to identify information items that gather much attention in such a real time information collective is a challenging task. In this paper, we consider the problem of early online identification of items that gather a lot of attention in social media. We model social media activity using ISIS, a stochastic model for Interacting Streaming Information Sources, that intuitively captures the concept of attention gathering information items. Given the challenge of the information overload characterizing digital social activity, we present sequential statistical tests that enable early identification of attention gathering items. This effectively reduces the set of items one has to monitor in real time in order to identify pieces of information attracting a lot of attention. Experiments on real data demonstrate the utility of our model, as well as the efficiency and effectiveness of the proposed sequential statistical tests.},
 acmid = {1718525},
 address = {New York, NY, USA},
 author = {Mathioudakis, Michael and Koudas, Nick and Marbach, Peter},
 booktitle = {Proceedings of the Third ACM International Conference on Web Search and Data Mining},
 doi = {10.1145/1718487.1718525},
 isbn = {978-1-60558-889-6},
 keyword = {social media analysis, user activity modeling and exploitation},
 link = {http://doi.acm.org/10.1145/1718487.1718525},
 location = {New York, New York, USA},
 numpages = {10},
 pages = {301--310},
 publisher = {ACM},
 series = {WSDM '10},
 title = {Early Online Identification of Attention Gathering Items in Social Media},
 year = {2010}
}


@inproceedings{Carterette:2010:MRT:1718487.1718516,
 abstract = {While test collection construction is a time-consuming and expensive process, the true cost is amortized by reusing the collection over hundreds or thousands of experiments. Some of these experiments may involve systems that retrieve documents not judged during the initial construction phase, and some of these systems may be "hard" to evaluate: depending on which judgments are missing and which judged documents were retrieved, the experimenter's confidence in an evaluation could potentially be very low. We propose two methods for quantifying the reusability of a test collection for evaluating new systems. The proposed methods provide simple yet highly effective tests for determining whether an existing set of judgments is useful for evaluating a new system. Empirical evaluations using TREC datasets confirm the usefulness of our proposed reusability measures. In particular, we show that our methods can reliably estimate confidence intervals that are indicative of collection reusability.},
 acmid = {1718516},
 address = {New York, NY, USA},
 author = {Carterette, Ben and Gabrilovich, Evgeniy and Josifovski, Vanja and Metzler, Donald},
 booktitle = {Proceedings of the Third ACM International Conference on Web Search and Data Mining},
 doi = {10.1145/1718487.1718516},
 isbn = {978-1-60558-889-6},
 keyword = {evaluation, reusability, test collections},
 link = {http://doi.acm.org/10.1145/1718487.1718516},
 location = {New York, New York, USA},
 numpages = {10},
 pages = {231--240},
 publisher = {ACM},
 series = {WSDM '10},
 title = {Measuring the Reusability of Test Collections},
 year = {2010}
}


@inproceedings{Du:2010:AIB:1718487.1718502,
 abstract = {Domain-oriented sentiment lexicons are widely used for fine-grained sentiment analysis on reviews; therefore, the automatic construction of domain-oriented sentiment lexicon is a fundamental and important task for sentiment analysis research. Most of existing construction approaches take only the kind of relationships between words into account, which makes them have a lot of room for improvement. This paper proposes an adapted information bottleneck method for the construction of domain-oriented sentiment lexicon. This approach can naturally make full use of the mutual reinforcement between documents and words by fusing three kinds of relationships either from words to documents or from words to words; either homogeneous or heterogeneous; either within-domain or cross-domain. The experimental results demonstrate that proposed method could dramatically improve the accuracy of the baseline approach on the construction of out-of-domain sentiment lexicon.},
 acmid = {1718502},
 address = {New York, NY, USA},
 author = {Du, Weifu and Tan, Songbo and Cheng, Xueqi and Yun, Xiaochun},
 booktitle = {Proceedings of the Third ACM International Conference on Web Search and Data Mining},
 doi = {10.1145/1718487.1718502},
 isbn = {978-1-60558-889-6},
 keyword = {information retrieval, opinion mining, sentiment analysis},
 link = {http://doi.acm.org/10.1145/1718487.1718502},
 location = {New York, New York, USA},
 numpages = {10},
 pages = {111--120},
 publisher = {ACM},
 series = {WSDM '10},
 title = {Adapting Information Bottleneck Method for Automatic Construction of Domain-oriented Sentiment Lexicon},
 year = {2010}
}


@inproceedings{Heymann:2010:THK:1718487.1718495,
 abstract = {A fundamental premise of tagging systems is that regular users can organize large collections for browsing and other tasks using uncontrolled vocabularies. Until now, that premise has remained relatively unexamined. Using library data, we test the tagging approach to organizing a collection. We find that tagging systems have three major large scale organizational features: consistency, quality, and completeness. In addition to testing these features, we present results suggesting that users produce tags similar to the topics designed by experts, that paid tagging can effectively supplement tags in a tagging system, and that information integration may be possible across tagging systems.},
 acmid = {1718495},
 address = {New York, NY, USA},
 author = {Heymann, Paul and Paepcke, Andreas and Garcia-Molina, Hector},
 booktitle = {Proceedings of the Third ACM International Conference on Web Search and Data Mining},
 doi = {10.1145/1718487.1718495},
 isbn = {978-1-60558-889-6},
 keyword = {library science, tagging systems},
 link = {http://doi.acm.org/10.1145/1718487.1718495},
 location = {New York, New York, USA},
 numpages = {10},
 pages = {51--60},
 publisher = {ACM},
 series = {WSDM '10},
 title = {Tagging Human Knowledge},
 year = {2010}
}


@inproceedings{Agarwal:2010:FMF:1718487.1718499,
 abstract = {We propose fLDA, a novel matrix factorization method to predict ratings in recommender system applications where a "bag-of-words" representation for item meta-data is natural. Such scenarios are commonplace in web applications like content recommendation, ad targeting and web search where items are articles, ads and web pages respectively. Because of data sparseness, regularization is key to good predictive accuracy. Our method works by regularizing both user and item factors simultaneously through user features and the bag of words associated with each item. Specifically, each word in an item is associated with a discrete latent factor often referred to as the topic of the word; item topics are obtained by averaging topics across all words in an item. Then, user rating on an item is modeled as user's affinity to the item's topics where user affinity to topics (user factors) and topic assignments to words in items (item factors) are learned jointly in a supervised fashion. To avoid overfitting, user and item factors are regularized through Gaussian linear regression and Latent Dirichlet Allocation (LDA) priors respectively. We show our model is accurate, interpretable and handles both cold-start and warm-start scenarios seamlessly through a single model. The efficacy of our method is illustrated on benchmark datasets and a new dataset from Yahoo! Buzz where fLDA provides superior predictive accuracy in cold-start scenarios and is comparable to state-of-the-art methods in warm-start scenarios. As a by-product, fLDA also identifies interesting topics that explains user-item interactions. Our method also generalizes a recently proposed technique called supervised LDA (sLDA) to collaborative filtering applications. While sLDA estimates item topic vectors in a supervised fashion for a single regression, fLDA incorporates multiple regressions (one for each user) in estimating the item factors.},
 acmid = {1718499},
 address = {New York, NY, USA},
 author = {Agarwal, Deepak and Chen, Bee-Chung},
 booktitle = {Proceedings of the Third ACM International Conference on Web Search and Data Mining},
 doi = {10.1145/1718487.1718499},
 isbn = {978-1-60558-889-6},
 keyword = {bayesian hierarchical model, collaborative filtering, graphical model, latent factor model, recommender systems, supervised topic model},
 link = {http://doi.acm.org/10.1145/1718487.1718499},
 location = {New York, New York, USA},
 numpages = {10},
 pages = {91--100},
 publisher = {ACM},
 series = {WSDM '10},
 title = {fLDA: Matrix Factorization Through Latent Dirichlet Allocation},
 year = {2010}
}


@inproceedings{Zhang:2010:RGS:1718487.1718534,
 abstract = {There has been a large amount of research on efficient document retrieval in both IR and web search areas. One important technique to improve retrieval efficiency is early termination, which speeds up query processing by avoiding scanning the entire inverted lists. Most early termination techniques first build new inverted indexes by sorting the inverted lists in the order of either the term-dependent information, e.g., term frequencies or term IR scores, or the term-independent information, e.g., static rank of the document; and then apply appropriate retrieval strategies on the resulting indexes. Although the methods based only on the static rank have been shown to be ineffective for the early termination, there are still many advantages of using the methods based on term-independent information. In this paper, we propose new techniques to organize inverted indexes based on the term-independent information beyond static rank and study the new retrieval strategies on the resulting indexes. We perform a detailed experimental evaluation on our new techniques and compare them with the existing approaches. Our results on the TREC GOV and GOV2 data sets show that our techniques can improve query efficiency significantly.},
 acmid = {1718534},
 address = {New York, NY, USA},
 author = {Zhang, Fan and Shi, Shuming and Yan, Hao and Wen, Ji-Rong},
 booktitle = {Proceedings of the Third ACM International Conference on Web Search and Data Mining},
 doi = {10.1145/1718487.1718534},
 isbn = {978-1-60558-889-6},
 keyword = {dynamic index pruning, globally-sorted index, top-k},
 link = {http://doi.acm.org/10.1145/1718487.1718534},
 location = {New York, New York, USA},
 numpages = {10},
 pages = {371--380},
 publisher = {ACM},
 series = {WSDM '10},
 title = {Revisiting Globally Sorted Indexes for Efficient Document Retrieval},
 year = {2010}
}


@proceedings{King:2011:1935826,
 abstract = {Welcome to the Fourth ACM International Conference on Web Search and Data Mining (WSDM 2011) held on February 9-12, 2011, in Hong Kong. As the premier ACM conference in the field, WSDM 2011 offers a highly competitive forum for reporting the latest developments in websearch, social search and data mining. We are pleased to present the proceedings of the conference as its published record. Although it is only in its fourth year, WSDM has already witnessed significant growth. We received a record 372 submissions, representing a 22% increase compared to WSDM 2010. 19 Senior PC members and 134 PC members conducted reviews to the submissions. The conference accepted 83 papers (22.3% acceptance rate). Among these, 32 papers were selected for oral and poster presentations and 51 papers were selected for poster only presentations. The authors of submitted papers were from 35 countries and regions, authors of accepted papers are from 13 countries and regions. The quality of accepted papers is very high, making WSDM a first tier conference in computer science.},
 address = {New York, NY, USA},
 isbn = {978-1-4503-0493-1},
 location = {Hong Kong, China},
 note = {618113},
 publisher = {ACM},
 title = {WSDM '11: Proceedings of the Fourth ACM International Conference on Web Search and Data Mining},
 year = {2011}
}


