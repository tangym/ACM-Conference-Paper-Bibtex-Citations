@article{Duan:2015:AMF:2786763.2694388,
 abstract = {There have been several recent efforts to improve the performance of fences. The most aggressive designs allow post-fence accesses to retire and complete before the fence completes. Unfortunately, such designs present implementation difficulties due to their reliance on global state and structures. This paper's goal is to optimize both the performance and the implementability of fences. We start-off with a design like the most aggressive ones but without the global state. We call it Weak Fence or wF. Since the concurrent execution of multiple wFs can deadlock, we combine wFs with a conventional fence (i.e., Strong Fence or sF) for the less performance-critical thread(s). We call the result an Asymmetric fence group. We also propose a taxonomy of Asymmetric fence groups under TSO. Compared to past aggressive fences, Asymmetric fence groups both are substantially easier to implement and have higher average performance. The two main designs presented (WS+ and W+) speed-up workloads under TSO by an average of 13% and 21%, respectively, over conventional fences.},
 acmid = {2694388},
 address = {New York, NY, USA},
 author = {Duan, Yuelu and Honarmand, Nima and Torrellas, Josep},
 doi = {10.1145/2786763.2694388},
 issn = {0163-5964},
 issue_date = {March 2015},
 journal = {SIGARCH Comput. Archit. News},
 keyword = {fences, parallel programming, sequential consistency, shared-memory machines, synchronization},
 link = {http://doi.acm.org/10.1145/2786763.2694388},
 month = {mar},
 number = {1},
 numpages = {13},
 pages = {531--543},
 publisher = {ACM},
 title = {Asymmetric Memory Fences: Optimizing Both Performance and Implementability},
 volume = {43},
 year = {2015}
}


@inproceedings{Gidra:2015:NGC:2694344.2694361,
 abstract = {On contemporary cache-coherent Non-Uniform Memory Access (ccNUMA) architectures, applications with a large memory footprint suffer from the cost of the garbage collector (GC), because, as the GC scans the reference graph, it makes many remote memory accesses, saturating the interconnect between memory nodes. We address this problem with NumaGiC, a GC with a mostly-distributed design. In order to maximise memory access locality during collection, a GC thread avoids accessing a different memory node, instead notifying a remote GC thread with a message; nonetheless, NumaGiC avoids the drawbacks of a pure distributed design, which tends to decrease parallelism. We compare NumaGiC with Parallel Scavenge and NAPS on two different ccNUMA architectures running on the Hotspot Java Virtual Machine of OpenJDK 7. On Spark and Neo4j, two industry-strength analytics applications, with heap sizes ranging from 160GB to 350GB, and on SPECjbb2013 and SPECjbb2005, ourgc improves overall performance by up to 45% over NAPS (up to 94% over Parallel Scavenge), and increases the performance of the collector itself by up to 3.6x over NAPS (up to 5.4x over Parallel Scavenge).},
 acmid = {2694361},
 address = {New York, NY, USA},
 author = {Gidra, Lokesh and Thomas, Ga\"{e}l and Sopena, Julien and Shapiro, Marc and Nguyen, Nhan},
 booktitle = {Proceedings of the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems},
 doi = {10.1145/2694344.2694361},
 isbn = {978-1-4503-2835-7},
 keyword = {NUMA, garbage collection, multicore},
 link = {http://doi.acm.org/10.1145/2694344.2694361},
 location = {Istanbul, Turkey},
 numpages = {13},
 pages = {661--673},
 publisher = {ACM},
 series = {ASPLOS '15},
 title = {NumaGiC: A Garbage Collector for Big Data on Big NUMA Machines},
 year = {2015}
}


@article{Zhang:2015:MRH:2786763.2694370,
 abstract = {Next-generation non-volatile memories (NVMs) promise DRAM-like performance, persistence, and high density. They can attach directly to processors to form non-volatile main memory (NVMM) and offer the opportunity to build very low-latency storage systems. These high-performance storage systems would be especially useful in large-scale data center environments where reliability and availability are critical. However, providing reliability and availability to NVMM is challenging, since the latency of data replication can overwhelm the low latency that NVMM should provide. We propose Mojim, a system that provides the reliability and availability that large-scale storage systems require, while preserving the performance of NVMM. Mojim achieves these goals by using a two-tier architecture in which the primary tier contains a mirrored pair of nodes and the secondary tier contains one or more secondary backup nodes with weakly consistent copies of data. Mojim uses highly-optimized replication protocols, software, and networking stacks to minimize replication costs and expose as much of NVMM?s performance as possible. We evaluate Mojim using raw DRAM as a proxy for NVMM and using an industrial NVMM emulation system. We find that Mojim provides replicated NVMM with similar or even better performance than un-replicated NVMM (reducing latency by 27% to 63% and delivering between 0.4 to 2.7X the throughput). We demonstrate that replacing MongoDB's built-in replication system with Mojim improves MongoDB's performance by 3.4 to 4X.},
 acmid = {2694370},
 address = {New York, NY, USA},
 author = {Zhang, Yiying and Yang, Jian and Memaripour, Amirsaman and Swanson, Steven},
 doi = {10.1145/2786763.2694370},
 issn = {0163-5964},
 issue_date = {March 2015},
 journal = {SIGARCH Comput. Archit. News},
 keyword = {availability, data center, distributed storage systems, keywords non-volatile memory, reliability, storage-class memory},
 link = {http://doi.acm.org/10.1145/2786763.2694370},
 month = {mar},
 number = {1},
 numpages = {16},
 pages = {3--18},
 publisher = {ACM},
 title = {Mojim: A Reliable and Highly-Available Non-Volatile Memory System},
 volume = {43},
 year = {2015}
}


@article{Mullapudi:2015:PAO:2786763.2694364,
 abstract = {This paper presents the design and implementation of PolyMage, a domain-specific language and compiler for image processing pipelines. An image processing pipeline can be viewed as a graph of interconnected stages which process images successively. Each stage typically performs one of point-wise, stencil, reduction or data-dependent operations on image pixels. Individual stages in a pipeline typically exhibit abundant data parallelism that can be exploited with relative ease. However, the stages also require high memory bandwidth preventing effective utilization of parallelism available on modern architectures. For applications that demand high performance, the traditional options are to use optimized libraries like OpenCV or to optimize manually. While using libraries precludes optimization across library routines, manual optimization accounting for both parallelism and locality is very tedious. The focus of our system, PolyMage, is on automatically generating high-performance implementations of image processing pipelines expressed in a high-level declarative language. Our optimization approach primarily relies on the transformation and code generation capabilities of the polyhedral compiler framework. To the best of our knowledge, this is the first model-driven compiler for image processing pipelines that performs complex fusion, tiling, and storage optimization automatically. Experimental results on a modern multicore system show that the performance achieved by our automatic approach is up to 1.81x better than that achieved through manual tuning in Halide, a state-of-the-art language and compiler for image processing pipelines. For a camera raw image processing pipeline, our performance is comparable to that of a hand-tuned implementation.},
 acmid = {2694364},
 address = {New York, NY, USA},
 author = {Mullapudi, Ravi Teja and Vasista, Vinay and Bondhugula, Uday},
 doi = {10.1145/2786763.2694364},
 issn = {0163-5964},
 issue_date = {March 2015},
 journal = {SIGARCH Comput. Archit. News},
 keyword = {domain-specific language, image processing, locality, multicores, parallelism, polyhedral optimization, tiling, vectorization},
 link = {http://doi.acm.org/10.1145/2786763.2694364},
 month = {mar},
 number = {1},
 numpages = {15},
 pages = {429--443},
 publisher = {ACM},
 title = {PolyMage: Automatic Optimization for Image Processing Pipelines},
 volume = {43},
 year = {2015}
}


@article{Zhang:2015:HDL:2775054.2694372,
 abstract = {Information security can be compromised by leakage via low-level hardware features. One recently prominent example is cache probing attacks, which rely on timing channels created by caches. We introduce a hardware design language, SecVerilog, which makes it possible to statically analyze information flow at the hardware level. With SecVerilog, systems can be built with verifiable control of timing channels and other information channels. SecVerilog is Verilog, extended with expressive type annotations that enable precise reasoning about information flow. It also comes with rigorous formal assurance: we prove that SecVerilog enforces timing-sensitive noninterference and thus ensures secure information flow. By building a secure MIPS processor and its caches, we demonstrate that SecVerilog makes it possible to build complex hardware designs with verified security, yet with low overhead in time, space, and HW designer effort.},
 acmid = {2694372},
 address = {New York, NY, USA},
 author = {Zhang, Danfeng and Wang, Yao and Suh, G. Edward and Myers, Andrew C.},
 doi = {10.1145/2775054.2694372},
 issn = {0362-1340},
 issue_date = {April 2015},
 journal = {SIGPLAN Not.},
 keyword = {dependent types, hardware description language, information flow control, timing channels},
 link = {http://doi.acm.org/10.1145/2775054.2694372},
 month = {mar},
 number = {4},
 numpages = {14},
 pages = {503--516},
 publisher = {ACM},
 title = {A Hardware Design Language for Timing-Sensitive Information-Flow Security},
 volume = {50},
 year = {2015}
}


@inproceedings{Fletcher:2015:FON:2694344.2694353,
 abstract = {Oblivious RAM (ORAM) is a cryptographic primitive that hides memory access patterns as seen by untrusted storage. Recently, ORAM has been architected into secure processors. A big challenge for hardware ORAM schemes is how to efficiently manage the Position Map (PosMap), a central component in modern ORAM algorithms. Implemented naively, the PosMap causes ORAM to be fundamentally unscalable in terms of on-chip area. On the other hand, a technique called Recursive ORAM fixes the area problem yet significantly increases ORAM's performance overhead. To address this challenge, we propose three new mechanisms. We propose a new ORAM structure called the PosMap Lookaside Buffer (PLB) and PosMap compression techniques to reduce the performance overhead from Recursive ORAM empirically (the latter also improves the construction asymptotically). Through simulation, we show that these techniques reduce the memory bandwidth overhead needed to support recursion by 95%, reduce overall ORAM bandwidth by 37% and improve overall SPEC benchmark performance by 1.27x. We then show how our PosMap compression techniques further facilitate an extremely efficient integrity verification scheme for ORAM which we call PosMap MAC (PMMAC). For a practical parameterization, PMMAC reduces the amount of hashing needed for integrity checking by >= 68x relative to prior schemes and introduces only 7% performance overhead. We prototype our mechanisms in hardware and report area and clock frequency for a complete ORAM design post-synthesis and post-layout using an ASIC flow in a 32~nm commercial process. With 2 DRAM channels, the design post-layout runs at 1~GHz and has a total area of .47~mm2. Depending on PLB-specific parameters, the PLB accounts for 10% to 26% area. PMMAC costs 12% of total design area. Our work is the first to prototype Recursive ORAM or ORAM with any integrity scheme in hardware.},
 acmid = {2694353},
 address = {New York, NY, USA},
 author = {Fletcher, Christopher W. and Ren, Ling and Kwon, Albert and van Dijk, Marten and Devadas, Srinivas},
 booktitle = {Proceedings of the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems},
 doi = {10.1145/2694344.2694353},
 isbn = {978-1-4503-2835-7},
 keyword = {oblivious ram, secure processor, security},
 link = {http://doi.acm.org/10.1145/2694344.2694353},
 location = {Istanbul, Turkey},
 numpages = {14},
 pages = {103--116},
 publisher = {ACM},
 series = {ASPLOS '15},
 title = {Freecursive ORAM: [Nearly] Free Recursion and Integrity Verification for Position-based Oblivious RAM},
 year = {2015}
}


@article{Liu:2015:GHS:2775054.2694385,
 abstract = {This paper presents a new, co-designed compiler and architecture called GhostRider for supporting privacy preserving computation in the cloud. GhostRider ensures all programs satisfy a property called memory-trace obliviousness (MTO): Even an adversary that observes memory, bus traffic, and access times while the program executes can learn nothing about the program's sensitive inputs and outputs. One way to achieve MTO is to employ Oblivious RAM (ORAM), allocating all code and data in a single ORAM bank, and to also disable caches or fix the rate of memory traffic. This baseline approach can be inefficient, and so GhostRider's compiler uses a program analysis to do better, allocating data to non-oblivious, encrypted RAM (ERAM) and employing a scratchpad when doing so will not compromise MTO. The compiler can also allocate to multiple ORAM banks, which sometimes significantly reduces access times.We have formalized our approach and proved it enjoys MTO. Our FPGA-based hardware prototype and simulation results show that GhostRider significantly outperforms the baseline strategy.},
 acmid = {2694385},
 address = {New York, NY, USA},
 author = {Liu, Chang and Harris, Austin and Maas, Martin and Hicks, Michael and Tiwari, Mohit and Shi, Elaine},
 doi = {10.1145/2775054.2694385},
 issn = {0362-1340},
 issue_date = {April 2015},
 journal = {SIGPLAN Not.},
 keyword = {memory trace obliviousness, oblivious ram, secure type system},
 link = {http://doi.acm.org/10.1145/2775054.2694385},
 month = {mar},
 number = {4},
 numpages = {15},
 pages = {87--101},
 publisher = {ACM},
 title = {GhostRider: A Hardware-Software System for Memory Trace Oblivious Computation},
 volume = {50},
 year = {2015}
}


@inproceedings{Heckey:2015:CMC:2694344.2694357,
 abstract = {Quantum computing (QC) offers huge promise to accelerate a range of computationally intensive benchmarks. Quantum computing is limited, however, by the challenges of decoherence: i.e., a quantum state can only be maintained for short windows of time before it decoheres. While quantum error correction codes can protect against decoherence, fast execution time is the best defense against decoherence, so efficient architectures and effective scheduling algorithms are necessary. This paper proposes the Multi-SIMD QC architecture and then proposes and evaluates effective schedulers to map benchmark descriptions onto Multi-SIMD architectures. The Multi-SIMD model consists of a small number of SIMD regions, each of which may support operations on up to thousands of qubits per cycle. Efficient Multi-SIMD operation requires efficient scheduling. This work develops schedulers to reduce communication requirements of qubits between operating regions, while also improving parallelism.We find that communication to global memory is a dominant cost in QC. We also note that many quantum benchmarks have long serial operation paths (although each operation may be data parallel). To exploit this characteristic, we introduce Longest-Path-First Scheduling (LPFS) which pins operations to SIMD regions to keep data in-place and reduce communication to memory. The use of small, local scratchpad memories also further reduces communication. Our results show a 3% to 308% improvement for LPFS over conventional scheduling algorithms, and an additional 3% to 64% improvement using scratchpad memories. Our work is the most comprehensive software-to-quantum toolflow published to date, with efficient and practical scheduling techniques that reduce communication and increase parallelism for full-scale quantum code executing up to a trillion quantum gate operations.},
 acmid = {2694357},
 address = {New York, NY, USA},
 author = {Heckey, Jeff and Patil, Shruti and JavadiAbhari, Ali and Holmes, Adam and Kudrow, Daniel and Brown, Kenneth R. and Franklin, Diana and Chong, Frederic T. and Martonosi, Margaret},
 booktitle = {Proceedings of the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems},
 doi = {10.1145/2694344.2694357},
 isbn = {978-1-4503-2835-7},
 keyword = {cached memories, design languages, performance metrics},
 link = {http://doi.acm.org/10.1145/2694344.2694357},
 location = {Istanbul, Turkey},
 numpages = {12},
 pages = {445--456},
 publisher = {ACM},
 series = {ASPLOS '15},
 title = {Compiler Management of Communication and Parallelism for Quantum Computation},
 year = {2015}
}


@article{Orr:2015:SUR:2786763.2694350,
 abstract = {Heterogeneous system architecture (HSA) and OpenCL define scoped synchronization to facilitate low overhead communication across a subset of threads. Scoped synchronization works well for static sharing patterns, where consumer threads are known a priori. It works poorly for dynamic sharing patterns (e.g., work stealing) where programmers cannot use a faster small scope due to the rare possibility that the work is stolen by a thread in a distant slower scope. This puts programmers in a conundrum: optimize the common case by synchronizing at a faster small scope or use work stealing at a slower large scope. In this paper, we propose to extend scoped synchronization with remote-scope promotion. This allows the most frequent sharers to synchronize through a small scope. Infrequent sharers synchronize by promoting that remote small scope to a larger shared scope. Synchronization using remote-scope promotion provides performance robustness for dynamic workloads, where the benefits provided by scoped synchronization and work stealing are hard to anticipate. Compared to a na√Øve baseline, static scoped synchronization alone achieves a 1.07x speedup on average and dynamic work stealing alone achieves a 1.18x speedup on average. In contrast, synchronization using remote-scope promotion achieves a robust 1.25x speedup on average, across a diverse set of graph benchmarks and inputs.},
 acmid = {2694350},
 address = {New York, NY, USA},
 author = {Orr, Marc S. and Che, Shuai and Yilmazer, Ayse and Beckmann, Bradford M. and Hill, Mark D. and Wood, David A.},
 doi = {10.1145/2786763.2694350},
 issn = {0163-5964},
 issue_date = {March 2015},
 journal = {SIGARCH Comput. Archit. News},
 keyword = {graphics processing unit (GPU), memory model, scope promotion, scoped synchronization, work stealing},
 link = {http://doi.acm.org/10.1145/2786763.2694350},
 month = {mar},
 number = {1},
 numpages = {14},
 pages = {73--86},
 publisher = {ACM},
 title = {Synchronization Using Remote-Scope Promotion},
 volume = {43},
 year = {2015}
}


@article{Young:2015:DWE:2786763.2694387,
 abstract = {Phase Change Memory (PCM) is an emerging Non Volatile Memory (NVM) technology that has the potential to provide scalable high-density memory systems. While the non-volatility of PCM is a desirable property in order to save leakage power, it also has the undesirable effect of making PCM main memories susceptible to newer modes of security vulnerabilities, for example, accessibility to sensitive data if a PCM DIMM gets stolen. PCM memories can be made secure by encrypting the data. Unfortunately, such encryption comes with a significant overhead in terms of bits written to PCM memory, causing half of the bits in the line to change on every write, even if the actual number of bits being written to memory is small. Our studies show that a typical writeback modifies, on average, only 12% of the bits in the cacheline. Thus, encryption causes almost a 4x increase in the number of bits written to PCM memories. Such extraneous bit writes cause significant increase in write power, reduction in write endurance, and reduction in write bandwidth. To provide the benefit of secure memory in a write efficient manner this paper proposes Dual Counter Encryption (DEUCE). DEUCE is based on the observation that a typical writeback only changes a few words, so DEUCE reencrypts only the words that have changed. We show that DEUCE reduces the number of modified bits per writeback for a secure memory from 50% to 24%, which improves performance by 27% and increases lifetime by 2x.},
 acmid = {2694387},
 address = {New York, NY, USA},
 author = {Young, Vinson and Nair, Prashant J. and Qureshi, Moinuddin K.},
 doi = {10.1145/2786763.2694387},
 issn = {0163-5964},
 issue_date = {March 2015},
 journal = {SIGARCH Comput. Archit. News},
 keyword = {NVM, bitflip, encryption, phase change memory, security},
 link = {http://doi.acm.org/10.1145/2786763.2694387},
 month = {mar},
 number = {1},
 numpages = {12},
 pages = {33--44},
 publisher = {ACM},
 title = {DEUCE: Write-Efficient Encryption for Non-Volatile Memories},
 volume = {43},
 year = {2015}
}


@article{Goiri:2015:ABA:2786763.2694351,
 abstract = {We propose and evaluate a framework for creating and running approximation-enabled MapReduce programs. Specifically, we propose approximation mechanisms that fit naturally into the MapReduce paradigm, including input data sampling, task dropping, and accepting and running a precise and a user-defined approximate version of the MapReduce code. We then show how to leverage statistical theories to compute error bounds for popular classes of MapReduce programs when approximating with input data sampling and/or task dropping. We implement the proposed mechanisms and error bound estimations in a prototype system called ApproxHadoop. Our evaluation uses MapReduce applications from different domains, including data analytics, scientific computing, video encoding, and machine learning. Our results show that ApproxHadoop can significantly reduce application execution time and/or energy consumption when the user is willing to tolerate small errors. For example, ApproxHadoop can reduce runtimes by up to 32x when the user can tolerate an error of 1% with 95% confidence. We conclude that our framework and system can make approximation easily accessible to many application domains using the MapReduce model.},
 acmid = {2694351},
 address = {New York, NY, USA},
 author = {Goiri, Inigo and Bianchini, Ricardo and Nagarakatte, Santosh and Nguyen, Thu D.},
 doi = {10.1145/2786763.2694351},
 issn = {0163-5964},
 issue_date = {March 2015},
 journal = {SIGARCH Comput. Archit. News},
 keyword = {MapReduce, approximation, extreme value theory, multi-stage sampling},
 link = {http://doi.acm.org/10.1145/2786763.2694351},
 month = {mar},
 number = {1},
 numpages = {15},
 pages = {383--397},
 publisher = {ACM},
 title = {ApproxHadoop: Bringing Approximations to MapReduce Frameworks},
 volume = {43},
 year = {2015}
}


@inproceedings{Goiri:2015:CTV:2694344.2694378,
 abstract = {Despite its benefits, free cooling may expose servers to high absolute temperatures, wide temperature variations, and high humidity when datacenters are sited at certain locations. Prior research (in non-free-cooled datacenters) has shown that high temperatures and/or wide temporal temperature variations can harm hardware reliability. In this paper, we identify the runtime management strategies required to limit absolute temperatures, temperature variations, humidity, and cooling energy in free-cooled datacenters. As the basis for our study, we propose CoolAir, a system that embodies these strategies. Using CoolAir and a real free-cooled datacenter prototype, we show that effective management requires cooling infrastructures that can act smoothly. In addition, we show that CoolAir can tightly manage temperature and significantly reduce temperature variation, often at a lower cooling cost than existing free-cooled datacenters. Perhaps most importantly, based on our results, we derive several principles and lessons that should guide the design of management systems for free-cooled datacenters of any size.},
 acmid = {2694378},
 address = {New York, NY, USA},
 author = {Goiri, \'{I}\~{n}igo and Nguyen, Thu D. and Bianchini, Ricardo},
 booktitle = {Proceedings of the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems},
 doi = {10.1145/2694344.2694378},
 isbn = {978-1-4503-2835-7},
 keyword = {datacenters, energy management, free cooling, thermal management},
 link = {http://doi.acm.org/10.1145/2694344.2694378},
 location = {Istanbul, Turkey},
 numpages = {13},
 pages = {253--265},
 publisher = {ACM},
 series = {ASPLOS '15},
 title = {CoolAir: Temperature- and Variation-Aware Management for Free-Cooled Datacenters},
 year = {2015}
}


@article{Hosek:2015:VUE:2775054.2694390,
 abstract = {With the widespread availability of multi-core processors, running multiple diversified variants or several different versions of an application in parallel is becoming a viable approach for increasing the reliability and security of software systems. The key component of such N-version execution (NVX) systems is a runtime monitor that enables the execution of multiple versions in parallel. Unfortunately, existing monitors impose either a large performance overhead or rely on intrusive kernel-level changes. Moreover, none of the existing solutions scales well with the number of versions, since the runtime monitor acts as a performance bottleneck. In this paper, we introduce Varan, an NVX framework that combines selective binary rewriting with a novel event-streaming architecture to significantly reduce performance overhead and scale well with the number of versions, without relying on intrusive kernel modifications. Our evaluation shows that Varan can run NVX systems based on popular C10k network servers with only a modest performance overhead, and can be effectively used to increase software reliability using techniques such as transparent failover, live sanitization and multi-revision execution.},
 acmid = {2694390},
 address = {New York, NY, USA},
 author = {Hosek, Petr and Cadar, Cristian},
 doi = {10.1145/2775054.2694390},
 issn = {0362-1340},
 issue_date = {April 2015},
 journal = {SIGPLAN Not.},
 keyword = {N-version execution, event streaming, live sanitization, multi-revision execution, record-replay, selective binary rewriting, transparent failover},
 link = {http://doi.acm.org/10.1145/2775054.2694390},
 month = {mar},
 number = {4},
 numpages = {15},
 pages = {339--353},
 publisher = {ACM},
 title = {VARAN the Unbelievable: An Efficient N-version Execution Framework},
 volume = {50},
 year = {2015}
}


@article{Alglave:2015:GCW:2775054.2694391,
 abstract = {Concurrency is pervasive and perplexing, particularly on graphics processing units (GPUs). Current specifications of languages and hardware are inconclusive; thus programmers often rely on folklore assumptions when writing software. To remedy this state of affairs, we conducted a large empirical study of the concurrent behaviour of deployed GPUs. Armed with litmus tests (i.e. short concurrent programs), we questioned the assumptions in programming guides and vendor documentation about the guarantees provided by hardware. We developed a tool to generate thousands of litmus tests and run them under stressful workloads. We observed a litany of previously elusive weak behaviours, and exposed folklore beliefs about GPU programming---often supported by official tutorials---as false. As a way forward, we propose a model of Nvidia GPU hardware, which correctly models every behaviour witnessed in our experiments. The model is a variant of SPARC Relaxed Memory Order (RMO), structured following the GPU concurrency hierarchy.},
 acmid = {2694391},
 address = {New York, NY, USA},
 author = {Alglave, Jade and Batty, Mark and Donaldson, Alastair F. and Gopalakrishnan, Ganesh and Ketema, Jeroen and Poetzl, Daniel and Sorensen, Tyler and Wickerson, John},
 doi = {10.1145/2775054.2694391},
 issn = {0362-1340},
 issue_date = {April 2015},
 journal = {SIGPLAN Not.},
 keyword = {GPU, Nvidia PTX, formal model, litmus testing, memory consistency, openCL, test generation},
 link = {http://doi.acm.org/10.1145/2775054.2694391},
 month = {mar},
 number = {4},
 numpages = {15},
 pages = {577--591},
 publisher = {ACM},
 title = {GPU Concurrency: Weak Behaviours and Programming Assumptions},
 volume = {50},
 year = {2015}
}


@article{Bhatotia:2015:ITL:2775054.2694371,
 abstract = {Incremental computation strives for efficient successive runs of applications by re-executing only those parts of the computation that are affected by a given input change instead of recomputing everything from scratch. To realize these benefits automatically, we describe iThreads, a threading library for parallel incremental computation. iThreads supports unmodified shared-memory multithreaded programs: it can be used as a replacement for pthreads by a simple exchange of dynamically linked libraries, without even recompiling the application code. To enable such an interface, we designed algorithms and an implementation to operate at the compiled binary code level by leveraging MMU-assisted memory access tracking and process-based thread isolation. Our evaluation on a multicore platform using applications from the PARSEC and Phoenix benchmarks and two case-studies shows significant performance gains.},
 acmid = {2694371},
 address = {New York, NY, USA},
 author = {Bhatotia, Pramod and Fonseca, Pedro and Acar, Umut A. and Brandenburg, Bj\"{o}rn B. and Rodrigues, Rodrigo},
 doi = {10.1145/2775054.2694371},
 issn = {0362-1340},
 issue_date = {April 2015},
 journal = {SIGPLAN Not.},
 keyword = {concurrent dynamic dependence graph (CDDG), incremental computation, memoization, release consistency (RC) memory model, self-adjusting computation, shared-memory multithreading},
 link = {http://doi.acm.org/10.1145/2775054.2694371},
 month = {mar},
 number = {4},
 numpages = {15},
 pages = {645--659},
 publisher = {ACM},
 title = {iThreads: A Threading Library for Parallel Incremental Computation},
 volume = {50},
 year = {2015}
}


@article{Dhawan:2015:ASS:2786763.2694383,
 abstract = {Optimized hardware for propagating and checking software-programmable metadata tags can achieve low runtime overhead. We generalize prior work on hardware tagging by considering a generic architecture that supports software-defined policies over metadata of arbitrary size and complexity; we introduce several novel microarchitectural optimizations that keep the overhead of this rich processing low. Our model thus achieves the efficiency of previous hardware-based approaches with the flexibility of the software-based ones. We demonstrate this by using it to enforce four diverse safety and security policies---spatial and temporal memory safety, taint tracking, control-flow integrity, and code and data separation---plus a composite policy that enforces all of them simultaneously. Experiments on SPEC CPU2006 benchmarks with a PUMP-enhanced RISC processor show modest impact on runtime (typically under 10%) and power ceiling (less than 10%), in return for some increase in energy usage (typically under 60%) and area for on-chip memory structures (110%).},
 acmid = {2694383},
 address = {New York, NY, USA},
 author = {Dhawan, Udit and Hritcu, Catalin and Rubin, Raphael and Vasilakis, Nikos and Chiricescu, Silviu and Smith, Jonathan M. and Knight,Jr., Thomas F. and Pierce, Benjamin C. and DeHon, Andre},
 doi = {10.1145/2786763.2694383},
 issn = {0163-5964},
 issue_date = {March 2015},
 journal = {SIGARCH Comput. Archit. News},
 keyword = {CFI, memory safety, metadata, security, tagged architecture, taint tracking},
 link = {http://doi.acm.org/10.1145/2786763.2694383},
 month = {mar},
 number = {1},
 numpages = {16},
 pages = {487--502},
 publisher = {ACM},
 title = {Architectural Support for Software-Defined Metadata Processing},
 volume = {43},
 year = {2015}
}


@article{Lee:2015:ASC:2775054.2694375,
 abstract = {Cyber-physical systems are integrations of computation, communication networks, and physical dynamics. Although time plays a central role in the physical world, all widely used software abstractions lack temporal semantics. The notion of correct execution of a program written in every widely-used programming language today does not depend on the temporal behavior of the program. But temporal behavior matters in almost all systems, and most particularly in cyber-physical systems. In this talk, I will argue that time can and must become part of the semantics of programs for a large class of applications. To illustrate that this is both practical and useful, we will describe a recent effort at Berkeley in the design and implementation of timing-centric software systems. Specifically, I will describe PRET machines, which redefine the instruction-set architecture (ISA) of a microprocessor to embrace temporal semantics. Such machines can be used in high-confidence and safety-critical systems, in energy-constrained systems, in mixed-criticality systems, and as a Real-Time Unit (RTU) that cooperates with a general-purpose processor to provide real-time services, in a manner similar to how a GPU provides graphics services.},
 acmid = {2694375},
 address = {New York, NY, USA},
 author = {Lee, Edward A.},
 doi = {10.1145/2775054.2694375},
 issn = {0362-1340},
 issue_date = {April 2015},
 journal = {SIGPLAN Not.},
 keyword = {computer architecture, cyber-physical systems, embedded systems, real-time systems},
 link = {http://doi.acm.org/10.1145/2775054.2694375},
 month = {mar},
 number = {4},
 numpages = {1},
 pages = {1--1},
 publisher = {ACM},
 title = {Architectural Support for Cyber-Physical Systems},
 volume = {50},
 year = {2015}
}


@article{Hauswald:2015:SOE:2775054.2694347,
 abstract = {As user demand scales for intelligent personal assistants (IPAs) such as Apple's Siri, Google's Google Now, and Microsoft's Cortana, we are approaching the computational limits of current datacenter architectures. It is an open question how future server architectures should evolve to enable this emerging class of applications, and the lack of an open-source IPA workload is an obstacle in addressing this question. In this paper, we present the design of Sirius, an open end-to-end IPA web-service application that accepts queries in the form of voice and images, and responds with natural language. We then use this workload to investigate the implications of four points in the design space of future accelerator-based server architectures spanning traditional CPUs, GPUs, manycore throughput co-processors, and FPGAs. To investigate future server designs for Sirius, we decompose Sirius into a suite of 7 benchmarks (Sirius Suite) comprising the computationally intensive bottlenecks of Sirius. We port Sirius Suite to a spectrum of accelerator platforms and use the performance and power trade-offs across these platforms to perform a total cost of ownership (TCO) analysis of various server design points. In our study, we find that accelerators are critical for the future scalability of IPA services. Our results show that GPU- and FPGA-accelerated servers improve the query latency on average by 10x and 16x. For a given throughput, GPU- and FPGA-accelerated servers can reduce the TCO of datacenters by 2.6x and 1.4x, respectively.},
 acmid = {2694347},
 address = {New York, NY, USA},
 author = {Hauswald, Johann and Laurenzano, Michael A. and Zhang, Yunqi and Li, Cheng and Rovinski, Austin and Khurana, Arjun and Dreslinski, Ronald G. and Mudge, Trevor and Petrucci, Vinicius and Tang, Lingjia and Mars, Jason},
 doi = {10.1145/2775054.2694347},
 issn = {0362-1340},
 issue_date = {April 2015},
 journal = {SIGPLAN Not.},
 keyword = {datacenters, emerging workloads, intelligent personal assistants, warehouse scale computers},
 link = {http://doi.acm.org/10.1145/2775054.2694347},
 month = {mar},
 number = {4},
 numpages = {16},
 pages = {223--238},
 publisher = {ACM},
 title = {Sirius: An Open End-to-End Voice and Vision Personal Assistant and Its Implications for Future Warehouse Scale Computers},
 volume = {50},
 year = {2015}
}


@article{Kim:2015:DEF:2775054.2694394,
 abstract = {Execution comparison has many applications in debugging, malware analysis, software feature identification, and intrusion detection. Existing comparison techniques have various limitations. Some can only compare at the system event level and require executions to take the same input. Some require storing instruction traces that are very space-consuming and have difficulty dealing with non-determinism. In this paper, we propose a novel dual execution technique that allows on-the-fly comparison at the instruction level. Only differences between the executions are recorded. It allows executions to proceed in a coupled mode such that they share the same input sequence with the same timing, reducing nondeterminism. It also allows them to proceed in a decoupled mode such that the user can interact with each one differently. Decoupled executions can be recoupled to share the same future inputs and facilitate further comparison. We have implemented a prototype and applied it to identifying functional components for reuse, comparative debugging with new GDB primitives, and understanding real world regression failures. Our results show that dual execution is a critical enabling technique for execution comparison.},
 acmid = {2694394},
 address = {New York, NY, USA},
 author = {Kim, Dohyeong and Kwon, Yonghwi and Sumner, William N. and Zhang, Xiangyu and Xu, Dongyan},
 doi = {10.1145/2775054.2694394},
 issn = {0362-1340},
 issue_date = {April 2015},
 journal = {SIGPLAN Not.},
 keyword = {dynamic analysis, execution comparison},
 link = {http://doi.acm.org/10.1145/2775054.2694394},
 month = {mar},
 number = {4},
 numpages = {14},
 pages = {325--338},
 publisher = {ACM},
 title = {Dual Execution for On the Fly Fine Grained Execution Comparison},
 volume = {50},
 year = {2015}
}


@article{Banavar:2015:WEC:2786763.2694376,
 abstract = {In the last decade, the availability of massive amounts of new data, and the development of new machine learning technologies, have augmented reasoning systems to give rise to a new class of computing systems. These "Cognitive Systems" learn from data, reason from models, and interact naturally with us, to perform complex tasks better than either humans or machines can do by themselves. In essence, cognitive systems help us perform like the best by penetrating the complexity of big data and leverage the power of models. One of the first cognitive systems, called Watson, demonstrated through a Jeopardy! exhibition match, that it was capable of answering complex factoid questions as effectively as the world's champions. Follow-on cognitive systems perform other tasks, such as discovery, reasoning, and multi-modal understanding in a variety of domains, such as healthcare, insurance, and education. We believe such cognitive systems will transform every industry and our everyday life for the better. In this talk, I will give an overview of the applications, the underlying capabilities, and some of the key challenges, of cognitive systems.},
 acmid = {2694376},
 address = {New York, NY, USA},
 author = {Banavar, Guruduth},
 doi = {10.1145/2786763.2694376},
 issn = {0163-5964},
 issue_date = {March 2015},
 journal = {SIGARCH Comput. Archit. News},
 keyword = {Watson, big data, cognitive computing, cognitive systems},
 link = {http://doi.acm.org/10.1145/2786763.2694376},
 month = {mar},
 number = {1},
 numpages = {1},
 pages = {413--413},
 publisher = {ACM},
 title = {Watson and the Era of Cognitive Computing},
 volume = {43},
 year = {2015}
}


@inproceedings{Mishra:2015:PGM:2694344.2694373,
 abstract = {In many deployments, computer systems are underutilized -- meaning that applications have performance requirements that demand less than full system capacity. Ideally, we would take advantage of this under-utilization by allocating system resources so that the performance requirements are met and energy is minimized. This optimization problem is complicated by the fact that the performance and power consumption of various system configurations are often application -- or even input -- dependent. Thus, practically, minimizing energy for a performance constraint requires fast, accurate estimations of application-dependent performance and power tradeoffs. This paper investigates machine learning techniques that enable energy savings by learning Pareto-optimal power and performance tradeoffs. Specifically, we propose LEO, a probabilistic graphical model-based learning system that provides accurate online estimates of an application's power and performance as a function of system configuration. We compare LEO to (1) offline learning, (2) online learning, (3) a heuristic approach, and (4) the true optimal solution. We find that LEO produces the most accurate estimates and near optimal energy savings.},
 acmid = {2694373},
 address = {New York, NY, USA},
 author = {Mishra, Nikita and Zhang, Huazhe and Lafferty, John D. and Hoffmann, Henry},
 booktitle = {Proceedings of the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems},
 doi = {10.1145/2694344.2694373},
 isbn = {978-1-4503-2835-7},
 keyword = {probabilistic graphical models},
 link = {http://doi.acm.org/10.1145/2694344.2694373},
 location = {Istanbul, Turkey},
 numpages = {15},
 pages = {267--281},
 publisher = {ACM},
 series = {ASPLOS '15},
 title = {A Probabilistic Graphical Model-based Approach for Minimizing Energy Under Performance Constraints},
 year = {2015}
}


@article{Matveev:2015:RHN:2775054.2694393,
 abstract = {Because of hardware TM limitations, software fallbacks are the only way to make TM algorithms guarantee progress. Nevertheless, all known software fallbacks to date, from simple locks to sophisticated versions of the NOrec Hybrid TM algorithm, have either limited scalability or weakened semantics. We propose a novel reduced-hardware (RH) version of the NOrec HyTM algorithm. Instead of an all-software slow path, in our RH NOrec the slow-path is a "mix" of hardware and software: one short hardware transaction executes a maximal amount of initial reads in the hardware, and the second executes all of the writes. This novel combination of the RH approach and the NOrec algorithm delivers the first Hybrid TM that scales while fully preserving the hardware's original semantics of opacity and privatization. Our GCC implementation of RH NOrec is promising in that it shows improved performance relative to all prior methods, at the concurrency levels we could test today.},
 acmid = {2694393},
 address = {New York, NY, USA},
 author = {Matveev, Alexander and Shavit, Nir},
 doi = {10.1145/2775054.2694393},
 issn = {0362-1340},
 issue_date = {April 2015},
 journal = {SIGPLAN Not.},
 keyword = {algorithms, design, transactional memory},
 link = {http://doi.acm.org/10.1145/2775054.2694393},
 month = {mar},
 number = {4},
 numpages = {13},
 pages = {59--71},
 publisher = {ACM},
 title = {Reduced Hardware NOrec: A Safe and Scalable Hybrid Transactional Memory},
 volume = {50},
 year = {2015}
}


@article{Sidiroglou-Douskos:2015:TAI:2775054.2694389,
 abstract = {We present a new technique and system, DIODE, for auto- matically generating inputs that trigger overflows at memory allocation sites. DIODE is designed to identify relevant sanity checks that inputs must satisfy to trigger overflows at target memory allocation sites, then generate inputs that satisfy these sanity checks to successfully trigger the overflow. DIODE works with off-the-shelf, production x86 binaries. Our results show that, for our benchmark set of applications, and for every target memory allocation site exercised by our seed inputs (which the applications process correctly with no overflows), either 1) DIODE is able to generate an input that triggers an overflow at that site or 2) there is no input that would trigger an overflow for the observed target expression at that site.},
 acmid = {2694389},
 address = {New York, NY, USA},
 author = {Sidiroglou-Douskos, Stelios and Lahtinen, Eric and Rittenhouse, Nathan and Piselli, Paolo and Long, Fan and Kim, Deokhwan and Rinard, Martin},
 doi = {10.1145/2775054.2694389},
 issn = {0362-1340},
 issue_date = {April 2015},
 journal = {SIGPLAN Not.},
 keyword = {bug detection, integer overflow, targeted symbolic execution},
 link = {http://doi.acm.org/10.1145/2775054.2694389},
 month = {mar},
 number = {4},
 numpages = {14},
 pages = {473--486},
 publisher = {ACM},
 title = {Targeted Automatic Integer Overflow Discovery Using Goal-Directed Conditional Branch Enforcement},
 volume = {50},
 year = {2015}
}


@article{Park:2015:CCP:2786763.2694346,
 abstract = {The demand for multitasking on graphics processing units (GPUs) is constantly increasing as they have become one of the default components on modern computer systems along with traditional processors (CPUs). Preemptive multitasking on CPUs has been primarily supported through context switching. However, the same preemption strategy incurs substantial overhead due to the large context in GPUs. The overhead comes in two dimensions: a preempting kernel suffers from a long preemption latency, and the system throughput is wasted during the switch. Without precise control over the large preemption overhead, multitasking on GPUs has little use for applications with strict latency requirements. In this paper, we propose Chimera, a collaborative preemption approach that can precisely control the overhead for multitasking on GPUs. Chimera first introduces streaming multiprocessor (SM) flushing, which can instantly preempt an SM by detecting and exploiting idempotent execution. Chimera utilizes flushing collaboratively with two previously proposed preemption techniques for GPUs, namely context switching and draining to minimize throughput overhead while achieving a required preemption latency. Evaluations show that Chimera violates the deadline for only 0.2% of preemption requests when a 15us preemption latency constraint is used. For multi-programmed workloads, Chimera can improve the average normalized turnaround time by 5.5x, and system throughput by 12.2%.},
 acmid = {2694346},
 address = {New York, NY, USA},
 author = {Park, Jason Jong Kyu and Park, Yongjun and Mahlke, Scott},
 doi = {10.1145/2786763.2694346},
 issn = {0163-5964},
 issue_date = {March 2015},
 journal = {SIGARCH Comput. Archit. News},
 keyword = {context switch, graphics processing unit, idempotence, preemptive multitasking},
 link = {http://doi.acm.org/10.1145/2786763.2694346},
 month = {mar},
 number = {1},
 numpages = {14},
 pages = {593--606},
 publisher = {ACM},
 title = {Chimera: Collaborative Preemption for Multitasking on a Shared GPU},
 volume = {43},
 year = {2015}
}


@article{Stewart:2015:ZDW:2775054.2694368,
 abstract = {Software-defined radio (SDR) brings the flexibility of software to wireless protocol design, promising an ideal platform for innovation and rapid protocol deployment. However, implementing modern wireless protocols on existing SDR platforms often requires careful hand-tuning of low-level code, which can undermine the advantages of software. Ziria is a new domain-specific language (DSL) that offers programming abstractions suitable for wireless physical (PHY) layer tasks while emphasizing the pipeline reconfiguration aspects of PHY programming. The Ziria compiler implements a rich set of specialized optimizations, such as lookup table generation and pipeline fusion. We also offer a novel -- due to pipeline reconfiguration -- algorithm to optimize the data widths of computations in Ziria pipelines. We demonstrate the programming flexibility of Ziria and the performance of the generated code through a detailed evaluation of a line-rate Ziria WiFi 802.11a/g implementation that is on par and in many cases outperforms a hand-tuned state-of-the-art C++ implementation on commodity CPUs.},
 acmid = {2694368},
 address = {New York, NY, USA},
 author = {Stewart, Gordon and Gowda, Mahanth and Mainland, Geoffrey and Radunovic, Bozidar and Vytiniotis, Dimitrios and Agullo, Cristina Luengo},
 doi = {10.1145/2775054.2694368},
 issn = {0362-1340},
 issue_date = {April 2015},
 journal = {SIGPLAN Not.},
 keyword = {domain-specific languages, wifi, wireless networking},
 link = {http://doi.acm.org/10.1145/2775054.2694368},
 month = {mar},
 number = {4},
 numpages = {14},
 pages = {415--428},
 publisher = {ACM},
 title = {Ziria: A DSL for Wireless Systems Programming},
 volume = {50},
 year = {2015}
}


@article{Yetim:2015:CMC:2786763.2694354,
 abstract = {As semiconductor technology scales towards ever-smaller transistor sizes, hardware fault rates are increasing. Since important application classes (e.g., multimedia, streaming workloads) are data-error-tolerant, recent research has proposed techniques that seek to save energy or improve yield by exploiting error tolerance at the architecture/microarchitecture level. Even seemingly error-tolerant applications, however, will crash or hang due to control-flow/memory addressing errors. In parallel computation, errors involving inter-thread communication can have equally catastrophic effects. Our work explores techniques that mitigate the impact of potentially catastrophic errors in parallel computation, while still garnering power, cost, or yield benefits from data error tolerance. Our proposed CommGuard solution uses FSM-based checkers to pad and discard data in order to maintain semantic alignment between program control flow and the data communicated between processors. CommGuard techniques are low overhead and they exploit application information already provided by some parallel programming languages (e.g. StreamIt). By converting potentially catastrophic communication errors into potentially tolerable data errors, CommGuard allows important streaming applications like JPEG and MP3 decoding to execute without crashing and to sustain good output quality, even for errors as frequent as every 500Œºs.},
 acmid = {2694354},
 address = {New York, NY, USA},
 author = {Yetim, Yavuz and Malik, Sharad and Martonosi, Margaret},
 doi = {10.1145/2786763.2694354},
 issn = {0163-5964},
 issue_date = {March 2015},
 journal = {SIGARCH Comput. Archit. News},
 keyword = {application-level error tolerance, high-level programming languages, parallel computing},
 link = {http://doi.acm.org/10.1145/2786763.2694354},
 month = {mar},
 number = {1},
 numpages = {13},
 pages = {311--323},
 publisher = {ACM},
 title = {CommGuard: Mitigating Communication Errors in Error-Prone Parallel Execution},
 volume = {43},
 year = {2015}
}


@inproceedings{Morrison:2015:TBT:2694344.2694374,
 abstract = {This paper introduces a temporally bounded total store ordering (TBTSO) memory model, and shows that it enables nonblocking fence-free solutions to asymmetric synchronization problems, such as those arising in memory reclamation and biased locking. TBTSO strengthens the TSO memory model by bounding the time it takes a store to drain from the store buffer into memory. This bound enables devising fence-free algorithms for asymmetric problems, which require a performance-critical fast path to synchronize with an infrequently executed slow path. We demonstrate this by constructing (1) a fence-free version of the hazard pointers memory reclamation scheme, and (2) a fence-free biased lock algorithm which is compatible with unmanaged environments as it does not rely on safe points or similar mechanisms. We further argue that TBTSO can be implemented in hardware with modest modifications to existing TSO architectures. However, our design makes assumptions about proprietary implementation details of commercial hardware; it thus best serves as a starting point for a discussion on the feasibility of hardware TBTSO implementation. We also show how minimal OS support enables the adaptation of TBTSO algorithms to x86 systems.},
 acmid = {2694374},
 address = {New York, NY, USA},
 author = {Morrison, Adam and Afek, Yehuda},
 booktitle = {Proceedings of the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems},
 doi = {10.1145/2694344.2694374},
 isbn = {978-1-4503-2835-7},
 keyword = {TSO, biased locks, bounded TSO, hazard pointers, memory fences},
 link = {http://doi.acm.org/10.1145/2694344.2694374},
 location = {Istanbul, Turkey},
 numpages = {14},
 pages = {45--58},
 publisher = {ACM},
 series = {ASPLOS '15},
 title = {Temporally Bounding TSO for Fence-Free Asymmetric Synchronization},
 year = {2015}
}


@inproceedings{Xu:2015:AOD:2694344.2694360,
 abstract = {Non-CPU devices on a modern system-on-a-chip (SoC), ranging from accelerators to I/O controllers, account for a significant portion of the chip area. It is therefore vital for system energy efficiency that idle devices can enter a low-power state while still meeting the performance expectation. This is called device runtime Power Management (PM) for which individual device drivers in commodity OSes are held responsible today. Based on the observations of existing drivers and their evolution, we consider it harmful to rely on drivers for device runtime PM. This paper identifies three pieces of information as essential to device runtime PM, and shows that they can be obtained without involving drivers, either by using a software-only approach, or more efficiently, by adding one register bit to each device. We thus suggest a structural change to the current Linux runtime PM framework, replacing the PM code in all applicable drivers with a single kernel module called the central PM agent. Experimental evaluations show that the central PM agent is just as effective as hand-tuned driver PM code. The paper also presents a tool called PowerAdvisor that simplifies driver PM efforts under the current Linux runtime PM framework. PowerAdvisor analyzes execution traces and suggests where to insert PM calls in driver source code. Despite being a best-effort tool, PowerAdvisor not only reproduces hand-tuned PM code from stock drivers, but also correctly suggests PM code never known before. Overall, our experience shows that it is promising to ultimately free driver developers from manual PM.},
 acmid = {2694360},
 address = {New York, NY, USA},
 author = {Xu, Chao and Lin, Felix Xiaozhu and Wang, Yuyang and Zhong, Lin},
 booktitle = {Proceedings of the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems},
 doi = {10.1145/2694344.2694360},
 isbn = {978-1-4503-2835-7},
 keyword = {mobile system, operating system, power management, system-on-a-chip},
 link = {http://doi.acm.org/10.1145/2694344.2694360},
 location = {Istanbul, Turkey},
 numpages = {14},
 pages = {239--252},
 publisher = {ACM},
 series = {ASPLOS '15},
 title = {Automated OS-level Device Runtime Power Management},
 year = {2015}
}


@article{Hicks:2015:SLR:2775054.2694366,
 abstract = {Processor implementation errata remain a problem, and worse, a subset of these bugs are security-critical. We classified 7 years of errata from recent commercial processors to understand the magnitude and severity of this problem, and found that of 301 errata analyzed, 28 are security-critical. We propose the SECURITY-CRITICAL PROCESSOR ER- RATA CATCHING SYSTEM (SPECS) as a low-overhead solution to this problem. SPECS employs a dynamic verification strategy that is made lightweight by limiting protection to only security-critical processor state. As a proof-of- concept, we implement a hardware prototype of SPECS in an open source processor. Using this prototype, we evaluate SPECS against a set of 14 bugs inspired by the types of security-critical errata we discovered in the classification phase. The evaluation shows that SPECS is 86% effective as a defense when deployed using only ISA-level state; incurs less than 5% area and power overhead; and has no software run-time overhead.},
 acmid = {2694366},
 address = {New York, NY, USA},
 author = {Hicks, Matthew and Sturton, Cynthia and King, Samuel T. and Smith, Jonathan M.},
 doi = {10.1145/2775054.2694366},
 issn = {0362-1340},
 issue_date = {April 2015},
 journal = {SIGPLAN Not.},
 keyword = {hardware security exploits, processor errata, security-critical processor errata},
 link = {http://doi.acm.org/10.1145/2775054.2694366},
 month = {mar},
 number = {4},
 numpages = {13},
 pages = {517--529},
 publisher = {ACM},
 title = {SPECS: A Lightweight Runtime Mechanism for Protecting Software from Security-Critical Processor Bugs},
 volume = {50},
 year = {2015}
}


@proceedings{Balasubramonian:2014:2541940,
 abstract = {It is our great pleasure to welcome you to the 2014 ACM International Conference on Architectural Support for Programming Languages and Operating Systems -- ASPLOS-XIX in Salt Lake City, UT on March 1-5, 2014. This year's ASPLOS finds a home in beautiful Salt Lake City, a thriving technology hub that is surrounded by snow-capped mountains and a handful of geologically diverse national parks. In fact, the large number of technology start-ups in the Salt Lake valley has inspired the moniker "Silicon Slopes". Visitors can choose among a wide array of activities while in town -- skiing or snowboarding in one of eight nearby world-class ski resorts, hiking on numerous local trails, visiting Historic Temple Square in downtown, or taking the five-hour drive to national parks in Utah and surrounding states. As the site of the 2002 Winter Olympic games, there are numerous Olympic class facilities nearby. The conference banquet will be held at the Olympic Oval, site of both the long and short track speed skating events. Apollo Anton Ohno appeared on the world stage as the gold medalist in the 1500 meter short-track event in the 2002 Olympics. We hope the banquet will not only provide the usual social opportunity to mingle with colleagues but also give you all the chance to feel some of the spirit that lies behind the Winter Olympic Games.},
 address = {New York, NY, USA},
 isbn = {978-1-4503-2305-5},
 location = {Salt Lake City, Utah, USA},
 publisher = {ACM},
 title = {ASPLOS '14: Proceedings of the 19th International Conference on Architectural Support for Programming Languages and Operating Systems},
 year = {2014}
}


@inproceedings{Pang:2015:MLL:2694344.2694377,
 abstract = {Molecular-scale Network-on-Chip (mNoC) crossbars use quantum dot LEDs as an on-chip light source, and chromophores to provide optical signal filtering for receivers. An mNoC reduces power consumption or enables scaling to larger crossbars for a reduced energy budget compared to current nanophotonic NoC crossbars. Since communication latency is reduced by using a high-radix crossbar, minimizing power consumption becomes a primary design target. Conventional Single Writer Multiple Reader (SWMR) photonic crossbar designs broadcast all packets, and incur the commensurate required power, even if only two nodes are communicating. This paper introduces power topologies, enabled by unique capabilities of mNoC technology, to reduce overall interconnect power consumption. A power topology corresponds to the logical connectivity provided by a given power mode. Broadcast is one power mode and it consumes the maximum power. Additional power modes consume less power but allow a source to communicate with only a statically defined, potentially non-contiguous, subset of nodes. Overall interconnect power is reduced if the more frequently communicating nodes use modes that consume less power, while less frequently communicating nodes use modes that consume more power. We also investigate thread mapping techniques to fully exploit power topologies. We explore various mNoC power topologies with one, two and four power modes for a radix-256 SWMR mNoC crossbar. Our results show that the combination of power topologies and intelligent thread mapping can reduce total mNoC power by up to 51% on average for a set of 12 SPLASH benchmarks. Furthermore performance is 10% better than conventional resonator-based photonic NoCs and energy is reduced by 72%.},
 acmid = {2694377},
 address = {New York, NY, USA},
 author = {Pang, Jun and Dwyer, Chris and Lebeck, Alvin R.},
 booktitle = {Proceedings of the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems},
 doi = {10.1145/2694344.2694377},
 isbn = {978-1-4503-2835-7},
 keyword = {energy efficiency, interconnection network},
 link = {http://doi.acm.org/10.1145/2694344.2694377},
 location = {Istanbul, Turkey},
 numpages = {14},
 pages = {283--296},
 publisher = {ACM},
 series = {ASPLOS '15},
 title = {More is Less, Less is More: Molecular-Scale Photonic NoC Power Topologies},
 year = {2015}
}


@article{Wang:2015:SCR:2775054.2694352,
 abstract = {Phase Change Memory (PCM) has better scalability and smaller cell size comparing to DRAM. However, further scaling PCM cell in deep sub-micron regime results in significant thermal based write disturbance (WD). Naively allocating large inter-cell space increases cell size from 4F2 ideal to 12F2. While a recent work mitigates WD along word-lines through disturbance resilient data encoding, it is ineffective for WD along bit-lines, which is more severe due to widely adopted $\mu$Trench structure in constructing PCM cell arrays. Without mitigating WD along bit-lines, a PCM cell still has 8F2, which is 100% larger than the ideal. In this paper, we propose SD-PCM for achieving reliable write operations in super dense PCM. In particular, we focus on mitigating WD along bit-lines such that we can construct super dense PCM chips with 4F2 cell size, i.e., the minimal for diode-switch based PCM. Based on simple verification-n-correction (VnC), we propose LazyCorrection and PreRead to effectively reduce VnC overhead and minimize cascading verification during write. We further propose (n:m)-Alloc for achieving good tradeoff between VnC overhead minimization and memory capacity loss. Our experimental results show that, comparing to a WD-free low density PCM, SD-PCM achieves 80% capacity improvement in cell arrays while incurring around 0-10% performance degradation when using different (n:m) allocators.},
 acmid = {2694352},
 address = {New York, NY, USA},
 author = {Wang, Rujia and Jiang, Lei and Zhang, Youtao and Yang, Jun},
 doi = {10.1145/2775054.2694352},
 issn = {0362-1340},
 issue_date = {April 2015},
 journal = {SIGPLAN Not.},
 keyword = {phase change memory, write disturbance},
 link = {http://doi.acm.org/10.1145/2775054.2694352},
 month = {mar},
 number = {4},
 numpages = {13},
 pages = {19--31},
 publisher = {ACM},
 title = {SD-PCM: Constructing Reliable Super Dense Phase Change Memory Under Write Disturbance},
 volume = {50},
 year = {2015}
}


@article{Tan:2015:DWC:2786763.2694362,
 abstract = {Motivated by rapid software and hardware innovation in warehouse-scale computing (WSC), we visit the problem of warehouse-scale network design evaluation. A WSC is composed of about 30 arrays or clusters, each of which contains about 3000 servers, leading to a total of about 100,000 servers per WSC. We found many prior experiments have been conducted on relatively small physical testbeds, and they often assume the workload is static and that computations are only loosely coupled with the adaptive networking stack. We present a novel and cost-efficient FPGAbased evaluation methodology, called Datacenter-In-A-Box at LOw cost (DIABLO), which treats arrays as whole computers with tightly integrated hardware and software. We have built a 3,000-node prototype running the full WSC software stack. Using our prototype, we have successfully reproduced a few WSC phenomena, such as TCP Incast and memcached request latency long tail, and found that results do indeed change with both scale and with version of the full software stack.},
 acmid = {2694362},
 address = {New York, NY, USA},
 author = {Tan, Zhangxi and Qian, Zhenghao and Chen, Xi and Asanovic, Krste and Patterson, David},
 doi = {10.1145/2786763.2694362},
 issn = {0163-5964},
 issue_date = {March 2015},
 journal = {SIGARCH Comput. Archit. News},
 keyword = {FPGA, evaluation, performance, warehouse-scale computing},
 link = {http://doi.acm.org/10.1145/2786763.2694362},
 month = {mar},
 number = {1},
 numpages = {15},
 pages = {207--221},
 publisher = {ACM},
 title = {DIABLO: A Warehouse-Scale Computer Network Simulator Using FPGAs},
 volume = {43},
 year = {2015}
}


@article{Liu:2015:PPM:2775054.2694358,
 abstract = {Machine Learning (ML) techniques are pervasive tools in various emerging commercial applications, but have to be accommodated by powerful computer systems to process very large data. Although general-purpose CPUs and GPUs have provided straightforward solutions, their energy-efficiencies are limited due to their excessive supports for flexibility. Hardware accelerators may achieve better energy-efficiencies, but each accelerator often accommodates only a single ML technique (family). According to the famous No-Free-Lunch theorem in the ML domain, however, an ML technique performs well on a dataset may perform poorly on another dataset, which implies that such accelerator may sometimes lead to poor learning accuracy. Even if regardless of the learning accuracy, such accelerator can still become inapplicable simply because the concrete ML task is altered, or the user chooses another ML technique. In this study, we present an ML accelerator called PuDianNao, which accommodates seven representative ML techniques, including k-means, k-nearest neighbors, naive bayes, support vector machine, linear regression, classification tree, and deep neural network. Benefited from our thorough analysis on computational primitives and locality properties of different ML techniques, PuDianNao can perform up to 1056 GOP/s (e.g., additions and multiplications) in an area of 3.51 mm^2, and consumes 596 mW only. Compared with the NVIDIA K20M GPU (28nm process), PuDianNao (65nm process) is 1.20x faster, and can reduce the energy by 128.41x.},
 acmid = {2694358},
 address = {New York, NY, USA},
 author = {Liu, Daofu and Chen, Tianshi and Liu, Shaoli and Zhou, Jinhong and Zhou, Shengyuan and Teman, Olivier and Feng, Xiaobing and Zhou, Xuehai and Chen, Yunji},
 doi = {10.1145/2775054.2694358},
 issn = {0362-1340},
 issue_date = {April 2015},
 journal = {SIGPLAN Not.},
 keyword = {accelerator, computer architecture, machine learning},
 link = {http://doi.acm.org/10.1145/2775054.2694358},
 month = {mar},
 number = {4},
 numpages = {13},
 pages = {369--381},
 publisher = {ACM},
 title = {PuDianNao: A Polyvalent Machine Learning Accelerator},
 volume = {50},
 year = {2015}
}


@article{Chisnall:2015:BPA:2786763.2694367,
 abstract = {We propose a new memory-safe interpretation of the C abstract machine that provides stronger protection to benefit security and debugging. Despite ambiguities in the specification intended to provide implementation flexibility, contemporary implementations of C have converged on a memory model similar to the PDP-11, the original target for C. This model lacks support for memory safety despite well-documented impacts on security and reliability. Attempts to change this model are often hampered by assumptions embedded in a large body of existing C code, dating back to the memory model exposed by the original C compiler for the PDP-11. Our experience with attempting to implement a memory-safe variant of C on the CHERI experimental microprocessor led us to identify a number of problematic idioms. We describe these as well as their interaction with existing memory safety schemes and the assumptions that they make beyond the requirements of the C specification. Finally, we refine the CHERI ISA and abstract model for C, by combining elements of the CHERI capability model and fat pointers, and present a softcore CPU that implements a C abstract machine that can run legacy C code with strong memory protection guarantees.},
 acmid = {2694367},
 address = {New York, NY, USA},
 author = {Chisnall, David and Rothwell, Colin and Watson, Robert N.M. and Woodruff, Jonathan and Vadera, Munraj and Moore, Simon W. and Roe, Michael and Davis, Brooks and Neumann, Peter G.},
 doi = {10.1145/2786763.2694367},
 issn = {0163-5964},
 issue_date = {March 2015},
 journal = {SIGARCH Comput. Archit. News},
 keyword = {C language, bounds checking, capabilities, compilers, memory protection, memory safety, processor design, security},
 link = {http://doi.acm.org/10.1145/2786763.2694367},
 month = {mar},
 number = {1},
 numpages = {14},
 pages = {117--130},
 publisher = {ACM},
 title = {Beyond the PDP-11: Architectural Support for a Memory-Safe C Abstract Machine},
 volume = {43},
 year = {2015}
}


@article{Malka:2015:REI:2775054.2694355,
 abstract = {The IOMMU allows the OS to encapsulate I/O devices in their own virtual memory spaces, thus restricting their DMAs to specific memory pages. The OS uses the IOMMU to protect itself against buggy drivers and malicious/errant devices. But the added protection comes at a cost, degrading the throughput of I/O-intensive workloads by up to an order of magnitude. This cost has motivated system designers to trade off some safety for performance, e.g., by leaving stale information in the IOTLB for a while so as to amortize costly invalidations. We observe that high-bandwidth devices---like network and PCIe SSD controllers---interact with the OS via circular ring buffers that induce a sequential, predictable workload. We design a ring IOMMU (rIOMMU) that leverages this characteristic by replacing the virtual memory page table hierarchy with a circular, flat table. A flat table is adequately supported by exactly one IOTLB entry, making every new translation an implicit invalidation of the former and thus requiring explicit invalidations only at the end of I/O bursts. Using standard networking benchmarks, we show that rIOMMU provides up to 7.56x higher throughput relative to the baseline IOMMU, and that it is within 0.77--1.00x the throughput of a system without IOMMU protection.},
 acmid = {2694355},
 address = {New York, NY, USA},
 author = {Malka, Moshe and Amit, Nadav and Ben-Yehuda, Muli and Tsafrir, Dan},
 doi = {10.1145/2775054.2694355},
 issn = {0362-1340},
 issue_date = {April 2015},
 journal = {SIGPLAN Not.},
 keyword = {I/O memory management unit},
 link = {http://doi.acm.org/10.1145/2775054.2694355},
 month = {mar},
 number = {4},
 numpages = {14},
 pages = {355--368},
 publisher = {ACM},
 title = {rIOMMU: Efficient IOMMU for I/O Devices That Employ Ring Buffers},
 volume = {50},
 year = {2015}
}


@article{Dautenhahn:2015:NKO:2786763.2694386,
 abstract = {Monolithic operating system designs undermine the security of computing systems by allowing single exploits anywhere in the kernel to enjoy full supervisor privilege. The nested kernel operating system architecture addresses this problem by "nesting" a small isolated kernel within a traditional monolithic kernel. The "nested kernel" interposes on all updates to virtual memory translations to assert protections on physical memory, thus significantly reducing the trusted computing base for memory access control enforcement. We incorporated the nested kernel architecture into FreeBSD on x86-64 hardware while allowing the entire operating system, including untrusted components, to operate at the highest hardware privilege level by write-protecting MMU translations and de-privileging the untrusted part of the kernel. Our implementation inherently enforces kernel code integrity while still allowing dynamically loaded kernel modules, thus defending against code injection attacks. We also demonstrate that the nested kernel architecture allows kernel developers to isolate memory in ways not possible in monolithic kernels by introducing write-mediation and write-logging services to protect critical system data structures. Performance of the nested kernel prototype shows modest overheads: <1% average for Apache and 2.7% for kernel compile. Overall, our results and experience show that the nested kernel design can be retrofitted to existing monolithic kernels, providing important security benefits.},
 acmid = {2694386},
 address = {New York, NY, USA},
 author = {Dautenhahn, Nathan and Kasampalis, Theodoros and Dietz, Will and Criswell, John and Adve, Vikram},
 doi = {10.1145/2786763.2694386},
 issn = {0163-5964},
 issue_date = {March 2015},
 journal = {SIGARCH Comput. Archit. News},
 keyword = {intra-kernel isolation, malicious operating systems, operating system architecture, virtual memory},
 link = {http://doi.acm.org/10.1145/2786763.2694386},
 month = {mar},
 number = {1},
 numpages = {16},
 pages = {191--206},
 publisher = {ACM},
 title = {Nested Kernel: An Operating System Architecture for Intra-Kernel Privilege Separation},
 volume = {43},
 year = {2015}
}


@inproceedings{Colp:2015:PDS:2694344.2694380,
 abstract = {Smartphones and tablets are easily lost or stolen. This makes them susceptible to an inexpensive class of memory attacks, such as cold-boot attacks, using a bus monitor to observe the memory bus, and DMA attacks. This paper describes Sentry, a system that allows applications and OS components to store their code and data on the System-on-Chip (SoC) rather than in DRAM. We use ARM-specific mechanisms originally designed for embedded systems, but still present in today's mobile devices, to protect applications and OS subsystems from memory attacks.},
 acmid = {2694380},
 address = {New York, NY, USA},
 author = {Colp, Patrick and Zhang, Jiawen and Gleeson, James and Suneja, Sahil and de Lara, Eyal and Raj, Himanshu and Saroiu, Stefan and Wolman, Alec},
 booktitle = {Proceedings of the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems},
 doi = {10.1145/2694344.2694380},
 isbn = {978-1-4503-2835-7},
 keyword = {AES, DMA attack, android, arm, bus monitoring, cache, cold boot, encrypted RAM, encrypted memory, iRAM, nexus, tegra},
 link = {http://doi.acm.org/10.1145/2694344.2694380},
 location = {Istanbul, Turkey},
 numpages = {13},
 pages = {177--189},
 publisher = {ACM},
 series = {ASPLOS '15},
 title = {Protecting Data on Smartphones and Tablets from Memory Attacks},
 year = {2015}
}


@article{Zhao:2015:OPS:2786763.2694369,
 abstract = {Finite State Machine (FSM) is the backbone of an important class of applications in many domains. Its parallelization has been extremely difficult due to inherent strong dependences in the computation. Recently, principled speculation shows good promise to solve the problem. However, the reliance on offline training makes the approach inconvenient to adopt and hard to apply to many practical FSM applications, which often deal with a large variety of inputs different from training inputs. This work presents an assembly of techniques that completely remove the needs for offline training. The techniques include a set of theoretical results on inherent properties of FSMs, and two newly designed dynamic optimizations for efficient FSM characterization. The new techniques, for the first time, make principle speculation applicable on the fly, and enables swift, automatic configuration of speculative parallelizations to best suit a given FSM and its current input. They eliminate the fundamental barrier for practical adoption of principle speculation for FSM parallelization. Experiments show that the new techniques give significantly higher speedups for some difficult FSM applications in the presence of input changes.},
 acmid = {2694369},
 address = {New York, NY, USA},
 author = {Zhao, Zhijia and Shen, Xipeng},
 doi = {10.1145/2786763.2694369},
 issn = {0163-5964},
 issue_date = {March 2015},
 journal = {SIGARCH Comput. Archit. News},
 keyword = {DFA, FSM, finite state machine, multicore, online profiling, speculative parallelization},
 link = {http://doi.acm.org/10.1145/2786763.2694369},
 month = {mar},
 number = {1},
 numpages = {12},
 pages = {619--630},
 publisher = {ACM},
 title = {On-the-Fly Principled Speculation for FSM Parallelization},
 volume = {43},
 year = {2015}
}


@article{Agarwal:2015:PPS:2775054.2694381,
 abstract = {Systems from smartphones to supercomputers are increasingly heterogeneous, being composed of both CPUs and GPUs. To maximize cost and energy efficiency, these systems will increasingly use globally-addressable heterogeneous memory systems, making choices about memory page placement critical to performance. In this work we show that current page placement policies are not sufficient to maximize GPU performance in these heterogeneous memory systems. We propose two new page placement policies that improve GPU performance: one application agnostic and one using application profile information. Our application agnostic policy, bandwidth-aware (BW-AWARE) placement, maximizes GPU throughput by balancing page placement across the memories based on the aggregate memory bandwidth available in a system. Our simulation-based results show that BW-AWARE placement outperforms the existing Linux INTERLEAVE and LOCAL policies by 35% and 18% on average for GPU compute workloads. We build upon BW-AWARE placement by developing a compiler-based profiling mechanism that provides programmers with information about GPU application data structure access patterns. Combining this information with simple program-annotated hints about memory placement, our hint-based page placement approach performs within 90% of oracular page placement on average, largely mitigating the need for costly dynamic page tracking and migration.},
 acmid = {2694381},
 address = {New York, NY, USA},
 author = {Agarwal, Neha and Nellans, David and Stephenson, Mark and O'Connor, Mike and Keckler, Stephen W.},
 doi = {10.1145/2775054.2694381},
 issn = {0362-1340},
 issue_date = {April 2015},
 journal = {SIGPLAN Not.},
 keyword = {bandwidth, linux, page placement, program annotation},
 link = {http://doi.acm.org/10.1145/2775054.2694381},
 month = {mar},
 number = {4},
 numpages = {12},
 pages = {607--618},
 publisher = {ACM},
 title = {Page Placement Strategies for GPUs Within Heterogeneous Memory Systems},
 volume = {50},
 year = {2015}
}


@article{Sung:2015:DES:2786763.2694356,
 abstract = {Current shared-memory hardware is complex and inefficient. Prior work on the DeNovo coherence protocol showed that disciplined shared-memory programming models can enable more complexity-, performance-, and energy-efficient hardware than the state-of-the-art MESI protocol. DeNovo, however, severely restricted the synchronization constructs an application can support. This paper proposes DeNovoSync, a technique to support arbitrary synchronization in DeNovo. The key challenge is that DeNovo exploits race-freedom to use reader-initiated local self-invalidations (instead of conventional writer-initiated remote cache invalidations) to ensure coherence. Synchronization accesses are inherently racy and not directly amenable to self-invalidations. DeNovoSync addresses this challenge using a novel combination of registration of all synchronization reads with a judicious hardware backoff to limit unnecessary registrations. For a wide variety of synchronization constructs and applications, compared to MESI, DeNovoSync shows comparable or up to 22% lower execution time and up to 58% lower network traffic, enabling DeNovo's advantages for a much broader class of software than previously possible.},
 acmid = {2694356},
 address = {New York, NY, USA},
 author = {Sung, Hyojin and Adve, Sarita V.},
 doi = {10.1145/2786763.2694356},
 issn = {0163-5964},
 issue_date = {March 2015},
 journal = {SIGARCH Comput. Archit. News},
 keyword = {cache coherence, consistency, shared memory, synchronization},
 link = {http://doi.acm.org/10.1145/2786763.2694356},
 month = {mar},
 number = {1},
 numpages = {15},
 pages = {545--559},
 publisher = {ACM},
 title = {DeNovoSync: Efficient Support for Arbitrary Synchronization Without Writer-Initiated Invalidations},
 volume = {43},
 year = {2015}
}


@proceedings{Ozturk:2015:2694344,
 abstract = {On behalf of the ASPLOS organizing committee, we are very pleased to welcome you to Istanbul, Turkey, for the 20th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-XX) on March 14-18, 2015. Istanbul is a jewel of a city, with its unique historical accumulation and splendid natural beauty, blended into a modern metropolis. Each civilization that has made Istanbul its home has left its mark in sublime and splendid ways, and the result is a city that gives one the feeling of universal history at every step from the Roman era to the Byzantine and Ottoman eras.  The 2015 conference saw a record 287 submissions (compared to 217 in 2014, an increase of 32%), with a total of 1,018 paper authors from 287 institutions spread across at least 22 countries and spanning 5 continents: a clear indication that our community is growing, and that ASPLOS is the premier venue of choice for disseminating high quality interdisciplinary work.  There was a wide diversity in topics, ranging from quantum computing to human computer interaction, with the most popular being scheduling and resource management, memory systems, power/energy/thermal management, and multicore and heterogeneous architectures. 97 papers self-identified as relating to architecture, 72 to parallelism, 70 to operating systems, 51 to programming models and languages, and 34 to compiler optimizations.  In addition to the 48 accepted papers, the conference includes two invited keynote speeches. Edward Lee from the University of California at Berkeley will talk about incorporating time into the semantics of programs in support of cyber-physical systems. Guruduth Banavar from IBM will give a talk on the capabilities of and the challenges in realizing cognitive computing. We will maintain the tradition of past ASPLOS conferences in convening a Wild and Crazy Ideas (WACI) session, organized by John Criswell, Arrvindh Shriraman, and Emmett Witchel, and a debate, organized by Dan Tsafrir. Lightning sessions each morning will provide a quick introduction to the key ideas that will be presented in the talks that day. New this year is the ASPLOS-ACM Student Research Competition, which will allow a showcase for student research via poster presentations during the poster session.  We hope you enjoy reading the papers in these proceedings and continue to submit your interdisciplinary work to ASPLOS. The conference provides a unique opportunity for stimulating interaction with experts across a diverse range of subdisciplines and we look forward to seeing many of you at the conference. },
 address = {New York, NY, USA},
 isbn = {978-1-4503-2835-7},
 location = {Istanbul, Turkey},
 note = {415155},
 publisher = {ACM},
 title = {ASPLOS '15: Proceedings of the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems},
 year = {2015}
}


@inproceedings{Nguyen:2015:FCR:2694344.2694345,
 abstract = {The past decade has witnessed the increasing demands on data-driven business intelligence that led to the proliferation of data-intensive applications. A managed object-oriented programming language such as Java is often the developer's choice for implementing such applications, due to its quick development cycle and rich community resource. While the use of such languages makes programming easier, their automated memory management comes at a cost. When the managed runtime meets Big Data, this cost is significantly magnified and becomes a scalability-prohibiting bottleneck. This paper presents a novel compiler framework, called Facade, that can generate highly-efficient data manipulation code by automatically transforming the data path of an existing Big Data application. The key treatment is that in the generated code, the number of runtime heap objects created for data types in each thread is (almost) statically bounded, leading to significantly reduced memory management cost and improved scalability. We have implemented Facade and used it to transform 7 common applications on 3 real-world, already well-optimized Big Data frameworks: GraphChi, Hyracks, and GPS. Our experimental results are very positive: the generated programs have (1) achieved a 3%--48% execution time reduction and an up to 88X GC reduction; (2) consumed up to 50% less memory, and (3) scaled to much larger datasets.},
 acmid = {2694345},
 address = {New York, NY, USA},
 author = {Nguyen, Khanh and Wang, Kai and Bu, Yingyi and Fang, Lu and Hu, Jianfei and Xu, Guoqing},
 booktitle = {Proceedings of the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems},
 doi = {10.1145/2694344.2694345},
 isbn = {978-1-4503-2835-7},
 keyword = {big data applications, managed languages, memory management, performance optimization},
 link = {http://doi.acm.org/10.1145/2694344.2694345},
 location = {Istanbul, Turkey},
 numpages = {16},
 pages = {675--690},
 publisher = {ACM},
 series = {ASPLOS '15},
 title = {FACADE: A Compiler and Runtime for (Almost) Object-Bounded Big Data Applications},
 year = {2015}
}


@article{Ma:2015:SDS:2786763.2694382,
 abstract = {This paper presents PARD, a programmable architecture for resourcing-on-demand that provides a new programming interface to convey an application's high-level information like quality-of-service requirements to the hardware. PARD enables new functionalities like fully hardware-supported virtualization and differentiated services in computers. PARD is inspired by the observation that a computer is inherently a network in which hardware components communicate via packets (e.g., over the NoC or PCIe). We apply principles of software-defined networking to this intra-computer network and address three major challenges. First, to deal with the semantic gap between high-level applications and underlying hardware packets, PARD attaches a high-level semantic tag (e.g., a virtual machine or thread ID) to each memory-access, I/O, or interrupt packet. Second, to make hardware components more manageable, PARD implements programmable control planes that can be integrated into various shared resources (e.g., cache, DRAM, and I/O devices) and can differentially process packets according to tag-based rules. Third, to facilitate programming, PARD abstracts all control planes as a device file tree to provide a uniform programming interface via which users create and apply tag-based rules. Full-system simulation results show that by co-locating latencycritical memcached applications with other workloads PARD can improve a four-core computer's CPU utilization by up to a factor of four without significantly increasing tail latency. FPGA emulation based on a preliminary RTL implementation demonstrates that the cache control plane introduces no extra latency and that the memory control plane can reduce queueing delay for high-priority memory-access requests by up to a factor of 5.6.},
 acmid = {2694382},
 address = {New York, NY, USA},
 author = {Ma, Jiuyue and Sui, Xiufeng and Sun, Ninghui and Li, Yupeng and Yu, Zihao and Huang, Bowen and Xu, Tianni and Yao, Zhicheng and Chen, Yun and Wang, Haibin and Zhang, Lixin and Bao, Yungang},
 doi = {10.1145/2786763.2694382},
 issn = {0163-5964},
 issue_date = {March 2015},
 journal = {SIGARCH Comput. Archit. News},
 keyword = {QoS, data center, hardware/software interface},
 link = {http://doi.acm.org/10.1145/2786763.2694382},
 month = {mar},
 number = {1},
 numpages = {13},
 pages = {131--143},
 publisher = {ACM},
 title = {Supporting Differentiated Services in Computers via Programmable Architecture for Resourcing-on-Demand (PARD)},
 volume = {43},
 year = {2015}
}


@inproceedings{Hassaan:2015:KDG:2694344.2694363,
 abstract = {Task graphs or dependence graphs are used in runtime systems to schedule tasks for parallel execution. In problem domains such as dense linear algebra and signal processing, dependence graphs can be generated from a program by static analysis. However, in emerging problem domains such as graph analytics, the set of tasks and dependences between tasks in a program are complex functions of runtime values and cannot be determined statically. In this paper, we introduce a novel approach for exploiting parallelism in such programs. This approach is based on a data structure called the kinetic dependence graph (KDG), which consists of a dependence graph together with update rules that incrementally update the graph to reflect changes in the dependence structure whenever a task is completed. We have implemented a simple programming model that allows programmers to write these applications at a high level of abstraction, and a runtime within the Galois system [15] that builds the KDG automatically and executes the program in parallel. On a suite of programs that are difficult to parallelize otherwise, we have obtained speedups of up to 33 on 40 cores, out-performing third-party implementations in many cases.},
 acmid = {2694363},
 address = {New York, NY, USA},
 author = {Hassaan, Muhammad Amber and Nguyen, Donald D. and Pingali, Keshav K.},
 booktitle = {Proceedings of the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems},
 doi = {10.1145/2694344.2694363},
 isbn = {978-1-4503-2835-7},
 keyword = {kinetic dependence graph, ordered algorithms, stable-source and unstable-source algorithms},
 link = {http://doi.acm.org/10.1145/2694344.2694363},
 location = {Istanbul, Turkey},
 numpages = {15},
 pages = {457--471},
 publisher = {ACM},
 series = {ASPLOS '15},
 title = {Kinetic Dependence Graphs},
 year = {2015}
}


@article{Agrawal:2015:ASD:2775054.2694392,
 abstract = {All software in use today relies on libraries, including standard libraries (e.g., C, C++) and application-specific libraries (e.g., libxml, libpng). Most libraries are loaded in memory and dynamically linked when programs are launched, resolving symbol addresses across the applications and libraries. Dynamic linking has many benefits: It allows code to be reused between applications, conserves memory (because only one copy of a library is kept in memory for all the applications that share it), and allows libraries to be patched and updated without modifying programs, among numerous other benefits. However, these benefits come at the cost of performance. For every call made to a function in a dynamically linked library, a trampoline is used to read the function address from a lookup table and branch to the function, incurring memory load and branch operations. Static linking avoids this performance penalty, but loses all the benefits of dynamic linking. Given its myriad benefits, dynamic linking is the predominant choice today, despite the performance cost. In this work, we propose a speculative hardware mechanism to optimize dynamic linking by avoiding executing the trampolines for library function calls, providing the benefits of dynamic linking with the performance of static linking. Speculatively skipping the memory load and branch operations of the library call trampolines improves performance by reducing the number of executed instructions and gains additional performance by reducing pressure on the instruction and data caches, TLBs, and branch predictors. Because the indirect targets of library call trampolines do not change during program execution, our speculative mechanism never misspeculates in practice. We evaluate our technique on real hardware with production software and observe up to 4% speedup using only 1.5KB of on-chip storage.},
 acmid = {2694392},
 address = {New York, NY, USA},
 author = {Agrawal, Varun and Dabral, Abhiroop and Palit, Tapti and Shen, Yongming and Ferdman, Michael},
 doi = {10.1145/2775054.2694392},
 issn = {0362-1340},
 issue_date = {April 2015},
 journal = {SIGPLAN Not.},
 keyword = {branch prediction, dynamic linking, hardware memoization, instruction elision},
 link = {http://doi.acm.org/10.1145/2775054.2694392},
 month = {mar},
 number = {4},
 numpages = {12},
 pages = {691--702},
 publisher = {ACM},
 title = {Architectural Support for Dynamic Linking},
 volume = {50},
 year = {2015}
}


@article{Sridharan:2015:MEM:2786763.2694348,
 abstract = {Several recent publications have shown that hardware faults in the memory subsystem are commonplace. These faults are predicted to become more frequent in future systems that contain orders of magnitude more DRAM and SRAM than found in current memory subsystems. These memory subsystems will need to provide resilience techniques to tolerate these faults when deployed in high-performance computing systems and data centers containing tens of thousands of nodes. Therefore, it is critical to understand the efficacy of current hardware resilience techniques to determine whether they will be suitable for future systems. In this paper, we present a study of DRAM and SRAM faults and errors from the field. We use data from two leadership-class high-performance computer systems to analyze the reliability impact of hardware resilience schemes that are deployed in current systems. Our study has several key findings about the efficacy of many currently deployed reliability techniques such as DRAM ECC, DDR address/command parity, and SRAM ECC and parity. We also perform a methodological study, and find that counting errors instead of faults, a common practice among researchers and data center operators, can lead to incorrect conclusions about system reliability. Finally, we use our data to project the needs of future large-scale systems. We find that SRAM faults are unlikely to pose a significantly larger reliability threat in the future, while DRAM faults will be a major concern and stronger DRAM resilience schemes will be needed to maintain acceptable failure rates similar to those found on today's systems.},
 acmid = {2694348},
 address = {New York, NY, USA},
 author = {Sridharan, Vilas and DeBardeleben, Nathan and Blanchard, Sean and Ferreira, Kurt B. and Stearley, Jon and Shalf, John and Gurumurthi, Sudhanva},
 doi = {10.1145/2786763.2694348},
 issn = {0163-5964},
 issue_date = {March 2015},
 journal = {SIGARCH Comput. Archit. News},
 keyword = {field studies, large-scale systems, reliability},
 link = {http://doi.acm.org/10.1145/2786763.2694348},
 month = {mar},
 number = {1},
 numpages = {14},
 pages = {297--310},
 publisher = {ACM},
 title = {Memory Errors in Modern Systems: The Good, The Bad, and The Ugly},
 volume = {43},
 year = {2015}
}


@article{Sengupta:2015:HSA:2786763.2694379,
 abstract = {Data races are common. They are difficult to detect, avoid, or eliminate, and programmers sometimes introduce them intentionally. However, shared-memory programs with data races have unexpected, erroneous behaviors. Intentional and unintentional data races lead to atomicity and sequential consistency (SC) violations, and they make it more difficult to understand, test, and verify software. Existing approaches for providing stronger guarantees for racy executions add high run-time overhead and/or rely on custom hardware. This paper shows how to provide stronger semantics for racy programs while providing relatively good performance on commodity systems. A novel hybrid static--dynamic analysis called \emph{EnfoRSer} provides end-to-end support for a memory model called \emph{statically bounded region serializability} (SBRS) that is not only stronger than weak memory models but is strictly stronger than SC. EnfoRSer uses static compiler analysis to transform regions, and dynamic analysis to detect and resolve conflicts at run time. By demonstrating commodity support for a reasonably strong memory model with reasonable overheads, we show its potential as an always-on execution model.},
 acmid = {2694379},
 address = {New York, NY, USA},
 author = {Sengupta, Aritra and Biswas, Swarnendu and Zhang, Minjia and Bond, Michael D. and Kulkarni, Milind},
 doi = {10.1145/2786763.2694379},
 issn = {0163-5964},
 issue_date = {March 2015},
 journal = {SIGARCH Comput. Archit. News},
 keyword = {atomicity, dynamic analysis, memory models, region serializability, static analysis, synchronization},
 link = {http://doi.acm.org/10.1145/2786763.2694379},
 month = {mar},
 number = {1},
 numpages = {15},
 pages = {561--575},
 publisher = {ACM},
 title = {Hybrid Static--Dynamic Analysis for Statically Bounded Region Serializability},
 volume = {43},
 year = {2015}
}


@article{David:2015:ACS:2786763.2694359,
 abstract = {We introduce "asynchronized concurrency (ASCY)," a paradigm consisting of four complementary programming patterns. ASCY calls for the design of concurrent search data structures (CSDSs) to resemble that of their sequential counterparts. We argue that ASCY leads to implementations which are portably scalable: they scale across different types of hardware platforms, including single and multi-socket ones, for various classes of workloads, such as read-only and read-write, and according to different performance metrics, including throughput, latency, and energy. We substantiate our thesis through the most exhaustive evaluation of CSDSs to date, involving 6 platforms, 22 state-of-the-art CSDS algorithms, 10 re-engineered state-of-the-art CSDS algorithms following the ASCY patterns, and 2 new CSDS algorithms designed with ASCY in mind. We observe up to 30% improvements in throughput in the re-engineered algorithms, while our new algorithms out-perform the state-of-the-art alternatives.},
 acmid = {2694359},
 address = {New York, NY, USA},
 author = {David, Tudor and Guerraoui, Rachid and Trigonakis, Vasileios},
 doi = {10.1145/2786763.2694359},
 issn = {0163-5964},
 issue_date = {March 2015},
 journal = {SIGARCH Comput. Archit. News},
 keyword = {concurrent data structures, multi-cores, portability, scalability},
 link = {http://doi.acm.org/10.1145/2786763.2694359},
 month = {mar},
 number = {1},
 numpages = {14},
 pages = {631--644},
 publisher = {ACM},
 title = {Asynchronized Concurrency: The Secret to Scaling Concurrent Search Data Structures},
 volume = {43},
 year = {2015}
}


@article{Ringenburg:2015:MDQ:2775054.2694365,
 abstract = {Energy efficiency is a key concern in the design of modern computer systems. One promising approach to energy-efficient computation, approximate computing, trades off output accuracy for significant gains in energy efficiency. However, debugging the actual cause of output quality problems in approximate programs is challenging. This paper presents dynamic techniques to debug and monitor the quality of approximate computations. We propose both offline debugging tools that instrument code to determine the key sources of output degradation and online approaches that monitor the quality of deployed applications. We present two offline debugging techniques and three online monitoring mechanisms. The first offline tool identifies correlations between output quality and the execution of individual approximate operations. The second tracks approximate operations that flow into a particular value. Our online monitoring mechanisms are complementary approaches designed for detecting quality problems in deployed applications, while still maintaining the energy savings from approximation. We present implementations of our techniques and describe their usage with seven applications. Our online monitors control output quality while still maintaining significant energy efficiency gains, and our offline tools provide new insights into the effects of approximation on output quality.},
 acmid = {2694365},
 address = {New York, NY, USA},
 author = {Ringenburg, Michael and Sampson, Adrian and Ackerman, Isaac and Ceze, Luis and Grossman, Dan},
 doi = {10.1145/2775054.2694365},
 issn = {0362-1340},
 issue_date = {April 2015},
 journal = {SIGPLAN Not.},
 keyword = {approximate computing, debugging, monitoring},
 link = {http://doi.acm.org/10.1145/2775054.2694365},
 month = {mar},
 number = {4},
 numpages = {13},
 pages = {399--411},
 publisher = {ACM},
 title = {Monitoring and Debugging the Quality of Results in Approximate Programs},
 volume = {50},
 year = {2015}
}


@article{Omote:2015:IAE:2775054.2694349,
 abstract = {Bare-metal clouds are an emerging infrastructure-as-a-service (IaaS) that leases physical machines (bare-metal instances) rather than virtual machines, allowing resource-intensive applications to have exclusive access to physical hardware. Unfortunately, bare-metal instances require time-consuming or OS-specific tasks for deployment due to the lack of virtualization layers, thereby sacrificing several beneficial features of traditional IaaS clouds such as agility, elasticity, and OS transparency. We present BMcast, an OS deployment system with a special-purpose de-virtualizable virtual machine monitor (VMM) that supports quick and OS-transparent startup of bare-metal instances. BMcast performs streaming OS deployment while allowing direct access to physical hardware from the guest OS, and then disappears after completing the deployment. Quick startup of instances improves agility and elasticity significantly, and OS transparency greatly simplifies management tasks for cloud customers. Experimental results have confirmed that BMcast initiated a bare-metal instance 8.6 times faster than image copying, and database performance on BMcast during streaming OS deployment was comparable to that on a state-of-the-art VMM without performing deployment. BMcast incurred zero overhead after de-virtualization.},
 acmid = {2694349},
 address = {New York, NY, USA},
 author = {Omote, Yushi and Shinagawa, Takahiro and Kato, Kazuhiko},
 doi = {10.1145/2775054.2694349},
 issn = {0362-1340},
 issue_date = {April 2015},
 journal = {SIGPLAN Not.},
 keyword = {bare-metal clouds, device mediators, operating systems, virtualization},
 link = {http://doi.acm.org/10.1145/2775054.2694349},
 month = {mar},
 number = {4},
 numpages = {15},
 pages = {145--159},
 publisher = {ACM},
 title = {Improving Agility and Elasticity in Bare-metal Clouds},
 volume = {50},
 year = {2015}
}


@article{Haque:2015:FIP:2775054.2694384,
 abstract = {Interactive services, such as Web search, recommendations, games, and finance, must respond quickly to satisfy customers. Achieving this goal requires optimizing tail (e.g., 99th+ percentile) latency. Although every server is multicore, parallelizing individual requests to reduce tail latency is challenging because (1) service demand is unknown when requests arrive; (2) blindly parallelizing all requests quickly oversubscribes hardware resources; and (3) parallelizing the numerous short requests will not improve tail latency. This paper introduces Few-to-Many (FM) incremental parallelization, which dynamically increases parallelism to reduce tail latency. FM uses request service demand profiles and hardware parallelism in an offline phase to compute a policy, represented as an interval table, which specifies when and how much software parallelism to add. At runtime, FM adds parallelism as specified by the interval table indexed by dynamic system load and request execution time progress. The longer a request executes, the more parallelism FM adds. We evaluate FM in Lucene, an open-source enterprise search engine, and in Bing, a commercial Web search engine. FM improves the 99th percentile response time up to 32% in Lucene and up to 26% in Bing, compared to prior state-of-the-art parallelization. Compared to running requests sequentially in Bing, FM improves tail latency by a factor of two. These results illustrate that incremental parallelism is a powerful tool for reducing tail latency.},
 acmid = {2694384},
 address = {New York, NY, USA},
 author = {Haque, Md E. and Eom, Yong hun and He, Yuxiong and Elnikety, Sameh and Bianchini, Ricardo and McKinley, Kathryn S.},
 doi = {10.1145/2775054.2694384},
 issn = {0362-1340},
 issue_date = {April 2015},
 journal = {SIGPLAN Not.},
 keyword = {dynamic parallelism, interactive services, multithreading, tail latency, thread scheduling, web search},
 link = {http://doi.acm.org/10.1145/2775054.2694384},
 month = {mar},
 number = {4},
 numpages = {15},
 pages = {161--175},
 publisher = {ACM},
 title = {Few-to-Many: Incremental Parallelism for Reducing Tail Latency in Interactive Services},
 volume = {50},
 year = {2015}
}


@proceedings{Conte:2016:2872362,
 abstract = {It is my pleasure and privilege to serve as program chair for ASPLOS 2016 -- the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems. This year's conference has set new records in terms of the number of submissions, and reinforces ASPLOS's tradition of encouraging work on innovative multidisciplinary research spanning computer architecture and hardware, programming languages and compilers, operating systems and networking, and applications. The 2016 conference saw a record, 232 submissions with a total of 986 (unique 877) paper authors from 240 institutions spread across at least 21 countries and spanning 5 continents: a clear indication that our community is growing, and that ASPLOS is the premier venue of choice for disseminating high quality interdisciplinary work. There was a wide diversity in topics, ranging from DNA computer storage to human computer interaction, with the most popular being heterogeneous architecture and accelerators, security, reliability and debugging, and memory management. 55 papers self-identified as relating to architecture, 55 to parallelism, 59 to operating systems, 40 to programming models and languages, and 20 to compiler optimizations. Some Notes on the Review Process: All reviewing and discussion, including that at the PC meeting, was double blind. As in past ASPLOS conferences, I used a 2-phase review process, with each paper receiving 3 reviews in round 1, and a minimum of an additional 2 reviews in round 2. In order to improve the quality of review assignment, in conjunction with the paper title and abstract (with sometimes a need to skim the paper directly), I used a combination of topic and interest match with reviewers, and suggestions for reviewers from both the authors and the round 1 reviewers (during the round 2 assignment). I continued to monitor reviews for papers through both rounds 1 and 2 as they came in for quality, substance, and tone, to correct any expertise mismatch, and to find experts in the multiple areas each paper might span, including experts outside of the program and external review committees, a step that is essential for a conference with the breadth that ASPLOS covers. Reviewer feedback in this process was extremely helpful. In keeping with ASPLOS'15 and other conferences, not all papers were moved to round 2. In particular, papers with no round 1 reviews advocating acceptance, and with clear consensus (based both on substantive review content and comment exchange) among the reviewers that the paper did not rise above the acceptance bar for the conference, did not move to round 2. Approximately 35.27% of the papers fell in this category. Each of these decisions involved the active participation of all the reviewers. After the rebuttal phase, each paper was assigned a discussion lead. The discussion lead's job was to carefully read all reviews, the rebuttal, and prior online comments (several papers had extensive online discussions after both rounds 1 and 2), and then initiate a discussion with the goal of reaching a conclusion on whether papers were to be accepted, rejected, or discussed at the PC meeting. The goal of the discussion lead (and my monitoring) was to ensure that every reviewer participated in the discussion after reading the other reviews and the rebuttal. During this process, if new reviewers were considered required based on the rebuttal content, they were sought. The program committee meeting was held at the Chicago O'Hare Hilton on November 7th, 2014. All but four PC members were in attendance, due to medical emergencies or health issues. PC members had access to the reviews for all papers for which they had no declared conflict. Paper authors were not revealed during the PC meeting, and since the discussion continued to be blind, PC papers were not singled out for separate discussion. PC members were asked to leave the room for papers for which they were declared as a conflict (which included any papers they were authors on) prior to revealing the paper title and number being discussed. During the PC meeting, all papers categorized as a preliminary accept (15) were discussed first. The PC also had a chance during and prior to the PC meeting to bring up papers for discussion thatwere classified as tentatively rejected (i.e., all papers were open for discussion at the PC meeting). The majority of the time during the PC meeting was spent on the papers categorized as needing discussion. The result of the extensive reviewing, online discussion, and PC meeting is now in your hands for your reading pleasure, with 53 accepted papers, 16 of which were shepherded. In addition to the decision process, for every paper where the authors chose to provide a rebuttal, the discussion lead, in collaboration with the other reviewers, provided the authors with a summary outlining the main criteria leading to the decision outcome for the paper (whether or not the rebuttal answered reviewer questions or addressed concerns or shortcomings expressed in the reviews), along with feedback for improvement. The Program: In addition to the 53 accepted papers, the conference includes two invited keynote speeches. Richard Stanley Williams, a senior fellow at HP, will talk about memristors and sensible machines. Kathryn McKinley from Microsoft Research will give a talk on how to program uncertain things. We will maintain the tradition of past ASPLOS conferences in convening a Wild and Crazy Ideas (WACI) session, organized by Dan Tsafrir, and a debate session organized by Emmett Witchel. Each of which has a group of inspiring speakers line up to provoke thoughts and discussion among the audience and the whole community. Lightning sessions, each morning, managed by Ding Yuan, will provide a quick introduction to the key ideas that will be presented in the talks that day. The authors also have one more chance in the poster session to present their work and get feedback.},
 address = {New York, NY, USA},
 isbn = {978-1-4503-4091-5},
 location = {Atlanta, Georgia, USA},
 note = {415165},
 publisher = {ACM},
 title = {ASPLOS '16: Proceedings of the Twenty-First International Conference on Architectural Support for Programming Languages and Operating Systems},
 year = {2016}
}


