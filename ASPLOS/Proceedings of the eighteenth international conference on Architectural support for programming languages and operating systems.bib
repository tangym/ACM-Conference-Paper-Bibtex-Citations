@inproceedings{Grossman:2013:HSF:2451116.2451175,
 abstract = {Exploiting parallelism to accelerate a computation typically involves dividing it into many small tasks that can be assigned to different processing elements. An efficient execution schedule for these tasks can be difficult or impossible to determine in advance, however, if there is uncertainty as to when each task's input data will be available. Ideally, each task would run in direct response to the arrival of its input data, thus allowing the computation to proceed in a fine-grained event-driven manner. Realizing this ideal is difficult in practice, and typically requires sacrificing flexibility for performance. In Anton 2, a massively parallel special-purpose supercomputer for molecular dynamics simulations, we addressed this challenge by including a hardware block, called the dispatch unit, that provides flexible and efficient support for fine-grained event-driven computation. Its novel features include a many-to-many mapping from input data to a set of synchronization counters, and the ability to prioritize tasks based on their type. To solve the additional problem of using a fixed set of synchronization counters to track input data for a potentially large number of tasks, we created a software library that allows programmers to treat Anton 2 as an idealized machine with infinitely many synchronization counters. The dispatch unit, together with this library, made it possible to simplify our molecular dynamics software by expressing it as a collection of independent tasks, and the resulting fine-grained execution schedule improved overall performance by up to 16% relative to a coarse-grained schedule for precisely the same computation.},
 acmid = {2451175},
 address = {New York, NY, USA},
 author = {Grossman, J. P. and Kuskin, Jeffrey S. and Bank, Joseph A. and Theobald, Michael and Dror, Ron O. and Ierardi, Douglas J. and Larson, Richard H. and Schafer, U. Ben and Towles, Brian and Young, Cliff and Shaw, David E.},
 booktitle = {Proceedings of the Eighteenth International Conference on Architectural Support for Programming Languages and Operating Systems},
 doi = {10.1145/2451116.2451175},
 isbn = {978-1-4503-1870-9},
 keyword = {anton 2, dispatch unit, event-driven, parallel, task scheduling},
 link = {http://doi.acm.org/10.1145/2451116.2451175},
 location = {Houston, Texas, USA},
 numpages = {12},
 pages = {549--560},
 publisher = {ACM},
 series = {ASPLOS '13},
 title = {Hardware Support for Fine-grained Event-driven Computation in Anton 2},
 year = {2013}
}


@article{Schkufza:2013:SS:2499368.2451150,
 abstract = {We formulate the loop-free binary superoptimization task as a stochastic search problem. The competing constraints of transformation correctness and performance improvement are encoded as terms in a cost function, and a Markov Chain Monte Carlo sampler is used to rapidly explore the space of all possible programs to find one that is an optimization of a given target program. Although our method sacrifices completeness, the scope of programs we are able to consider, and the resulting quality of the programs that we produce, far exceed those of existing superoptimizers. Beginning from binaries compiled by llvm -O0 for 64-bit x86, our prototype implementation, STOKE, is able to produce programs which either match or outperform the code produced by gcc -O3, icc -O3, and in some cases, expert handwritten assembly.},
 acmid = {2451150},
 address = {New York, NY, USA},
 author = {Schkufza, Eric and Sharma, Rahul and Aiken, Alex},
 doi = {10.1145/2499368.2451150},
 issn = {0362-1340},
 issue_date = {April 2013},
 journal = {SIGPLAN Not.},
 keyword = {64-bit, binary, markov chain monte carlo, mcmc, smt, stochastic search, superoptimization, x86, x86-64},
 link = {http://doi.acm.org/10.1145/2499368.2451150},
 month = {mar},
 number = {4},
 numpages = {12},
 pages = {305--316},
 publisher = {ACM},
 title = {Stochastic Superoptimization},
 volume = {48},
 year = {2013}
}


@article{Madhavapeddy:2013:ULO:2499368.2451167,
 abstract = {We present unikernels, a new approach to deploying cloud services via applications written in high-level source code. Unikernels are single-purpose appliances that are compile-time specialised into standalone kernels, and sealed against modification when deployed to a cloud platform. In return they offer significant reduction in image sizes, improved efficiency and security, and should reduce operational costs. Our Mirage prototype compiles OCaml code into unikernels that run on commodity clouds and offer an order of magnitude reduction in code size without significant performance penalty. The architecture combines static type-safety with a single address-space layout that can be made immutable via a hypervisor extension. Mirage contributes a suite of type-safe protocol libraries, and our results demonstrate that the hypervisor is a platform that overcomes the hardware compatibility issues that have made past library operating systems impractical to deploy in the real-world.},
 acmid = {2451167},
 address = {New York, NY, USA},
 author = {Madhavapeddy, Anil and Mortier, Richard and Rotsos, Charalampos and Scott, David and Singh, Balraj and Gazagnaire, Thomas and Smith, Steven and Hand, Steven and Crowcroft, Jon},
 doi = {10.1145/2499368.2451167},
 issn = {0362-1340},
 issue_date = {April 2013},
 journal = {SIGPLAN Not.},
 keyword = {functional programming, hypervisor, microkernel},
 link = {http://doi.acm.org/10.1145/2499368.2451167},
 month = {mar},
 number = {4},
 numpages = {12},
 pages = {461--472},
 publisher = {ACM},
 title = {Unikernels: Library Operating Systems for the Cloud},
 volume = {48},
 year = {2013}
}


@article{Mai:2013:VSI:2490301.2451148,
 abstract = {Security for applications running on mobile devices is important. In this paper we present ExpressOS, a new OS for enabling high-assurance applications to run on commodity mobile devices securely. Our main contributions are a new OS architecture and our use of formal methods for proving key security invariants about our implementation. In our use of formal methods, we focus solely on proving that our OS implements our security invariants correctly, rather than striving for full functional correctness, requiring significantly less verification effort while still proving the security relevant aspects of our system. We built ExpressOS, analyzed its security, and tested its performance. Our evaluation shows that the performance of ExpressOS is comparable to an Android-based system. In one test, we ran the same web browser on ExpressOS and on an Android-based system, and found that ExpressOS adds 16% overhead on average to the page load latency time for nine popular web sites.},
 acmid = {2451148},
 address = {New York, NY, USA},
 author = {Mai, Haohui and Pek, Edgar and Xue, Hui and King, Samuel Talmadge and Madhusudan, Parthasarathy},
 doi = {10.1145/2490301.2451148},
 issn = {0163-5964},
 issue_date = {March 2013},
 journal = {SIGARCH Comput. Archit. News},
 keyword = {automatic theorem proving, microkernel, mobile security, programming by con- tracts},
 link = {http://doi.acm.org/10.1145/2490301.2451148},
 month = {mar},
 number = {1},
 numpages = {12},
 pages = {293--304},
 publisher = {ACM},
 title = {Verifying Security Invariants in ExpressOS},
 volume = {41},
 year = {2013}
}


@article{Lucia:2013:CEF:2499368.2451121,
 abstract = {Concurrency errors in multithreaded programs are difficult to find and fix. We propose Aviso, a system for avoiding schedule-dependent failures. Aviso monitors events during a program's execution and, when a failure occurs, records a history of events from the failing execution. It uses this history to generate schedule constraints that perturb the order of events in the execution and thereby avoids schedules that lead to failures in future program executions. Aviso leverages scenarios where many instances of the same software run, using a statistical model of program behavior and experimentation to determine which constraints most effectively avoid failures. After implementing Aviso, we showed that it decreased failure rates for a variety of important desktop, server, and cloud applications by orders of magnitude, with an average overhead of less than 20% and, in some cases, as low as 5%.},
 acmid = {2451121},
 address = {New York, NY, USA},
 author = {Lucia, Brandon and Ceze, Luis},
 doi = {10.1145/2499368.2451121},
 issn = {0362-1340},
 issue_date = {April 2013},
 journal = {SIGPLAN Not.},
 keyword = {concurrency, cooperative failure avoidance},
 link = {http://doi.acm.org/10.1145/2499368.2451121},
 month = {mar},
 number = {4},
 numpages = {12},
 pages = {39--50},
 publisher = {ACM},
 title = {Cooperative Empirical Failure Avoidance for Multithreaded Programs},
 volume = {48},
 year = {2013}
}


@article{Xiang:2013:HHO:2490301.2451153,
 abstract = {The locality metrics are many, for example, miss ratio to test performance, data footprint to manage cache sharing, and reuse distance to analyze and optimize a program. It is unclear how different metrics are related, whether one subsumes another, and what combination may represent locality completely. This paper first derives a set of formulas to convert between five locality metrics and gives the condition for correctness. The transformation is analogous to differentiation and integration used to convert between higher order polynomials. As a result, these metrics can be assigned an order and organized into a hierarchy. Using the new theory, the paper then develops two techniques: one measures the locality in real time without special hardware support, and the other predicts multicore cache interference without parallel testing. The paper evaluates them using sequential and parallel programs as well as for a parallel mix of sequential programs.},
 acmid = {2451153},
 address = {New York, NY, USA},
 author = {Xiang, Xiaoya and Ding, Chen and Luo, Hao and Bao, Bin},
 doi = {10.1145/2490301.2451153},
 issn = {0163-5964},
 issue_date = {March 2013},
 journal = {SIGARCH Comput. Archit. News},
 keyword = {locality metrics, locality modeling},
 link = {http://doi.acm.org/10.1145/2490301.2451153},
 month = {mar},
 number = {1},
 numpages = {14},
 pages = {343--356},
 publisher = {ACM},
 title = {HOTL: A Higher Order Theory of Locality},
 volume = {41},
 year = {2013}
}


@article{Arulraj:2013:PSF:2490301.2451128,
 abstract = {Sequential and concurrency bugs are widespread in deployed software. They cause severe failures and huge financial loss during production runs. Tools that diagnose production-run failures with low overhead are needed. The state-of-the-art diagnosis techniques use software instrumentation to sample program properties at run time and use off-line statistical analysis to identify properties most correlated with failures. Although promising, these techniques suffer from high run-time overhead, which is sometimes over 100%, for concurrency-bug failure diagnosis and hence are not suitable for production-run usage. We present PBI, a system that uses existing hardware performance counters to diagnose production-run failures caused by sequential and concurrency bugs with low overhead. PBI is designed based on several key observations. First, a few widely supported performance counter events can reflect a wide variety of common software bugs and can be monitored by hardware with almost no overhead. Second, the counter overflow interrupt supported by existing hardware and operating systems provides a natural and effective mechanism to conduct event sampling at user level. Third, the noise and non-determinism in interrupt delivery complements well with statistical processing. We evaluate PBI using 13 real-world concurrency and sequential bugs from representative open-source server, client, and utility programs, and 10 bugs from a widely used software-testing benchmark. Quantitatively, PBI can effectively diagnose failures caused by these bugs with a small overhead that is never higher than 10%. Qualitatively, PBI does not require any change to software and presents a novel use of existing hardware performance counters.},
 acmid = {2451128},
 address = {New York, NY, USA},
 author = {Arulraj, Joy and Chang, Po-Chun and Jin, Guoliang and Lu, Shan},
 doi = {10.1145/2490301.2451128},
 issn = {0163-5964},
 issue_date = {March 2013},
 journal = {SIGARCH Comput. Archit. News},
 keyword = {concurrency bugs, failure diagnosis, performance counters, production run},
 link = {http://doi.acm.org/10.1145/2490301.2451128},
 month = {mar},
 number = {1},
 numpages = {12},
 pages = {101--112},
 publisher = {ACM},
 title = {Production-run Software Failure Diagnosis via Hardware Performance Counters},
 volume = {41},
 year = {2013}
}


@article{Gidra:2013:SSS:2490301.2451142,
 abstract = {Large-scale multicore architectures create new challenges for garbage collectors (GCs). In particular, throughput-oriented stop-the-world algorithms demonstrate good performance with a small number of cores, but have been shown to degrade badly beyond approximately 8 cores on a 48-core with OpenJDK 7. This negative result raises the question whether the stop-the-world design has intrinsic limitations that would require a radically different approach. Our study suggests that the answer is no, and that there is no compelling scalability reason to discard the existing highly-optimised throughput-oriented GC code on contemporary hardware. This paper studies the default throughput-oriented garbage collector of OpenJDK 7, called Parallel Scavenge. We identify its bottlenecks, and show how to eliminate them using well-established parallel programming techniques. On the SPECjbb2005, SPECjvm2008 and DaCapo 9.12 benchmarks, the improved GC matches the performance of Parallel Scavenge at low core count, but scales well, up to 48~cores.},
 acmid = {2451142},
 address = {New York, NY, USA},
 author = {Gidra, Lokesh and Thomas, Ga\"{e}l and Sopena, Julien and Shapiro, Marc},
 doi = {10.1145/2490301.2451142},
 issn = {0163-5964},
 issue_date = {March 2013},
 journal = {SIGARCH Comput. Archit. News},
 keyword = {garbage collection, multicore, numa},
 link = {http://doi.acm.org/10.1145/2490301.2451142},
 month = {mar},
 number = {1},
 numpages = {12},
 pages = {229--240},
 publisher = {ACM},
 title = {A Study of the Scalability of Stop-the-world Garbage Collectors on Multicores},
 volume = {41},
 year = {2013}
}


@article{Zhang:2013:CFC:2499368.2451129,
 abstract = {Many concurrency bugs are hidden in deployed software and cause severe failures for end-users. When they finally manifest and become known by developers, they are difficult to fix correctly. To support end-users, we need techniques that help software survive hidden concurrency bugs during production runs. To help developers, we need techniques that fix exposed concurrency bugs. The state-of-the-art techniques on concurrency-bug fixing and survival only satisfy a subset of four important properties: compatibility, correctness, generality, and performance.We aim to develop a system that satisfies all of these four properties. To achieve this goal, we leverage two observations: (1) rolling back a single thread is sufficient to recover from most concurrency-bug failures; (2) reexecuting an idempotent region, which requires no memory-state checkpoint, is sufficient to recover from many concurrency-bug failures. Our system ConAir includes a static analysis component that automatically identifies potential failure sites, a static analysis component that automatically identifies the idempotent code regions around every failure site, and a code-transformation component that inserts rollback-recovery code around the identified idempotent regions. We evaluated ConAir on 10 real-world concurrency bugs in widely used C/C++ open-source applications. These bugs cover different types of failure symptoms and root causes. Quantitatively, ConAir helps software survive failures caused by all of these bugs with negligible run-time overhead (<1%) and short recovery time. Qualitatively, ConAir can help recover from failures caused by unknown bugs. It guarantees that program semantics remain unchanged and requires no change to operating systems or hardware.},
 acmid = {2451129},
 address = {New York, NY, USA},
 author = {Zhang, Wei and de Kruijf, Marc and Li, Ang and Lu, Shan and Sankaralingam, Karthikeyan},
 doi = {10.1145/2499368.2451129},
 issn = {0362-1340},
 issue_date = {April 2013},
 journal = {SIGPLAN Not.},
 keyword = {bug fixing, concurrency bugs, failure recovery, idempotency, static analysis},
 link = {http://doi.acm.org/10.1145/2499368.2451129},
 month = {mar},
 number = {4},
 numpages = {14},
 pages = {113--126},
 publisher = {ACM},
 title = {ConAir: Featherweight Concurrency Bug Recovery via Single-threaded Idempotent Execution},
 volume = {48},
 year = {2013}
}


@article{Sahoo:2013:ULI:2499368.2451131,
 abstract = {We propose an automatic diagnosis technique for isolating the root cause(s) of software failures. We use likely program invariants, automatically generated using correct inputs that are close to the fault-triggering input, to select a set of candidate program locations which are possible root causes. We then trim the set of candidate root causes using software-implemented dynamic backwards slicing, plus two new filtering heuristics: dependence filtering, and filtering via multiple failing inputs that are also close to the failing input. Experimental results on reported software bugs of three large open-source servers show that we are able to narrow down the number of candidate bug locations to between 5 and 17 program expressions, even in programs that are hundreds of thousands of lines long.},
 acmid = {2451131},
 address = {New York, NY, USA},
 author = {Sahoo, Swarup Kumar and Criswell, John and Geigle, Chase and Adve, Vikram},
 doi = {10.1145/2499368.2451131},
 issn = {0362-1340},
 issue_date = {April 2013},
 journal = {SIGPLAN Not.},
 keyword = {bug diagnosis, debugging, fault localization, invariants, program analysis, root cause, software reliability, testing},
 link = {http://doi.acm.org/10.1145/2499368.2451131},
 month = {mar},
 number = {4},
 numpages = {14},
 pages = {139--152},
 publisher = {ACM},
 title = {Using Likely Invariants for Automated Software Fault Localization},
 volume = {48},
 year = {2013}
}


@article{Silberstein:2013:GIF:2490301.2451169,
 abstract = {PU hardware is becoming increasingly general purpose, quickly outgrowing the traditional but constrained GPU-as-coprocessor programming model. To make GPUs easier to program and easier to integrate with existing systems, we propose making the host's file system directly accessible from GPU code. GPUfs provides a POSIX-like API for GPU programs, exploits GPU parallelism for efficiency, and optimizes GPU file access by extending the buffer cache into GPU memory. Our experiments, based on a set of real benchmarks adopted to use our file system, demonstrate the feasibility and benefits of our approach. For example, we demonstrate a simple self-contained GPU program which searches for a set of strings in the entire tree of Linux kernel source files over seven times faster than an eight-core CPU run.},
 acmid = {2451169},
 address = {New York, NY, USA},
 author = {Silberstein, Mark and Ford, Bryan and Keidar, Idit and Witchel, Emmett},
 doi = {10.1145/2490301.2451169},
 issn = {0163-5964},
 issue_date = {March 2013},
 journal = {SIGARCH Comput. Archit. News},
 keyword = {accelerators, file systems, gpgpus, operating systems design},
 link = {http://doi.acm.org/10.1145/2490301.2451169},
 month = {mar},
 number = {1},
 numpages = {14},
 pages = {485--498},
 publisher = {ACM},
 title = {GPUfs: Integrating a File System with GPUs},
 volume = {41},
 year = {2013}
}


@inproceedings{Sung:2013:DEH:2451116.2451119,
 abstract = {Recent work has shown that disciplined shared-memory programming models that provide deterministic-by-default semantics can simplify both parallel software and hardware. Specifically, the DeNovo hardware system has shown that the software guarantees of such models (e.g., data-race-freedom and explicit side-effects) can enable simpler, higher performance, and more energy-efficient hardware than the current state-of-the-art for deterministic programs. Many applications, however, contain non-deterministic parts; e.g., using lock synchronization. For commercial hardware to exploit the benefits of DeNovo, it is therefore necessary to extend DeNovo to support non-deterministic applications. This paper proposes DeNovoND, a system that supports lock-based, disciplined non-determinism, with the simplicity, performance, and energy benefits of DeNovo. We use a combination of distributed queue-based locks and access signatures to implement simple memory consistency semantics for safe non-determinism, with a coherence protocol that does not require transient states, invalidation traffic, or directories, and does not incur false sharing. The resulting system is simpler, shows comparable or better execution time, and has 33% less network traffic on average (translating directly into energy savings) relative to a state-of-the-art invalidation-based protocol for 8 applications designed for lock synchronization.},
 acmid = {2451119},
 address = {New York, NY, USA},
 author = {Sung, Hyojin and Komuravelli, Rakesh and Adve, Sarita V.},
 booktitle = {Proceedings of the Eighteenth International Conference on Architectural Support for Programming Languages and Operating Systems},
 doi = {10.1145/2451116.2451119},
 isbn = {978-1-4503-1870-9},
 keyword = {cache coherence, disciplined parallelism, memory consistency, non-determinism, shared memory},
 link = {http://doi.acm.org/10.1145/2451116.2451119},
 location = {Houston, Texas, USA},
 numpages = {14},
 pages = {13--26},
 publisher = {ACM},
 series = {ASPLOS '13},
 title = {DeNovoND: Efficient Hardware Support for Disciplined Non-determinism},
 year = {2013}
}


@article{Pai:2013:IGC:2499368.2451160,
 abstract = {Each new generation of GPUs vastly increases the resources available to GPGPU programs. GPU programming models (like CUDA) were designed to scale to use these resources. However, we find that CUDA programs actually do not scale to utilize all available resources, with over 30% of resources going unused on average for programs of the Parboil2 suite that we used in our work. Current GPUs therefore allow concurrent execution of kernels to improve utilization. In this work, we study concurrent execution of GPU kernels using multiprogram workloads on current NVIDIA Fermi GPUs. On two-program workloads from the Parboil2 benchmark suite we find concurrent execution is often no better than serialized execution. We identify that the lack of control over resource allocation to kernels is a major serialization bottleneck. We propose transformations that convert CUDA kernels into elastic kernels which permit fine-grained control over their resource usage. We then propose several elastic-kernel aware concurrency policies that offer significantly better performance and concurrency compared to the current CUDA policy. We evaluate our proposals on real hardware using multiprogrammed workloads constructed from benchmarks in the Parboil 2 suite. On average, our proposals increase system throughput (STP) by 1.21x and improve the average normalized turnaround time (ANTT) by 3.73x for two-program workloads when compared to the current CUDA concurrency implementation.},
 acmid = {2451160},
 address = {New York, NY, USA},
 author = {Pai, Sreepathi and Thazhuthaveetil, Matthew J. and Govindarajan, R.},
 doi = {10.1145/2499368.2451160},
 issn = {0362-1340},
 issue_date = {April 2013},
 journal = {SIGPLAN Not.},
 keyword = {concurrent kernels, cuda, gpgpu},
 link = {http://doi.acm.org/10.1145/2499368.2451160},
 month = {mar},
 number = {4},
 numpages = {12},
 pages = {407--418},
 publisher = {ACM},
 title = {Improving GPGPU Concurrency with Elastic Kernels},
 volume = {48},
 year = {2013}
}


@article{Delimitrou:2013:PQS:2499368.2451125,
 abstract = {Large-scale datacenters (DCs) host tens of thousands of diverse applications each day. However, interference between colocated workloads and the difficulty to match applications to one of the many hardware platforms available can degrade performance, violating the quality of service (QoS) guarantees that many cloud workloads require. While previous work has identified the impact of heterogeneity and interference, existing solutions are computationally intensive, cannot be applied online and do not scale beyond few applications. We present Paragon, an online and scalable DC scheduler that is heterogeneity and interference-aware. Paragon is derived from robust analytical methods and instead of profiling each application in detail, it leverages information the system already has about applications it has previously seen. It uses collaborative filtering techniques to quickly and accurately classify an unknown, incoming workload with respect to heterogeneity and interference in multiple shared resources, by identifying similarities to previously scheduled applications. The classification allows Paragon to greedily schedule applications in a manner that minimizes interference and maximizes server utilization. Paragon scales to tens of thousands of servers with marginal scheduling overheads in terms of time or state. We evaluate Paragon with a wide range of workload scenarios, on both small and large-scale systems, including 1,000 servers on EC2. For a 2,500-workload scenario, Paragon enforces performance guarantees for 91% of applications, while significantly improving utilization. In comparison, heterogeneity-oblivious, interference-oblivious and least-loaded schedulers only provide similar guarantees for 14%, 11% and 3% of workloads. The differences are more striking in oversubscribed scenarios where resource efficiency is more critical.},
 acmid = {2451125},
 address = {New York, NY, USA},
 author = {Delimitrou, Christina and Kozyrakis, Christos},
 doi = {10.1145/2499368.2451125},
 issn = {0362-1340},
 issue_date = {April 2013},
 journal = {SIGPLAN Not.},
 keyword = {cloud computing, datacenter, heterogeneity, interference, qos, scheduling},
 link = {http://doi.acm.org/10.1145/2499368.2451125},
 month = {mar},
 number = {4},
 numpages = {12},
 pages = {77--88},
 publisher = {ACM},
 title = {Paragon: QoS-aware Scheduling for Heterogeneous Datacenters},
 volume = {48},
 year = {2013}
}


@article{Phothilimthana:2013:PPH:2490301.2451162,
 abstract = {Trends in both consumer and high performance computing are bringing not only more cores, but also increased heterogeneity among the computational resources within a single machine. In many machines, one of the greatest computational resources is now their graphics coprocessors (GPUs), not just their primary CPUs. But GPU programming and memory models differ dramatically from conventional CPUs, and the relative performance characteristics of the different processors vary widely between machines. Different processors within a system often perform best with different algorithms and memory usage patterns, and achieving the best overall performance may require mapping portions of programs across all types of resources in the machine. To address the problem of efficiently programming machines with increasingly heterogeneous computational resources, we propose a programming model in which the best mapping of programs to processors and memories is determined empirically. Programs define choices in how their individual algorithms may work, and the compiler generates further choices in how they can map to CPU and GPU processors and memory systems. These choices are given to an empirical autotuning framework that allows the space of possible implementations to be searched at installation time. The rich choice space allows the autotuner to construct poly-algorithms that combine many different algorithmic techniques, using both the CPU and the GPU, to obtain better performance than any one technique alone. Experimental results show that algorithmic changes, and the varied use of both CPUs and GPUs, are necessary to obtain up to a 16.5x speedup over using a single program configuration for all architectures.},
 acmid = {2451162},
 address = {New York, NY, USA},
 author = {Phothilimthana, Phitchaya Mangpo and Ansel, Jason and Ragan-Kelley, Jonathan and Amarasinghe, Saman},
 doi = {10.1145/2490301.2451162},
 issn = {0163-5964},
 issue_date = {March 2013},
 journal = {SIGARCH Comput. Archit. News},
 keyword = {autotuning, compilers, gpgpu, heterogeneous},
 link = {http://doi.acm.org/10.1145/2490301.2451162},
 month = {mar},
 number = {1},
 numpages = {14},
 pages = {431--444},
 publisher = {ACM},
 title = {Portable Performance on Heterogeneous Architectures},
 volume = {41},
 year = {2013}
}


@article{Curtsinger:2013:SSS:2490301.2451141,
 abstract = {Researchers and software developers require effective performance evaluation. Researchers must evaluate optimizations or measure overhead. Software developers use automatic performance regression tests to discover when changes improve or degrade performance. The standard methodology is to compare execution times before and after applying changes. Unfortunately, modern architectural features make this approach unsound. Statistically sound evaluation requires multiple samples to test whether one can or cannot (with high confidence) reject the null hypothesis that results are the same before and after. However, caches and branch predictors make performance dependent on machine-specific parameters and the exact layout of code, stack frames, and heap objects. A single binary constitutes just one sample from the space of program layouts, regardless of the number of runs. Since compiler optimizations and code changes also alter layout, it is currently impossible to distinguish the impact of an optimization from that of its layout effects. This paper presents Stabilizer, a system that enables the use of the powerful statistical techniques required for sound performance evaluation on modern architectures. Stabilizer forces executions to sample the space of memory configurations by repeatedly re-randomizing layouts of code, stack, and heap objects at runtime. Stabilizer thus makes it possible to control for layout effects. Re-randomization also ensures that layout effects follow a Gaussian distribution, enabling the use of statistical tests like ANOVA. We demonstrate Stabilizer's efficiency (<7% median overhead) and its effectiveness by evaluating the impact of LLVM's optimizations on the SPEC CPU2006 benchmark suite. We find that, while -O2 has a significant impact relative to -O1, the performance impact of -O3 over -O2 optimizations is indistinguishable from random noise.},
 acmid = {2451141},
 address = {New York, NY, USA},
 author = {Curtsinger, Charlie and Berger, Emery D.},
 doi = {10.1145/2490301.2451141},
 issn = {0163-5964},
 issue_date = {March 2013},
 journal = {SIGARCH Comput. Archit. News},
 keyword = {measurement bias, performance evaluation, randomization},
 link = {http://doi.acm.org/10.1145/2490301.2451141},
 month = {mar},
 number = {1},
 numpages = {10},
 pages = {219--228},
 publisher = {ACM},
 title = {STABILIZER: Statistically Sound Performance Evaluation},
 volume = {41},
 year = {2013}
}


@proceedings{Harris:2012:2150976,
 abstract = {Welcome to the seventeenth edition of the ASPLOS conference series, and the first to be held outside of North America. We have assembled what we believe to be an outstanding technical program, and are delighted to be presenting it in the historic chambers of the Royal Society in the UK. The program continues and reinforces the ASPLOS tradition of publishing innovative multidisciplinary research spanning the boundaries of hardware, computer architecture, compilers, languages, operating systems, networking, and applications. We received 172 full paper submissions, which was 13% higher than last year's 152, and 5% lower than the record of 181 set in 2010. 14 submissions had program committee members as co-authors. Once again, the dominant technical theme was parallelism: 70% of both the submitted and accepted papers explicitly indicated a technical keyword specific to parallel processing. The single most popular keyword was "multicore systems," selected by 35% of both the submitted and accepted papers. Interestingly, not one of the 172 submissions selected "VLSI or process technology." There was a nice mixture of submissions across operating systems (34%), compilers and programming languages (28%), and core topics in computer architecture (37%), with 23% of submissions spanning more than one of these. (Most submissions had something to do with computer architecture; the characterization of "core areas" is somewhat arbitrary.) As in recent years, we employed two rounds of reviewing. An External Review Committee, preselected by the Program Chair, assisted in round one. All submissions received at least three round one reviews. Based on these, we eliminated 71 papers whose average overall score was below 3.5 (on a scale of 1-6), and whose maximum score was below 5 (i.e., no better than "weak accept"). The remaining 101 papers received two additional (PC-only) reviews in round two. This process allowed the program committee to focus the bulk of its attention on papers with a significant chance of being accepted. The typical member of the External Review Committee completed six reviews; the typical Program Committee member did 14. Almost all reviews were completed by early October, at which point authors had several days to compose a formal response, if they desired (most did). After this rebuttal period, PC members took the author comments into account as they discussed the papers online and, in some cases, updated their scores accordingly. The PC gathered in Rochester, NY, for a full-day meeting on October 20th, 2011. 74 papers were discussed at the meeting. PC authors stepped out of the room for discussion of their papers; those with conflicts of interest remained in the room, but did not participate in discussion. Papers on which the Program Chair had a conflict of interest (4 in number) had been reviewed in a separate process managed by Jim Larus, and were discussed when the Chair was not present. At the end of the day, 37 papers were accepted for presentation at the conference, for an acceptance rate of 21.5% --- a slightly larger number than last year, but almost the same percentage. Four of the accepts have PC members as co-authors. Nine papers were assigned a "shepherd" to make sure that final versions addressed issues (not necessarily weaknesses) of concern to the program committee. The logistics behind organizing this conference have been handled by the tireless effort of a team of volunteers: Luis Ceze (University of Washington), Alexandra Fedorova (Simon Fraser University), Steven Hand (University of Cambridge), Timothy Jones (University of Cambridge), Paul Kelly (Imperial College), Mikel LujÃ¡n (University of Manchester), Robert Mullins (University of Cambridge), Onur Mutlu (Carnegie Mellon University), and Derek Murray (Microsoft Research). Each member of this team played an important role in making ASPLOS a success. The ASPLOS Steering Committee gave us valuable assistance over the last year, and Todd Mowry (CMU) was particularly generous with his advice from ASPLOS 2010 and ASPLOS 2011. In addition, Jean Bacon (University of Cambridge), Andrew Herbert, Graham Hutton (University of Nottingham), and Joe Sventek (University of Glasgow) provided advice on organizing a conference in the UK. As always, the support and guidance of the ACM staff was very valuable.},
 address = {New York, NY, USA},
 isbn = {978-1-4503-0759-8},
 location = {London, England, UK},
 note = {415125},
 publisher = {ACM},
 title = {ASPLOS XVII: Proceedings of the Seventeenth International Conference on Architectural Support for Programming Languages and Operating Systems},
 year = {2012}
}


@inproceedings{Viennot:2013:TMR:2451116.2451130,
 abstract = {We present Dora, a mutable record-replay system which allows a recorded execution of an application to be replayed with a modified version of the application. This feature, not available in previous record-replay systems, enables powerful new functionality. In particular, Dora can help reproduce, diagnose, and fix software bugs by replaying a version of a recorded application that is recompiled with debugging information, reconfigured to produce verbose log output, modified to include additional print statements, or patched to fix a bug. Dora uses lightweight operating system mechanisms to record an application execution by capturing nondeterministic events to a log without imposing unnecessary timing and ordering constraints. It replays the log using a modified version of the application even in the presence of added, deleted, or modified operations that do not match events in the log. Dora searches for a replay that minimizes differences between the log and the replayed execution of the modified program. If there are no modifications, Dora provides deterministic replay of the unmodified program. We have implemented a Linux prototype which provides transparent mutable replay without recompiling or relinking applications. We show that Dora is useful for reproducing, diagnosing, and fixing software bugs in real-world applications, including Apache and MySQL. Our results show that Dora (1) captures bugs and replays them with applications modified or reconfigured to produce additional debugging output for root cause diagnosis, (2) captures exploits and replays them with patched applications to validate that the patches successfully eliminate vulnerabilities, (3) records production workloads and replays them with patched applications to validate patches with realistic workloads, and (4) maintains low recording overhead on commodity multicore hardware, making it suitable for production systems.},
 acmid = {2451130},
 address = {New York, NY, USA},
 author = {Viennot, Nicolas and Nair, Siddharth and Nieh, Jason},
 booktitle = {Proceedings of the Eighteenth International Conference on Architectural Support for Programming Languages and Operating Systems},
 doi = {10.1145/2451116.2451130},
 isbn = {978-1-4503-1870-9},
 keyword = {debugging, deterministic replay, multicore, mutable replay},
 link = {http://doi.acm.org/10.1145/2451116.2451130},
 location = {Houston, Texas, USA},
 numpages = {12},
 pages = {127--138},
 publisher = {ACM},
 series = {ASPLOS '13},
 title = {Transparent Mutable Replay for Multicore Debugging and Patch Validation},
 year = {2013}
}


@article{Checkoway:2013:IAW:2499368.2451145,
 abstract = {In recent years, researchers have proposed systems for running trusted code on an untrusted operating system. Protection mechanisms deployed by such systems keep a malicious kernel from directly manipulating a trusted application's state. Under such systems, the application and kernel are, conceptually, peers, and the system call API defines an RPC interface between them. We introduce Iago attacks, attacks that a malicious kernel can mount in this model. We show how a carefully chosen sequence of integer return values to Linux system calls can lead a supposedly protected process to act against its interests, and even to undertake arbitrary computation at the malicious kernel's behest. Iago attacks are evidence that protecting applications from malicious kernels is more difficult than previously realized.},
 acmid = {2451145},
 address = {New York, NY, USA},
 author = {Checkoway, Stephen and Shacham, Hovav},
 doi = {10.1145/2499368.2451145},
 issn = {0362-1340},
 issue_date = {April 2013},
 journal = {SIGPLAN Not.},
 keyword = {iago attacks, overshadow, system call},
 link = {http://doi.acm.org/10.1145/2499368.2451145},
 month = {mar},
 number = {4},
 numpages = {12},
 pages = {253--264},
 publisher = {ACM},
 title = {Iago Attacks: Why the System Call API is a Bad Untrusted RPC Interface},
 volume = {48},
 year = {2013}
}


@article{Qian:2013:VSP:2499368.2451174,
 abstract = {Sequential Consistency (SC) is the most intuitive memory model, and SC Violations (SCVs) produce unintuitive, typically incorrect executions. Most prior SCV detection schemes have used data races as proxies for SCVs, which is highly imprecise. Other schemes that have targeted data-race cycles are either too conservative or are designed only for two-processor cycles and snoopy-based systems. This paper presents Volition, the first hardware scheme that detects SCVs in a relaxed-consistency machine precisely, in a scalable manner, and for an arbitrary number of processors in the cycle. Volition leverages cache coherence protocol transactions to dynamically detect cycles in memory-access orders across threads. When a cycle is about to occur, an exception is triggered. Volition can be used in both directory- and snoopy-based coherence protocols. Our simulations of Volition in a 64-processor multicore with directory-based coherence running SPLASH-2 and Parsec programs shows that Volition induces negligible traffic and execution overhead. In addition, it can detect SCVs with several processors. Volition is suitable for on-the-fly use.},
 acmid = {2451174},
 address = {New York, NY, USA},
 author = {Qian, Xuehai and Torrellas, Josep and Sahelices, Benjamin and Qian, Depei},
 doi = {10.1145/2499368.2451174},
 issn = {0362-1340},
 issue_date = {April 2013},
 journal = {SIGPLAN Not.},
 keyword = {memory consistency, parallel programming, sequential consistency, shared-memory multiprocessors},
 link = {http://doi.acm.org/10.1145/2499368.2451174},
 month = {mar},
 number = {4},
 numpages = {14},
 pages = {535--548},
 publisher = {ACM},
 title = {Volition: Scalable and Precise Sequential Consistency Violation Detection},
 volume = {48},
 year = {2013}
}


@article{Goiri:2013:PGM:2490301.2451123,
 abstract = {Several companies have recently announced plans to build "green" datacenters, i.e. datacenters partially or completely powered by renewable energy. These datacenters will either generate their own renewable energy or draw it directly from an existing nearby plant. Besides reducing carbon footprints, renewable energy can potentially reduce energy costs, reduce peak power costs, or both. However, certain renewable fuels are intermittent, which requires approaches for tackling the energy supply variability. One approach is to use batteries and/or the electrical grid as a backup for the renewable energy. It may also be possible to adapt the workload to match the renewable energy supply. For highest benefits, green datacenter operators must intelligently manage their workloads and the sources of energy at their disposal. In this paper, we first discuss the tradeoffs involved in building green datacenters today and in the future. Second, we present Parasol, a prototype green datacenter that we have built as a research platform. Parasol comprises a small container, a set of solar panels, a battery bank, and a grid-tie. Third, we describe GreenSwitch, our model-based approach for dynamically scheduling the workload and selecting the source of energy to use. Our real experiments with Parasol, GreenSwitch, and MapReduce workloads demonstrate that intelligent workload and energy source management can produce significant cost reductions. Our results also isolate the cost implications of peak power management, storing energy on the grid, and the ability to delay the MapReduce jobs. Finally, our results demonstrate that careful workload and energy source management can minimize the negative impact of electrical grid outages.},
 acmid = {2451123},
 address = {New York, NY, USA},
 author = {Goiri, \'{I}\~{n}igo and Katsak, William and Le, Kien and Nguyen, Thu D. and Bianchini, Ricardo},
 doi = {10.1145/2490301.2451123},
 issn = {0163-5964},
 issue_date = {March 2013},
 journal = {SIGARCH Comput. Archit. News},
 keyword = {batteries, datacenters, renewable energy, scheduling},
 link = {http://doi.acm.org/10.1145/2490301.2451123},
 month = {mar},
 number = {1},
 numpages = {14},
 pages = {51--64},
 publisher = {ACM},
 title = {Parasol and GreenSwitch: Managing Datacenters Powered by Renewable Energy},
 volume = {41},
 year = {2013}
}


@proceedings{Balasubramonian:2014:2541940,
 abstract = {It is our great pleasure to welcome you to the 2014 ACM International Conference on Architectural Support for Programming Languages and Operating Systems -- ASPLOS-XIX in Salt Lake City, UT on March 1-5, 2014. This year's ASPLOS finds a home in beautiful Salt Lake City, a thriving technology hub that is surrounded by snow-capped mountains and a handful of geologically diverse national parks. In fact, the large number of technology start-ups in the Salt Lake valley has inspired the moniker "Silicon Slopes". Visitors can choose among a wide array of activities while in town -- skiing or snowboarding in one of eight nearby world-class ski resorts, hiking on numerous local trails, visiting Historic Temple Square in downtown, or taking the five-hour drive to national parks in Utah and surrounding states. As the site of the 2002 Winter Olympic games, there are numerous Olympic class facilities nearby. The conference banquet will be held at the Olympic Oval, site of both the long and short track speed skating events. Apollo Anton Ohno appeared on the world stage as the gold medalist in the 1500 meter short-track event in the 2002 Olympics. We hope the banquet will not only provide the usual social opportunity to mingle with colleagues but also give you all the chance to feel some of the spirit that lies behind the Winter Olympic Games.},
 address = {New York, NY, USA},
 isbn = {978-1-4503-2305-5},
 location = {Salt Lake City, Utah, USA},
 publisher = {ACM},
 title = {ASPLOS '14: Proceedings of the 19th International Conference on Architectural Support for Programming Languages and Operating Systems},
 year = {2014}
}


@article{Tang:2013:RRS:2490301.2451126,
 abstract = {As multicore processors with expanding core counts continue to dominate the server market, the overall utilization of the class of datacenters known as warehouse scale computers (WSCs) depends heavily on colocation of multiple workloads on each server to take advantage of the computational power provided by modern processors. However, many of the applications running in WSCs, such as websearch, are user-facing and have quality of service (QoS) requirements. When multiple applications are co-located on a multicore machine, contention for shared memory resources threatens application QoS as severe cross-core performance interference may occur. WSC operators are left with two options: either disregard QoS to maximize WSC utilization, or disallow the co-location of high-priority user-facing applications with other applications, resulting in low machine utilization and millions of dollars wasted. This paper presents ReQoS, a static/dynamic compilation approach that enables low-priority applications to adaptively manipulate their own contentiousness to ensure the QoS of high-priority co-runners. ReQoS is composed of a profile guided compilation technique that identifies and inserts markers in contentious code regions in low-priority applications, and a lightweight runtime that monitors the QoS of high-priority applications and reactively reduces the pressure low-priority applications generate to the memory subsystem when cross-core interference is detected. In this work, we show that ReQoS can accurately diagnose contention and significantly reduce performance interference to ensure application QoS. Applying ReQoS to SPEC2006 and SmashBench workloads on real multicore machines, we are able to improve machine utilization by more than 70% in many cases, and more than 50% on average, while enforcing a 90% QoS threshold. We are also able to improve the energy efficiency of modern multicore machines by 47% on average over a policy of disallowing co-locations.},
 acmid = {2451126},
 address = {New York, NY, USA},
 author = {Tang, Lingjia and Mars, Jason and Wang, Wei and Dey, Tanima and Soffa, Mary Lou},
 doi = {10.1145/2490301.2451126},
 issn = {0163-5964},
 issue_date = {March 2013},
 journal = {SIGARCH Comput. Archit. News},
 keyword = {compiler, contention, cross-core interference, datacenter, dynamic techniques, multicore, online adaptation, quality of service, runtime systems, warehouse scale computers},
 link = {http://doi.acm.org/10.1145/2490301.2451126},
 month = {mar},
 number = {1},
 numpages = {12},
 pages = {89--100},
 publisher = {ACM},
 title = {ReQoS: Reactive Static/Dynamic Compilation for QoS in Warehouse Scale Computers},
 volume = {41},
 year = {2013}
}


@inproceedings{Honarmand:2013:CUA:2451116.2451138,
 abstract = {Architectures for deterministic record-replay (R&R) of multithreaded code are attractive for program debugging, intrusion analysis, and fault-tolerance uses. However, very few of the proposed designs have focused on maximizing replay speed -- a key enabling property of these systems. The few efforts that focus on replay speed require intrusive hardware or software modifications, or target whole-system R&R rather than the more useful application-level R&R. This paper presents the first hardware-based scheme for unintrusive, application-level R&R that explicitly targets high replay speed. Our scheme, called Cyrus, requires no modification to commodity snoopy cache coherence. It introduces the concept of an on-the-fly software Backend Pass during recording which, as the log is being generated, transforms it for high replay parallelism. This pass also fixes-up the log, and can flexibly trade-off replay parallelism for log size. We analyze the performance of Cyrus using full system (OS plus hardware) simulation. Our results show that Cyrus has negligible recording overhead. In addition, for 8-processor runs of SPLASH-2, Cyrus attains an average replay parallelism of 5, and a replay speed that is, on average, only about 50% lower than the recording speed.},
 acmid = {2451138},
 address = {New York, NY, USA},
 author = {Honarmand, Nima and Dautenhahn, Nathan and Torrellas, Josep and King, Samuel T. and Pokam, Gilles and Pereira, Cristiano},
 booktitle = {Proceedings of the Eighteenth International Conference on Architectural Support for Programming Languages and Operating Systems},
 doi = {10.1145/2451116.2451138},
 isbn = {978-1-4503-1870-9},
 keyword = {application-level parallel replay, backend log processing, deterministic replay, source-only recording, unintrusive hardware-assisted recording},
 link = {http://doi.acm.org/10.1145/2451116.2451138},
 location = {Houston, Texas, USA},
 numpages = {14},
 pages = {193--206},
 publisher = {ACM},
 series = {ASPLOS '13},
 title = {Cyrus: Unintrusive Application-level Record-replay for Replay Parallelism},
 year = {2013}
}


@inproceedings{Mittal:2013:EVE:2451116.2451163,
 abstract = {Power ArchitectureÂ® processors are popular and widespread on embedded systems, and such platforms are increasingly being used to run virtual machines. While the Power Architecture meets the Popek-and-Goldberg virtualization requirements for traditional trap-and-emulate style virtualization, the performance overhead of virtualization remains high. For example, workloads exhibiting a large amount of kernel activity typically show 3-5x slowdowns over bare-metal. Recent additions to the Linux kernel contain guest and host side paravirtual extensions for Power Architecture platforms. While these extensions improve performance significantly, they are guest-specific, guest-intrusive, and cover only a subset of all possible virtualization optimizations. We present a set of host-side optimizations that achieve comparable performance to the aforementioned paravirtual extensions, on an unmodified guest. Our optimizations are based on adaptive in-place binary translation. Unlike the paravirtual approach, our solution is guest neutral. We implement our ideas in a prototype based on Qemu/KVM. After our modifications, KVM can boot an unmodified Linux guest around 2.5x faster. We contrast our optimization approach with previous similar binary translation based approaches for the x86 architecture; in our experience, each architecture presents a unique set of challenges and optimization opportunities.},
 acmid = {2451163},
 address = {New York, NY, USA},
 author = {Mittal, Aashish and Bansal, Dushyant and Bansal, Sorav and Sethi, Varun},
 booktitle = {Proceedings of the Eighteenth International Conference on Architectural Support for Programming Languages and Operating Systems},
 doi = {10.1145/2451116.2451163},
 isbn = {978-1-4503-1870-9},
 keyword = {adaptive data mirroring, adaptive page resizing, architecture design, code patching, dynamic binary translation, in-place binary translation, power architecture platforms, read/write tracing, tlb, virtual machine monitor, virtualization},
 link = {http://doi.acm.org/10.1145/2451116.2451163},
 location = {Houston, Texas, USA},
 numpages = {14},
 pages = {445--458},
 publisher = {ACM},
 series = {ASPLOS '13},
 title = {Efficient Virtualization on Embedded Power Architecture\textregistered Platforms},
 year = {2013}
}


@article{Kang:2013:HPP:2499368.2451155,
 abstract = {Most hardware and software venders suggest disabling hardware prefetching in virtualized environments. They claim that prefetching is detrimental to application performance due to inaccurate prediction caused by workload diversity and VM interference on shared cache. However, no comprehensive or quantitative measurements to support this belief have been performed. This paper is the first to systematically measure the influence of hardware prefetching in virtualized environments. We examine a wide variety of benchmarks on three types of chip-multiprocessors (CMPs) to analyze the hardware prefetching performance. We conduct extensive experiments by taking into account a number of important virtualization factors. We find that hardware prefetching has minimal destructive influence under most configurations. Only with certain application combinations does prefetching influence the overall performance. To leverage these findings and make hardware prefetching effective across a diversity of virtualized environments, we propose a dynamic prefetching-aware VCPU-core binding approach (PAVCB), which includes two phases - classifying and binding. The workload of each VM is classified into different cache sharing constraint categories based upon its cache access characteristics, considering both prefetch requests and demand requests. Then following heuristic rules, the VCPUs of each VM are scheduled onto appropriate cores subject to cache sharing constraints. We show that the proposed approach can improve performance by 12% on average over the default scheduler and 46% over manual system administrator bindings across different workload combinations in the presence of hardware prefetching.},
 acmid = {2451155},
 address = {New York, NY, USA},
 author = {Kang, Hui and Wong, Jennifer L.},
 doi = {10.1145/2499368.2451155},
 issn = {0362-1340},
 issue_date = {April 2013},
 journal = {SIGPLAN Not.},
 keyword = {prefetching, shared cache, virtualization},
 link = {http://doi.acm.org/10.1145/2499368.2451155},
 month = {mar},
 number = {4},
 numpages = {12},
 pages = {357--368},
 publisher = {ACM},
 title = {To Hardware Prefetch or Not to Prefetch?: A Virtualized Environment Study and Core Binding Approach},
 volume = {48},
 year = {2013}
}


@inproceedings{Park:2013:RCH:2451116.2451137,
 abstract = {We propose a novel kernel-level memory allocator, called M3 (M-cube, Multi-core Multi-bank Memory allocator), that has the following two features. First, it introduces and makes use of a notion of a memory container, which is defined as a unit of memory that comprises the minimum number of page frames that can cover all the banks of the memory organization, by exclusively assigning a container to a core so that each core achieves bank parallelism as much as possible. Second, it orchestrates page frame allocation so that pages that threads access are dispersed randomly across multiple banks so that each thread's access pattern is randomized. The development of M3 is based on a tool that we develop to fully understand the architectural characteristics of the underlying memory organization. Using an extension of this tool, we observe that the same application that accesses pages in a random manner outperforms one that accesses pages in a regular pattern such as sequential or same ordered accesses. This is because such randomized accesses reduces inter-thread access interference on the row-buffer in memory banks. We implement M3 in the Linux kernel version 2.6.32 on the Intel Xeon system that has 16 cores and 32GB DRAM. Performance evaluation with various workloads show that M3 improves the overall performance for memory intensive benchmarks by up to 85% with an average of about 40%.},
 acmid = {2451137},
 address = {New York, NY, USA},
 author = {Park, Heekwon and Baek, Seungjae and Choi, Jongmoo and Lee, Donghee and Noh, Sam H.},
 booktitle = {Proceedings of the Eighteenth International Conference on Architectural Support for Programming Languages and Operating Systems},
 doi = {10.1145/2451116.2451137},
 isbn = {978-1-4503-1870-9},
 keyword = {analysis tool, memory container, memory management, randomized algorithm, row-buffer conflict},
 link = {http://doi.acm.org/10.1145/2451116.2451137},
 location = {Houston, Texas, USA},
 numpages = {12},
 pages = {181--192},
 publisher = {ACM},
 series = {ASPLOS '13},
 title = {Regularities Considered Harmful: Forcing Randomness to Memory Accesses to Reduce Row Buffer Conflicts for Multi-core, Multi-bank Systems},
 year = {2013}
}


@article{Hofmann:2013:ISA:2499368.2451146,
 abstract = {InkTag is a virtualization-based architecture that gives strong safety guarantees to high-assurance processes even in the presence of a malicious operating system. InkTag advances the state of the art in untrusted operating systems in both the design of its hypervisor and in the ability to run useful applications without trusting the operating system. We introduce paraverification, a technique that simplifies the InkTag hypervisor by forcing the untrusted operating system to participate in its own verification. Attribute-based access control allows trusted applications to create decentralized access control policies. InkTag is also the first system of its kind to ensure consistency between secure data and metadata, ensuring recoverability in the face of system crashes.},
 acmid = {2451146},
 address = {New York, NY, USA},
 author = {Hofmann, Owen S. and Kim, Sangman and Dunn, Alan M. and Lee, Michael Z. and Witchel, Emmett},
 doi = {10.1145/2499368.2451146},
 issn = {0362-1340},
 issue_date = {April 2013},
 journal = {SIGPLAN Not.},
 keyword = {application protection, paraverification, virtualization-based security},
 link = {http://doi.acm.org/10.1145/2499368.2451146},
 month = {mar},
 number = {4},
 numpages = {14},
 pages = {265--278},
 publisher = {ACM},
 title = {InkTag: Secure Applications on an Untrusted Operating System},
 volume = {48},
 year = {2013}
}


@article{Raghavan:2013:CSH:2490301.2451135,
 abstract = {CMOS scaling trends have led to an inflection point where thermal constraints (especially in mobile devices that employ only passive cooling) preclude sustained operation of all transistors on a chip --- a phenomenon called "dark silicon." Recent research proposed computational sprinting --- exceeding sustainable thermal limits for short intervals --- to improve responsiveness in light of the bursty computation demands of many media-rich interactive mobile applications. Computational sprinting improves responsiveness by activating reserve cores (parallel sprinting) and/or boosting frequency/voltage (frequency sprinting) to power levels that far exceed the system's sustainable cooling capabilities, relying on thermal capacitance to buffer heat. Prior work analyzed the feasibility of sprinting through modeling and simulation. In this work, we investigate sprinting using a hardware/software testbed. First, we study unabridged sprints, wherein the computation completes before temperature becomes critical, demonstrating a 6.3x responsiveness gain, and a 6% energy efficiency improvement by racing to idle. We then analyze truncated sprints, wherein our software runtime system must intervene to prevent overheating by throttling parallelism and frequency before the computation is complete. To avoid oversubscription penalties (context switching inefficiencies after a truncated parallel sprint), we develop a sprint-aware task-based parallel runtime. We find that maximal-intensity sprinting is not always best, introduce the concept of sprint pacing, and evaluate an adaptive policy for selecting sprint intensity. We report initial results using a phase change heat sink to extend maximum sprint duration. Finally, we demonstrate that a sprint-and-rest operating regime can actually outperform thermally-limited sustained execution.},
 acmid = {2451135},
 address = {New York, NY, USA},
 author = {Raghavan, Arun and Emurian, Laurel and Shao, Lei and Papaefthymiou, Marios and Pipe, Kevin P. and Wenisch, Thomas F. and Martin, Milo M.K.},
 doi = {10.1145/2490301.2451135},
 issn = {0163-5964},
 issue_date = {March 2013},
 journal = {SIGARCH Comput. Archit. News},
 keyword = {computer architecture, energy-efficiency, phase change material, real-system measurement, task-based parallelism, thermal-aware computing, voltage-frequency scaling},
 link = {http://doi.acm.org/10.1145/2490301.2451135},
 month = {mar},
 number = {1},
 numpages = {12},
 pages = {155--166},
 publisher = {ACM},
 title = {Computational Sprinting on a Hardware/Software Testbed},
 volume = {41},
 year = {2013}
}


@proceedings{Sarkar:2013:2451116,
 abstract = {It is our great pleasure to welcome you to the Eighteenth International Conference on Architectural Support for Programming Languages and Operating Systems --- ASPLOS 2013! This year's conference continues and reinforces the ASPLOS tradition of publishing innovative multidisciplinary research spanning the boundaries of hardware, computer architecture, compilers, languages, operating systems, networking, and applications. This year, authors submitted 193 papers, 12% more than last year when 172 papers were submitted, and 7% more than in 2010, the previous record year, when 180 papers were submitted. PC members co-authored 22 submissions. The following ten topic keywords were marked by at least 20 submissions: compilation, parallelization and optimization (40 submissions), programming models (32), OS scheduling and resource management (31), OS abstractions (29), testing and debugging (27), power/energy/thermal management (27), memory optimizations (25), heterogeneous architectures (24), parallel programming languages (23), and virtualization (21). The 2013 ASPLOS Program Committee (PC) consisted of 35 members; each member was assigned 18 submissions, a workload of four more papers than the previous year. As in previous years, the Program Chair recruited an External Review Committee (ERC) whose expertise complemented that of the PC. The 2013 ERC consisted of 63 members, assigned 5 papers each. Additionally, 90 sub reviewers prepared 143 reviews. In total, we received 943 reviews, an average of 4.89 reviews per paper. 90% of submissions received at least five reviews, and all received at least four. As in previous years, we employed two rounds of reviews. A significant difference this year was that no papers were rejected after the first round. The two phases were employed merely to spread out the review workload and to allocate reviewer expertise based on first-round reviews. After a three-day author response period, the PC and ERC members entered online discussion lasting about a week. The online discussion, moderated by paper discussion leaders assigned to each paper, was unusually lively. The discussion resulted in 71 papers being selected for deliberation at the physical PC meeting. During the entire review and selection process, scores alone were never used to reject a submission. The PC gathered in Berkeley, CA, on Nov. 1, 2012, for a one-day meeting. Three committee members were unable to attend, mostly due to travel disruptions from the Hurricane Sandy. These members participated in online discussions prior to the meeting. The meeting followed the protocol of allocating a fixed amount of time for the discussion of each paper. If a consensus was not reached, the paper advanced to a discussion phase, which had the form of small groups composed of paper reviewers and other relevant PC members. In the late afternoon, these groups reported back to the PC with arguments for or against accepting the paper. PC and ERC members volunteered to shepherd 7 papers to ensure reviewer comments were properly addressed. The PC-authored submissions were not discussed at the meeting (see below). PC members with conflicts left the room during discussions, as is customary. Vivek Sarkar, the General Chair, attended the PC meeting as an observer and handled conflicts involving the Program Chair. In the end, the PC and ERC accepted 44 papers, 7 more than the previous year. The acceptance rate is 22.8%, compared to 21.5% the previous year. The 22 PC-authored submissions were reviewed only by the ERC. The ERC discussed and accepted these submissions in an online discussion, conducted mostly prior to the physical PC meeting. The ERC held the PC submissions to the same conference standard as other papers. Preliminary acceptance decisions, done by the ERC before the PC meeting, were calibrated against the papers accepted at the PC meeting with the help of the Program and General Chairs. In the end, 5 PC submissions were accepted. The acceptance rate was the same as for non-PC submissions.},
 address = {New York, NY, USA},
 isbn = {978-1-4503-1870-9},
 location = {Houston, Texas, USA},
 publisher = {ACM},
 title = {ASPLOS '13: Proceedings of the Eighteenth International Conference on Architectural Support for Programming Languages and Operating Systems},
 year = {2013}
}


@article{Jooybar:2013:GDG:2490301.2451118,
 abstract = {Nondeterminism is a key challenge in developing multithreaded applications. Even with the same input, each execution of a multithreaded program may produce a different output. This behavior complicates debugging and limits one's ability to test for correctness. This non-reproducibility situation is aggravated on massively parallel architectures like graphics processing units (GPUs) with thousands of concurrent threads. We believe providing a deterministic environment to ease debugging and testing of GPU applications is essential to enable a broader class of software to use GPUs. Many hardware and software techniques have been proposed for providing determinism on general-purpose multi-core processors. However, these techniques are designed for small numbers of threads. Scaling them to thousands of threads on a GPU is a major challenge. This paper proposes a scalable hardware mechanism, GPUDet, to provide determinism in GPU architectures. In this paper we characterize the existing deterministic and nondeterministic aspects of current GPU execution models, and we use these observations to inform GPUDet's design. For example, GPUDet leverages the inherent determinism of the SIMD hardware in GPUs to provide determinism within a wavefront at no cost. GPUDet also exploits the Z-Buffer Unit, an existing GPU hardware unit for graphics rendering, to allow parallel out-of-order memory writes to produce a deterministic output. Other optimizations in GPUDet include deterministic parallel execution of atomic operations and a workgroup-aware algorithm that eliminates unnecessary global synchronizations. Our simulation results indicate that GPUDet incurs only 2X slowdown on average over a baseline nondeterministic architecture, with runtime overheads as low as 4% for compute-bound applications, despite running GPU kernels with thousands of threads. We also characterize the sources of overhead for deterministic execution on GPUs to provide insights for further optimizations.},
 acmid = {2451118},
 address = {New York, NY, USA},
 author = {Jooybar, Hadi and Fung, Wilson W.L. and O'Connor, Mike and Devietti, Joseph and Aamodt, Tor M.},
 doi = {10.1145/2490301.2451118},
 issn = {0163-5964},
 issue_date = {March 2013},
 journal = {SIGARCH Comput. Archit. News},
 keyword = {deterministic parallelism, gpu},
 link = {http://doi.acm.org/10.1145/2490301.2451118},
 month = {mar},
 number = {1},
 numpages = {12},
 pages = {1--12},
 publisher = {ACM},
 title = {GPUDet: A Deterministic GPU Architecture},
 volume = {41},
 year = {2013}
}


@article{Ahn:2013:DAS:2490301.2451136,
 abstract = {Alias analysis is a critical component in many compiler optimizations. A promising approach to reduce the complexity of alias analysis is to use speculation. The approach consists of performing optimizations assuming the alias relationships that are true most of the time, and repairing the code when such relationships are found not to hold through runtime checks. This paper proposes a general alias speculation scheme that leverages upcoming hardware support for transactions with the help of some ISA extensions. The ability of transactions to checkpoint and roll back frees the compiler to pursue aggressive optimizations without having to worry about recovery code. Also, exposing the memory conflict detection hardware in transactions to software allows runtime checking of aliases with little or no overhead. We test the potential of the novel alias speculation approach with Loop Invariant Code Motion (LICM), Global Value Numbering (GVN), and Partial Redundancy Elimination (PRE) optimization passes. On average, they are shown to reduce program execution time by 9% in SPEC FP2006 applications and 3% in SPEC INT2006 applications over the alias analysis of a state-of-the-art compiler.},
 acmid = {2451136},
 address = {New York, NY, USA},
 author = {Ahn, Wonsun and Duan, Yuelu and Torrellas, Josep},
 doi = {10.1145/2490301.2451136},
 issn = {0163-5964},
 issue_date = {March 2013},
 journal = {SIGARCH Comput. Archit. News},
 keyword = {alias analysis, atomic region, compiler optimization, transactional memory},
 link = {http://doi.acm.org/10.1145/2490301.2451136},
 month = {mar},
 number = {1},
 numpages = {14},
 pages = {167--180},
 publisher = {ACM},
 title = {DeAliaser: Alias Speculation Using Atomic Region Support},
 volume = {41},
 year = {2013}
}


@article{Giuffrida:2013:SAL:2499368.2451147,
 abstract = {Increasingly many systems have to run all the time with no downtime allowed. Consider, for example, systems controlling electric power plants and e-banking servers. Nevertheless, security patches and a constant stream of new operating system versions need to be deployed without stopping running programs. These factors naturally lead to a pressing demand for live update---upgrading all or parts of the operating system without rebooting. Unfortunately, existing solutions require significant manual intervention and thus work reliably only for small operating system patches. In this paper, we describe an automated system for live update that can safely and automatically handle major upgrades without rebooting. We have implemented our ideas in Proteos, a new research OS designed with live update in mind. Proteos relies on system support and nonintrusive instrumentation to handle even very complex updates with minimal manual effort. The key novelty is the idea of state quiescence, which allows updates to happen only in safe and predictable system states. A second novelty is the ability to automatically perform transactional live updates at the process level, ensuring a safe and stable update process. Unlike prior solutions, Proteos supports automated state transfer, state checking, and hot rollback. We have evaluated Proteos on 50 real updates and on novel live update scenarios. The results show that our techniques can effectively support both simple and complex updates, while outperforming prior solutions in terms of flexibility, security, reliability, and stability of the update process.},
 acmid = {2451147},
 address = {New York, NY, USA},
 author = {Giuffrida, Cristiano and Kuijsten, Anton and Tanenbaum, Andrew S.},
 doi = {10.1145/2499368.2451147},
 issn = {0362-1340},
 issue_date = {April 2013},
 journal = {SIGPLAN Not.},
 keyword = {automatic updates, live update, operating systems, state checking, state transfer, update safety},
 link = {http://doi.acm.org/10.1145/2499368.2451147},
 month = {mar},
 number = {4},
 numpages = {14},
 pages = {279--292},
 publisher = {ACM},
 title = {Safe and Automatic Live Update for Operating Systems},
 volume = {48},
 year = {2013}
}


@article{Kadav:2013:FFT:2490301.2451168,
 abstract = {Recovering faults in drivers is difficult compared to other code because their state is spread across both memory and a device. Existing driver fault-tolerance mechanisms either restart the driver and discard its state, which can break applications, or require an extensive logging mechanism to replay requests and recreate driver state. Even logging may be insufficient, though, if the semantics of requests are ambiguous. In addition, these systems either require large subsystems that must be kept up-to-date as the kernel changes, or require substantial rewriting of drivers. We present a new driver fault-tolerance mechanism that provides fine-grained control over the code protected. Fine-Grained Fault Tolerance (FGFT) isolates driver code at the granularity of a single entry point. It executes driver code as a transaction, allowing roll back if the driver fails. We develop a novel checkpointing mechanism to save and restore device state using existing power management code. Unlike past systems, FGFT can be incrementally deployed in a single driver without the need for a large kernel subsystem, but at the cost of small modifications to the driver. In the evaluation, we show that FGFT can have almost zero runtime cost in many cases, and that checkpoint-based recovery can reduce the duration of a failure by 79% compared to restarting the driver. Finally, we show that applying FGFT to a driver requires little effort, and the majority of drivers in common classes already contain the power-management code needed for checkpoint/restore.},
 acmid = {2451168},
 address = {New York, NY, USA},
 author = {Kadav, Asim and Renzelmann, Matthew J. and Swift, Michael M.},
 doi = {10.1145/2490301.2451168},
 issn = {0163-5964},
 issue_date = {March 2013},
 journal = {SIGARCH Comput. Archit. News},
 keyword = {checkpoints, device drivers},
 link = {http://doi.acm.org/10.1145/2490301.2451168},
 month = {mar},
 number = {1},
 numpages = {12},
 pages = {473--484},
 publisher = {ACM},
 title = {Fine-grained Fault Tolerance Using Device Checkpoints},
 volume = {41},
 year = {2013}
}


@article{Jafri:2013:WIH:2490301.2451173,
 abstract = {Transactional memory (TM) has been proposed to alleviate some key programmability problems in chip multiprocessors. Most TMs optimistically allow concurrent transactions, detecting read-write or write-write conflicts. Upon conflicts, existing hardware TMs (HTMs) use one of three conflict-resolution policies: (1) always-abort, (2) always-wait for some conflicting transactions to complete, or (3) always-go past conflicts and resolve acyclic conflicts at commit or abort upon cyclic dependencies. While each policy has advantages, the policies degrade performance under contention by limiting concurrency (always-abort, always-wait) or incurring late aborts due to cyclic dependencies (always-go). Thus, while always-go avoids acyclic aborts, no policy avoids cyclic aborts. We propose Wait-n-GoTM (WnGTM) to increase concurrency while avoiding cyclic aborts. We observe that most cyclic dependencies are caused by threads interleaving multiple accesses to a few heavily-read-write-shared delinquent data cache blocks. These accesses occur in code sections called cycle inducer sections (CISTs). Accordingly, we propose Wait-n-Go (WnG) conflict-resolution to avoid many cyclic aborts by predicting and serializing the CISTs. To support the WnG policy, we extend previous HTMs to (1) allow multiple readers and writers, (2) scalably identify dependencies, and (3) detect cyclic dependencies via new mechanisms, namely, conflict transactional state, order-capture, and hardware timestamps, respectively. In 16-core simulations of STAMP, WnGTM achieves average speedups of 46% for higher-contention benchmarks and 28% for all benchmarks over always-abort (TokenTM) with low-contention benchmarks remaining unchanged, compared to always-go (DATM) and always-wait (LogTM-SE), which perform worse than and 6% better than TokenTM, respectively.},
 acmid = {2451173},
 address = {New York, NY, USA},
 author = {Jafri, Syed Ali Raza and Voskuilen, Gwendolyn and Vijaykumar, T. N.},
 doi = {10.1145/2490301.2451173},
 issn = {0163-5964},
 issue_date = {March 2013},
 journal = {SIGARCH Comput. Archit. News},
 keyword = {cyclic dependencies, serializing transactions, transactional conflicts, transactional memory},
 link = {http://doi.acm.org/10.1145/2490301.2451173},
 month = {mar},
 number = {1},
 numpages = {14},
 pages = {521--534},
 publisher = {ACM},
 title = {Wait-n-GoTM: Improving HTM Performance by Serializing Cyclic Dependencies},
 volume = {41},
 year = {2013}
}


@article{Hill:2013:RDC:2490301.2451165,
 abstract = {Four recent efforts call out architectural challenges and opportunities up and down the software/hardware stack. This panel will discuss, "What should the community do to facilitate, transcend, or refute these partially overlapping visions?" The panel is chaired by Mark D. Hill with other panel members not finalized for the ASPLOS'13 proceedings.},
 acmid = {2451165},
 address = {New York, NY, USA},
 author = {Hill, Mark D.},
 doi = {10.1145/2490301.2451165},
 issn = {0163-5964},
 issue_date = {March 2013},
 journal = {SIGARCH Comput. Archit. News},
 keyword = {architecture, computer systems, energy, new technology, performance, programming methods},
 link = {http://doi.acm.org/10.1145/2490301.2451165},
 month = {mar},
 number = {1},
 numpages = {2},
 pages = {459--460},
 publisher = {ACM},
 title = {Research Directions for 21st Century Computer Systems: Asplos 2013 Panel},
 volume = {41},
 year = {2013}
}


@article{Hunt:2013:DTN:2499368.2451170,
 abstract = {Nondeterminism complicates the development and management of distributed systems, and arises from two main sources: the local behavior of each individual node as well as the behavior of the network connecting them. Taming nondeterminism effectively requires dealing with both sources. This paper proposes DDOS, a system that leverages prior work on deterministic multithreading to offer: 1) space-efficient record/replay of distributed systems; and 2) fully deterministic distributed behavior. Leveraging deterministic behavior at each node makes outgoing messages strictly a function of explicit inputs. This allows us to record the system by logging just message's arrival time, not the contents. Going further, we propose and implement an algorithm that makes all communication between nodes deterministic by scheduling communication onto a global logical timeline. We implement both algorithms in a system called DDOS and evaluate our system with parallel scientific applications, an HTTP/memcached system and a distributed microbenchmark with a high volume of peer-to-peer communication. Our results show up to two orders of magnitude reduction in log size of record/replay, and that distributed systems can be made deterministic with an order of magnitude of overhead.},
 acmid = {2451170},
 address = {New York, NY, USA},
 author = {Hunt, Nicholas and Bergan, Tom and Ceze, Luis and Gribble, Steven D.},
 doi = {10.1145/2499368.2451170},
 issn = {0362-1340},
 issue_date = {April 2013},
 journal = {SIGPLAN Not.},
 keyword = {determinism, distributed systems, record/replay},
 link = {http://doi.acm.org/10.1145/2499368.2451170},
 month = {mar},
 number = {4},
 numpages = {10},
 pages = {499--508},
 publisher = {ACM},
 title = {DDOS: Taming Nondeterminism in Distributed Systems},
 volume = {48},
 year = {2013}
}


@article{Dashti:2013:TMH:2490301.2451157,
 abstract = {NUMA systems are characterized by Non-Uniform Memory Access times, where accessing data in a remote node takes longer than a local access. NUMA hardware has been built since the late 80's, and the operating systems designed for it were optimized for access locality. They co-located memory pages with the threads that accessed them, so as to avoid the cost of remote accesses. Contrary to older systems, modern NUMA hardware has much smaller remote wire delays, and so remote access costs per se are not the main concern for performance, as we discovered in this work. Instead, congestion on memory controllers and interconnects, caused by memory traffic from data-intensive applications, hurts performance a lot more. Because of that, memory placement algorithms must be redesigned to target traffic congestion. This requires an arsenal of techniques that go beyond optimizing locality. In this paper we describe Carrefour, an algorithm that addresses this goal. We implemented Carrefour in Linux and obtained performance improvements of up to 3.6 relative to the default kernel, as well as significant improvements compared to NUMA-aware patchsets available for Linux. Carrefour never hurts performance by more than 4% when memory placement cannot be improved. We present the design of Carrefour, the challenges of implementing it on modern hardware, and draw insights about hardware support that would help optimize system software on future NUMA systems.},
 acmid = {2451157},
 address = {New York, NY, USA},
 author = {Dashti, Mohammad and Fedorova, Alexandra and Funston, Justin and Gaud, Fabien and Lachaize, Renaud and Lepers, Baptiste and Quema, Vivien and Roth, Mark},
 doi = {10.1145/2490301.2451157},
 issn = {0163-5964},
 issue_date = {March 2013},
 journal = {SIGARCH Comput. Archit. News},
 keyword = {multicore, numa, operating systems, scheduling},
 link = {http://doi.acm.org/10.1145/2490301.2451157},
 month = {mar},
 number = {1},
 numpages = {14},
 pages = {381--394},
 publisher = {ACM},
 title = {Traffic Management: A Holistic Approach to Memory Placement on NUMA Systems},
 volume = {41},
 year = {2013}
}


@article{McFarlin:2013:DDO:2490301.2451143,
 abstract = {In this paper, we set out to study the performance advantages of an Out-of-Order (OOO) processor relative to in-order processors with similar execution resources. In particular, we try to tease apart the performance contributions from two sources: the improved sched- ules enabled by OOO hardware speculation support and its ability to generate different schedules on different occurrences of the same instructions based on operand and functional unit availability. We find that the ability to express good static schedules achieves the bulk of the speedup resulting from OOO. Specifically, of the 53% speedup achieved by OOO relative to a similarly provisioned in- order machine, we find that 88% of that speedup can be achieved by using a single "best" static schedule as suggested by observing an OOO schedule of the code. We discuss the ISA mechanisms that would be required to express these static schedules. Furthermore, we find that the benefits of dynamism largely come from two kinds of events that influence the application's critical path: load instructions that miss in the cache only part of the time and branch mispredictions. We find that much of the benefit of OOO dynamism can be achieved by the potentially simpler task of addressing these two behaviors directly.},
 acmid = {2451143},
 address = {New York, NY, USA},
 author = {McFarlin, Daniel S. and Tucker, Charles and Zilles, Craig},
 doi = {10.1145/2490301.2451143},
 issn = {0163-5964},
 issue_date = {March 2013},
 journal = {SIGARCH Comput. Archit. News},
 keyword = {dynamic scheduling, hw/sw co-design},
 link = {http://doi.acm.org/10.1145/2490301.2451143},
 month = {mar},
 number = {1},
 numpages = {12},
 pages = {241--252},
 publisher = {ACM},
 title = {Discerning the Dominant Out-of-order Performance Advantage: Is It Speculation or Dynamism?},
 volume = {41},
 year = {2013}
}


@article{Jog:2013:OCT:2499368.2451158,
 abstract = {Emerging GPGPU architectures, along with programming models like CUDA and OpenCL, offer a cost-effective platform for many applications by providing high thread level parallelism at lower energy budgets. Unfortunately, for many general-purpose applications, available hardware resources of a GPGPU are not efficiently utilized, leading to lost opportunity in improving performance. A major cause of this is the inefficiency of current warp scheduling policies in tolerating long memory latencies. In this paper, we identify that the scheduling decisions made by such policies are agnostic to thread-block, or cooperative thread array (CTA), behavior, and as a result inefficient. We present a coordinated CTA-aware scheduling policy that utilizes four schemes to minimize the impact of long memory latencies. The first two schemes, CTA-aware two-level warp scheduling and locality aware warp scheduling, enhance per-core performance by effectively reducing cache contention and improving latency hiding capability. The third scheme, bank-level parallelism aware warp scheduling, improves overall GPGPU performance by enhancing DRAM bank-level parallelism. The fourth scheme employs opportunistic memory-side prefetching to further enhance performance by taking advantage of open DRAM rows. Evaluations on a 28-core GPGPU platform with highly memory-intensive applications indicate that our proposed mechanism can provide 33% average performance improvement compared to the commonly-employed round-robin warp scheduling policy.},
 acmid = {2451158},
 address = {New York, NY, USA},
 author = {Jog, Adwait and Kayiran, Onur and Chidambaram Nachiappan, Nachiappan and Mishra, Asit K. and Kandemir, Mahmut T. and Mutlu, Onur and Iyer, Ravishankar and Das, Chita R.},
 doi = {10.1145/2499368.2451158},
 issn = {0362-1340},
 issue_date = {April 2013},
 journal = {SIGPLAN Not.},
 keyword = {GPGPUs, latency tolerance, prefetching, scheduling},
 link = {http://doi.acm.org/10.1145/2499368.2451158},
 month = {mar},
 number = {4},
 numpages = {12},
 pages = {395--406},
 publisher = {ACM},
 title = {OWL: Cooperative Thread Array Aware Scheduling Techniques for Improving GPGPU Performance},
 volume = {48},
 year = {2013}
}


@article{Oh:2013:PAL:2499368.2451161,
 abstract = {Program specialization optimizes a program with respect to program invariants, including known, fixed inputs. These invariants can be used to enable optimizations that are otherwise unsound. In many applications, a program input induces predictable patterns of values across loop iterations, yet existing specializers cannot fully capitalize on this opportunity. To address this limitation, we present Invariant-induced Pattern based Loop Specialization (IPLS), the first fully-automatic specialization technique designed for everyday use on real applications. Using dynamic information-flow tracking, IPLS profiles the values of instructions that depend solely on invariants and recognizes repeating patterns across multiple iterations of hot loops. IPLS then specializes these loops, using those patterns to predict values across a large window of loop iterations. This enables aggressive optimization of the loop; conceptually, this optimization reconstructs recurring patterns induced by the input as concrete loops in the specialized binary. IPLS specializes real-world programs that prior techniques fail to specialize without requiring hints from the user. Experiments demonstrate a geomean speedup of 14.1% with a maximum speedup of 138% over the original codes when evaluated on three script interpreters and eleven scripts each.},
 acmid = {2451161},
 address = {New York, NY, USA},
 author = {Oh, Taewook and Kim, Hanjun and Johnson, Nick P. and Lee, Jae W. and August, David I.},
 doi = {10.1145/2499368.2451161},
 issn = {0362-1340},
 issue_date = {April 2013},
 journal = {SIGPLAN Not.},
 keyword = {loop specialization, partial evaluation, profile based optimization, program specialization},
 link = {http://doi.acm.org/10.1145/2499368.2451161},
 month = {mar},
 number = {4},
 numpages = {12},
 pages = {419--430},
 publisher = {ACM},
 title = {Practical Automatic Loop Specialization},
 volume = {48},
 year = {2013}
}


@article{Wang:2013:TEH:2490301.2451172,
 abstract = {Program optimizations based on data dependences may not preserve the memory consistency in the programs. Previous works leverage a hardware ATOMICITY primitive to restrict the thread interleaving for preserving sequential consistency in region optimizations. However, ATOMICITY primitive is over restrictive on the thread interleaving for optimizing real-world applications developed with the popular Total-Store-Ordering (TSO) memory consistency, which is weaker than sequential consistency. In this paper, we present a novel hardware TSO_ATOMICITY primitive, which has less restriction on the thread interleaving than ATOMICITY primitive to permit more efficient program execution than ATOMICITY primitive, but can still preserve TSO memory consistency in all region optimizations. Furthermore, TSO_ATOMICITY primitive requires similar architecture support as ATOMICITY primitive and can be implemented with only slight change to the existing ATOMICITY primitive implementation. Our experimental results show that in a start-of-art dynamic binary optimization system on a large set of workloads, ATOMICITY primitive can only improve the performance by 4% on average. TSO_ATOMICITY primitive can reduce the overhead associated with ATOMICITY primitive and improve the performance by 12% on average.},
 acmid = {2451172},
 address = {New York, NY, USA},
 author = {Wang, Cheng and Wu, Youfeng},
 doi = {10.1145/2490301.2451172},
 issn = {0163-5964},
 issue_date = {March 2013},
 journal = {SIGARCH Comput. Archit. News},
 keyword = {atomicity, dynamic optimization, memory consistency},
 link = {http://doi.acm.org/10.1145/2490301.2451172},
 month = {mar},
 number = {1},
 numpages = {12},
 pages = {509--520},
 publisher = {ACM},
 title = {TSO_ATOMICITY: Efficient Hardware Primitive for TSO-preserving Region Optimizations},
 volume = {41},
 year = {2013}
}


@article{Schulte:2013:ARB:2490301.2451151,
 abstract = {We present a method for automatically repairing arbitrary software defects in embedded systems, which have limited memory, disk and CPU capacities, but exist in great numbers. We extend evolutionary computation (EC) algorithms that search for valid repairs at the source code level to assembly and ELF format binaries, compensating for limited system resources with several algorithmic innovations. Our method does not require access to the source code or build toolchain of the software under repair, does not require program instrumentation, specialized execution environments, or virtual machines, or prior knowledge of the bug type. We repair defects in ARM and x86 assembly as well as ELF binaries, observing decreases of 86% in memory and 95% in disk requirements, with 62% decrease in repair time, compared to similar source-level techniques. These advances allow repairs previously possible only with C source code to be applied to any ARM or x86 assembly or ELF executable. Efficiency gains are achieved by introducing stochastic fault localization, with much lower overhead than comparable deterministic methods, and low-level program representations. When distributed over multiple devices, our algorithm finds repairs faster than predicted by naive parallelism. Four devices using our approach are five times more efficient than a single device because of our collaboration model. The algorithm is implemented on Nokia N900 smartphones, with inter-phone communication fitting in 900 bytes sent in 7 SMS text messages per device per repair on average.},
 acmid = {2451151},
 address = {New York, NY, USA},
 author = {Schulte, Eric and DiLorenzo, Jonathan and Weimer, Westley and Forrest, Stephanie},
 doi = {10.1145/2490301.2451151},
 issn = {0163-5964},
 issue_date = {March 2013},
 journal = {SIGARCH Comput. Archit. News},
 keyword = {assembly code, automated program repair, bytecode, evolutionary computation, fault localization, legacy software},
 link = {http://doi.acm.org/10.1145/2490301.2451151},
 month = {mar},
 number = {1},
 numpages = {12},
 pages = {317--328},
 publisher = {ACM},
 title = {Automated Repair of Binary and Assembly Programs for Cooperating Embedded Devices},
 volume = {41},
 year = {2013}
}


@article{deOliveira:2013:WYC:2490301.2451140,
 abstract = {Research has shown that correctly conducting and analysing computer performance experiments is difficult. This paper investigates what is necessary to conduct successful computer performance evaluation by attempting to repeat a prior experiment: the comparison between two Linux schedulers. In our efforts, we found that exploring an experimental space through a series of incremental experiments can be inconclusive, and there may be no indication of how much experimentation will be enough. Analysis of variance (ANOVA), a traditional analysis method, is able to partly solve the problems with the previous approach, but we demonstrate that ANOVA can be insufficient for proper analysis due to the requirements it imposes on the data. Finally, we demonstrate the successful application of quantile regression, a recent development in statistics, to computer performance experiments. Quantile regression can provide more insight into the experiment than ANOVA, with the additional benefit of being applicable to data from any distribution. This property makes it especially useful in our field, since non-normally distributed data is common in computer experiments.},
 acmid = {2451140},
 address = {New York, NY, USA},
 author = {de Oliveira, Augusto Born and Fischmeister, Sebastian and Diwan, Amer and Hauswirth, Matthias and Sweeney, Peter F.},
 doi = {10.1145/2490301.2451140},
 issn = {0163-5964},
 issue_date = {March 2013},
 journal = {SIGARCH Comput. Archit. News},
 keyword = {anova, latency, quantile regression, scheduling},
 link = {http://doi.acm.org/10.1145/2490301.2451140},
 month = {mar},
 number = {1},
 numpages = {12},
 pages = {207--218},
 publisher = {ACM},
 title = {Why You Should Care About Quantile Regression},
 volume = {41},
 year = {2013}
}


@inproceedings{Cui:2013:VSR:2451116.2451152,
 abstract = {Systems code must obey many rules, such as "opened files must be closed." One approach to verifying rules is static analysis, but this technique cannot infer precise runtime effects of code, often emitting many false positives. An alternative is symbolic execution, a technique that verifies program paths over all inputs up to a bounded size. However, when applied to verify rules, existing symbolic execution systems often blindly explore many redundant program paths while missing relevant ones that may contain bugs. Our key insight is that only a small portion of paths are relevant to rules, and the rest (majority) of paths are irrelevant and do not need to be verified. Based on this insight, we create WOODPECKER, a new symbolic execution system for effectively checking rules on systems programs. It provides a set of builtin checkers for common rules, and an interface for users to easily check new rules. It directs symbolic execution toward the program paths relevant to a checked rule, and soundly prunes redundant paths, exponentially speeding up symbolic execution. It is designed to be heuristic-agnostic, enabling users to leverage existing powerful search heuristics. Evaluation on 136 systems programs totaling 545K lines of code, including some of the most widely used programs, shows that, with a time limit of typically just one hour for each verification run, WOODPECKER effectively verifies 28.7% of the program and rule combinations over bounded input, whereas an existing symbolic execution system KLEE verifies only 8.5%. For the remaining combinations, WOODPECKER verifies 4.6 times as many relevant paths as KLEE. With a longer time limit, WOODPECKER verifies much more paths than KLEE, e.g., 17 times as many with a fourhour limit. WOODPECKER detects 113 rule violations, including 10 serious data loss errors with 2 most serious ones already confirmed by the corresponding developers.},
 acmid = {2451152},
 address = {New York, NY, USA},
 author = {Cui, Heming and Hu, Gang and Wu, Jingyue and Yang, Junfeng},
 booktitle = {Proceedings of the Eighteenth International Conference on Architectural Support for Programming Languages and Operating Systems},
 doi = {10.1145/2451116.2451152},
 isbn = {978-1-4503-1870-9},
 keyword = {error detection, path slicing, symbolic execution, systems rules, verification},
 link = {http://doi.acm.org/10.1145/2451116.2451152},
 location = {Houston, Texas, USA},
 numpages = {14},
 pages = {329--342},
 publisher = {ACM},
 series = {ASPLOS '13},
 title = {Verifying Systems Rules Using Rule-directed Symbolic Execution},
 year = {2013}
}


@inproceedings{Paulos:2013:REA:2451116.2451133,
 abstract = {We are at an important technological inflection point. Most of our computing systems have been designed and built by professionally trained experts (i.e. us -- computer scientists, engineers, and designers) for use in specific domains and to solve explicit prob-lems. Artifacts often called "user manuals" traditionally prescribed the appropriate usage of these tools and implied an acceptable etiquette for interaction and experience. A fringe group of individuals usually labeled "hackers" or "amateurs" or "makers" have challenged this producer-consumer model of technology by creating novel hardware and software features to "improve" our research and products while a similar creative group of technicians called "artists" have redirected the techniques, tools, and tenets of accepted technological usage away from their typical manifestations in practicality and product. Over time the technological artifacts of these fringe groups and the support for their rhetoric have gained them a foothold into computing culture and eroded the established power discontinuities within the practice of computing research. We now expect our computing tools to be driven by an architecture of open participation and democracy that encourages users to add value to their tools and applications as they use them. Similarly, the bar for enabling the design of novel, personal computing systems and "hardware remixes" has fallen to the point where many non-experts and novices are readily embracing and creating fascinating and ingenious computing artifacts outside of our official and traditionally sanctioned academic and industrial research communities. But how have we as "expert" practitioners been influencing this discussion? By constructing a practice around the design and development of technology for task based and problem solving applications, we have unintentionally established such work as the status quo for the human computing experience. We have failed in our duty to open up alternate forums for technology to express itself and touch our lives beyond productivity and efficiency. Blinded by our quest for "smart technologies" we have forgotten to contemplate the design of technologies to inspire us to be smarter, more curious, and more inquisitive. We owe it to ourselves to rethink the impact we desire to have on this historic moment in computing culture. We must choose to participate in and perhaps lead a dialogue that heralds an expansive new acceptable practice of designing to enable participation by experts and non-experts alike. We are in the milieu of the rise of the "expert amateur". We must change our mantra -- not just performance, completeness, and usability but openness, usefulness and relevancy to our world, its citizens, and our environment. This talk will explore elements of the DIY and maker culture and its relevancy to research questions across computational hardware, languages, and systems. Ultimately, this talk will outline and argue for expanding the design territory and potential opportunities for all of us to collaborate and benefit as a society from this cultural movement.},
 acmid = {2451133},
 address = {New York, NY, USA},
 author = {Paulos, Eric},
 booktitle = {Proceedings of the Eighteenth International Conference on Architectural Support for Programming Languages and Operating Systems},
 doi = {10.1145/2451116.2451133},
 isbn = {978-1-4503-1870-9},
 link = {http://doi.acm.org/10.1145/2451116.2451133},
 location = {Houston, Texas, USA},
 numpages = {2},
 pages = {153--154},
 publisher = {ACM},
 series = {ASPLOS '13},
 title = {The Rise of the Expert Amateur: DIY Culture and the Evolution of Computer Science},
 year = {2013}
}


@article{Shen:2013:PCO:2499368.2451124,
 abstract = {Energy efficiency and power capping are critical concerns in server and cloud computing systems. They face growing challenges due to dynamic power variations from new client-directed web applications, as well as complex behaviors due to multicore resource sharing and hardware heterogeneity. This paper presents a new operating system facility called "power containers" that accounts for and controls the power and energy usage of individual fine-grained requests in multicore servers. This facility relies on three key techniques---1) online model that attributes multicore power (including shared maintenance power) to concurrently running tasks, 2) alignment of actual power measurements and model estimates to enable online model recalibration, and 3) on-the-fly application-transparent request tracking in multi-stage servers to isolate the power and energy contributions and customize per-request control. Our mechanisms enable new multicore server management capabilities including fair power capping that only penalizes power-hungry requests, and energy-aware request distribution between heterogeneous servers. Our evaluation uses three multicore processors (Intel Woodcrest, Westmere, and SandyBridge) and a variety of server and cloud computing (Google App Engine) workloads. Our results demonstrate the high accuracy of our request power accounting (no more than 11% errors) and the effectiveness of container-enabled power virus isolation and throttling. Our request distribution case study shows up to 25% energy saving compared to an alternative approach that recognizes machine heterogeneity but not fine-grained workload affinity.},
 acmid = {2451124},
 address = {New York, NY, USA},
 author = {Shen, Kai and Shriraman, Arrvindh and Dwarkadas, Sandhya and Zhang, Xiao and Chen, Zhuan},
 doi = {10.1145/2499368.2451124},
 issn = {0362-1340},
 issue_date = {April 2013},
 journal = {SIGPLAN Not.},
 keyword = {hardware counters, multicore, operating system, power modeling, power virus, server and cloud computing},
 link = {http://doi.acm.org/10.1145/2499368.2451124},
 month = {mar},
 number = {4},
 numpages = {12},
 pages = {65--76},
 publisher = {ACM},
 title = {Power Containers: An OS Facility for Fine-grained Power and Energy Management on Multicore Servers},
 volume = {48},
 year = {2013}
}


@article{Wester:2013:PDR:2499368.2451120,
 abstract = {Detecting data races in multithreaded programs is a crucial part of debugging such programs, but traditional data race detectors are too slow to use routinely. This paper shows how to speed up race detection by spreading the work across multiple cores. Our strategy relies on uniparallelism, which executes time intervals of a program (called epochs) in parallel to provide scalability, but executes all threads from a single epoch on a single core to eliminate locking overhead. We use several techniques to make parallelization effective: dividing race detection into three phases, predicting a subset of the analysis state, eliminating sequential work via transitive reduction, and reducing the work needed to maintain multiple versions of analysis via factorization. We demonstrate our strategy by parallelizing a happens-before detector and a lockset-based detector. We find that uniparallelism can significantly speed up data race detection. With 4x the number of cores as the original application, our strategy speeds up the median execution time by 4.4x for a happens-before detector and 3.3x for a lockset race detector. Even on the same number of cores as the conventional detectors, the ability for uniparallelism to elide analysis locks allows it to reduce the median overhead by 13% for a happens-before detector and 8% for a lockset detector.},
 acmid = {2451120},
 address = {New York, NY, USA},
 author = {Wester, Benjamin and Devecsery, David and Chen, Peter M. and Flinn, Jason and Narayanasamy, Satish},
 doi = {10.1145/2499368.2451120},
 issn = {0362-1340},
 issue_date = {April 2013},
 journal = {SIGPLAN Not.},
 keyword = {data race detection, uniparallelism},
 link = {http://doi.acm.org/10.1145/2499368.2451120},
 month = {mar},
 number = {4},
 numpages = {12},
 pages = {27--38},
 publisher = {ACM},
 title = {Parallelizing Data Race Detection},
 volume = {48},
 year = {2013}
}


@article{Kim:2013:DCS:2499368.2451156,
 abstract = {As processor architectures have been enhancing their computing capacity by increasing core counts, independent workloads can be consolidated on a single node for the sake of high resource efficiency in data centers. With the prevalence of virtualization technology, each individual workload can be hosted on a virtual machine for strong isolation between co-located workloads. Along with this trend, hosted applications have increasingly been multithreaded to take advantage of improved hardware parallelism. Although the performance of many multithreaded applications highly depends on communication (or synchronization) latency, existing schemes of virtual machine scheduling do not explicitly coordinate virtual CPUs based on their communication behaviors. This paper presents a demand-based coordinated scheduling scheme for consolidated virtual machines that host multithreaded workloads. To this end, we propose communication-driven scheduling that controls time-sharing in response to inter-processor interrupts (IPIs) between virtual CPUs. On the basis of in-depth analysis on the relationship between IPI communications and coordination demands, we devise IPI-driven coscheduling and delayed preemption schemes, which effectively reduce synchronization latency and unnecessary CPU consumption. In addition, we introduce a load-conscious CPU allocation policy in order to address load imbalance in heterogeneously consolidated environments. The proposed schemes are evaluated with respect to various scenarios of mixed workloads using the PARSEC multithreaded applications. In the evaluation, our scheme improves the overall performance of consolidated workloads, especially communication-intensive applications, by reducing inefficient synchronization latency.},
 acmid = {2451156},
 address = {New York, NY, USA},
 author = {Kim, Hwanju and Kim, Sangwook and Jeong, Jinkyu and Lee, Joonwon and Maeng, Seungryoul},
 doi = {10.1145/2499368.2451156},
 issn = {0362-1340},
 issue_date = {April 2013},
 journal = {SIGPLAN Not.},
 keyword = {synchronization, coscheduling, virtualization},
 link = {http://doi.acm.org/10.1145/2499368.2451156},
 month = {mar},
 number = {4},
 numpages = {12},
 pages = {369--380},
 publisher = {ACM},
 title = {Demand-based Coordinated Scheduling for SMP VMs},
 volume = {48},
 year = {2013}
}


