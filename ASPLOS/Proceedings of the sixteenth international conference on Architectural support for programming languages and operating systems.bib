@article{Singh:2011:EPS:1961295.1950375,
 abstract = {A longstanding challenge of shared-memory concurrency is to provide a memory model that allows for efficient implementation while providing strong and simple guarantees to programmers. The C++0x and Java memory models admit a wide variety of compiler and hardwareoptimizations and provide sequentially consistent (SC) semantics for data-race-free programs. However, they either do not provide any semantics (C++0x) or provide a hard-to-understand semantics (Java) for racy programs, compromising the safety and debuggability of such programs. In earlier work we proposed the DRFx memory model, which addresses this problem by dynamically detecting potential violations of SC due to the interaction of compiler or hardware optimizations with data races and halting execution upon detection. In this paper, we present a detailed micro-architecture design for supporting the DRFx memory model, formalize the design and prove its correctness, and evaluate the design using a hardware simulator. We describe a set of DRFx-compliant complexity-effective optimizations which allow us to attain performance close to that of TSO (Total Store Model) and DRF0 while providing strong guarantees for all programs.},
 acmid = {1950375},
 address = {New York, NY, USA},
 author = {Singh, Abhayendra and Marino, Daniel and Narayanasamy, Satish and Millstein, Todd and Musuvathi, Madan},
 doi = {10.1145/1961295.1950375},
 issn = {0163-5964},
 issue_date = {March 2011},
 journal = {SIGARCH Comput. Archit. News},
 keyword = {data-races, memory model exception, memory models, soft fences},
 link = {http://doi.acm.org/10.1145/1961295.1950375},
 month = {mar},
 number = {1},
 numpages = {14},
 pages = {53--66},
 publisher = {ACM},
 title = {Efficient Processor Support for DRFx, a Memory Model with Exceptions},
 volume = {39},
 year = {2011}
}


@article{Dalessandro:2011:HNC:1961296.1950373,
 abstract = {Transactional memory (TM) is a promising synchronization mechanism for the next generation of multicore processors. Best-effort Hardware Transactional Memory (HTM) designs, such as Sun's prototype Rock processor and AMD's proposed Advanced Synchronization Facility (ASF), can efficiently execute many transactions, but abort in some cases due to various limitations. Hybrid TM systems can use a compatible software TM (STM) in such cases. We introduce a family of hybrid TMs built using the recent NOrec STM algorithm that, unlike existing hybrid approaches, provide both low overhead on hardware transactions and concurrent execution of hardware and software transactions. We evaluate implementations for Rock and ASF, exploring how the differing HTM designs affect optimization choices. Our investigation yields valuable input for designers of future best-effort HTMs.},
 acmid = {1950373},
 address = {New York, NY, USA},
 author = {Dalessandro, Luke and Carouge, Fran\c{c}ois and White, Sean and Lev, Yossi and Moir, Mark and Scott, Michael L. and Spear, Michael F.},
 doi = {10.1145/1961296.1950373},
 issn = {0362-1340},
 issue_date = {March 2011},
 journal = {SIGPLAN Not.},
 keyword = {transactional memory},
 link = {http://doi.acm.org/10.1145/1961296.1950373},
 month = {mar},
 number = {3},
 numpages = {14},
 pages = {39--52},
 publisher = {ACM},
 title = {Hybrid NOrec: A Case Study in the Effectiveness of Best Effort Hardware Transactional Memory},
 volume = {46},
 year = {2011}
}


@article{Deng:2011:MAL:2248487.1950392,
 abstract = {Main memory is responsible for a large and increasing fraction of the energy consumed by servers. Prior work has focused on exploiting DRAM low-power states to conserve energy. However, these states require entire DRAM ranks to be idled, which is difficult to achieve even in lightly loaded servers. In this paper, we propose to conserve memory energy while improving its energy-proportionality by creating active low-power modes for it. Specifically, we propose MemScale, a scheme wherein we apply dynamic voltage and frequency scaling (DVFS) to the memory controller and dynamic frequency scaling (DFS) to the memory channels and DRAM devices. MemScale is guided by an operating system policy that determines the DVFS/DFS mode of the memory subsystem based on the current need for memory bandwidth, the potential energy savings, and the performance degradation that applications are willing to withstand. Our results demonstrate that MemScale reduces energy consumption significantly compared to modern memory energy management approaches. We conclude that the potential benefits of the MemScale mechanisms and policy more than compensate for their small hardware cost.},
 acmid = {1950392},
 address = {New York, NY, USA},
 author = {Deng, Qingyuan and Meisner, David and Ramos, Luiz and Wenisch, Thomas F. and Bianchini, Ricardo},
 doi = {10.1145/2248487.1950392},
 issn = {0362-1340},
 issue_date = {April 2012},
 journal = {SIGPLAN Not.},
 keyword = {dynamic voltage and frequency scaling, energy conservation, memory subsystem},
 link = {http://doi.acm.org/10.1145/2248487.1950392},
 month = {mar},
 number = {4},
 numpages = {14},
 pages = {225--238},
 publisher = {ACM},
 title = {MemScale: Active Low-power Modes for Main Memory},
 volume = {47},
 year = {2011}
}


@article{Hoang:2011:ECT:2248487.1950405,
 abstract = {By adjusting the design of the ISA and enabling circuit timing-sensitive optimizations in a compiler, we can more effectively exploit timing speculation. While there has been growing interest in systems that leverage circuit-level timing speculation to improve the performance and power-efficiency of processors, most of the innovation has been at the microarchitectural level. We make the observation that some code sequences place greater demand on circuit timing deadlines than others. Furthermore, by selectively replacing these codes with instruction sequences which are semantically equivalent but reduce activity on timing critical circuit paths, we can trigger fewer timing errors and hence reduce recovery costs.},
 acmid = {1950405},
 address = {New York, NY, USA},
 author = {Hoang, Giang and Findler, Robby Bruce and Joseph, Russ},
 doi = {10.1145/2248487.1950405},
 issn = {0362-1340},
 issue_date = {April 2012},
 journal = {SIGPLAN Not.},
 keyword = {compiler, isa design, timing speculation},
 link = {http://doi.acm.org/10.1145/2248487.1950405},
 month = {mar},
 number = {4},
 numpages = {12},
 pages = {345--356},
 publisher = {ACM},
 title = {Exploring Circuit Timing-aware Language and Compilation},
 volume = {47},
 year = {2011}
}


@proceedings{Gupta:2011:1950365,
 abstract = {It is our great pleasure to welcome you to the sixteenth edition of ASPLOS and to introduce what we feel will be an outstanding and thought-provoking technical program. This year's conference continues and reinforces the ASPLOS tradition of publishing innovative multidisciplinary research spanning the boundaries of hardware, computer architecture, compilers, languages, operating systems, networking, and applications. We received 152 full paper submissions by the July 28th deadline, which was fewer than last year's record-setting year, but above average over the past three years. (127, 113 and 181 full papers were submitted in 2008, 2009, and 2010, respectively.) 22 submissions had PC members as co-authors. The dominant technical theme this year was parallelism (once again), with more than half of the submissions explicitly indicating a technical keyword specific to parallel processing. There was a nice mixture of submissions spanning the disciplines of operating systems (36%), compilers and programming languages (34%), and architecture (60%), with 38% of submissions spanning multiple of these areas. Regarding the paper reviewing process, we continued with two changes that had been adopted the previous year: we had two rounds of reviews, and we used an External Review Committee that was preselected by the Program Chair. Each paper was initially reviewed by one PC member and two ERC members. The rationale for having two ERC reviews during the first round (as opposed to one, as was the case last year) was that it gave us the most flexibility to match a paper with the experts who were most qualified to review that specific topic. Based upon these reviews (as well as follow-up discussions for any borderline cases), 103 papers were selected for a second round of reviews, where they received two more reviews from PC members. In general, any paper that received at least one vote for acceptance during the first round was selected for second round reviews. Through this two-round mechanism, we were able to limit the reviewing load per PC member to no more than 14 papers, and more importantly we allowed them to spend the bulk of their time on papers with a significant chance of being accepted. We would especially like to thank the ERC members for their hard work and dedication to the reviewing process; the typical ERC member reviewed 5 papers this year. One change that we made to the reviewing form this year was that in addition to evaluation categories that had been used in previous years (e.g., the degree of novelty, whether the paper was convincing, etc.), we added a new category where we asked reviewers to rate the extent to which a paper would be thoughtprovoking. Our goal was to make sure that we paid special attention to submissions that might inspire people to change the way that they think about how they approach their own technical research problems. We are pleased that this emphasis appears to have translated well into a highly thought-provoking technical program. We are also pleased to report that all reviews had been submitted in time for the authors to respond to them during the author rebuttal period. After the rebuttal period, the PC members and ERC members took the rebuttal comments into account as they discussed the papers online and potentially updated their scores accordingly. The PC met in person on the Carnegie Mellon University campus for a one-day meeting on October 21st, 2010. 81 papers were discussed at the meeting. PC-authored papers were handled roughly halfway through the meeting, using the hotseat approach. Jim Larus, Margaret Martonosi, and David Patterson managed the discussion of papers where the PC Chair had a conflict. The PC chose 32 papers to be presented at the conference, for an overall acceptance rate of 21.1%. 7 of the 32 papers have PC members as co-authors. Continuing with another change that was made last year, all papers were assigned a shepherd to ensure that the final papers adequately addressed the concerns of the reviewers.},
 address = {New York, NY, USA},
 isbn = {978-1-4503-0266-1},
 location = {Newport Beach, California, USA},
 note = {415115},
 publisher = {ACM},
 title = {ASPLOS XVI: Proceedings of the Sixteenth International Conference on Architectural Support for Programming Languages and Operating Systems},
 year = {2011}
}


@article{Ransford:2011:MSS:1961295.1950386,
 abstract = {Transiently powered computing devices such as RFID tags, kinetic energy harvesters, and smart cards typically rely on programs that complete a task under tight time constraints before energy starvation leads to complete loss of volatile memory. Mementos is a software system that transforms general-purpose programs into interruptible computations that are protected from frequent power losses by automatic, energy-aware state checkpointing. Mementos comprises a collection of optimization passes for the LLVM compiler infrastructure and a linkable library that exercises hardware support for energy measurement while managing state checkpoints stored in nonvolatile memory. We evaluate Mementos against diverse test cases in a trace-driven simulator of transiently powered RFID-scale devices. Although Mementos's energy checks increase run time when energy is plentiful, they allow Mementos to safely suspend execution when energy dwindles, effectively spreading computation across zero or more power failures. This paper's contributions are: a study of the runtime environment for programs on RFID-scale devices; an energy-aware state checkpointing system for these devices that is implemented for the MSP430 family of microcontrollers; and a trace-driven simulator of transiently powered RFID-scale devices.},
 acmid = {1950386},
 address = {New York, NY, USA},
 author = {Ransford, Benjamin and Sorber, Jacob and Fu, Kevin},
 doi = {10.1145/1961295.1950386},
 issn = {0163-5964},
 issue_date = {March 2011},
 journal = {SIGARCH Comput. Archit. News},
 keyword = {computational rfid, energy-aware checkpointing, mementos, rfid-scale devices},
 link = {http://doi.acm.org/10.1145/1961295.1950386},
 month = {mar},
 number = {1},
 numpages = {12},
 pages = {159--170},
 publisher = {ACM},
 title = {Mementos: System Support for Long-running Computation on RFID-scale Devices},
 volume = {39},
 year = {2011}
}


@article{Zhang:2011:CDC:1961296.1950395,
 abstract = {Concurrency bugs are caused by non-deterministic interleavings between shared memory accesses. Their effects propagate through data and control dependences until they cause software to crash, hang, produce incorrect output, etc. The lifecycle of a bug thus consists of three phases: (1) triggering, (2) propagation, and (3) failure. Traditional techniques for detecting concurrency bugs mostly focus on phase (1)--i.e., on finding certain structural patterns of interleavings that are common triggers of concurrency bugs, such as data races. This paper explores a consequence-oriented approach to improving the accuracy and coverage of state-space search and bug detection. The proposed approach first statically identifies potential failure sites in a program binary (i.e., it first considers a phase (3) issue). It then uses static slicing to identify critical read instructions that are highly likely to affect potential failure sites through control and data dependences (phase (2)). Finally, it monitors a single (correct) execution of a concurrent program and identifies suspicious interleavings that could cause an incorrect state to arise at a critical read and then lead to a software failure (phase (1)). ConSeq's backwards approach, (3)!(2)!(1), provides advantages in bug-detection coverage and accuracy but is challenging to carry out. ConSeq makes it feasible by exploiting the empirical observationthat phases (2) and (3) usually are short and occur within one thread. Our evaluation on large, real-world C/C++ applications shows that ConSeq detects more bugs than traditional approaches and has a much lower false-positive rate.},
 acmid = {1950395},
 address = {New York, NY, USA},
 author = {Zhang, Wei and Lim, Junghee and Olichandran, Ramya and Scherpelz, Joel and Jin, Guoliang and Lu, Shan and Reps, Thomas},
 doi = {10.1145/1961296.1950395},
 issn = {0362-1340},
 issue_date = {March 2011},
 journal = {SIGPLAN Not.},
 keyword = {concurrency bugs, software testing},
 link = {http://doi.acm.org/10.1145/1961296.1950395},
 month = {mar},
 number = {3},
 numpages = {14},
 pages = {251--264},
 publisher = {ACM},
 title = {ConSeq: Detecting Concurrency Bugs Through Sequential Errors},
 volume = {46},
 year = {2011}
}


@article{Volos:2011:MLP:2248487.1950379,
 abstract = {New storage-class memory (SCM) technologies, such as phase-change memory, STT-RAM, and memristors, promise user-level access to non-volatile storage through regular memory instructions. These memory devices enable fast user-mode access to persistence, allowing regular in-memory data structures to survive system crashes. In this paper, we present Mnemosyne, a simple interface for programming with persistent memory. Mnemosyne addresses two challenges: how to create and manage such memory, and how to ensure consistency in the presence of failures. Without additional mechanisms, a system failure may leave data structures in SCM in an invalid state, crashing the program the next time it starts. In Mnemosyne, programmers declare global persistent data with the keyword "pstatic" or allocate it dynamically. Mnemosyne provides primitives for directly modifying persistent variables and supports consistent updates through a lightweight transaction mechanism. Compared to past work on disk-based persistent memory, Mnemosyne reduces latency to storage by writing data directly to memory at the granularity of an update rather than writing memory pages back to disk through the file system. In tests emulating the performance characteristics of forthcoming SCMs, we show that Mnemosyne can persist data as fast as 3 microseconds. Furthermore, it provides a 35 percent performance increase when applied in the OpenLDAP directory server. In microbenchmark studies we find that Mnemosyne can be up to 1400% faster than alternative persistence strategies, such as Berkeley DB or Boost serialization, that are designed for disks.},
 acmid = {1950379},
 address = {New York, NY, USA},
 author = {Volos, Haris and Tack, Andres Jaan and Swift, Michael M.},
 doi = {10.1145/2248487.1950379},
 issn = {0362-1340},
 issue_date = {April 2012},
 journal = {SIGPLAN Not.},
 keyword = {memory transactions, performance, persistence, persistent memory, storage-class memory},
 link = {http://doi.acm.org/10.1145/2248487.1950379},
 month = {mar},
 number = {4},
 numpages = {14},
 pages = {91--104},
 publisher = {ACM},
 title = {Mnemosyne: Lightweight Persistent Memory},
 volume = {47},
 year = {2011}
}


@article{Veeraraghavan:2011:DPS:2248487.1950370,
 abstract = {Deterministic replay systems record and reproduce the execution of a hardware or software system. In contrast to replaying execution on uniprocessors, deterministic replay on multiprocessors is very challenging to implement efficiently because of the need to reproduce the order or values read by shared memory operations performed by multiple threads. In this paper, we present DoublePlay, a new way to efficiently guarantee replay on commodity multiprocessors. Our key insight is that one can use the simpler and faster mechanisms of single-processor record and replay, yet still achieve the scalability offered by multiple cores, by using an additional execution to parallelize the record and replay of an application. DoublePlay timeslices multiple threads on a single processor, then runs multiple time intervals (epochs) of the program concurrently on separate processors. This strategy, which we call uniparallelism, makes logging much easier because each epoch runs on a single processor (so threads in an epoch never simultaneously access the same memory) and different epochs operate on different copies of the memory. Thus, rather than logging the order of shared-memory accesses, we need only log the order in which threads in an epoch are timesliced on the processor. DoublePlay runs an additional execution of the program on multiple processors to generate checkpoints so that epochs run in parallel. We evaluate DoublePlay on a variety of client, server, and scientific parallel benchmarks; with spare cores, DoublePlay reduces logging overhead to an average of 15% with two worker threads and 28% with four threads.},
 acmid = {1950370},
 address = {New York, NY, USA},
 author = {Veeraraghavan, Kaushik and Lee, Dongyoon and Wester, Benjamin and Ouyang, Jessica and Chen, Peter M. and Flinn, Jason and Narayanasamy, Satish},
 doi = {10.1145/2248487.1950370},
 issn = {0362-1340},
 issue_date = {April 2012},
 journal = {SIGPLAN Not.},
 keyword = {deterministic replay, uniparallelism},
 link = {http://doi.acm.org/10.1145/2248487.1950370},
 month = {mar},
 number = {4},
 numpages = {12},
 pages = {15--26},
 publisher = {ACM},
 title = {DoublePlay: Parallelizing Sequential Logging and Replay},
 volume = {47},
 year = {2011}
}


@inproceedings{Hofmann:2011:EOS:1950365.1950398,
 abstract = {Kernel rootkits that modify operating system state to avoid detection are a dangerous threat to system security. This paper presents OSck, a system that discovers kernel rootkits by detecting malicious modifications to operating system data. OSck integrates and extends existing techniques for detecting rootkits, and verifies safety properties for large portions of the kernel heap with minimal overhead. We deduce type information for verification by analyzing unmodified kernel source code and in-memory kernel data structures. High-performance integrity checks that execute concurrently with a running operating system create data races, and we demonstrate a deterministic solution for ensuring kernel memory is in a consistent state. We introduce two new classes of kernel rootkits that are undetectable by current systems, motivating the need for the OSck API that allows kernel developers to conveniently specify arbitrary integrity properties.},
 acmid = {1950398},
 address = {New York, NY, USA},
 author = {Hofmann, Owen S. and Dunn, Alan M. and Kim, Sangman and Roy, Indrajit and Witchel, Emmett},
 booktitle = {Proceedings of the Sixteenth International Conference on Architectural Support for Programming Languages and Operating Systems},
 doi = {10.1145/1950365.1950398},
 isbn = {978-1-4503-0266-1},
 keyword = {rootkit detection},
 link = {http://doi.acm.org/10.1145/1950365.1950398},
 location = {Newport Beach, California, USA},
 numpages = {12},
 pages = {279--290},
 publisher = {ACM},
 series = {ASPLOS XVI},
 title = {Ensuring Operating System Kernel Integrity with OSck},
 year = {2011}
}


@inproceedings{Palix:2011:FLT:1950365.1950401,
 abstract = {In 2001, Chou et al. published a study of faults found by applying a static analyzer to Linux versions 1.0 through 2.4.1. A major result of their work was that the drivers directory contained up to 7 times more of certain kinds of faults than other directories. This result inspired a number of development and research efforts on improving the reliability of driver code. Today Linux is used in a much wider range of environments, provides a much wider range of services, and has adopted a new development and release model. What has been the impact of these changes on code quality? Are drivers still a major problem? To answer these questions, we have transported the experiments of Chou et al. to Linux versions 2.6.0 to 2.6.33, released between late 2003 and early 2010. We find that Linux has more than doubled in size during this period, but that the number of faults per line of code has been decreasing. And, even though drivers still accounts for a large part of the kernel code and contains the most faults, its fault rate is now below that of other directories, such as arch (HAL) and fs (file systems). These results can guide further development and research efforts. To enable others to continually update these results as Linux evolves, we define our experimental protocol and make our checkers and results available in a public archive.},
 acmid = {1950401},
 address = {New York, NY, USA},
 author = {Palix, Nicolas and Thomas, Ga\"{e}l and Saha, Suman and Calv\`{e}s, Christophe and Lawall, Julia and Muller, Gilles},
 booktitle = {Proceedings of the Sixteenth International Conference on Architectural Support for Programming Languages and Operating Systems},
 doi = {10.1145/1950365.1950401},
 isbn = {978-1-4503-0266-1},
 keyword = {fault-finding tools, linux},
 link = {http://doi.acm.org/10.1145/1950365.1950401},
 location = {Newport Beach, California, USA},
 numpages = {14},
 pages = {305--318},
 publisher = {ACM},
 series = {ASPLOS XVI},
 title = {Faults in Linux: Ten Years Later},
 year = {2011}
}


@article{Burnim:2011:SCS:1961296.1950377,
 abstract = {In practice, it is quite difficult to write correct multithreaded programs due to the potential for unintended and nondeterministic interference between parallel threads. A fundamental correctness property for such programs is atomicity---a block of code in a program is atomic if, for any parallel execution of the program, there is an execution with the same overall program behavior in which the block is executed serially. We propose semantic atomicity, a generalization of atomicity with respect to a programmer-defined notion of equivalent behavior. We propose an assertion framework in which a programmer can use bridge predicates to specify noninterference properties at the level of abstraction of their application. Further, we propose a novel algorithm for systematically testing atomicity specifications on parallel executions with a bounded number of interruptions---i.e. atomic blocks whose execution is interleaved with that of other threads. We further propose a set of sound heuristics and optional user annotations that increase the efficiency of checking atomicity specifications in the common case where the specifications hold. We have implemented our assertion framework for specifying and checking semantic atomicity for parallel Java programs, and we have written semantic atomicity specifications for a number of benchmarks. We found that using bridge predicates allowed us to specify the natural and intended atomic behavior of a wider range of programs than did previous approaches. Further, in checking our specifications, we found several previously unknown bugs, including in the widely-used java.util.concurrent library.},
 acmid = {1950377},
 address = {New York, NY, USA},
 author = {Burnim, Jacob and Necula, George and Sen, Koushik},
 doi = {10.1145/1961296.1950377},
 issn = {0362-1340},
 issue_date = {March 2011},
 journal = {SIGPLAN Not.},
 keyword = {atomicity, concurrency, linearizability},
 link = {http://doi.acm.org/10.1145/1961296.1950377},
 month = {mar},
 number = {3},
 numpages = {12},
 pages = {79--90},
 publisher = {ACM},
 title = {Specifying and Checking Semantic Atomicity for Multithreaded Programs},
 volume = {46},
 year = {2011}
}


@article{Hashmi:2011:CNI:1961295.1950385,
 abstract = {The desire to create novel computing systems, paired with recent advances in neuroscientific understanding of the brain, has led researchers to develop neuromorphic architectures that emulate the brain. To date, such models are developed, trained, and deployed on the same substrate. However, excessive co-dependence between the substrate and the algorithm prevents portability, or at the very least requires reconstructing and retraining the model whenever the substrate changes. This paper proposes a well-defined abstraction layer -- the Neuromorphic instruction set architecture, or NISA -- that separates a neural application's algorithmic specification from the underlying execution substrate, and describes the Aivo framework, which demonstrates the concrete advantages of such an abstraction layer. Aivo consists of a NISA implementation for a rate-encoded neuromorphic system based on the cortical column abstraction, a state-of-the-art integrated development and runtime environment (IDE), and various profile-based optimization tools. Aivo's IDE generates code for emulating cortical networks on the host CPU, multiple GPGPUs, or as boolean functions. Its runtime system can deploy and adaptively optimize cortical networks in a manner similar to conventional just-in-time compilers in managed runtime systems (e.g. Java, C#). We demonstrate the abilities of the NISA abstraction by constructing a cortical network model of the mammalian visual cortex, deploying on multiple execution substrates, and utilizing the various optimization tools we have created. For this hierarchical configuration, Aivo's profiling based network optimization tools reduce the memory footprint by 50% and improve the execution time by a factor of 3x on the host CPU. Deploying the same network on a single GPGPU results in a 30x speedup. We further demonstrate that a speedup of 480x can be achieved by deploying a massively scaled cortical network across three GPGPUs. Finally, converting a trained hierarchical network to C/C++ boolean constructs on the host CPU results in 44x speedup.},
 acmid = {1950385},
 address = {New York, NY, USA},
 author = {Hashmi, Atif and Nere, Andrew and Thomas, James Jamal and Lipasti, Mikko},
 doi = {10.1145/1961295.1950385},
 issn = {0163-5964},
 issue_date = {March 2011},
 journal = {SIGARCH Comput. Archit. News},
 keyword = {cortical learning algorithms, gpgpu, neuromorphic archi- tectures},
 link = {http://doi.acm.org/10.1145/1961295.1950385},
 month = {mar},
 number = {1},
 numpages = {14},
 pages = {145--158},
 publisher = {ACM},
 title = {A Case for Neuromorphic ISAs},
 volume = {39},
 year = {2011}
}


@article{Schupbach:2011:DLA:1961295.1950382,
 abstract = {C remains the language of choice for hardware programming (device drivers, bus configuration, etc.): it is fast, allows low-level access, and is trusted by OS developers. However, the algorithms required to configure and reconfigure hardware devices and interconnects are becoming more complex and diverse, with the added burden of legacy support, quirks, and hardware bugs to work around. Even programming PCI bridges in a modern PC is a surprisingly complex problem, and is getting worse as new functionality such as hotplug appears. Existing approaches use relatively simple algorithms, hard-coded in C and closely coupled with low-level register access code, generally leading to suboptimal configurations. We investigate the merits and drawbacks of a new approach: separating hardware configuration logic (algorithms to determine configuration parameter values) from mechanism (programming device registers). The latter we keep in C, and the former we encode in a declarative programming language with constraint-satisfaction extensions. As a test case, we have implemented full PCI configuration, resource allocation, and interrupt assignment in the Barrelfish research operating system, using a concise expression of efficient algorithms in constraint logic programming. We show that the approach is tractable, and can successfully configure a wide range of PCs with competitive runtime cost. Moreover, it requires about half the code of the C-based approach in Linux while offering considerably more functionality. Additionally it easily accommodates adaptations such as hotplug, fixed regions, and quirks.},
 acmid = {1950382},
 address = {New York, NY, USA},
 author = {Sch\"{u}pbach, Adrian and Baumann, Andrew and Roscoe, Timothy and Peter, Simon},
 doi = {10.1145/1961295.1950382},
 issn = {0163-5964},
 issue_date = {March 2011},
 journal = {SIGARCH Comput. Archit. News},
 keyword = {constraint logic programming, eclipse clp, hardware programming, pci configuration},
 link = {http://doi.acm.org/10.1145/1961295.1950382},
 month = {mar},
 number = {1},
 numpages = {14},
 pages = {119--132},
 publisher = {ACM},
 title = {A Declarative Language Approach to Device Configuration},
 volume = {39},
 year = {2011}
}


@article{Ryzhyk:2011:IDD:2248487.1950383,
 abstract = {Faulty device drivers are a major source of operating system failures. We argue that the underlying cause of many driver faults is the separation of two highly-related tasks: device verification and driver development. These two tasks have a lot in common, and result in software that is conceptually and functionally similar, yet kept totally separate. The result is a particularly bad case of duplication of effort: the verification code is correct, but is discarded after the device has been manufactured; the driver code is inferior, but used in actual device operation. We claim that the two tasks, and the software they produce, can and should be unified, and this will result in drastic improvement of device-driver quality and reduction in the development cost and time to market. In this paper we propose a device driver design and verification workflow that achieves such unification. We apply this workflow to develop and test drivers for four different I/O devices and demonstrate that it improves the driver test coverage and allows detecting driver defects that are extremely hard to find using conventional testing techniques.},
 acmid = {1950383},
 address = {New York, NY, USA},
 author = {Ryzhyk, Leonid and Keys, John and Mirla, Balachandra and Raghunath, Arun and Vij, Mona and Heiser, Gernot},
 doi = {10.1145/2248487.1950383},
 issn = {0362-1340},
 issue_date = {April 2012},
 journal = {SIGPLAN Not.},
 keyword = {automated testing, co-verification, device drivers, reliability, rtl testbenches},
 link = {http://doi.acm.org/10.1145/2248487.1950383},
 month = {mar},
 number = {4},
 numpages = {12},
 pages = {133--144},
 publisher = {ACM},
 title = {Improved Device Driver Reliability Through Hardware Verification Reuse},
 volume = {47},
 year = {2011}
}


@article{Gao:2011:TMH:1961296.1950394,
 abstract = {Concurrency bugs are becoming increasingly prevalent in the multi-core era. Recently, much research has focused on data races and atomicity violation bugs, which are related to low-level memory accesses. However, a large number of concurrency typestate bugs such as "invalid reads to a closed file from a different thread" are under-studied. These concurrency typestate bugs are important yet challenging to study since they are mostly relevant to high-level program semantics. This paper presents 2ndStrike, a method to manifest hidden concurrency typestate bugs in software testing. Given a state machine describing correct program behavior on certain object typestates, 2ndStrike profiles runtime events related to the typestates and thread synchronization. Based on the profiling results, 2ndStrike then identifies bug candidates, each of which is a pair of runtime events that would cause typestate violation if the event order is reversed. Finally, 2ndStrike re-executes the program with controlled thread interleaving to manifest bug candidates. We have implemented a prototype of 2ndStrike on Linux and have illustrated our idea using three types of concurrency typestate bugs, including invalid file operation, invalid pointer dereference, and invalid lock operation. We have evaluated 2ndStrike with six real world bugs (including one previously unknown bug) from three open-source server and desktop programs (i.e., MySQL, Mozilla, pbzip2). Our experimental results show that 2ndStrike can effectively and efficiently manifest all six software bugs, most of which are difficult or impossible to manifest using stress testing or active testing techniques that are based on data race/atomicity violation. Additionally, 2ndStrike reports no false positives, provides detailed bug reports for each manifested bug, and can consistently reproduce the bug after manifesting it once.},
 acmid = {1950394},
 address = {New York, NY, USA},
 author = {Gao, Qi and Zhang, Wenbin and Chen, Zhezhe and Zheng, Mai and Qin, Feng},
 doi = {10.1145/1961296.1950394},
 issn = {0362-1340},
 issue_date = {March 2011},
 journal = {SIGPLAN Not.},
 keyword = {concurrency bugs, software testing, typestate bugs},
 link = {http://doi.acm.org/10.1145/1961296.1950394},
 month = {mar},
 number = {3},
 numpages = {12},
 pages = {239--250},
 publisher = {ACM},
 title = {2ndStrike: Toward Manifesting Hidden Concurrency Typestate Bugs},
 volume = {46},
 year = {2011}
}


@proceedings{Harris:2012:2150976,
 abstract = {Welcome to the seventeenth edition of the ASPLOS conference series, and the first to be held outside of North America. We have assembled what we believe to be an outstanding technical program, and are delighted to be presenting it in the historic chambers of the Royal Society in the UK. The program continues and reinforces the ASPLOS tradition of publishing innovative multidisciplinary research spanning the boundaries of hardware, computer architecture, compilers, languages, operating systems, networking, and applications. We received 172 full paper submissions, which was 13% higher than last year's 152, and 5% lower than the record of 181 set in 2010. 14 submissions had program committee members as co-authors. Once again, the dominant technical theme was parallelism: 70% of both the submitted and accepted papers explicitly indicated a technical keyword specific to parallel processing. The single most popular keyword was "multicore systems," selected by 35% of both the submitted and accepted papers. Interestingly, not one of the 172 submissions selected "VLSI or process technology." There was a nice mixture of submissions across operating systems (34%), compilers and programming languages (28%), and core topics in computer architecture (37%), with 23% of submissions spanning more than one of these. (Most submissions had something to do with computer architecture; the characterization of "core areas" is somewhat arbitrary.) As in recent years, we employed two rounds of reviewing. An External Review Committee, preselected by the Program Chair, assisted in round one. All submissions received at least three round one reviews. Based on these, we eliminated 71 papers whose average overall score was below 3.5 (on a scale of 1-6), and whose maximum score was below 5 (i.e., no better than "weak accept"). The remaining 101 papers received two additional (PC-only) reviews in round two. This process allowed the program committee to focus the bulk of its attention on papers with a significant chance of being accepted. The typical member of the External Review Committee completed six reviews; the typical Program Committee member did 14. Almost all reviews were completed by early October, at which point authors had several days to compose a formal response, if they desired (most did). After this rebuttal period, PC members took the author comments into account as they discussed the papers online and, in some cases, updated their scores accordingly. The PC gathered in Rochester, NY, for a full-day meeting on October 20th, 2011. 74 papers were discussed at the meeting. PC authors stepped out of the room for discussion of their papers; those with conflicts of interest remained in the room, but did not participate in discussion. Papers on which the Program Chair had a conflict of interest (4 in number) had been reviewed in a separate process managed by Jim Larus, and were discussed when the Chair was not present. At the end of the day, 37 papers were accepted for presentation at the conference, for an acceptance rate of 21.5% --- a slightly larger number than last year, but almost the same percentage. Four of the accepts have PC members as co-authors. Nine papers were assigned a "shepherd" to make sure that final versions addressed issues (not necessarily weaknesses) of concern to the program committee. The logistics behind organizing this conference have been handled by the tireless effort of a team of volunteers: Luis Ceze (University of Washington), Alexandra Fedorova (Simon Fraser University), Steven Hand (University of Cambridge), Timothy Jones (University of Cambridge), Paul Kelly (Imperial College), Mikel Luján (University of Manchester), Robert Mullins (University of Cambridge), Onur Mutlu (Carnegie Mellon University), and Derek Murray (Microsoft Research). Each member of this team played an important role in making ASPLOS a success. The ASPLOS Steering Committee gave us valuable assistance over the last year, and Todd Mowry (CMU) was particularly generous with his advice from ASPLOS 2010 and ASPLOS 2011. In addition, Jean Bacon (University of Cambridge), Andrew Herbert, Graham Hutton (University of Nottingham), and Joe Sventek (University of Glasgow) provided advice on organizing a conference in the UK. As always, the support and guidance of the ACM staff was very valuable.},
 address = {New York, NY, USA},
 isbn = {978-1-4503-0759-8},
 location = {London, England, UK},
 note = {415125},
 publisher = {ACM},
 title = {ASPLOS XVII: Proceedings of the Seventeenth International Conference on Architectural Support for Programming Languages and Operating Systems},
 year = {2012}
}


@article{Chipounov:2011:SPI:2248487.1950396,
 abstract = {This paper presents S2E, a platform for analyzing the properties and behavior of software systems. We demonstrate S2E's use in developing practical tools for comprehensive performance profiling, reverse engineering of proprietary software, and bug finding for both kernel-mode and user-mode binaries. Building these tools on top of S2E took less than 770 LOC and 40 person-hours each. S2E's novelty consists of its ability to scale to large real systems, such as a full Windows stack. S2E is based on two new ideas: selective symbolic execution, a way to automatically minimize the amount of code that has to be executed symbolically given a target analysis, and relaxed execution consistency models, a way to make principled performance/accuracy trade-offs in complex analyses. These techniques give S2E three key abilities: to simultaneously analyze entire families of execution paths, instead of just one execution at a time; to perform the analyses in-vivo within a real software stack--user programs, libraries, kernel, drivers, etc.--instead of using abstract models of these layers; and to operate directly on binaries, thus being able to analyze even proprietary software. Conceptually, S2E is an automated path explorer with modular path analyzers: the explorer drives the target system down all execution paths of interest, while analyzers check properties of each such path (e.g., to look for bugs) or simply collect information (e.g., count page faults). Desired paths can be specified in multiple ways, and S2E users can either combine existing analyzers to build a custom analysis tool, or write new analyzers using the S2E API.},
 acmid = {1950396},
 address = {New York, NY, USA},
 author = {Chipounov, Vitaly and Kuznetsov, Volodymyr and Candea, George},
 doi = {10.1145/2248487.1950396},
 issn = {0362-1340},
 issue_date = {April 2012},
 journal = {SIGPLAN Not.},
 keyword = {analysis, binary, consistency models, dbt, framework, in-vivo, performance, symbolic execution, testing, virtualization},
 link = {http://doi.acm.org/10.1145/2248487.1950396},
 month = {mar},
 number = {4},
 numpages = {14},
 pages = {265--278},
 publisher = {ACM},
 title = {S2E: A Platform for In-vivo Multi-path Analysis of Software Systems},
 volume = {47},
 year = {2011}
}


@inproceedings{Hayashizaki:2011:IPT:1950365.1950412,
 abstract = {Trace-based compilation is a promising technique for language compilers and binary translators. It offers the potential to expand the compilation scopes that have traditionally been limited by method boundaries. Detecting repeating cyclic execution paths and capturing the detected repetitions into traces is a key requirement for trace selection algorithms to achieve good optimization and performance with small amounts of code. One important class of repetition detection is cyclic-path-based repetition detection, where a cyclic execution path (a path that starts and ends at the same instruction address) is detected as a repeating cyclic execution path. However, we found many cyclic paths that are not repeating cyclic execution paths, which we call false loops. A common class of false loops occurs when a method is invoked from multiple call-sites. A cycle is formed between two invocations of the method from different call-sites, but which does not represent loops or recursion. False loops can result in shorter traces and smaller compilation scopes, and degrade the performance. We propose false loop filtering, an approach to reject false loops in the repetition detection step of trace selection, and a technique called false loop filtering by call-stack-comparison, which rejects a cyclic path as a false loop if the call stacks at the beginning and the end of the cycle are different. We applied false loop filtering to our trace-based Java™ JIT compiler that is based on IBM's J9 JVM. We found that false loop filtering achieved an average improvement of 16% and 10% for the DaCapo benchmark when applied to two baseline trace selection algorithms, respectively, with up to 37% improvement for individual benchmarks. In the end, with false loop filtering, our trace-based JIT achieves a performance comparable to that of the method-based J9 JVM/JIT using the corresponding optimization level.},
 acmid = {1950412},
 address = {New York, NY, USA},
 author = {Hayashizaki, Hiroshige and Wu, Peng and Inoue, Hiroshi and Serrano, Mauricio J. and Nakatani, Toshio},
 booktitle = {Proceedings of the Sixteenth International Conference on Architectural Support for Programming Languages and Operating Systems},
 doi = {10.1145/1950365.1950412},
 isbn = {978-1-4503-0266-1},
 keyword = {repetition detection, trace selection, trace-based compilation},
 link = {http://doi.acm.org/10.1145/1950365.1950412},
 location = {Newport Beach, California, USA},
 numpages = {14},
 pages = {405--418},
 publisher = {ACM},
 series = {ASPLOS XVI},
 title = {Improving the Performance of Trace-based Systems by False Loop Filtering},
 year = {2011}
}


@article{Zhang:2011:OED:1961296.1950408,
 abstract = {The power-efficient massively parallel Graphics Processing Units (GPUs) have become increasingly influential for general-purpose computing over the past few years. However, their efficiency is sensitive to dynamic irregular memory references and control flows in an application. Experiments have shown great performance gains when these irregularities are removed. But it remains an open question how to achieve those gains through software approaches on modern GPUs. This paper presents a systematic exploration to tackle dynamic irregularities in both control flows and memory references. It reveals some properties of dynamic irregularities in both control flows and memory references, their interactions, and their relations with program data and threads. It describes several heuristics-based algorithms and runtime adaptation techniques for effectively removing dynamic irregularities through data reordering and job swapping. It presents a framework, G-Streamline, as a unified software solution to dynamic irregularities in GPU computing. G-Streamline has several distinctive properties. It is a pure software solution and works on the fly, requiring no hardware extensions or offline profiling. It treats both types of irregularities at the same time in a holistic fashion, maximizing the whole-program performance by resolving conflicts among optimizations. Its optimization overhead is largely transparent to GPU kernel executions, jeopardizing no basic efficiency of the GPU application. Finally, it is robust to the presence of various complexities in GPU applications. Experiments show that G-Streamline is effective in reducing dynamic irregularities in GPU computing, producing speedups between 1.07 and 2.5 for a variety of applications.},
 acmid = {1950408},
 address = {New York, NY, USA},
 author = {Zhang, Eddy Z. and Jiang, Yunlian and Guo, Ziyu and Tian, Kai and Shen, Xipeng},
 doi = {10.1145/1961296.1950408},
 issn = {0362-1340},
 issue_date = {March 2011},
 journal = {SIGPLAN Not.},
 keyword = {cpu-gpu pipelining, data transformation, gpgpu, memory coalescing, thread data remapping, thread divergence},
 link = {http://doi.acm.org/10.1145/1961296.1950408},
 month = {mar},
 number = {3},
 numpages = {12},
 pages = {369--380},
 publisher = {ACM},
 title = {On-the-fly Elimination of Dynamic Irregularities for GPU Computing},
 volume = {46},
 year = {2011}
}


@article{Farhad:2011:OAM:2248487.1950406,
 abstract = {We present a novel 2-approximation algorithm for deploying stream graphs on multicore computers and a stream graph transformation that eliminates bottlenecks. The key technical insight is a data rate transfer model that enables the computation of a "closed form", i.e., the data rate transfer function of an actor depending on the arrival rate of the stream program. A combinatorial optimization problem uses the closed form to maximize the throughput of the stream program. Although the problem is inherently NP-hard, we present an efficient and effective 2-approximation algorithm that provides a lower bound on the quality of the solution. We introduce a transformation that uses the closed form to identify and eliminate bottlenecks. We show experimentally that state-of-the art integer linear programming approaches for orchestrating stream graphs are (1) intractable or at least impractical for larger stream graphs and larger number of processors and (2)our 2-approximation algorithm is highly efficient and its results are close to the optimal solution for a standard set of StreamIt benchmark programs.},
 acmid = {1950406},
 address = {New York, NY, USA},
 author = {Farhad, Sardar M. and Ko, Yousun and Burgstaller, Bernd and Scholz, Bernhard},
 doi = {10.1145/2248487.1950406},
 issn = {0362-1340},
 issue_date = {April 2012},
 journal = {SIGPLAN Not.},
 keyword = {multicore, stream programming, streamit},
 link = {http://doi.acm.org/10.1145/2248487.1950406},
 month = {mar},
 number = {4},
 numpages = {12},
 pages = {357--368},
 publisher = {ACM},
 title = {Orchestration by Approximation: Mapping Stream Programs Onto Multicore Architectures},
 volume = {47},
 year = {2011}
}


@article{Kamruzzaman:2011:IPM:1961296.1950411,
 abstract = {Multicore processors have become ubiquitous in today's systems, but exploiting the parallelism they offer remains difficult, especially for legacy application and applications with large serial components. The challenge, then, is to develop techniques that allow multiple cores to work in concert to accelerate a single thread. This paper describes inter-core prefetching, a technique to exploit multiple cores to accelerate a single thread. Inter-core prefetching extends existing work on helper threads for SMT machines to multicore machines. Inter-core prefetching uses one compute thread and one or more prefetching threads. The prefetching threads execute on cores that would otherwise be idle, prefetching the data that the compute thread will need. The compute thread then migrates between cores, following the path of the prefetch threads, and finds the data already waiting for it. Inter-core prefetching works with existing hardware and existing instruction set architectures. Using a range of state-of-the-art multiprocessors, this paper characterizes the potential benefits of the technique with microbenchmarks and then measures its impact on a range of memory intensive applications. The results show that inter-core prefetching improves performance by an average of 31 to 63%, depending on the architecture, and speeds up some applications by as much as 2.8×. It also demonstrates that inter-core prefetching reduces energy consumption by between 11 and 26% on average.},
 acmid = {1950411},
 address = {New York, NY, USA},
 author = {Kamruzzaman, Md and Swanson, Steven and Tullsen, Dean M.},
 doi = {10.1145/1961296.1950411},
 issn = {0362-1340},
 issue_date = {March 2011},
 journal = {SIGPLAN Not.},
 keyword = {chip multiprocessors, compilers, helper threads, single-thread performance},
 link = {http://doi.acm.org/10.1145/1961296.1950411},
 month = {mar},
 number = {3},
 numpages = {12},
 pages = {393--404},
 publisher = {ACM},
 title = {Inter-core Prefetching for Multicore Processors Using Migrating Helper Threads},
 volume = {46},
 year = {2011}
}


@article{Coburn:2011:NMP:1961296.1950380,
 abstract = {Persistent, user-defined objects present an attractive abstraction for working with non-volatile program state. However, the slow speed of persistent storage (i.e., disk) has restricted their design and limited their performance. Fast, byte-addressable, non-volatile technologies, such as phase change memory, will remove this constraint and allow programmers to build high-performance, persistent data structures in non-volatile storage that is almost as fast as DRAM. Creating these data structures requires a system that is lightweight enough to expose the performance of the underlying memories but also ensures safety in the presence of application and system failures by avoiding familiar bugs such as dangling pointers, multiple free()s, and locking errors. In addition, the system must prevent new types of hard-to-find pointer safety bugs that only arise with persistent objects. These bugs are especially dangerous since any corruption they cause will be permanent. We have implemented a lightweight, high-performance persistent object system called NV-heaps that provides transactional semantics while preventing these errors and providing a model for persistence that is easy to use and reason about. We implement search trees, hash tables, sparse graphs, and arrays using NV-heaps, BerkeleyDB, and Stasis. Our results show that NV-heap performance scales with thread count and that data structures implemented using NV-heaps out-perform BerkeleyDB and Stasis implementations by 32x and 244x, respectively, by avoiding the operating system and minimizing other software overheads. We also quantify the cost of enforcing the safety guarantees that NV-heaps provide and measure the costs of NV-heap primitive operations.},
 acmid = {1950380},
 address = {New York, NY, USA},
 author = {Coburn, Joel and Caulfield, Adrian M. and Akel, Ameen and Grupp, Laura M. and Gupta, Rajesh K. and Jhala, Ranjit and Swanson, Steven},
 doi = {10.1145/1961296.1950380},
 issn = {0362-1340},
 issue_date = {March 2011},
 journal = {SIGPLAN Not.},
 keyword = {acid transactions, memory mangement, non-volatile heap, persistent objects, phase-change memory, pointer safety, spin-torque transfer memory, transactional memory},
 link = {http://doi.acm.org/10.1145/1961296.1950380},
 month = {mar},
 number = {3},
 numpages = {14},
 pages = {105--118},
 publisher = {ACM},
 title = {NV-Heaps: Making Persistent Objects Fast and Safe with Next-generation, Non-volatile Memories},
 volume = {46},
 year = {2011}
}


@inproceedings{Nguyen:2011:SCS:1950365.1950404,
 abstract = {Scheduling is the assignment of tasks or activities to processors for execution, and it is an important concern in parallel programming. Most prior work on scheduling has focused either on static scheduling of applications in which the dependence graph is known at compile-time or on dynamic scheduling of independent loop iterations such as in OpenMP. In irregular algorithms, dependences between activities are complex functions of runtime values so these algorithms are not amenable to compile-time analysis nor do they consist of independent activities. Moreover, the amount of work can vary dramatically with the scheduling policy. To handle these complexities, implementations of irregular algorithms employ carefully handcrafted, algorithm-specific schedulers but these schedulers are themselves parallel programs, complicating the parallel programming problem further. In this paper, we present a flexible and efficient approach for specifying and synthesizing scheduling policies for irregular algorithms. We develop a simple compositional specification language and show how it can concisely encode scheduling policies in the literature. Then, we show how to synthesize efficient parallel schedulers from these specifications. We evaluate our approach for five irregular algorithms on three multicore architectures and show that (1) the performance of some algorithms can improve by orders of magnitude with the right scheduling policy, and (2) for the same policy, the overheads of our synthesized schedulers are comparable to those of fixed-function schedulers.},
 acmid = {1950404},
 address = {New York, NY, USA},
 author = {Nguyen, Donald and Pingali, Keshav},
 booktitle = {Proceedings of the Sixteenth International Conference on Architectural Support for Programming Languages and Operating Systems},
 doi = {10.1145/1950365.1950404},
 isbn = {978-1-4503-0266-1},
 keyword = {scheduling, synthesis},
 link = {http://doi.acm.org/10.1145/1950365.1950404},
 location = {Newport Beach, California, USA},
 numpages = {12},
 pages = {333--344},
 publisher = {ACM},
 series = {ASPLOS XVI},
 title = {Synthesizing Concurrent Schedulers for Irregular Algorithms},
 year = {2011}
}


@proceedings{Hoe:2010:1736020,
 abstract = {It is our pleasure to introduce the technical program for the fifteenth edition of ASPLOS. The conference this year continues and reinforces the ASPLOS tradition of publishing innovative research spanning the boundaries of hardware, computer architecture, compilers, programming languages, operating systems, and distributed computing. We received 181 submissions this year, which was a record and a significant reversal of a trend from the last 3 years (158, 127 and 113 in 2006, 2008 and 2009, respectively). 19 submissions had PC members as co-authors. The most dominant theme, by far, was parallel processing: over half of the submissions were directly related to this topic, most of which mentioned multicore, manycore, SIMD, or heterogeneous architectures as a target. There were two key changes in the ASPLOS paper reviewing process this year, both of which have been used in other systems conferences: two rounds of reviews, and the use of an External Review Committee preselected by the Program Chair. Each paper was initially reviewed by two PC members and one ERC member. Based on these reviews, 91 papers were selected for a second round of reviews; see the Program Chair's Report on the ASPLOS Web site for the details on how these were selected. These papers received one or two more PC reviews plus one more ERC review. Together, these two changes held the average reviewing load per PC member to under 18 papers, while allowing them to spend more time on papers with a significant chance of being accepted. The PC met in person at the Chicago O'Hare Hilton for a one-day meeting on Oct. 30, 2009. 73 papers were discussed at the meeting. The PC chose 32 papers to be presented at the conference, for an overall acceptance rate of 17.7%. 6 of the 32 have PC members as co-authors. In another departure from previous years, all papers were assigned a shepherd to ensure that the final papers adequately addressed the concerns of the reviewers. The PC also awarded a Best Paper Award and nominated three papers for the CACM Research Highlights. If ASPLOS XV is successful, as we hope, numerous people should share the credit. Most of all, the technical program directly reflects the high quality of the submitted papers; we thank all the authors of the submissions for their effort. The members of the PC and ERC put in a very substantial effort for reviewing and shepherding, and for selecting the award papers. Mike Hind (IBM Research), in particular, also handled the review process and discussions for papers where the PC chair had conflicts, and provided valuable advice during the reviewing process. Seth Goldstein (CMU) once again organized and put his inimitable stamp on the ASPLOS Wild and Crazy Ideas session. We owe them all a debt of gratitude. The extensive logistics behind organizing this conference have been handled by the tireless effort of a large team of Pittsburgh volunteers: Shimin Chen (Intel), Allen Cheng (Pitt), Sangyeun Cho (Pitt), Franz Franchetti (CMU), Ken Mai (CMU), Todd Mowry (CMU), Onur Mutlu (CMU), Jun Yang (Pitt) and Youtao Zhang (Pitt). Each member of this team played an important role in making ASPLOS in Pittsburgh a success. In addition, the ASPLOS Steering Committee gave us timely and valuable guidance over the last year. We want to thank all members of the organizing committee and the steering committee for the time and attention they have invested in this conference.},
 address = {New York, NY, USA},
 isbn = {978-1-60558-839-1},
 location = {Pittsburgh, Pennsylvania, USA},
 note = {415105},
 publisher = {ACM},
 title = {ASPLOS XV: Proceedings of the Fifteenth Edition of ASPLOS on Architectural Support for Programming Languages and Operating Systems},
 year = {2010}
}


@article{Porter:2011:RLO:1961295.1950399,
 abstract = {This paper revisits an old approach to operating system construc-tion, the library OS, in a new context. The idea of the library OS is that the personality of the OS on which an application depends runs in the address space of the application. A small, fixed set of abstractions connects the library OS to the host OS kernel, offering the promise of better system security and more rapid independent evolution of OS components. We describe a working prototype of a Windows 7 library OS that runs the latest releases of major applications such as Microsoft Excel, PowerPoint, and Internet Explorer. We demonstrate that desktop sharing across independent, securely isolated, library OS instances can be achieved through the pragmatic reuse of net-working protocols. Each instance has significantly lower overhead than a full VM bundled with an application: a typical application adds just 16MB of working set and 64MB of disk footprint. We contribute a new ABI below the library OS that enables application mobility. We also show that our library OS can address many of the current uses of hardware virtual machines at a fraction of the overheads. This paper describes the first working prototype of a full commercial OS redesigned as a library OS capable of running significant applications. Our experience shows that the long-promised benefits of the library OS approach better protection of system integrity and rapid system evolution are readily obtainable.},
 acmid = {1950399},
 address = {New York, NY, USA},
 author = {Porter, Donald E. and Boyd-Wickizer, Silas and Howell, Jon and Olinsky, Reuben and Hunt, Galen C.},
 doi = {10.1145/1961295.1950399},
 issn = {0163-5964},
 issue_date = {March 2011},
 journal = {SIGARCH Comput. Archit. News},
 keyword = {drawbridge, libos, library os},
 link = {http://doi.acm.org/10.1145/1961295.1950399},
 month = {mar},
 number = {1},
 numpages = {14},
 pages = {291--304},
 publisher = {ACM},
 title = {Rethinking the Library OS from the Top Down},
 volume = {39},
 year = {2011}
}


@article{Sharma:2011:BMS:1961295.1950389,
 abstract = {Reducing the energy footprint of data centers continues to receive significant attention due to both its financial and environmental impact. There are numerous methods that limit the impact of both factors, such as expanding the use of renewable energy or participating in automated demand-response programs. To take advantage of these methods, servers and applications must gracefully handle intermittent constraints in their power supply. In this paper, we propose blinking---metered transitions between a high-power active state and a low-power inactive state---as the primary abstraction for conforming to intermittent power constraints. We design Blink, an application-independent hardware-software platform for developing and evaluating blinking applications, and define multiple types of blinking policies. We then use Blink to design BlinkCache, a blinking version of memcached, to demonstrate the effect of blinking on an example application. Our results show that a load-proportional blinking policy combines the advantages of both activation and synchronous blinking for realistic Zipf-like popularity distributions and wind/solar power signals by achieving near optimal hit rates (within 15% of an activation policy), while also providing fairer access to the cache (within 2% of a syn- chronous policy) for equally popular objects.},
 acmid = {1950389},
 address = {New York, NY, USA},
 author = {Sharma, Navin and Barker, Sean and Irwin, David and Shenoy, Prashant},
 doi = {10.1145/1961295.1950389},
 issn = {0163-5964},
 issue_date = {March 2011},
 journal = {SIGARCH Comput. Archit. News},
 keyword = {blink, intermittent, power, renewable energy},
 link = {http://doi.acm.org/10.1145/1961295.1950389},
 month = {mar},
 number = {1},
 numpages = {14},
 pages = {185--198},
 publisher = {ACM},
 title = {Blink: Managing Server Clusters on Intermittent Power},
 volume = {39},
 year = {2011}
}


@article{Larus:2011:CCE:1961296.1950367,
 abstract = {Cloud computing is fast on its way to becoming a meaningless, oversold marketing slogan. In the midst of this hype, it is easy to overlook the fundamental change that is occurring. Computation, which used to be confined to the machine beside your desk, is increasingly centralized in vast shared facilities and at the same time liberated by battery-powered, wireless devices. Performance, security, and reliability are no longer problems that can be considered in isolation -- the wires and software connecting pieces offer more challenges and opportunities than components themselves. The eXtreme Computing Group (XCG) in Microsoft Research is taking a holistic approach to research in this area, by bring together researchers and developers with expertise in data center design, computer architecture, operating systems, computer security, programming language, mobile computation, and user interfaces to tackle the challenges of cloud computing.},
 acmid = {1950367},
 address = {New York, NY, USA},
 author = {Larus, James R.},
 doi = {10.1145/1961296.1950367},
 issn = {0362-1340},
 issue_date = {March 2011},
 journal = {SIGPLAN Not.},
 keyword = {cloud computing},
 link = {http://doi.acm.org/10.1145/1961296.1950367},
 month = {mar},
 number = {3},
 numpages = {2},
 pages = {1--2},
 publisher = {ACM},
 title = {The Cloud Will Change Everything},
 volume = {46},
 year = {2011}
}


@inproceedings{Yuan:2011:ISD:1950365.1950369,
 abstract = {Diagnosing software failures in the field is notoriously difficult, in part due to the fundamental complexity of trouble-shooting any complex software system, but further exacerbated by the paucity of information that is typically available in the production setting. Indeed, for reasons of both overhead and privacy, it is common that only the run-time log generated by a system (e.g., syslog) can be shared with the developers. Unfortunately, the ad-hoc nature of such reports are frequently insufficient for detailed failure diagnosis. This paper seeks to improve this situation within the rubric of existing practice. We describe a tool, LogEnhancer that automatically "enhances" existing logging code to aid in future post-failure debugging. We evaluate LogEnhancer on eight large, real-world applications and demonstrate that it can dramatically reduce the set of potential root failure causes that must be considered during diagnosis while imposing negligible overheads.},
 acmid = {1950369},
 address = {New York, NY, USA},
 author = {Yuan, Ding and Zheng, Jing and Park, Soyeon and Zhou, Yuanyuan and Savage, Stefan},
 booktitle = {Proceedings of the Sixteenth International Conference on Architectural Support for Programming Languages and Operating Systems},
 doi = {10.1145/1950365.1950369},
 isbn = {978-1-4503-0266-1},
 keyword = {log, software diagnosability, static analysis},
 link = {http://doi.acm.org/10.1145/1950365.1950369},
 location = {Newport Beach, California, USA},
 numpages = {12},
 pages = {3--14},
 publisher = {ACM},
 series = {ASPLOS XVI},
 title = {Improving Software Diagnosability via Log Enhancement},
 year = {2011}
}


@article{Esmaeilzadeh:2011:LBL:1961295.1950402,
 abstract = {This paper reports and analyzes measured chip power and performance on five process technology generations executing 61 diverse benchmarks with a rigorous methodology. We measure representative Intel IA32 processors with technologies ranging from 130nm to 32nm while they execute sequential and parallel benchmarks written in native and managed languages. During this period, hardware and software changed substantially: (1) hardware vendors delivered chip multiprocessors instead of uniprocessors, and independently (2) software developers increasingly chose managed languages instead of native languages. This quantitative data reveals the extent of some known and previously unobserved hardware and software trends. Two themes emerge. (I) Workload: The power, performance, and energy trends of native workloads do not approximate managed workloads. For example, (a) the SPEC CPU2006 native benchmarks on the i7 (45) and i5 (32) draw significantly less power than managed or scalable native benchmarks; and (b) managed runtimes exploit parallelism even when running single-threaded applications. The results recommend architects always include native and managed workloads when designing and evaluating energy efficient hardware. (II) Architecture: Clock scaling, microarchitecture, simultaneous multithreading, and chip multiprocessors each elicit a huge variety of power, performance, and energy responses. This variety and the difficulty of obtaining power measurements recommends exposing on-chip power meters and when possible structure specific power meters for cores, caches, and other structures. Just as hardware event counters provide a quantitative grounding for performance innovations, power meters are necessary for optimizing energy.},
 acmid = {1950402},
 address = {New York, NY, USA},
 author = {Esmaeilzadeh, Hadi and Cao, Ting and Xi, Yang and Blackburn, Stephen M. and McKinley, Kathryn S.},
 doi = {10.1145/1961295.1950402},
 issn = {0163-5964},
 issue_date = {March 2011},
 journal = {SIGARCH Comput. Archit. News},
 keyword = {energy, managed languages, measurement, native languages, performance, power},
 link = {http://doi.acm.org/10.1145/1961295.1950402},
 month = {mar},
 number = {1},
 numpages = {14},
 pages = {319--332},
 publisher = {ACM},
 title = {Looking Back on the Language and Hardware Revolutions: Measured Power, Performance, and Scaling},
 volume = {39},
 year = {2011}
}


@article{Hormati:2011:SPS:1961295.1950409,
 abstract = {Graphics processing units (GPUs) provide a low cost platform for accelerating high performance computations. The introduction of new programming languages, such as CUDA and OpenCL, makes GPU programming attractive to a wide variety of programmers. However, programming GPUs is still a cumbersome task for two primary reasons: tedious performance optimizations and lack of portability. First, optimizing an algorithm for a specific GPU is a time-consuming task that requires a thorough understanding of both the algorithm and the underlying hardware. Unoptimized CUDA programs typically only achieve a small fraction of the peak GPU performance. Second, GPU code lacks efficient portability as code written for one GPU can be inefficient when executed on another. Moving code from one GPU to another while maintaining the desired performance is a non-trivial task often requiring significant modifications to account for the hardware differences. In this work, we propose Sponge, a compilation framework for GPUs using synchronous data flow streaming languages. Sponge is capable of performing a wide variety of optimizations to generate efficient code for graphics engines. Sponge alleviates the problems associated with current GPU programming methods by providing portability across different generations of GPUs and CPUs, and a better abstraction of the hardware details, such as the memory hierarchy and threading model. Using streaming, we provide a write-once software paradigm and rely on the compiler to automatically create optimized CUDA code for a wide variety of GPU targets. Sponge's compiler optimizations improve the performance of the baseline CUDA implementations by an average of 3.2x.},
 acmid = {1950409},
 address = {New York, NY, USA},
 author = {Hormati, Amir H. and Samadi, Mehrzad and Woh, Mark and Mudge, Trevor and Mahlke, Scott},
 doi = {10.1145/1961295.1950409},
 issn = {0163-5964},
 issue_date = {March 2011},
 journal = {SIGARCH Comput. Archit. News},
 keyword = {compiler, gpu, optimization, portability, streaming},
 link = {http://doi.acm.org/10.1145/1961295.1950409},
 month = {mar},
 number = {1},
 numpages = {12},
 pages = {381--392},
 publisher = {ACM},
 title = {Sponge: Portable Stream Programming on Graphics Engines},
 volume = {39},
 year = {2011}
}


@article{Devietti:2011:RRC:1961296.1950376,
 abstract = {Providing deterministic execution significantly simplifies the debugging, testing, replication, and deployment of multithreaded programs. Recent work has developed deterministic multiprocessor architectures as well as compiler and runtime systems that enforce determinism in current hardware. Such work has incidentally imposed strong memory-ordering properties. Historically, memory ordering has been relaxed in favor of higher performance in shared memory multiprocessors and, interestingly, determinism exacerbates the cost of strong memory ordering. Consequently, we argue that relaxed memory ordering is vital to achieving faster deterministic execution. This paper introduces RCDC, a deterministic multiprocessor architecture that takes advantage of relaxed memory orderings to provide high-performance deterministic execution with low hardware complexity. RCDC has two key innovations: a hybrid HW/SW approach to enforcing determinism; and a new deterministic execution strategy that leverages data-race-free-based memory models (e.g., the models for Java and C++) to improve performance and scalability without sacrificing determinism, even in the presence of races. In our hybrid HW/SW approach, the only hardware mechanisms required are software-controlled store buffering and support for precise instruction counting; we do not require speculation. A runtime system uses these mechanisms to enforce determinism for arbitrary programs. We evaluate RCDC using PARSEC benchmarks and show that relaxing memory ordering leads to performance and scalability close to nondeterministic execution without requiring any form of speculation. We also compare our new execution strategy to one based on TSO (total-store-ordering) and show that some applications benefit significantly from the extra relaxation. We also evaluate a software-only implementation of our new deterministic execution strategy.},
 acmid = {1950376},
 address = {New York, NY, USA},
 author = {Devietti, Joseph and Nelson, Jacob and Bergan, Tom and Ceze, Luis and Grossman, Dan},
 doi = {10.1145/1961296.1950376},
 issn = {0362-1340},
 issue_date = {March 2011},
 journal = {SIGPLAN Not.},
 keyword = {determinism, multicore, parallel programming, relaxed consistency},
 link = {http://doi.acm.org/10.1145/1961296.1950376},
 month = {mar},
 number = {3},
 numpages = {12},
 pages = {67--78},
 publisher = {ACM},
 title = {RCDC: A Relaxed Consistency Deterministic Computer},
 volume = {46},
 year = {2011}
}


@article{Hoffmann:2011:DKR:1961296.1950390,
 abstract = {We present PowerDial, a system for dynamically adapting application behavior to execute successfully in the face of load and power fluctuations. PowerDial transforms static configuration parameters into dynamic knobs that the PowerDial control system can manipulate to dynamically trade off the accuracy of the computation in return for reductions in the computational resources that the application requires to produce its results. These reductions translate directly into performance improvements and power savings. Our experimental results show that PowerDial can enable our benchmark applications to execute responsively in the face of power caps that would otherwise significantly impair responsiveness. They also show that PowerDial can significantly reduce the number of machines required to service intermittent load spikes, enabling reductions in power and capital costs.},
 acmid = {1950390},
 address = {New York, NY, USA},
 author = {Hoffmann, Henry and Sidiroglou, Stelios and Carbin, Michael and Misailovic, Sasa and Agarwal, Anant and Rinard, Martin},
 doi = {10.1145/1961296.1950390},
 issn = {0362-1340},
 issue_date = {March 2011},
 journal = {SIGPLAN Not.},
 keyword = {accuracy-aware computing, power-aware computing, self-aware systems},
 link = {http://doi.acm.org/10.1145/1961296.1950390},
 month = {mar},
 number = {3},
 numpages = {14},
 pages = {199--212},
 publisher = {ACM},
 title = {Dynamic Knobs for Responsive Power-aware Computing},
 volume = {46},
 year = {2011}
}


@article{Koukoumidis:2011:PC:1961295.1950387,
 abstract = {Cloud services accessed through mobile devices suffer from high network access latencies and are constrained by energy budgets dictated by the devices' batteries. Radio and battery technologies will improve over time, but are still expected to be the bottlenecks in future systems. Non-volatile memories (NVM), however, may continue experiencing significant and steady improvements in density for at least ten more years. In this paper, we propose to leverage the abundance in memory capacity of mobile devices to mitigate latency and energy issues when accessing cloud services. We first analyze NVM technology scaling trends, and then propose a cloud service cache architecture that resides on the mobile device's NVM (pocket cloudlet). This architecture utilizes both individual user and community access models to maximize its hit rate, and subsequently reduce overall service latency and energy consumption. As a showcase we present the design, implementation and evaluation of PocketSearch, a search and advertisement pocket cloudlet. We perform mobile search characterization to guide the design of PocketSearch and evaluate it with 200 million mobile queries from the search logs of m.bing.com. We show that PocketSearch can serve, on average, 66% of the web search queries submitted by an individual user without having to use the slow 3G link, leading to 16x service access speedup. Finally, based on experience with PocketSearch we provide additional insight and guidelines on how future pocket cloudlets should be organized, from both an architectural and an operating system perspective.},
 acmid = {1950387},
 address = {New York, NY, USA},
 author = {Koukoumidis, Emmanouil and Lymberopoulos, Dimitrios and Strauss, Karin and Liu, Jie and Burger, Doug},
 doi = {10.1145/1961295.1950387},
 issn = {0163-5964},
 issue_date = {March 2011},
 journal = {SIGARCH Comput. Archit. News},
 keyword = {flash storage, mobile cloud, mobile search},
 link = {http://doi.acm.org/10.1145/1961295.1950387},
 month = {mar},
 number = {1},
 numpages = {14},
 pages = {171--184},
 publisher = {ACM},
 title = {Pocket Cloudlets},
 volume = {39},
 year = {2011}
}


@article{Casper:2011:HAT:1961296.1950372,
 abstract = {The adoption of transactional memory is hindered by the high overhead of software transactional memory and the intrusive design changes required by previously proposed TM hardware. We propose that hardware to accelerate software transactional memory (STM) can reside outside an unmodified commodity processor core, thereby substantially reducing implementation costs. This paper introduces Transactional Memory Acceleration using Commodity Cores (TMACC), a hardware-accelerated TM system that does not modify the processor, caches, or coherence protocol. We present a complete hardware implementation of TMACC using a rapid prototyping platform. Using this hardware, we implement two unique conflict detection schemes which are accelerated using Bloom filters on an FPGA. These schemes employ novel techniques for tolerating the latency of fine-grained asynchronous communication with an out-of-core accelerator. We then conduct experiments to explore the feasibility of accelerating TM without modifying existing system hardware. We show that, for all but short transactions, it is not necessary to modify the processor to obtain substantial improvement in TM performance. In these cases, TMACC outperforms an STM by an average of 69% in applications using moderate-length transactions, showing maximum speedup within 8% of an upper bound on TM acceleration. Overall, we demonstrate that hardware can substantially accelerate the performance of an STM on unmodified commodity processors.},
 acmid = {1950372},
 address = {New York, NY, USA},
 author = {Casper, Jared and Oguntebi, Tayo and Hong, Sungpack and Bronson, Nathan G. and Kozyrakis, Christos and Olukotun, Kunle},
 doi = {10.1145/1961296.1950372},
 issn = {0362-1340},
 issue_date = {March 2011},
 journal = {SIGPLAN Not.},
 keyword = {fpga, hardware acceleration, transactional memory},
 link = {http://doi.acm.org/10.1145/1961296.1950372},
 month = {mar},
 number = {3},
 numpages = {12},
 pages = {27--38},
 publisher = {ACM},
 title = {Hardware Acceleration of Transactional Memory on Commodity Systems},
 volume = {46},
 year = {2011}
}


@article{Liu:2011:FSD:1961296.1950391,
 abstract = {Energy has become a first-class design constraint in computer systems. Memory is a significant contributor to total system power. This paper introduces Flikker, an application-level technique to reduce refresh power in DRAM memories. Flikker enables developers to specify critical and non-critical data in programs and the runtime system allocates this data in separate parts of memory. The portion of memory containing critical data is refreshed at the regular refresh-rate, while the portion containing non-critical data is refreshed at substantially lower rates. This partitioning saves energy at the cost of a modest increase in data corruption in the non-critical data. Flikker thus exposes and leverages an interesting trade-off between energy consumption and hardware correctness. We show that many applications are naturally tolerant to errors in the non-critical data, and in the vast majority of cases, the errors have little or no impact on the application's final outcome. We also find that Flikker can save between 20-25% of the power consumed by the memory sub-system in a mobile device, with negligible impact on application performance. Flikker is implemented almost entirely in software, and requires only modest changes to the hardware.},
 acmid = {1950391},
 address = {New York, NY, USA},
 author = {Liu, Song and Pattabiraman, Karthik and Moscibroda, Thomas and Zorn, Benjamin G.},
 doi = {10.1145/1961296.1950391},
 issn = {0362-1340},
 issue_date = {March 2011},
 journal = {SIGPLAN Not.},
 keyword = {allocation, critical data, dram refresh, power-savings, soft errors},
 link = {http://doi.acm.org/10.1145/1961296.1950391},
 month = {mar},
 number = {3},
 numpages = {12},
 pages = {213--224},
 publisher = {ACM},
 title = {Flikker: Saving DRAM Refresh-power Through Critical Data Partitioning},
 volume = {46},
 year = {2011}
}


