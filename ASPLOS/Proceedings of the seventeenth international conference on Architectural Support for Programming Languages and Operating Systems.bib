@inproceedings{Lin:2012:ESC:2150976.2151006,
 abstract = {Although the sequential consistency (SC) model is the most intuitive, processor designers often choose to support relaxed memory consistency models for higher performance. This is because SC implementations that match the performance of relaxed memory models require post-retirement speculation and its associated hardware costs. In this paper we propose an efficient approach for enforcing SC without requiring post-retirement speculation. While prior SC implementations guarantee SC by explicitly completing memory operations within a processor in program order, we guarantee SC by completing conflicting memory operations, within and across processors, in an order that is consistent with the program order. More specifically, we identify those conflicting memory operations whose ordering is critical for the maintenance of SC and explicitly order them. This allows us to safely (non-speculatively) complete memory operations past pending writes, thus reducing memory ordering stalls. Our experiments with SPLASH-2 programs show that SC can be achieved efficiently, with performance comparable to RMO (relaxed memory order).},
 acmid = {2151006},
 address = {New York, NY, USA},
 author = {Lin, Changhui and Nagarajan, Vijay and Gupta, Rajiv and Rajaram, Bharghava},
 booktitle = {Proceedings of the Seventeenth International Conference on Architectural Support for Programming Languages and Operating Systems},
 doi = {10.1145/2150976.2151006},
 isbn = {978-1-4503-0759-8},
 keyword = {conflict ordering, sequential consistency},
 link = {http://doi.acm.org/10.1145/2150976.2151006},
 location = {London, England, UK},
 numpages = {14},
 pages = {273--286},
 publisher = {ACM},
 series = {ASPLOS XVII},
 title = {Efficient Sequential Consistency via Conflict Ordering},
 year = {2012}
}


@inproceedings{Hong:2012:GDE:2150976.2151013,
 abstract = {The increasing importance of graph-data based applications is fueling the need for highly efficient and parallel implementations of graph analysis software. In this paper we describe Green-Marl, a domain-specific language (DSL) whose high level language constructs allow developers to describe their graph analysis algorithms intuitively, but expose the data-level parallelism inherent in the algorithms. We also present our Green-Marl compiler which translates high-level algorithmic description written in Green-Marl into an efficient C++ implementation by exploiting this exposed data-level parallelism. Furthermore, our Green-Marl compiler applies a set of optimizations that take advantage of the high-level semantic knowledge encoded in the Green-Marl DSL. We demonstrate that graph analysis algorithms can be written very intuitively with Green-Marl through some examples, and our experimental results show that the compiler-generated implementation out of such descriptions performs as well as or better than highly-tuned hand-coded implementations.},
 acmid = {2151013},
 address = {New York, NY, USA},
 author = {Hong, Sungpack and Chafi, Hassan and Sedlar, Edic and Olukotun, Kunle},
 booktitle = {Proceedings of the Seventeenth International Conference on Architectural Support for Programming Languages and Operating Systems},
 doi = {10.1145/2150976.2151013},
 isbn = {978-1-4503-0759-8},
 keyword = {domain-specific language, graph, parallel programming},
 link = {http://doi.acm.org/10.1145/2150976.2151013},
 location = {London, England, UK},
 numpages = {14},
 pages = {349--362},
 publisher = {ACM},
 series = {ASPLOS XVII},
 title = {Green-Marl: A DSL for Easy and Efficient Graph Analysis},
 year = {2012}
}


@article{Hari:2012:REA:2248487.2150990,
 abstract = {Future microprocessors need low-cost solutions for reliable operation in the presence of failure-prone devices. A promising approach is to detect hardware faults by deploying low-cost monitors of software-level symptoms of such faults. Recently, researchers have shown these mechanisms work well, but there remains a non-negligible risk that several faults may escape the symptom detectors and result in silent data corruptions (SDCs). Most prior evaluations of symptom-based detectors perform fault injection campaigns on application benchmarks, where each run simulates the impact of a fault injected at a hardware site at a certain point in the application's execution (application fault site). Since the total number of application fault sites is very large (trillions for standard benchmark suites), it is not feasible to study all possible faults. Previous work therefore typically studies a randomly selected sample of faults. Such studies do not provide any feedback on the portions of the application where faults were not injected. Some of those instructions may be vulnerable to SDCs, and identifying them could allow protecting them through other means if needed. This paper presents Relyzer, an approach that systematically analyzes all application fault sites and carefully picks a small subset to perform selective fault injections for transient faults. Relyzer employs novel fault pruning techniques that prune faults that need detailed study by either predicting their outcomes or showing them equivalent to other faults. We find that Relyzer prunes about 99.78% of the total faults across twelve applications studied here, reducing the faults that require detailed simulation by 3 to 5 orders of magnitude for most of the applications. Fault injection simulations on the remaining faults can identify SDC causing faults in the entire application. Some of Relyzer's techniques rely on heuristics to determine fault equivalence. Our validation efforts show that Relyzer determines fault outcomes with 96% accuracy, averaged across all the applications studied here.},
 acmid = {2150990},
 address = {New York, NY, USA},
 author = {Hari, Siva Kumar Sastry and Adve, Sarita V. and Naeimi, Helia and Ramachandran, Pradeep},
 doi = {10.1145/2248487.2150990},
 issn = {0362-1340},
 issue_date = {April 2012},
 journal = {SIGPLAN Not.},
 keyword = {architecture, hardware reliability evaluation, low-cost hardware resiliency, silent data corruption, transient faults},
 link = {http://doi.acm.org/10.1145/2248487.2150990},
 month = {mar},
 number = {4},
 numpages = {12},
 pages = {123--134},
 publisher = {ACM},
 title = {Relyzer: Exploiting Application-level Fault Equivalence to Analyze Application Resiliency to Transient Faults},
 volume = {47},
 year = {2012}
}


@article{Ahmad:2012:TOM:2189750.2150984,
 abstract = {Data center-scale clusters are evolving towards heterogeneous hardware for power, cost, differentiated price-performance, and other reasons. MapReduce is a well-known programming model to process large amount of data on data center-scale clusters. Most MapReduce implementations have been designed and optimized for homogeneous clusters. Unfortunately, these implementations perform poorly on heterogeneous clusters (e.g., on a 90-node cluster that contains 10 Xeon-based servers and 80 Atom-based servers, Hadoop performs worse than on 10-node Xeon-only or 80-node Atom-only homogeneous sub-clusters for many of our benchmarks). This poor performance remains despite previously proposed optimizations related to management of straggler tasks. In this paper, we address MapReduce's poor performance on heterogeneous clusters. Our first contribution is that the poor performance is due to two key factors: (1) the non-intuitive effect that MapReduce's built-in load balancing results in excessive and bursty network communication during the Map phase, and (2) the intuitive effect that the heterogeneity amplifies load imbalance in the Reduce computation. Our second contribution is Tarazu, a suite of optimizations to improve MapReduce performance on heterogeneous clusters. Tarazu consists of (1) Communication-Aware Load Balancing of Map computation (CALB) across the nodes, (2) Communication-Aware Scheduling of Map computation (CAS) to avoid bursty network traffic and (3) Predictive Load Balancing of Reduce computation (PLB) across the nodes. Using the above 90-node cluster, we show that Tarazu significantly improves performance over a baseline of Hadoop with straightforward tuning for hardware heterogeneity.},
 acmid = {2150984},
 address = {New York, NY, USA},
 author = {Ahmad, Faraz and Chakradhar, Srimat T. and Raghunathan, Anand and Vijaykumar, T. N.},
 doi = {10.1145/2189750.2150984},
 issn = {0163-5964},
 issue_date = {March 2012},
 journal = {SIGARCH Comput. Archit. News},
 keyword = {MapReduce, cluster scheduling, heterogeneous clusters, load imbalance, shuffle},
 link = {http://doi.acm.org/10.1145/2189750.2150984},
 month = {mar},
 number = {1},
 numpages = {14},
 pages = {61--74},
 publisher = {ACM},
 title = {Tarazu: Optimizing MapReduce on Heterogeneous Clusters},
 volume = {40},
 year = {2012}
}


@article{Govindan:2012:LSE:2189750.2150985,
 abstract = {Datacenters spend $10-25 per watt in provisioning their power infrastructure, regardless of the watts actually consumed. Since peak power needs arise rarely, provisioning power infrastructure for them can be expensive. One can, thus, aggressively under-provision infrastructure assuming that simultaneous peak draw across all equipment will happen rarely. The resulting non-zero probability of emergency events where power needs exceed provisioned capacity, however small, mandates graceful reaction mechanisms to cap the power draw instead of leaving it to disruptive circuit breakers/fuses. Existing strategies for power capping use temporal knobs local to a server that throttle the rate of execution (using power modes), and/or spatial knobs that redirect/migrate excess load to regions of the datacenter with more power headroom. We show these mechanisms to have performance degrading ramifications, and propose an entirely orthogonal solution that leverages existing UPS batteries to temporarily augment the utility supply during emergencies. We build an experimental prototype to demonstrate such power capping on a cluster of 8 servers, each with an individual battery, and implement several online heuristics in the context of different datacenter workloads to evaluate their effectiveness in handling power emergencies. We show that: (i) our battery-based solution can handle emergencies of short duration on its own, (ii) supplement existing reaction mechanisms to enhance their efficacy for longer emergencies, and (iii) battery even provide feasible options when other knobs do not suffice.},
 acmid = {2150985},
 address = {New York, NY, USA},
 author = {Govindan, Sriram and Wang, Di and Sivasubramaniam, Anand and Urgaonkar, Bhuvan},
 doi = {10.1145/2189750.2150985},
 issn = {0163-5964},
 issue_date = {March 2012},
 journal = {SIGARCH Comput. Archit. News},
 keyword = {UPS batteries, cap-ex, data center, peak power, peak shaving, power capping, provisioning, stored energy},
 link = {http://doi.acm.org/10.1145/2189750.2150985},
 month = {mar},
 number = {1},
 numpages = {12},
 pages = {75--86},
 publisher = {ACM},
 title = {Leveraging Stored Energy for Handling Power Emergencies in Aggressively Provisioned Datacenters},
 volume = {40},
 year = {2012}
}


@proceedings{Gupta:2011:1950365,
 abstract = {It is our great pleasure to welcome you to the sixteenth edition of ASPLOS and to introduce what we feel will be an outstanding and thought-provoking technical program. This year's conference continues and reinforces the ASPLOS tradition of publishing innovative multidisciplinary research spanning the boundaries of hardware, computer architecture, compilers, languages, operating systems, networking, and applications. We received 152 full paper submissions by the July 28th deadline, which was fewer than last year's record-setting year, but above average over the past three years. (127, 113 and 181 full papers were submitted in 2008, 2009, and 2010, respectively.) 22 submissions had PC members as co-authors. The dominant technical theme this year was parallelism (once again), with more than half of the submissions explicitly indicating a technical keyword specific to parallel processing. There was a nice mixture of submissions spanning the disciplines of operating systems (36%), compilers and programming languages (34%), and architecture (60%), with 38% of submissions spanning multiple of these areas. Regarding the paper reviewing process, we continued with two changes that had been adopted the previous year: we had two rounds of reviews, and we used an External Review Committee that was preselected by the Program Chair. Each paper was initially reviewed by one PC member and two ERC members. The rationale for having two ERC reviews during the first round (as opposed to one, as was the case last year) was that it gave us the most flexibility to match a paper with the experts who were most qualified to review that specific topic. Based upon these reviews (as well as follow-up discussions for any borderline cases), 103 papers were selected for a second round of reviews, where they received two more reviews from PC members. In general, any paper that received at least one vote for acceptance during the first round was selected for second round reviews. Through this two-round mechanism, we were able to limit the reviewing load per PC member to no more than 14 papers, and more importantly we allowed them to spend the bulk of their time on papers with a significant chance of being accepted. We would especially like to thank the ERC members for their hard work and dedication to the reviewing process; the typical ERC member reviewed 5 papers this year. One change that we made to the reviewing form this year was that in addition to evaluation categories that had been used in previous years (e.g., the degree of novelty, whether the paper was convincing, etc.), we added a new category where we asked reviewers to rate the extent to which a paper would be thoughtprovoking. Our goal was to make sure that we paid special attention to submissions that might inspire people to change the way that they think about how they approach their own technical research problems. We are pleased that this emphasis appears to have translated well into a highly thought-provoking technical program. We are also pleased to report that all reviews had been submitted in time for the authors to respond to them during the author rebuttal period. After the rebuttal period, the PC members and ERC members took the rebuttal comments into account as they discussed the papers online and potentially updated their scores accordingly. The PC met in person on the Carnegie Mellon University campus for a one-day meeting on October 21st, 2010. 81 papers were discussed at the meeting. PC-authored papers were handled roughly halfway through the meeting, using the hotseat approach. Jim Larus, Margaret Martonosi, and David Patterson managed the discussion of papers where the PC Chair had a conflict. The PC chose 32 papers to be presented at the conference, for an overall acceptance rate of 21.1%. 7 of the 32 papers have PC members as co-authors. Continuing with another change that was made last year, all papers were assigned a shepherd to ensure that the final papers adequately addressed the concerns of the reviewers.},
 address = {New York, NY, USA},
 isbn = {978-1-4503-0266-1},
 location = {Newport Beach, California, USA},
 note = {415115},
 publisher = {ACM},
 title = {ASPLOS XVI: Proceedings of the Sixteenth International Conference on Architectural Support for Programming Languages and Operating Systems},
 year = {2011}
}


@article{Panneerselvam:2012:COS:2189750.2150988,
 abstract = {The rise of multi-core processors has shifted performance efforts towards parallel programs. However, single-threaded code, whether from legacy programs or ones difficult to parallelize, remains important. Proposed asymmetric multicore processors statically dedicate hardware to improve sequential performance, but at the cost of reduced parallel performance. However, several proposed mechanisms provide the best-of-both-worlds by combining multiple cores into a single, more powerful processor for sequential code. For example, Core Fusion merges multiple cores to pool caches and functional units, and Intel's Turbo Boost raises the clock speed of a core if the other cores on a chip are powered down. These reconfiguration mechanisms have two important properties. First the set of available cores and their capabilities can vary over short time scales. Current operating systems are not designed for rapidly changing hardware: the existing hotplug mechanisms for reconfiguring processors require global operations and hundreds of milliseconds to complete. Second, configurations may be mutually exclusive: using power to speed one core means it cannot be used to speed another. Current schedulers cannot manage this requirement. We present Chameleon, an extension to Linux to support dynamic processors that can reconfigure their cores at runtime. Chameleon provides processor proxies to enable rapid reconfiguration, execution objects to abstract the processing capabilities of physical CPUs, and a cluster scheduler to balance the needs of sequential and parallel programs. In experiments that emulate a dynamic processor, we find that Chameleon can reconfigure processors 100,000 times faster than Linux and allows applications full access to hardware capabilities: sequential code runs at full speed on a powerful execution context, while parallel code runs on as many cores as possible.},
 acmid = {2150988},
 address = {New York, NY, USA},
 author = {Panneerselvam, Sankaralingam and Swift, Michael M.},
 doi = {10.1145/2189750.2150988},
 issn = {0163-5964},
 issue_date = {March 2012},
 journal = {SIGARCH Comput. Archit. News},
 keyword = {dynamic processors, hotplug, processor proxy, reconfiguration, scheduling},
 link = {http://doi.acm.org/10.1145/2189750.2150988},
 month = {mar},
 number = {1},
 numpages = {12},
 pages = {99--110},
 publisher = {ACM},
 title = {Chameleon: Operating System Support for Dynamic Processors},
 volume = {40},
 year = {2012}
}


@article{King:2012:AGH:2248487.2151011,
 abstract = {Enabling new applications for mobile devices often requires the use of specialized hardware to reduce power consumption. Because of time-to-market pressure, current design methodologies for embedded applications require an early partitioning of the design, allowing the hardware and software to be developed simultaneously, each adhering to a rigid interface contract. This approach is problematic for two reasons: (1) a detailed hardware-software interface is difficult to specify until one is deep into the design process, and (2) it prevents the later migration of functionality across the interface motivated by efficiency concerns or the addition of features. We address this problem using the Bluespec Codesign Language~(BCL) which permits the designer to specify the hardware-software partition in the source code, allowing the compiler to synthesize efficient software and hardware along with transactors for communication between the partitions. The movement of functionality across the hardware-software boundary is accomplished by simply specifying a new partitioning, and since the compiler automatically generates the desired interface specifications, it eliminates yet another error-prone design task. In this paper we present BCL, an extension of a commercially available hardware design language (Bluespec SystemVerilog), a new software compiling scheme, and preliminary results generated using our compiler for various hardware-software decompositions of an Ogg Vorbis audio decoder, and a ray-tracing application.},
 acmid = {2151011},
 address = {New York, NY, USA},
 author = {King, Myron and Dave, Nirav and Arvind},
 doi = {10.1145/2248487.2151011},
 issn = {0362-1340},
 issue_date = {April 2012},
 journal = {SIGPLAN Not.},
 keyword = {hardware/software codesign},
 link = {http://doi.acm.org/10.1145/2248487.2151011},
 month = {mar},
 number = {4},
 numpages = {12},
 pages = {325--336},
 publisher = {ACM},
 title = {Automatic Generation of Hardware/Software Interfaces},
 volume = {47},
 year = {2012}
}


@article{Feiner:2012:CKI:2248487.2150992,
 abstract = {Dynamic binary translation (DBT) is a powerful technique that enables fine-grained monitoring and manipulation of an existing program binary. At the user level, it has been employed extensively to develop various analysis, bug-finding, and security tools. Such tools are currently not available for operating system (OS) binaries since no comprehensive DBT framework exists for the OS kernel. To address this problem, we have developed a DBT framework that runs as a Linux kernel module, based on the user-level DynamoRIO framework. Our approach is unique in that it controls all kernel execution, including interrupt and exception handlers and device drivers, enabling comprehensive instrumentation of the OS without imposing any overhead on user-level code. In this paper, we discuss the key challenges in designing and building an in-kernel DBT framework and how the design differs from user-space. We use our framework to build several sample instrumentations, including simple instruction counting as well as an implementation of shadow memory for the kernel. Using the shadow memory, we build a kernel stack overflow protection tool and a memory addressability checking tool. Qualitatively, the system is fast enough and stable enough to run the normal desktop workload of one of the authors for several weeks.},
 acmid = {2150992},
 address = {New York, NY, USA},
 author = {Feiner, Peter and Brown, Angela Demke and Goel, Ashvin},
 doi = {10.1145/2248487.2150992},
 issn = {0362-1340},
 issue_date = {April 2012},
 journal = {SIGPLAN Not.},
 keyword = {Linux, dynamic binary translation, interrupts, operating system instrumentation},
 link = {http://doi.acm.org/10.1145/2248487.2150992},
 month = {mar},
 number = {4},
 numpages = {12},
 pages = {135--146},
 publisher = {ACM},
 title = {Comprehensive Kernel Instrumentation via Dynamic Binary Translation},
 volume = {47},
 year = {2012}
}


@article{Narayanan:2012:WP:2248487.2151018,
 abstract = {Today's databases and key-value stores commonly keep all their data in main memory. A single server can have over 100 GB of memory, and a cluster of such servers can have 10s to 100s of TB. However, a storage back end is still required for recovery from failures. Recovery can last for minutes for a single server or hours for a whole cluster, causing heavy load on the back end. Non-volatile main memory (NVRAM) technologies can help by allowing near-instantaneous recovery of in-memory state. However, today's software does not support this well. Block-based approaches such as persistent buffer caches suffer from data duplication and block transfer overheads. Recently, user-level persistent heaps have been shown to have much better performance than these. However they require substantial application modification and still have significant runtime overheads. This paper proposes whole-system persistence (WSP) as an alternative. WSP is aimed at systems where all memory is non-volatile. It transparently recovers an application's entire state, making a failure appear as a suspend/resume event. Runtime overheads are eliminated by using "flush on fail": transient state in processor registers and caches is flushed to NVRAM only on failure, using the residual energy from the system power supply. Our evaluation shows that this approach has 1.6--13 times better runtime performance than a persistent heap, and that flush-on-fail can complete safely within 2--35\% of the residual energy window provided by standard power supplies.},
 acmid = {2151018},
 address = {New York, NY, USA},
 author = {Narayanan, Dushyanth and Hodson, Orion},
 doi = {10.1145/2248487.2151018},
 issn = {0362-1340},
 issue_date = {April 2012},
 journal = {SIGPLAN Not.},
 keyword = {NVRAM, persistence},
 link = {http://doi.acm.org/10.1145/2248487.2151018},
 month = {mar},
 number = {4},
 numpages = {10},
 pages = {401--410},
 publisher = {ACM},
 title = {Whole-system Persistence},
 volume = {47},
 year = {2012}
}


@article{Chang:2012:TGE:2189750.2150980,
 abstract = {The environmental impact of servers and datacenters is an important future challenge. System architects have traditionally focused on operational energy as a proxy for designing green servers, but this ignores important environmental implications from server production (materials, manufacturing, etc.). In contrast, this paper argues for a lifecycle focus on the environmental impact of future server designs, to include both operation and production. We present a new methodology to quantify the total environmental impact of system design decisions. Our approach uses the thermodynamic metric of exergy consumption, adapted and validated for use by system architects. Using this methodology, we evaluate the lifecycle impact of several example system designs with environment-friendly optimizations. Our results show that environmental impact from production can be important (around 20% on current servers and growing) and system design choices can reduce this component (by 30--40%). Our results also highlight several, sometimes unexpected, cross-interactions between the environmental impact of production and operation that further motivate a total lifecycle emphasis for future green server designs.},
 acmid = {2150980},
 address = {New York, NY, USA},
 author = {Chang, Jichuan and Meza, Justin and Ranganathan, Parthasarathy and Shah, Amip and Shih, Rocky and Bash, Cullen},
 doi = {10.1145/2189750.2150980},
 issn = {0163-5964},
 issue_date = {March 2012},
 journal = {SIGARCH Comput. Archit. News},
 keyword = {datacenter design, dematerialization, disaggregation, environmental sustainability, exergy, green computing, lifecycle impact, server architecture},
 link = {http://doi.acm.org/10.1145/2189750.2150980},
 month = {mar},
 number = {1},
 numpages = {12},
 pages = {25--36},
 publisher = {ACM},
 title = {Totally Green: Evaluating and Designing Servers for Lifecycle Environmental Impact},
 volume = {40},
 year = {2012}
}


@article{Greathouse:2012:CUW:2248487.2150994,
 abstract = {Numerous tools have been proposed to help developers fix software errors and inefficiencies. Widely-used techniques such as memory checking suffer from overheads that limit their use to pre-deployment testing, while more advanced systems have such severe performance impacts that they may require special-purpose hardware. Previous works have described hardware that can accelerate individual analyses, but such specialization stymies adoption; generalized mechanisms are more likely to be added to commercial processors. This paper demonstrates that the ability to set an unlimited number of fine-grain data watchpoints can reduce the runtime overheads of numerous dynamic software analysis techniques. We detail the watchpoint capabilities required to accelerate these analyses while remaining general enough to be useful in the future. We describe a hardware design that stores watchpoints in main memory and utilizes two different on-chip caches to accelerate performance. The first is a bitmap lookaside buffer that stores fine-grained watchpoints, while the second is a range cache that can efficiently hold large contiguous regions of watchpoints. As an example of the power of such a system, it is possible to use watchpoints to accelerate read/write set checks in a software data race detector by nearly 9x.},
 acmid = {2150994},
 address = {New York, NY, USA},
 author = {Greathouse, Joseph L. and Xin, Hongyi and Luo, Yixin and Austin, Todd},
 doi = {10.1145/2248487.2150994},
 issn = {0362-1340},
 issue_date = {April 2012},
 journal = {SIGPLAN Not.},
 keyword = {data race detection, demand-driven analysis, deterministic concurrent execution, taint analysis, watchpoints},
 link = {http://doi.acm.org/10.1145/2248487.2150994},
 month = {mar},
 number = {4},
 numpages = {14},
 pages = {159--172},
 publisher = {ACM},
 title = {A Case for Unlimited Watchpoints},
 volume = {47},
 year = {2012}
}


@article{Meisner:2012:DAS:2189750.2151009,
 abstract = {Numerous data center services exhibit low average utilization leading to poor energy efficiency. Although CPU voltage and frequency scaling historically has been an effective means to scale down power with utilization, transistor scaling trends are limiting its effectiveness and the CPU is accounting for a shrinking fraction of system power. Recent research advocates the use of full-system idle low-power modes to combat energy losses, as such modes provide the deepest power savings with bounded response time impact. However, the trend towards increasing cores per die is undermining the effectiveness of these sleep modes, particularly for request-parallel data center applications, because the independent idle periods across individual cores are unlikely to align by happenstance. We propose DreamWeaver, architectural support to facilitate deep sleep for request-parallel applications on multicore servers. DreamWeaver comprises two elements: Weave Scheduling, a scheduling policy to coalesce idle and busy periods across cores to create opportunities for system-wide deep sleep; and the Dream Processor, a light-weight co-processor that monitors incoming network traffic and suspended work during sleep to determine when the system must wake. DreamWeaver is based on two key concepts: (1) stall execution and sleep anytime any core is unoccupied, but (2) constrain the maximum time any request may be stalled. Unlike prior scheduling approaches, DreamWeaver will preempt execution to sleep, maximizing time spent at the systems' most efficient operating point. We demonstrate that DreamWeaver can smoothly trade-off bounded, predictable increases in 99th-percentile response time for increasing power savings, and strictly dominates the savings available with voltage and frequency scaling and timeout-based request batching schemes.},
 acmid = {2151009},
 address = {New York, NY, USA},
 author = {Meisner, David and Wenisch, Thomas F.},
 doi = {10.1145/2189750.2151009},
 issn = {0163-5964},
 issue_date = {March 2012},
 journal = {SIGARCH Comput. Archit. News},
 keyword = {power management, servers},
 link = {http://doi.acm.org/10.1145/2189750.2151009},
 month = {mar},
 number = {1},
 numpages = {12},
 pages = {313--324},
 publisher = {ACM},
 title = {DreamWeaver: Architectural Support for Deep Sleep},
 volume = {40},
 year = {2012}
}


@article{Caulfield:2012:PSU:2189750.2151017,
 abstract = {Emerging fast, non-volatile memories (e.g., phase change memories, spin-torque MRAMs, and the memristor) reduce storage access latencies by an order of magnitude compared to state-of-the-art flash-based SSDs. This improved performance means that software overheads that had little impact on the performance of flash-based systems can present serious bottlenecks in systems that incorporate these new technologies. We describe a novel storage hardware and software architecture that nearly eliminates two sources of this overhead: Entering the kernel and performing file system permission checks. The new architecture provides a private, virtualized interface for each process and moves file system protection checks into hardware. As a result, applications can access file data without operating system intervention, eliminating OS and file system costs entirely for most accesses. We describe the support the system provides for fast permission checks in hardware, our approach to notifying applications when requests complete, and the small, easily portable changes required in the file system to support the new access model. Existing applications require no modification to use the new interface. We evaluate the performance of the system using a suite of microbenchmarks and database workloads and show that the new interface improves latency and bandwidth for 4 KB writes by 60% and 7.2x, respectively, OLTP database transaction throughput by up to 2.0x, and Berkeley-DB throughput by up to 5.7x. A streamlined asynchronous file IO interface built to fully utilize the new interface enables an additional 5.5x increase in throughput with 1 thread and 2.8x increase in efficiency for 512 B transfers.},
 acmid = {2151017},
 address = {New York, NY, USA},
 author = {Caulfield, Adrian M. and Mollov, Todor I. and Eisner, Louis Alex and De, Arup and Coburn, Joel and Swanson, Steven},
 doi = {10.1145/2189750.2151017},
 issn = {0163-5964},
 issue_date = {March 2012},
 journal = {SIGARCH Comput. Archit. News},
 keyword = {I/O performance, file systems, non-volatile memory, storage systems, virtualization},
 link = {http://doi.acm.org/10.1145/2189750.2151017},
 month = {mar},
 number = {1},
 numpages = {14},
 pages = {387--400},
 publisher = {ACM},
 title = {Providing Safe, User Space Access to Fast, Solid State Disks},
 volume = {40},
 year = {2012}
}


@inproceedings{Kadav:2012:UMD:2150976.2150987,
 abstract = {Device drivers are the single largest contributor to operating-system kernel code with over 5 million lines of code in the Linux kernel, and cause significant complexity, bugs and development costs. Recent years have seen a flurry of research aimed at improving the reliability and simplifying the development of drivers. However, little is known about what constitutes this huge body of code beyond the small set of drivers used for research. In this paper, we study the source code of Linux drivers to understand what drivers actually do, how current research applies to them and what opportunities exist for future research. We determine whether assumptions made by most driver research, such as that all drivers belong to a class, are indeed true. We also analyze driver code and abstractions to determine whether drivers can benefit from code re-organization or hardware trends. We develop a set of static-analysis tools to analyze driver code across various axes. Broadly, our study looks at three aspects of driver code (i) what are the characteristics of driver code functionality and how applicable is driver research to all drivers, (ii) how do drivers interact with the kernel, devices, and buses, and (iii) are there similarities that can be abstracted into libraries to reduce driver size and complexity? We find that many assumptions made by driver research do not apply to all drivers. At least 44% of drivers have code that is not captured by a class definition, 28% of drivers support more than one device per driver, and 15% of drivers do significant computation over data. From the driver interactions study, we find USB bus offers an efficient bus interface with significant standardized code and coarse-grained access, ideal for executing drivers in isolation. We also find that drivers for different buses and classes have widely varying levels of device interaction, which indicates that the cost of isolation will vary by class. Finally, from our driver similarity study, we find 8% of all driver code is substantially similar to code elsewhere and may be removed with new abstractions or libraries.},
 acmid = {2150987},
 address = {New York, NY, USA},
 author = {Kadav, Asim and Swift, Michael M.},
 booktitle = {Proceedings of the Seventeenth International Conference on Architectural Support for Programming Languages and Operating Systems},
 doi = {10.1145/2150976.2150987},
 isbn = {978-1-4503-0759-8},
 keyword = {device drivers, measurement},
 link = {http://doi.acm.org/10.1145/2150976.2150987},
 location = {London, England, UK},
 numpages = {12},
 pages = {87--98},
 publisher = {ACM},
 series = {ASPLOS XVII},
 title = {Understanding Modern Device Drivers},
 year = {2012}
}


@article{Szefer:2012:ASH:2248487.2151022,
 abstract = {Virtualization has become a standard part of many computer systems. A key part of virtualization is the all-powerful hypervisor which manages the physical platform and can access all of its resources, including memory assigned to the guest virtual machines (VMs). Continuing releases of bug reports and exploits in the virtualization software show that defending the hypervisor against attacks is very difficult. In this work, we present hypervisor-secure virtualization - a new research direction with the goal of protecting the guest VMs from an untrusted hypervisor. We also present the HyperWall architecture which achieves hypervisor-secure virtualization, using hardware to provide the protections. HyperWall allows a hypervisor to freely manage the memory, processor cores and other resources of a platform. Yet once VMs are created, our new Confidentiality and Integrity Protection (CIP) tables protect the memory of the guest VMs from accesses by the hypervisor or by DMA, depending on the customer's specification. If a hypervisor does become compromised, e.g. by an attack from a malicious VM, it cannot be used in turn to attack other VMs. The protections are enabled through minimal modifications to the microprocessor and memory management units. Whereas much of the previous work concentrates on protecting the hypervisor from attacks by guest VMs, we tackle the problem of protecting the guest VMs from the hypervisor.},
 acmid = {2151022},
 address = {New York, NY, USA},
 author = {Szefer, Jakub and Lee, Ruby B.},
 doi = {10.1145/2248487.2151022},
 issn = {0362-1340},
 issue_date = {April 2012},
 journal = {SIGPLAN Not.},
 keyword = {attestation, cloud computing, computer architecture, confidentiality, hardware security, hypervisor, integrity, security, trust evidence, virtualization},
 link = {http://doi.acm.org/10.1145/2248487.2151022},
 month = {mar},
 number = {4},
 numpages = {14},
 pages = {437--450},
 publisher = {ACM},
 title = {Architectural Support for Hypervisor-secure Virtualization},
 volume = {47},
 year = {2012}
}


@article{Odaira:2012:COA:2248487.2150993,
 abstract = {Future microprocessors will have more serious memory wall problems since they will include more cores and threads in each chip. Similarly, future applications will have more serious memory bloat problems since they are more often written using object-oriented languages and reusable frameworks. To overcome such problems, the language runtime environments must accurately and efficiently profile how programs access objects. We propose Barrier Profiler, a low-overhead object access profiler using a memory-protection-based approach called pointer barrierization and adaptive overhead reduction techniques. Unlike previous memory-protection-based techniques, pointer barrierization offers per-object protection by converting all of the pointers to a given object to corresponding barrier pointers that point to protected pages. Barrier Profiler achieves low overhead by not causing signals at object accesses that are unrelated to the needed profiles, based on profile feedback and a compiler analysis. Our experimental results showed Barrier Profiler provided sufficiently accurate profiles with 1.3% on average and at most 3.4% performance overhead for allocation-intensive benchmarks, while previous code-instrumentation-based techniques suffered from 9.2% on average and at most 12.6% overhead. The low overhead allows Barrier Profiler to be run continuously on production systems. Using Barrier Profiler, we implemented two new online optimizations to compress write-only character arrays and to adjust the initial sizes of mostly non-accessed arrays. They resulted in speed-ups of up to 8.6% and 36%, respectively.},
 acmid = {2150993},
 address = {New York, NY, USA},
 author = {Odaira, Rei and Nakatani, Toshio},
 doi = {10.1145/2248487.2150993},
 issn = {0362-1340},
 issue_date = {April 2012},
 journal = {SIGPLAN Not.},
 keyword = {memory management, memory protection, profiling},
 link = {http://doi.acm.org/10.1145/2248487.2150993},
 month = {mar},
 number = {4},
 numpages = {12},
 pages = {147--158},
 publisher = {ACM},
 title = {Continuous Object Access Profiling and Optimizations to Overcome the Memory Wall and Bloat},
 volume = {47},
 year = {2012}
}


@article{Olszewski:2012:AAS:2189750.2150995,
 abstract = {Despite a burgeoning demand for parallel programs, the tools available to developers working on shared-memory multicore processors have lagged behind. One reason for this is the lack of hardware support for inspecting the complex behavior of these parallel programs. Inter-thread communication, which must be instrumented for many types of analyses, may occur with any memory operation. To detect such thread communication in software, many existing tools require the instrumentation of all memory operations, which leads to significant performance overheads. To reduce this overhead, some existing tools resort to random sampling of memory operations, which introduces false negatives. Unfortunately, neither of these approaches provide the speed and accuracy programmers have traditionally expected from their tools. In this work, we present Aikido, a new system and framework that enables the development of efficient and transparent analyses that operate on shared data. Aikido uses a hybrid of existing hardware features and dynamic binary rewriting to detect thread communication with low overhead. Aikido runs a custom hypervisor below the operating system, which exposes per-thread hardware protection mechanisms not available in any widely used operating system. This hybrid approach allows us to benefit from the low cost of detecting memory accesses with hardware, while maintaining the word-level accuracy of a software-only approach. To evaluate our framework, we have implemented an Aikido-enabled vector clock race detector. Our results show that the Aikido enabled race-detector outperforms existing techniques that provide similar accuracy by up to 6.0x, and 76% on average, on the PARSEC benchmark suite.},
 acmid = {2150995},
 address = {New York, NY, USA},
 author = {Olszewski, Marek and Zhao, Qin and Koh, David and Ansel, Jason and Amarasinghe, Saman},
 doi = {10.1145/2189750.2150995},
 issn = {0163-5964},
 issue_date = {March 2012},
 journal = {SIGARCH Comput. Archit. News},
 keyword = {data race detection, debugging, multicore},
 link = {http://doi.acm.org/10.1145/2189750.2150995},
 month = {mar},
 number = {1},
 numpages = {12},
 pages = {173--184},
 publisher = {ACM},
 title = {Aikido: Accelerating Shared Data Dynamic Analyses},
 volume = {40},
 year = {2012}
}


@proceedings{Harris:2012:2150976,
 abstract = {Welcome to the seventeenth edition of the ASPLOS conference series, and the first to be held outside of North America. We have assembled what we believe to be an outstanding technical program, and are delighted to be presenting it in the historic chambers of the Royal Society in the UK. The program continues and reinforces the ASPLOS tradition of publishing innovative multidisciplinary research spanning the boundaries of hardware, computer architecture, compilers, languages, operating systems, networking, and applications. We received 172 full paper submissions, which was 13% higher than last year's 152, and 5% lower than the record of 181 set in 2010. 14 submissions had program committee members as co-authors. Once again, the dominant technical theme was parallelism: 70% of both the submitted and accepted papers explicitly indicated a technical keyword specific to parallel processing. The single most popular keyword was "multicore systems," selected by 35% of both the submitted and accepted papers. Interestingly, not one of the 172 submissions selected "VLSI or process technology." There was a nice mixture of submissions across operating systems (34%), compilers and programming languages (28%), and core topics in computer architecture (37%), with 23% of submissions spanning more than one of these. (Most submissions had something to do with computer architecture; the characterization of "core areas" is somewhat arbitrary.) As in recent years, we employed two rounds of reviewing. An External Review Committee, preselected by the Program Chair, assisted in round one. All submissions received at least three round one reviews. Based on these, we eliminated 71 papers whose average overall score was below 3.5 (on a scale of 1-6), and whose maximum score was below 5 (i.e., no better than "weak accept"). The remaining 101 papers received two additional (PC-only) reviews in round two. This process allowed the program committee to focus the bulk of its attention on papers with a significant chance of being accepted. The typical member of the External Review Committee completed six reviews; the typical Program Committee member did 14. Almost all reviews were completed by early October, at which point authors had several days to compose a formal response, if they desired (most did). After this rebuttal period, PC members took the author comments into account as they discussed the papers online and, in some cases, updated their scores accordingly. The PC gathered in Rochester, NY, for a full-day meeting on October 20th, 2011. 74 papers were discussed at the meeting. PC authors stepped out of the room for discussion of their papers; those with conflicts of interest remained in the room, but did not participate in discussion. Papers on which the Program Chair had a conflict of interest (4 in number) had been reviewed in a separate process managed by Jim Larus, and were discussed when the Chair was not present. At the end of the day, 37 papers were accepted for presentation at the conference, for an acceptance rate of 21.5% --- a slightly larger number than last year, but almost the same percentage. Four of the accepts have PC members as co-authors. Nine papers were assigned a "shepherd" to make sure that final versions addressed issues (not necessarily weaknesses) of concern to the program committee. The logistics behind organizing this conference have been handled by the tireless effort of a team of volunteers: Luis Ceze (University of Washington), Alexandra Fedorova (Simon Fraser University), Steven Hand (University of Cambridge), Timothy Jones (University of Cambridge), Paul Kelly (Imperial College), Mikel Luj√°n (University of Manchester), Robert Mullins (University of Cambridge), Onur Mutlu (Carnegie Mellon University), and Derek Murray (Microsoft Research). Each member of this team played an important role in making ASPLOS a success. The ASPLOS Steering Committee gave us valuable assistance over the last year, and Todd Mowry (CMU) was particularly generous with his advice from ASPLOS 2010 and ASPLOS 2011. In addition, Jean Bacon (University of Cambridge), Andrew Herbert, Graham Hutton (University of Nottingham), and Joe Sventek (University of Glasgow) provided advice on organizing a conference in the UK. As always, the support and guidance of the ACM staff was very valuable.},
 address = {New York, NY, USA},
 isbn = {978-1-4503-0759-8},
 location = {London, England, UK},
 note = {415125},
 publisher = {ACM},
 title = {ASPLOS XVII: Proceedings of the Seventeenth International Conference on Architectural Support for Programming Languages and Operating Systems},
 year = {2012}
}


@article{Jaleel:2012:CCR:2189750.2151003,
 abstract = {When several applications are co-scheduled to run on a system with multiple shared LLCs, there is opportunity to improve system performance. This opportunity can be exploited by the hardware, software, or a combination of both hardware and software. The software, i.e., an operating system or hypervisor, can improve system performance by co-scheduling jobs on LLCs to minimize shared cache contention. The hardware can improve system throughput through better replacement policies by allocating more cache resources to applications that benefit from the cache and less to those applications that do not. This study presents a detailed analysis on the interactions between intelligent scheduling and smart cache replacement policies. We find that smart cache replacement reduces the burden on software to provide intelligent scheduling decisions. However, under smart cache replacement, there is still room to improve performance from better application co-scheduling. We find that co-scheduling decisions are a function of the underlying LLC replacement policy. We propose Cache Replacement and Utility-aware Scheduling (CRUISE)-a hardware/software co-designed approach for shared cache management. For 4-core and 8-core CMPs, we find that CRUISE approaches the performance of an ideal job co-scheduling policy under different LLC replacement policies.},
 acmid = {2151003},
 address = {New York, NY, USA},
 author = {Jaleel, Aamer and Najaf-abadi, Hashem H. and Subramaniam, Samantika and Steely, Simon C. and Emer, Joel},
 doi = {10.1145/2189750.2151003},
 issn = {0163-5964},
 issue_date = {March 2012},
 journal = {SIGARCH Comput. Archit. News},
 keyword = {cache replacement, scheduling, shared cache},
 link = {http://doi.acm.org/10.1145/2189750.2151003},
 month = {mar},
 number = {1},
 numpages = {12},
 pages = {249--260},
 publisher = {ACM},
 title = {CRUISE: Cache Replacement and Utility-aware Scheduling},
 volume = {40},
 year = {2012}
}


@inproceedings{Joao:2012:BIS:2150976.2151001,
 abstract = {Performance of multithreaded applications is limited by a variety of bottlenecks, e.g. critical sections, barriers and slow pipeline stages. These bottlenecks serialize execution, waste valuable execution cycles, and limit scalability of applications. This paper proposes Bottleneck Identification and Scheduling in Multithreaded Applications (BIS), a cooperative software-hardware mechanism to identify and accelerate the most critical bottlenecks. BIS identifies which bottlenecks are likely to reduce performance by measuring the number of cycles threads have to wait for each bottleneck, and accelerates those bottlenecks using one or more fast cores on an Asymmetric Chip Multi-Processor (ACMP). Unlike previous work that targets specific bottlenecks, BIS can identify and accelerate bottlenecks regardless of their type. We compare BIS to four previous approaches and show that it outperforms the best of them by 15% on average. BIS' performance improvement increases as the number of cores and the number of fast cores in the system increase.},
 acmid = {2151001},
 address = {New York, NY, USA},
 author = {Joao, Jos{\'e} A. and Suleman, M. Aater and Mutlu, Onur and Patt, Yale N.},
 booktitle = {Proceedings of the Seventeenth International Conference on Architectural Support for Programming Languages and Operating Systems},
 doi = {10.1145/2150976.2151001},
 isbn = {978-1-4503-0759-8},
 keyword = {asymmetric CMPs, barriers, critical sections, heterogeneous CMPs, multicore, pipeline parallelism},
 link = {http://doi.acm.org/10.1145/2150976.2151001},
 location = {London, England, UK},
 numpages = {12},
 pages = {223--234},
 publisher = {ACM},
 series = {ASPLOS XVII},
 title = {Bottleneck Identification and Scheduling in Multithreaded Applications},
 year = {2012}
}


@inproceedings{Clements:2012:SAS:2150976.2150998,
 abstract = {Software developers commonly exploit multicore processors by building multithreaded software in which all threads of an application share a single address space. This shared address space has a cost: kernel virtual memory operations such as handling soft page faults, growing the address space, mapping files, etc. can limit the scalability of these applications. In widely-used operating systems, all of these operations are synchronized by a single per-process lock. This paper contributes a new design for increasing the concurrency of kernel operations on a shared address space by exploiting read-copy-update (RCU) so that soft page faults can both run in parallel with operations that mutate the same address space and avoid contending with other page faults on shared cache lines. To enable such parallelism, this paper also introduces an RCU-based binary balanced tree for storing memory mappings. An experimental evaluation using three multithreaded applications shows performance improvements on 80 cores ranging from 1.7x to 3.4x for an implementation of this design in the Linux 2.6.37 kernel. The RCU-based binary tree enables soft page faults to run at a constant cost with an increasing number of cores,suggesting that the design will scale well beyond 80 cores.},
 acmid = {2150998},
 address = {New York, NY, USA},
 author = {Clements, Austin T. and Kaashoek, M. Frans and Zeldovich, Nickolai},
 booktitle = {Proceedings of the Seventeenth International Conference on Architectural Support for Programming Languages and Operating Systems},
 doi = {10.1145/2150976.2150998},
 isbn = {978-1-4503-0759-8},
 keyword = {RCU, concurrent balanced trees, lock-free algorithms, multicore, scalability, virtual memory},
 link = {http://doi.acm.org/10.1145/2150976.2150998},
 location = {London, England, UK},
 numpages = {12},
 pages = {199--210},
 publisher = {ACM},
 series = {ASPLOS XVII},
 title = {Scalable Address Spaces Using RCU Balanced Trees},
 year = {2012}
}


@article{Ferdman:2012:CCS:2248487.2150982,
 abstract = {Emerging scale-out workloads require extensive amounts of computational resources. However, data centers using modern server hardware face physical constraints in space and power, limiting further expansion and calling for improvements in the computational density per server and in the per-operation energy. Continuing to improve the computational resources of the cloud while staying within physical constraints mandates optimizing server efficiency to ensure that server hardware closely matches the needs of scale-out workloads. In this work, we introduce CloudSuite, a benchmark suite of emerging scale-out workloads. We use performance counters on modern servers to study scale-out workloads, finding that today's predominant processor micro-architecture is inefficient for running these workloads. We find that inefficiency comes from the mismatch between the workload needs and modern processors, particularly in the organization of instruction and data memory systems and the processor core micro-architecture. Moreover, while today's predominant micro-architecture is inefficient when executing scale-out workloads, we find that continuing the current trends will further exacerbate the inefficiency in the future. In this work, we identify the key micro-architectural needs of scale-out workloads, calling for a change in the trajectory of server processors that would lead to improved computational density and power efficiency in data centers.},
 acmid = {2150982},
 address = {New York, NY, USA},
 author = {Ferdman, Michael and Adileh, Almutaz and Kocberber, Onur and Volos, Stavros and Alisafaee, Mohammad and Jevdjic, Djordje and Kaynak, Cansu and Popescu, Adrian Daniel and Ailamaki, Anastasia and Falsafi, Babak},
 doi = {10.1145/2248487.2150982},
 issn = {0362-1340},
 issue_date = {April 2012},
 journal = {SIGPLAN Not.},
 keyword = {architectural evaluation, cloud computing, design insights, workload characterization},
 link = {http://doi.acm.org/10.1145/2248487.2150982},
 month = {mar},
 number = {4},
 numpages = {12},
 pages = {37--48},
 publisher = {ACM},
 title = {Clearing the Clouds: A Study of Emerging Scale-out Workloads on Modern Hardware},
 volume = {47},
 year = {2012}
}


@article{DeVuyst:2012:EMH:2248487.2151004,
 abstract = {Prior research has shown that single-ISA heterogeneous chip multiprocessors have the potential for greater performance and energy efficiency than homogeneous CMPs. However, restricting the cores to a single ISA removes an important opportunity for greater heterogeneity. To take full advantage of a heterogeneous-ISA CMP, however, we must be able to migrate execution among heterogeneous cores in order to adapt to program phase changes and changing external conditions (e.g., system power state). This paper explores migration on heterogeneous-ISA CMPs. This is non-trivial because program state is kept in an architecture-specific form; therefore, state transformation is necessary for migration. To keep migration cost low, the amount of state that requires transformation must be minimized. This work identifies large portions of program state whose form is not critical for performance; the compiler is modified to produce programs that keep most of their state in an architecture-neutral form so that only a small number of data items must be repositioned and no pointers need to be changed. The result is low migration cost with minimal sacrifice of non-migration performance. Additionally, this work leverages binary translation to enable instantaneous migration. When migration is requested, the program is immediately migrated to a different core where binary translation runs for a short time until a function call is reached, at which point program state is transformed and execution continues natively on the new core. This system can tolerate migrations as often as every 100 ms and still retain 95% of the performance of a system that does not do, or support, migration.},
 acmid = {2151004},
 address = {New York, NY, USA},
 author = {DeVuyst, Matthew and Venkat, Ashish and Tullsen, Dean M.},
 doi = {10.1145/2248487.2151004},
 issn = {0362-1340},
 issue_date = {April 2012},
 journal = {SIGPLAN Not.},
 keyword = {heterogeneous CMP, thread migration},
 link = {http://doi.acm.org/10.1145/2248487.2151004},
 month = {mar},
 number = {4},
 numpages = {12},
 pages = {261--272},
 publisher = {ACM},
 title = {Execution Migration in a heterogeneous-ISA Chip Multiprocessor},
 volume = {47},
 year = {2012}
}


@article{Vasic:2012:DAR:2189750.2151021,
 abstract = {Effective resource management of virtualized environments is a challenging task. State-of-the-art management systems either rely on analytical models or evaluate resource allocations by running actual experiments. However, both approaches incur a significant overhead once the workload changes. The former needs to re-calibrate and re-validate models, whereas the latter has to run a new set of experiments to select a new resource allocation. During the adaptation period, the system may run with an inefficient configuration. In this paper, we propose DejaVu - a framework that (1) minimizes the resource management overhead by identifying a small set of workload classes for which it needs to evaluate resource allocation decisions, (2) quickly adapts to workload changes by classifying workloads using signatures and caching their preferred resource allocations at runtime, and (3) deals with interference by estimating an "interference index". We evaluate DejaVu by running representative network services on Amazon EC2. DejaVu achieves more than 10x speedup in adaptation time for each workload change relative to the state-of-the-art. By enabling quick adaptation, DejaVu saves up to 60% of the service provisioning cost. Finally, DejaVu is easily deployable as it does not require any extensive instrumentation or human intervention.},
 acmid = {2151021},
 address = {New York, NY, USA},
 author = {Vasi\'{c}, Nedeljko and Novakovi\'{c}, Dejan and Miu\v{c}in, Svetozar and Kosti\'{c}, Dejan and Bianchini, Ricardo},
 doi = {10.1145/2189750.2151021},
 issn = {0163-5964},
 issue_date = {March 2012},
 journal = {SIGARCH Comput. Archit. News},
 keyword = {data center, resource management, virtualization},
 link = {http://doi.acm.org/10.1145/2189750.2151021},
 month = {mar},
 number = {1},
 numpages = {14},
 pages = {423--436},
 publisher = {ACM},
 title = {DejaVu: Accelerating Resource Allocation in Virtualized Environments},
 volume = {40},
 year = {2012}
}


@inproceedings{Simha:2012:USS:2150976.2151016,
 abstract = {Traditional storage systems provide a simple read/write interface, which is inadequate for low-locality update-intensive workloads because it limits the disk scheduling flexibility and results in inefficient use of buffer memory and raw disk bandwidth. This paper describes an update-aware disk access interface that allows applications to explicitly specify disk update requests and associate with such requests call-back functions that will be invoked when the requested disk blocks are brought into memory. Because call-back functions offer a continuation mechanism after retrieval of requested blocks, storage systems supporting this interface are given more flexibility in scheduling pending disk update requests. In particular, this interface enables a simple but effective technique called Batching mOdifications with Sequential Commit (BOSC), which greatly improves the sustained throughput of a storage system under low-locality update-intensive workloads. In addition, together with a space-efficient low-latency disk logging technique, BOSC is able to deliver the same durability guarantee as synchronous disk updates. Empirical measurements show that the random update throughput of a BOSC-based B+ tree is more than an order of magnitude higher than that of the same B+ tree implementation on a traditional storage system.},
 acmid = {2151016},
 address = {New York, NY, USA},
 author = {Simha, Dilip Nijagal and Lu, Maohua and Chiueh, Tzi-cker},
 booktitle = {Proceedings of the Seventeenth International Conference on Architectural Support for Programming Languages and Operating Systems},
 doi = {10.1145/2150976.2151016},
 isbn = {978-1-4503-0759-8},
 keyword = {B+ trees, BOSC, buffered writes, fast logging, hard disks, low-locality, storage, update interface, update-intensive},
 link = {http://doi.acm.org/10.1145/2150976.2151016},
 location = {London, England, UK},
 numpages = {12},
 pages = {375--386},
 publisher = {ACM},
 series = {ASPLOS XVII},
 title = {An Update-aware Storage System for Low-locality Update-intensive Workloads},
 year = {2012}
}


@article{Hwang:2012:CRD:2248487.2150989,
 abstract = {Main memory is one of the leading hardware causes for machine crashes in today's datacenters. Designing, evaluating and modeling systems that are resilient against memory errors requires a good understanding of the underlying characteristics of errors in DRAM in the field. While there have recently been a few first studies on DRAM errors in production systems, these have been too limited in either the size of the data set or the granularity of the data to conclusively answer many of the open questions on DRAM errors. Such questions include, for example, the prevalence of soft errors compared to hard errors, or the analysis of typical patterns of hard errors. In this paper, we study data on DRAM errors collected on a diverse range of production systems in total covering nearly 300 terabyte-years of main memory. As a first contribution, we provide a detailed analytical study of DRAM error characteristics, including both hard and soft errors. We find that a large fraction of DRAM errors in the field can be attributed to hard errors and we provide a detailed analytical study of their characteristics. As a second contribution, the paper uses the results from the measurement study to identify a number of promising directions for designing more resilient systems and evaluates the potential of different protection mechanisms in the light of realistic error patterns. One of our findings is that simple page retirement policies might be able to mask a large number of DRAM errors in production systems, while sacrificing only a negligible fraction of the total DRAM in the system.},
 acmid = {2150989},
 address = {New York, NY, USA},
 author = {Hwang, Andy A. and Stefanovici, Ioan A. and Schroeder, Bianca},
 doi = {10.1145/2248487.2150989},
 issn = {0362-1340},
 issue_date = {April 2012},
 journal = {SIGPLAN Not.},
 keyword = {DRAM errors, correctable errors, field study, reliability, uncorrectable errors},
 link = {http://doi.acm.org/10.1145/2248487.2150989},
 month = {mar},
 number = {4},
 numpages = {12},
 pages = {111--122},
 publisher = {ACM},
 title = {Cosmic Rays Don'T Strike Twice: Understanding the Nature of DRAM Errors and the Implications for System Design},
 volume = {47},
 year = {2012}
}


@article{Cheriton:2012:HAS:2248487.2151007,
 abstract = {Programming language and operating system support for efficient concurrency-safe access to shared data is a key concern for the effective use of multi-core processors. Most research has focused on the software model of multiple threads accessing this data within a single shared address space. However, many real applications are actually structured as multiple separate processes for fault isolation and simplified synchronization. In this paper, we describe the HICAMP architecture and its innovative memory system, which supports efficient concurrency safe access to structured shared data without incurring the overhead of inter-process communication. The HICAMP architecture also provides support for programming language and OS structures such as threads, iterators, read-only access and atomic update. In addition to demonstrating that HICAMP is beneficial for multi-process structured applications, our evaluation shows that the same mechanisms provide substantial benefits for other areas, including sparse matrix computations and virtualization.},
 acmid = {2151007},
 address = {New York, NY, USA},
 author = {Cheriton, David and Firoozshahian, Amin and Solomatnikov, Alex and Stevenson, John P. and Azizi, Omid},
 doi = {10.1145/2248487.2151007},
 issn = {0362-1340},
 issue_date = {April 2012},
 journal = {SIGPLAN Not.},
 keyword = {fault isolation, iterator register, memory deduplication, memory model, memory sharing, parallel architecture, parallel programming, snapshot semantics},
 link = {http://doi.acm.org/10.1145/2248487.2151007},
 month = {mar},
 number = {4},
 numpages = {14},
 pages = {287--300},
 publisher = {ACM},
 title = {HICAMP: Architectural Support for Efficient Concurrency-safe Shared Structured Data Access},
 volume = {47},
 year = {2012}
}


@inproceedings{Kasikci:2012:DRV:2150976.2150997,
 abstract = {Even though most data races are harmless, the harmful ones are at the heart of some of the worst concurrency bugs. Alas, spotting just the harmful data races in programs is like finding a needle in a haystack: 76%-90% of the true data races reported by state-of-the-art race detectors turn out to be harmless [45]. We present Portend, a tool that not only detects races but also automatically classifies them based on their potential consequences: Could they lead to crashes or hangs? Could their effects be visible outside the program? Are they harmless? Our proposed technique achieves high accuracy by efficiently analyzing multiple paths and multiple thread schedules in combination, and by performing symbolic comparison between program outputs. We ran Portend on 7 real-world applications: it detected 93 true data races and correctly classified 92 of them, with no human effort. 6 of them are harmful races. Portend's classification accuracy is up to 88% higher than that of existing tools, and it produces easy-to-understand evidence of the consequences of harmful races, thus both proving their harmfulness and making debugging easier. We envision Portend being used for testing and debugging, as well as for automatically triaging bug reports.},
 acmid = {2150997},
 address = {New York, NY, USA},
 author = {Kasikci, Baris and Zamfir, Cristian and Candea, George},
 booktitle = {Proceedings of the Seventeenth International Conference on Architectural Support for Programming Languages and Operating Systems},
 doi = {10.1145/2150976.2150997},
 isbn = {978-1-4503-0759-8},
 keyword = {concurrency, data races, testing, triage},
 link = {http://doi.acm.org/10.1145/2150976.2150997},
 location = {London, England, UK},
 numpages = {14},
 pages = {185--198},
 publisher = {ACM},
 series = {ASPLOS XVII},
 title = {Data Races vs. Data Race Bugs: Telling the Difference with Portend},
 year = {2012}
}


@proceedings{Sarkar:2013:2451116,
 abstract = {It is our great pleasure to welcome you to the Eighteenth International Conference on Architectural Support for Programming Languages and Operating Systems --- ASPLOS 2013! This year's conference continues and reinforces the ASPLOS tradition of publishing innovative multidisciplinary research spanning the boundaries of hardware, computer architecture, compilers, languages, operating systems, networking, and applications. This year, authors submitted 193 papers, 12% more than last year when 172 papers were submitted, and 7% more than in 2010, the previous record year, when 180 papers were submitted. PC members co-authored 22 submissions. The following ten topic keywords were marked by at least 20 submissions: compilation, parallelization and optimization (40 submissions), programming models (32), OS scheduling and resource management (31), OS abstractions (29), testing and debugging (27), power/energy/thermal management (27), memory optimizations (25), heterogeneous architectures (24), parallel programming languages (23), and virtualization (21). The 2013 ASPLOS Program Committee (PC) consisted of 35 members; each member was assigned 18 submissions, a workload of four more papers than the previous year. As in previous years, the Program Chair recruited an External Review Committee (ERC) whose expertise complemented that of the PC. The 2013 ERC consisted of 63 members, assigned 5 papers each. Additionally, 90 sub reviewers prepared 143 reviews. In total, we received 943 reviews, an average of 4.89 reviews per paper. 90% of submissions received at least five reviews, and all received at least four. As in previous years, we employed two rounds of reviews. A significant difference this year was that no papers were rejected after the first round. The two phases were employed merely to spread out the review workload and to allocate reviewer expertise based on first-round reviews. After a three-day author response period, the PC and ERC members entered online discussion lasting about a week. The online discussion, moderated by paper discussion leaders assigned to each paper, was unusually lively. The discussion resulted in 71 papers being selected for deliberation at the physical PC meeting. During the entire review and selection process, scores alone were never used to reject a submission. The PC gathered in Berkeley, CA, on Nov. 1, 2012, for a one-day meeting. Three committee members were unable to attend, mostly due to travel disruptions from the Hurricane Sandy. These members participated in online discussions prior to the meeting. The meeting followed the protocol of allocating a fixed amount of time for the discussion of each paper. If a consensus was not reached, the paper advanced to a discussion phase, which had the form of small groups composed of paper reviewers and other relevant PC members. In the late afternoon, these groups reported back to the PC with arguments for or against accepting the paper. PC and ERC members volunteered to shepherd 7 papers to ensure reviewer comments were properly addressed. The PC-authored submissions were not discussed at the meeting (see below). PC members with conflicts left the room during discussions, as is customary. Vivek Sarkar, the General Chair, attended the PC meeting as an observer and handled conflicts involving the Program Chair. In the end, the PC and ERC accepted 44 papers, 7 more than the previous year. The acceptance rate is 22.8%, compared to 21.5% the previous year. The 22 PC-authored submissions were reviewed only by the ERC. The ERC discussed and accepted these submissions in an online discussion, conducted mostly prior to the physical PC meeting. The ERC held the PC submissions to the same conference standard as other papers. Preliminary acceptance decisions, done by the ERC before the PC meeting, were calibrated against the papers accepted at the PC meeting with the help of the Program and General Chairs. In the end, 5 PC submissions were accepted. The acceptance rate was the same as for non-PC submissions.},
 address = {New York, NY, USA},
 isbn = {978-1-4503-1870-9},
 location = {Houston, Texas, USA},
 publisher = {ACM},
 title = {ASPLOS '13: Proceedings of the Eighteenth International Conference on Architectural Support for Programming Languages and Operating Systems},
 year = {2013}
}


@article{Lee:2012:RSE:2248487.2151023,
 abstract = {The performance of modern many-core platforms strongly depends on the effectiveness of using their complex cache and memory structures. This indicates the need for a memory-centric approach to platform scheduling, in which it is the locations of memory blocks in caches rather than CPU idleness that determines where application processes are run. Using the term 'memory region' to denote the current set of physical memory pages actively used by an application, this paper presents and evaluates region-based scheduling methods for multicore platforms. This involves (i) continuously and at runtime identifying the memory regions used by executable entities, and their sizes, (ii) mapping these regions to caches to match performance goals, and (iii) maintaining region to cache mappings by ensuring that entities run on processors with direct access to the caches containing their regions. Region scheduling can implement policies that (i) offer improved performance to applications by 'unifying' the multiple caches present on the underlying physical machine and/or by 'balancing' cache usage to take maximum advantage of available cache space, (ii) better isolate applications from each other, particularly when their performance is strongly affected by cache availability, and also (iii) take advantage of standard scheduling and CPU-based load balancing when regioning is ineffective. The paper describes region scheduling and its system-level implementation and evaluates its performance with micro-benchmarks and representative multi-core applications. Single applications see performance improvements of up to 15% with region scheduling, and we observe 40% latency improvements when a platform is shared by multiple applications. Superior isolation is shown to be particularly important for cache-sensitive or real-time codes.},
 acmid = {2151023},
 address = {New York, NY, USA},
 author = {Lee, Min and Schwan, Karsten},
 doi = {10.1145/2248487.2151023},
 issn = {0362-1340},
 issue_date = {April 2012},
 journal = {SIGPLAN Not.},
 keyword = {cache, memory, region, server consolidation, virtualization, xen},
 link = {http://doi.acm.org/10.1145/2248487.2151023},
 month = {mar},
 number = {4},
 numpages = {12},
 pages = {451--462},
 publisher = {ACM},
 title = {Region Scheduling: Efficiently Using the Cache Architectures via Page-level Affinity},
 volume = {47},
 year = {2012}
}


@article{Volos:2012:ATM:2248487.2150999,
 abstract = {Multithreaded programs often suffer from synchronization bugs such as atomicity violations and deadlocks. These bugs arise from complicated locking strategies and ad hoc synchronization methods to avoid the use of locks. A survey of the bug databases of major open-source applications shows that concurrency bugs often take multiple fix attempts, and that fixes often introduce yet more concurrency bugs. Transactional memory (TM) enables programmers to declare regions of code atomic without specifying a lock and has the potential to avoid these bugs. Where most previous studies have focused on using TM to write new programs from scratch, we consider its utility in fixing existing programs with concurrency bugs. We therefore investigate four methods of using TM on three concurrent programs. Overall, we find that 29% of the bugs are not fixable by transactional memory, showing that TM does not address many important types of concurrency bugs. In particular, TM works poorly with extremely long critical sections and with deadlocks involving both condition variables and I/O. Conversely, we find that for 56% of the bugs, transactional memory offers demonstrable value by simplifying the reasoning behind a fix or the effort to implement a fix, and using transactions in the first place would have avoided 71% of the bugs examined. We also find that ad hoc synchronization put in place to avoid the overhead of locking can be greatly simplified with TM, but requires hardware support to perform well.},
 acmid = {2150999},
 address = {New York, NY, USA},
 author = {Volos, Haris and Tack, Andres Jaan and Swift, Michael M. and Lu, Shan},
 doi = {10.1145/2248487.2150999},
 issn = {0362-1340},
 issue_date = {April 2012},
 journal = {SIGPLAN Not.},
 keyword = {atomicity violation, concurrency bug, concurrent program, deadlock, debugging, transactional memory},
 link = {http://doi.acm.org/10.1145/2248487.2150999},
 month = {mar},
 number = {4},
 numpages = {12},
 pages = {211--222},
 publisher = {ACM},
 title = {Applying Transactional Memory to Concurrency Bugs},
 volume = {47},
 year = {2012}
}


@article{Esmaeilzadeh:2012:ASD:2248487.2151008,
 abstract = {Disciplined approximate programming lets programmers declare which parts of a program can be computed approximately and consequently at a lower energy cost. The compiler proves statically that all approximate computation is properly isolated from precise computation. The hardware is then free to selectively apply approximate storage and approximate computation with no need to perform dynamic correctness checks. In this paper, we propose an efficient mapping of disciplined approximate programming onto hardware. We describe an ISA extension that provides approximate operations and storage, which give the hardware freedom to save energy at the cost of accuracy. We then propose Truffle, a microarchitecture design that efficiently supports the ISA extensions. The basis of our design is dual-voltage operation, with a high voltage for precise operations and a low voltage for approximate operations. The key aspect of the microarchitecture is its dependence on the instruction stream to determine when to use the low voltage. We evaluate the power savings potential of in-order and out-of-order Truffle configurations and explore the resulting quality of service degradation. We evaluate several applications and demonstrate energy savings up to 43%.},
 acmid = {2151008},
 address = {New York, NY, USA},
 author = {Esmaeilzadeh, Hadi and Sampson, Adrian and Ceze, Luis and Burger, Doug},
 doi = {10.1145/2248487.2151008},
 issn = {0362-1340},
 issue_date = {April 2012},
 journal = {SIGPLAN Not.},
 keyword = {architecture, disciplined approximate computation, energy, power-aware computing},
 link = {http://doi.acm.org/10.1145/2248487.2151008},
 month = {mar},
 number = {4},
 numpages = {12},
 pages = {301--312},
 publisher = {ACM},
 title = {Architecture Support for Disciplined Approximate Programming},
 volume = {47},
 year = {2012}
}


@inproceedings{Chen:2012:IOD:2150976.2150983,
 abstract = {Iterative optimization is a simple but powerful approach that searches for the best possible combination of compiler optimizations for a given workload. However, each program, if not each data set, potentially favors a different combination. As a result, iterative optimization is plagued by several practical issues that prevent it from being widely used in practice: a large number of runs are required for finding the best combination; the process can be data set dependent; and the exploration process incurs significant overhead that needs to be compensated for by performance benefits.Therefore, while iterative optimization has been shown to have significant performance potential, it is seldomly used in production compilers. In this paper, we propose Iterative Optimization for the Data Center (IODC): we show that servers and data centers offer a context in which all of the above hurdles can be overcome. The basic idea is to spawn different combinations across workers and recollect performance statistics at the master, which then evolves to the optimum combination of compiler optimizations. IODC carefully manages costs and benefits, and is transparent to the end user. We evaluate IODC using both MapReduce and throughput compute-intensive server applications. In order to reflect the large number of users interacting with the system, we gather a very large collection of data sets (at least 1000 and up to several million unique data sets per program), for a total storage of 10.7TB, and 568 days of CPU time. We report an average performance improvement of 1.48√ó, and up to 2.08√ó, for the MapReduce applications, and 1.14√ó, and up to 1.39√ó, for the throughput compute-intensive server applications.},
 acmid = {2150983},
 address = {New York, NY, USA},
 author = {Chen, Yang and Fang, Shuangde and Eeckhout, Lieven and Temam, Olivier and Wu, Chengyong},
 booktitle = {Proceedings of the Seventeenth International Conference on Architectural Support for Programming Languages and Operating Systems},
 doi = {10.1145/2150976.2150983},
 isbn = {978-1-4503-0759-8},
 keyword = {compiler, data center, iterative optimization, mapreduce, server},
 link = {http://doi.acm.org/10.1145/2150976.2150983},
 location = {London, England, UK},
 numpages = {12},
 pages = {49--60},
 publisher = {ACM},
 series = {ASPLOS XVII},
 title = {Iterative Optimization for the Data Center},
 year = {2012}
}


@article{Park:2012:SDE:2189750.2151014,
 abstract = {Single-instruction multiple-data (SIMD) accelerators provide an energy-efficient platform to scale the performance of mobile systems while still retaining post-programmability. The central challenge is translating the parallel resources of the SIMD hardware into real application performance. In scientific applications, automatic vectorization techniques have proven quite effective at extracting large levels of data-level parallelism (DLP). However, vectorization is often much less effective for media applications due to low trip count loops, complex control flow, and non-uniform execution behavior. As a result, SIMD lanes remain idle due to insufficient DLP. To attack this problem, this paper proposes a new vectorization pass called SIMD Defragmenter to uncover hidden DLP that lurks below the surface in the form of instruction-level parallelism (ILP). The difficulty is managing the data packing/unpacking overhead that can easily exceed the benefits gained through SIMD execution. The SIMD degragmenter overcomes this problem by identifying groups of compatible instructions (subgraphs) that can be executed in parallel across the SIMD lanes. By SIMDizing in bulk at the subgraph level, packing/unpacking overhead is minimized. On a 16-lane SIMD processor, experimental results show that SIMD defragmentation achieves a mean 1.6x speedup over traditional loop vectorization and a 31% gain over prior research approaches for converting ILP to DLP.},
 acmid = {2151014},
 address = {New York, NY, USA},
 author = {Park, Yongjun and Seo, Sangwon and Park, Hyunchul and Cho, Hyoun Kyu and Mahlke, Scott},
 doi = {10.1145/2189750.2151014},
 issn = {0163-5964},
 issue_date = {March 2012},
 journal = {SIGARCH Comput. Archit. News},
 keyword = {SIMD architecture, compiler, optimization},
 link = {http://doi.acm.org/10.1145/2189750.2151014},
 month = {mar},
 number = {1},
 numpages = {12},
 pages = {363--374},
 publisher = {ACM},
 title = {SIMD Defragmenter: Efficient ILP Realization on Data-parallel Architectures},
 volume = {40},
 year = {2012}
}


@article{Gordon:2012:EBP:2248487.2151020,
 abstract = {Direct device assignment enhances the performance of guest virtual machines by allowing them to communicate with I/O devices without host involvement. But even with device assignment, guests are still unable to approach bare-metal performance, because the host intercepts all interrupts, including those interrupts generated by assigned devices to signal to guests the completion of their I/O requests. The host involvement induces multiple unwarranted guest/host context switches, which significantly hamper the performance of I/O intensive workloads. To solve this problem, we present ELI (ExitLess Interrupts), a software-only approach for handling interrupts within guest virtual machines directly and securely. By removing the host from the interrupt handling path, ELI manages to improve the throughput and latency of unmodified, untrusted guests by 1.3x-1.6x, allowing them to reach 97%-100% of bare-metal performance even for the most demanding I/O-intensive workloads.},
 acmid = {2151020},
 address = {New York, NY, USA},
 author = {Gordon, Abel and Amit, Nadav and Har'El, Nadav and Ben-Yehuda, Muli and Landau, Alex and Schuster, Assaf and Tsafrir, Dan},
 doi = {10.1145/2248487.2151020},
 issn = {0362-1340},
 issue_date = {April 2012},
 journal = {SIGPLAN Not.},
 keyword = {I/O performance, I/O virtualization, SR-IOV, device assignment, interrupts},
 link = {http://doi.acm.org/10.1145/2248487.2151020},
 month = {mar},
 number = {4},
 numpages = {12},
 pages = {411--422},
 publisher = {ACM},
 title = {ELI: Bare-metal Performance for I/O Virtualization},
 volume = {47},
 year = {2012}
}


@inproceedings{Lymberopoulos:2012:PIW:2150976.2150978,
 abstract = {The high network latencies and limited battery life of mobile phones can make mobile web browsing a frustrating experience. In prior work, we proposed trading memory capacity for lower web access latency and a more convenient data transfer schedule from an energy perspective by prefetching slowly-changing data (search queries and results) nightly, when the phone is charging. However, most web content is intrinsically much more dynamic and may be updated multiple times a day, thus eliminating the effectiveness of periodic updates. This paper addresses the challenge of prefetching dynamic web content in a timely fashion, giving the user an instant web browsing experience but without aggravating the battery lifetime issue. We start by analyzing the web access traces of 8,000 users, and observe that mobile web browsing exhibits a strong spatiotemporal signature, which is different for every user. We propose to use a machine learning approach based on stochastic gradient boosting techniques to efficiently model this signature on a per user basis. The machine learning model is capable of accurately predicting future web accesses and prefetching the content in a timely manner. Our experimental evaluation with 48,000 models trained on real user datasets shows that we can accurately prefetch 60% of the URLs for about 80-90% of the users within 2 minutes before the request. The system prototype we built not only provides more than 80% lower web access time for more than 80% of the users, but it also achieves the same or lower radio energy dissipation by more than 50% for the majority of mobile users.},
 acmid = {2150978},
 address = {New York, NY, USA},
 author = {Lymberopoulos, Dimitrios and Riva, Oriana and Strauss, Karin and Mittal, Akshay and Ntoulas, Alexandros},
 booktitle = {Proceedings of the Seventeenth International Conference on Architectural Support for Programming Languages and Operating Systems},
 doi = {10.1145/2150976.2150978},
 isbn = {978-1-4503-0759-8},
 keyword = {mobile browsing, timely prefetching},
 link = {http://doi.acm.org/10.1145/2150976.2150978},
 location = {London, England, UK},
 numpages = {12},
 pages = {1--12},
 publisher = {ACM},
 series = {ASPLOS XVII},
 title = {PocketWeb: Instant Web Browsing for Mobile Devices},
 year = {2012}
}


@article{Lin:2012:RUL:2248487.2150979,
 abstract = {To accomplish frequent, simple tasks with high efficiency, it is necessary to leverage low-power, microcontroller-like processors that are increasingly available on mobile systems. However, existing solutions require developers to directly program the low-power processors and carefully manage inter-processor communication. We present Reflex, a suite of compiler and runtime techniques that significantly lower the barrier for developers to leverage such low-power processors. The heart of Reflex is a software Distributed Shared Memory (DSM) that enables shared memory objects with release consistency among code running on loosely coupled processors. In order to achieve high energy efficiency without sacrificing performance much, the Reflex DSM leverages (i) extreme architectural asymmetry between low-power processors and powerful central processors, (ii) aggressive compile-time optimization, and (iii) a minimalist runtime that supports efficient message passing and event-driven execution. We report a complete realization of Reflex that runs on a TI OMAP4430-based development platform as well as on a custom tri-processor mobile platform. Using smartphone sensing applications reported in recent literature, we show that Reflex supports a programming style very close to contemporary smartphone programming. Compared to message passing, the Reflex DSM greatly reduces efforts in programming heterogeneous smartphones, eliminating up to 38% of the source lines of application code. Compared to running the same applications on existing smartphones, Reflex reduces the average system power consumption by up to 81%.},
 acmid = {2150979},
 address = {New York, NY, USA},
 author = {Lin, Felix Xiaozhu and Wang, Zhen and LiKamWa, Robert and Zhong, Lin},
 doi = {10.1145/2248487.2150979},
 issn = {0362-1340},
 issue_date = {April 2012},
 journal = {SIGPLAN Not.},
 keyword = {distributed shared memory, energy-efficiency, heterogeneous systems, mobile systems},
 link = {http://doi.acm.org/10.1145/2248487.2150979},
 month = {mar},
 number = {4},
 numpages = {12},
 pages = {13--24},
 publisher = {ACM},
 title = {Reflex: Using Low-power Processors in Smartphones Without Knowing Them},
 volume = {47},
 year = {2012}
}


@article{Martignoni:2012:PLH:2248487.2151012,
 abstract = {Processor emulators are widely used to provide isolation and instrumentation of binary software. However they have proved difficult to implement correctly: processor specifications have many corner cases that are not exercised by common workloads. It is untenable to base other system security properties on the correctness of emulators that have received only ad-hoc testing. To obtain emulators that are worthy of the required trust, we propose a technique to explore a high-fidelity emulator with symbolic execution, and then lift those test cases to test a lower-fidelity emulator. The high-fidelity emulator serves as a proxy for the hardware specification, but we can also further validate by running the tests on real hardware. We implement our approach and apply it to generate about 610,000 test cases; for about 95% of the instructions we achieve complete path coverage. The tests reveal thousands of individual differences; we analyze those differences to shed light on a number of root causes, such as atomicity violations and missing security features.},
 acmid = {2151012},
 address = {New York, NY, USA},
 author = {Martignoni, Lorenzo and McCamant, Stephen and Poosankam, Pongsin and Song, Dawn and Maniatis, Petros},
 doi = {10.1145/2248487.2151012},
 issn = {0362-1340},
 issue_date = {April 2012},
 journal = {SIGPLAN Not.},
 keyword = {CPU emulators, cross validation, symbolic binary execution},
 link = {http://doi.acm.org/10.1145/2248487.2151012},
 month = {mar},
 number = {4},
 numpages = {12},
 pages = {337--348},
 publisher = {ACM},
 title = {Path-exploration Lifting: Hi-fi Tests for Lo-fi Emulators},
 volume = {47},
 year = {2012}
}


@article{Radojkovic:2012:OTA:2189750.2151002,
 abstract = {The introduction of massively multithreaded (MMT) processors, comprised of a large number of cores with many shared resources, has made task scheduling, in particular task to hardware thread assignment, one of the most promising ways to improve system performance. However, finding an optimal task assignment for a workload running on MMT processors is an NP-complete problem. Due to the fact that the performance of the best possible task assignment is unknown, the room for improvement of current task-assignment algorithms cannot be determined. This is a major problem for the industry because it could lead to: (1)~A waste of resources if excessive effort is devoted to improving a task assignment algorithm that already provides a performance that is close to the optimal one, or (2)~significant performance loss if insufficient effort is devoted to improving poorly-performing task assignment algorithms. In this paper, we present a method based on Extreme Value Theory that allows the prediction of the performance of the optimal task assignment in MMT processors. We further show that executing a sample of several hundred or several thousand random task assignments is enough to obtain, with very high confidence, an assignment with a performance that is close to the optimal one. We validate our method with an industrial case study for a set of multithreaded network applications running on an UltraSPARC~T2 processor.},
 acmid = {2151002},
 address = {New York, NY, USA},
 author = {Radojkovi\'{c}, Petar and \v{C}akarevi\'{c}, Vladimir and Moret\'{o}, Miquel and Verd\'{u}, Javier and Pajuelo, Alex and Cazorla, Francisco J. and Nemirovsky, Mario and Valero, Mateo},
 doi = {10.1145/2189750.2151002},
 issn = {0163-5964},
 issue_date = {March 2012},
 journal = {SIGARCH Comput. Archit. News},
 keyword = {extreme value theory, multithreading, scheduling, statistical estimation, task assignment},
 link = {http://doi.acm.org/10.1145/2189750.2151002},
 month = {mar},
 number = {1},
 numpages = {14},
 pages = {235--248},
 publisher = {ACM},
 title = {Optimal Task Assignment in Multithreaded Processors: A Statistical Approach},
 volume = {40},
 year = {2012}
}


