@proceedings{Soffa:2009:1508244,
 abstract = {It is our pleasure to welcome you to the Fourteenth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS XIV). This year's symposium continues its tradition of being the premier forum for presentation of research results on leading edge issues that cross the boundaries of computer architecture, programming languages and compilers, and operating systems. The call for papers attracted abstracts from around the world and resulted in 113 full papers being submitted to the Program Committee (PC). This year we used Borbala Online Conference Services's CyberChairPRO submission and review software. Papers were submitted for double-blind review without authors' names or identifying information. PC members were limited to no more than two paper submissions; a total of 13 papers were submitted which had a PC members as a (co)author. The Program Chair assigned each paper to three reviewers from the PC and two external reviewers. 524 of the 543 assigned reviews were submitted, giving an impressive return rate of 96%. On papers where the Program Chair had a conflict-of-interest, Prof. Margaret Martonosi from Princeton chose the reviewer assignments and also ran the discussions at the PC meeting. Prior to the PC meeting, there was an author rebuttal period during which authors could see and respond to their reviews. The Program Committee met on Saturday, November 1, 2008 at the Chicago O'Hare Hilton. The PC discussed the most highly ranked 51 papers (including 10 PC papers) during the meeting. Each paper discussed had a PC member assigned as lead discussant. PC (co)authored papers were discussed as a group roughly midway through the meeting; these papers were held to a higher standard. The PC selected 29 papers (including 7 PC papers) for an acceptance rate of 25.7%. Seven of these papers were conditionally accepted with shepherding provided by a PC member to ensure that the final papers adequately addressed concerns expressed in the reviews. This year, for the first time, we decided to give a Best Paper Award selected by a sub-committee of the PC from papers nominated by the entire PC.},
 address = {New York, NY, USA},
 isbn = {978-1-60558-406-5},
 location = {Washington, DC, USA},
 note = {415095},
 publisher = {ACM},
 title = {ASPLOS XIV: Proceedings of the 14th International Conference on Architectural Support for Programming Languages and Operating Systems},
 year = {2009}
}


@article{Bergan:2010:CCR:1735970.1736029,
 abstract = {The behavior of a multithreaded program does not depend only on its inputs. Scheduling, memory reordering, timing, and low-level hardware effects all introduce nondeterminism in the execution of multithreaded programs. This severely complicates many tasks, including debugging, testing, and automatic replication. In this work, we avoid these complications by eliminating their root cause: we develop a compiler and runtime system that runs arbitrary multithreaded C/C++ POSIX Threads programs deterministically. A trivial non-performant approach to providing determinism is simply deterministically serializing execution. Instead, we present a compiler and runtime infrastructure that ensures determinism but resorts to serialization rarely, for handling interthread communication and synchronization. We develop two basic approaches, both of which are largely dynamic with performance improved by some static compiler optimizations. First, an ownership-based approach detects interthread communication via an evolving table that tracks ownership of memory regions by threads. Second, a buffering approach uses versioned memory and employs a deterministic commit protocol to make changes visible to other threads. While buffering has larger single-threaded overhead than ownership, it tends to scale better (serializing less often). A hybrid system sometimes performs and scales better than either approach individually. Our implementation is based on the LLVM compiler infrastructure. It needs neither programmer annotations nor special hardware. Our empirical evaluation uses the PARSEC and SPLASH2 benchmarks and shows that our approach scales comparably to nondeterministic execution.},
 acmid = {1736029},
 address = {New York, NY, USA},
 author = {Bergan, Tom and Anderson, Owen and Devietti, Joseph and Ceze, Luis and Grossman, Dan},
 doi = {10.1145/1735970.1736029},
 issn = {0163-5964},
 issue_date = {March 2010},
 journal = {SIGARCH Comput. Archit. News},
 keyword = {compilers, determinism, multicore, multithreading},
 link = {http://doi.acm.org/10.1145/1735970.1736029},
 month = {mar},
 number = {1},
 numpages = {12},
 pages = {53--64},
 publisher = {ACM},
 title = {CoreDet: A Compiler and Runtime System for Deterministic Multithreaded Execution},
 volume = {38},
 year = {2010}
}


@article{Zhang:2010:CDS:1735971.1736041,
 abstract = {Multicore technology is making concurrent programs increasingly pervasive. Unfortunately, it is difficult to deliver reliable concurrent programs, because of the huge and non-deterministic interleaving space. In reality, without the resources to thoroughly check the interleaving space, critical concurrency bugs can slip into production runs and cause failures in the field. Approaches to making the best use of the limited resources and exposing severe concurrency bugs before software release would be desirable. Unlike previous work that focuses on bugs caused by specific interleavings (e.g., races and atomicity-violations), this paper targets concurrency bugs that result in one type of severe effects: program crashes. Our study of the error-propagation process of realworld concurrency bugs reveals a common pattern (50% in our non-deadlock concurrency bug set) that is highly correlated with program crashes. We call this pattern concurrency-memory bugs: buggy interleavings directly cause memory bugs (NULL-pointer-dereference, dangling-pointer, buffer-overflow, uninitialized-read) on shared memory objects. Guided by this study, we built ConMem to monitor program execution, analyze memory accesses and synchronizations, and predicatively detect these common and severe concurrency-memory bugs. We also built a validator ConMem-v to automatically prune false positives by enforcing potential bug-triggering interleavings. We evaluated ConMem using 7 open-source programs with 9 real-world severe concurrency bugs. ConMem detects more tested bugs (8 out of 9 bugs) than a lock-set-based race detector and an unserializable-interleaving detector that detect 4 and 5 bugs respectively, with a false positive rate about one tenth of the compared tools. ConMem-v further prunes out all the false positives. ConMem has reasonable overhead suitable for development usage.},
 acmid = {1736041},
 address = {New York, NY, USA},
 author = {Zhang, Wei and Sun, Chong and Lu, Shan},
 doi = {10.1145/1735971.1736041},
 issn = {0362-1340},
 issue_date = {March 2010},
 journal = {SIGPLAN Not.},
 keyword = {concurrency bugs, software testing},
 link = {http://doi.acm.org/10.1145/1735971.1736041},
 month = {mar},
 number = {3},
 numpages = {14},
 pages = {179--192},
 publisher = {ACM},
 title = {ConMem: Detecting Severe Concurrency Bugs Through an Effect-oriented Approach},
 volume = {45},
 year = {2010}
}


@proceedings{Gupta:2011:1950365,
 abstract = {It is our great pleasure to welcome you to the sixteenth edition of ASPLOS and to introduce what we feel will be an outstanding and thought-provoking technical program. This year's conference continues and reinforces the ASPLOS tradition of publishing innovative multidisciplinary research spanning the boundaries of hardware, computer architecture, compilers, languages, operating systems, networking, and applications. We received 152 full paper submissions by the July 28th deadline, which was fewer than last year's record-setting year, but above average over the past three years. (127, 113 and 181 full papers were submitted in 2008, 2009, and 2010, respectively.) 22 submissions had PC members as co-authors. The dominant technical theme this year was parallelism (once again), with more than half of the submissions explicitly indicating a technical keyword specific to parallel processing. There was a nice mixture of submissions spanning the disciplines of operating systems (36%), compilers and programming languages (34%), and architecture (60%), with 38% of submissions spanning multiple of these areas. Regarding the paper reviewing process, we continued with two changes that had been adopted the previous year: we had two rounds of reviews, and we used an External Review Committee that was preselected by the Program Chair. Each paper was initially reviewed by one PC member and two ERC members. The rationale for having two ERC reviews during the first round (as opposed to one, as was the case last year) was that it gave us the most flexibility to match a paper with the experts who were most qualified to review that specific topic. Based upon these reviews (as well as follow-up discussions for any borderline cases), 103 papers were selected for a second round of reviews, where they received two more reviews from PC members. In general, any paper that received at least one vote for acceptance during the first round was selected for second round reviews. Through this two-round mechanism, we were able to limit the reviewing load per PC member to no more than 14 papers, and more importantly we allowed them to spend the bulk of their time on papers with a significant chance of being accepted. We would especially like to thank the ERC members for their hard work and dedication to the reviewing process; the typical ERC member reviewed 5 papers this year. One change that we made to the reviewing form this year was that in addition to evaluation categories that had been used in previous years (e.g., the degree of novelty, whether the paper was convincing, etc.), we added a new category where we asked reviewers to rate the extent to which a paper would be thoughtprovoking. Our goal was to make sure that we paid special attention to submissions that might inspire people to change the way that they think about how they approach their own technical research problems. We are pleased that this emphasis appears to have translated well into a highly thought-provoking technical program. We are also pleased to report that all reviews had been submitted in time for the authors to respond to them during the author rebuttal period. After the rebuttal period, the PC members and ERC members took the rebuttal comments into account as they discussed the papers online and potentially updated their scores accordingly. The PC met in person on the Carnegie Mellon University campus for a one-day meeting on October 21st, 2010. 81 papers were discussed at the meeting. PC-authored papers were handled roughly halfway through the meeting, using the hotseat approach. Jim Larus, Margaret Martonosi, and David Patterson managed the discussion of papers where the PC Chair had a conflict. The PC chose 32 papers to be presented at the conference, for an overall acceptance rate of 21.1%. 7 of the 32 papers have PC members as co-authors. Continuing with another change that was made last year, all papers were assigned a shepherd to ensure that the final papers adequately addressed the concerns of the reviewers.},
 address = {New York, NY, USA},
 isbn = {978-1-4503-0266-1},
 location = {Newport Beach, California, USA},
 note = {415115},
 publisher = {ACM},
 title = {ASPLOS XVI: Proceedings of the Sixteenth International Conference on Architectural Support for Programming Languages and Operating Systems},
 year = {2011}
}


@article{Woo:2010:CPD:1735971.1736054,
 abstract = {A traditional fixed-function graphics accelerator has evolved into a programmable general-purpose graphics processing unit over the last few years. These powerful computing cores are mainly used for accelerating graphics applications or enabling low-cost scientific computing. To further reduce the cost and form factor, an emerging trend is to integrate GPU along with the memory controllers onto the same die with the processor cores. However, given such a system-on-chip, the GPU, while occupying a substantial part of the silicon, will sit idle and contribute nothing to the overall system performance when running non-graphics workloads or applications lack of data-level parallelism. In this paper, we propose COMPASS, a compute shader-assisted data prefetching scheme, to leverage the GPU resource for improving single-threaded performance on an integrated system. By harnessing the GPU shader cores with very lightweight architectural support, COMPASS can emulate the functionality of a hardware-based prefetcher using the idle GPU and successfully improve the memory performance of single-thread applications. Moreover, thanks to its flexibility and programmability, one can implement the best performing prefetch scheme to improve each specific application as demonstrated in this paper. With COMPASS, we envision that a future application vendor can provide a custom-designed COMPASS shader bundled with its software to be loaded at runtime to optimize the performance. Our simulation results show that COMPASS can improve the single-thread performance of memory-intensive applications by 68% on average.},
 acmid = {1736054},
 address = {New York, NY, USA},
 author = {Woo, Dong Hyuk and Lee, Hsien-Hsin S.},
 doi = {10.1145/1735971.1736054},
 issn = {0362-1340},
 issue_date = {March 2010},
 journal = {SIGPLAN Not.},
 keyword = {GPU, compute shader, prefetch},
 link = {http://doi.acm.org/10.1145/1735971.1736054},
 month = {mar},
 number = {3},
 numpages = {14},
 pages = {297--310},
 publisher = {ACM},
 title = {COMPASS: A Programmable Data Prefetcher Using Idle GPU Shaders},
 volume = {45},
 year = {2010}
}


@article{Yoon:2010:VFE:1735971.1736064,
 abstract = {We present a general scheme for virtualizing main memory error-correction mechanisms, which map redundant information needed to correct errors into the memory namespace itself. We rely on this basic idea, which increases flexibility to increase error protection capabilities, improve power efficiency, and reduce system cost; with only small performance overheads. We augment the virtual memory system architecture to detach the physical mapping of data from the physical mapping of its associated ECC information. We then use this mechanism to develop two-tiered error protection techniques that separate the process of detecting errors from the rare need to also correct errors, and thus save energy. We describe how to provide strong chipkill and double-chip kill protection using existing DRAM and packaging technology. We show how to maintain access granularity and redundancy overheads, even when using ×8 DRAM chips. We also evaluate error correction for systems that do not use ECC DIMMs. Overall, analysis of demanding SPEC CPU 2006 and PARSEC benchmarks indicates that performance overhead is only 1% with ECC DIMMs and less than 10% using standard Non-ECC DIMM configurations, that DRAM power savings can be as high as 27%, and that the system energy-delay product is improved by 12% on average.},
 acmid = {1736064},
 address = {New York, NY, USA},
 author = {Yoon, Doe Hyun and Erez, Mattan},
 doi = {10.1145/1735971.1736064},
 issn = {0362-1340},
 issue_date = {March 2010},
 journal = {SIGPLAN Not.},
 keyword = {error correction, fault tolerance, memory systems, reliability},
 link = {http://doi.acm.org/10.1145/1735971.1736064},
 month = {mar},
 number = {3},
 numpages = {12},
 pages = {397--408},
 publisher = {ACM},
 title = {Virtualized and Flexible ECC for Main Memory},
 volume = {45},
 year = {2010}
}


@article{Weeratunge:2010:AMD:1735970.1736039,
 abstract = {Debugging concurrent programs is difficult. This is primarily because the inherent non-determinism that arises because of scheduler interleavings makes it hard to easily reproduce bugs that may manifest only under certain interleavings. The problem is exacerbated in multi-core environments where there are multiple schedulers, one for each core. In this paper, we propose a reproduction technique for concurrent programs that execute on multi-core platforms. Our technique performs a lightweight analysis of a failing execution that occurs in a multi-core environment, and uses the result of the analysis to enable reproduction of the bug in a single-core system, under the control of a deterministic scheduler. More specifically, our approach automatically identifies the execution point in the re-execution that corresponds to the failure point. It does so by analyzing the failure core dump and leveraging a technique called execution indexing that identifies a related point in the re-execution. By generating a core dump at this point, and comparing the differences betwen the two dumps, we are able to guide a search algorithm to efficiently generate a failure inducing schedule. Our experiments show that our technique is highly effective and has reasonable overhead.},
 acmid = {1736039},
 address = {New York, NY, USA},
 author = {Weeratunge, Dasarath and Zhang, Xiangyu and Jagannathan, Suresh},
 doi = {10.1145/1735970.1736039},
 issn = {0163-5964},
 issue_date = {March 2010},
 journal = {SIGARCH Comput. Archit. News},
 keyword = {concurrency bugs, execution indexing, multi-core, reproduction},
 link = {http://doi.acm.org/10.1145/1735970.1736039},
 month = {mar},
 number = {1},
 numpages = {12},
 pages = {155--166},
 publisher = {ACM},
 title = {Analyzing Multicore Dumps to Facilitate Concurrency Bug Reproduction},
 volume = {38},
 year = {2010}
}


@article{Pelley:2010:PRD:1735971.1736047,
 abstract = {Data center power infrastructure incurs massive capital costs, which typically exceed energy costs over the life of the facility. To squeeze maximum value from the infrastructure, researchers have proposed over-subscribing power circuits, relying on the observation that peak loads are rare. To ensure availability, these proposals employ power capping, which throttles server performance during utilization spikes to enforce safe power budgets. However, because budgets must be enforced locally -- at each power distribution unit (PDU) -- local utilization spikes may force throttling even when power delivery capacity is available elsewhere. Moreover, the need to maintain reserve capacity for fault tolerance on power delivery paths magnifies the impact of utilization spikes. In this paper, we develop mechanisms to better utilize installed power infrastructure, reducing reserve capacity margins and avoiding performance throttling. Unlike conventional high-availability data centers, where collocated servers share identical primary and secondary power feeds, we reorganize power feeds to create shuffled power distribution topologies. Shuffled topologies spread secondary power feeds over numerous PDUs, reducing reserve capacity requirements to tolerate a single PDU failure. Second, we propose Power Routing, which schedules IT load dynamically across redundant power feeds to: (1) shift slack to servers with growing power demands, and (2) balance power draw across AC phases to reduce heating and improve electrical stability. We describe efficient heuristics for scheduling servers to PDUs (an NP-complete problem). Using data collected from nearly 1000 servers in three production facilities, we demonstrate that these mechanisms can reduce the required power infrastructure capacity relative to conventional high-availability data centers by 32% without performance degradation.},
 acmid = {1736047},
 address = {New York, NY, USA},
 author = {Pelley, Steven and Meisner, David and Zandevakili, Pooya and Wenisch, Thomas F. and Underwood, Jack},
 doi = {10.1145/1735971.1736047},
 issn = {0362-1340},
 issue_date = {March 2010},
 journal = {SIGPLAN Not.},
 keyword = {data centers, power infrastructure},
 link = {http://doi.acm.org/10.1145/1735971.1736047},
 month = {mar},
 number = {3},
 numpages = {12},
 pages = {231--242},
 publisher = {ACM},
 title = {Power Routing: Dynamic Power Provisioning in the Data Center},
 volume = {45},
 year = {2010}
}


@inproceedings{Ebrahimi:2010:FVS:1736020.1736058,
 abstract = {Cores in a chip-multiprocessor (CMP) system share multiple hardware resources in the memory subsystem. If resource sharing is unfair, some applications can be delayed significantly while others are unfairly prioritized. Previous research proposed separate fairness mechanisms in each individual resource. Such resource-based fairness mechanisms implemented independently in each resource can make contradictory decisions, leading to low fairness and loss of performance. Therefore, a coordinated mechanism that provides fairness in the entire shared memory system is desirable. This paper proposes a new approach that provides fairness in the entire shared memory system, thereby eliminating the need for and complexity of developing fairness mechanisms for each individual resource. Our technique, Fairness via Source Throttling (FST), estimates the unfairness in the entire shared memory system. If the estimated unfairness is above a threshold set by system software, FST throttles down cores causing unfairness by limiting the number of requests they can inject into the system and the frequency at which they do. As such, our source-based fairness control ensures fairness decisions are made in tandem in the entire memory system. FST also enforces thread priorities/weights, and enables system software to enforce different fairness objectives and fairness-performance tradeoffs in the memory system. Our evaluations show that FST provides the best system fairness and performance compared to four systems with no fairness control and with state-of-the-art fairness mechanisms implemented in both shared caches and memory controllers.},
 acmid = {1736058},
 address = {New York, NY, USA},
 author = {Ebrahimi, Eiman and Lee, Chang Joo and Mutlu, Onur and Patt, Yale N.},
 booktitle = {Proceedings of the Fifteenth Edition of ASPLOS on Architectural Support for Programming Languages and Operating Systems},
 doi = {10.1145/1736020.1736058},
 isbn = {978-1-60558-839-1},
 keyword = {fairness, multi-core systems, shared memory systems, system performance},
 link = {http://doi.acm.org/10.1145/1736020.1736058},
 location = {Pittsburgh, Pennsylvania, USA},
 numpages = {12},
 pages = {335--346},
 publisher = {ACM},
 series = {ASPLOS XV},
 title = {Fairness via Source Throttling: A Configurable and High-performance Fairness Substrate for Multi-core Memory Systems},
 year = {2010}
}


@inproceedings{Goodstein:2010:BAA:1736020.1736050,
 abstract = {Online program monitoring is an effective technique for detecting bugs and security attacks in running applications. Extending these tools to monitor parallel programs is challenging because the tools must account for inter-thread dependences and relaxed memory consistency models. Existing tools assume sequential consistency and often slow down the monitored program by orders of magnitude. In this paper, we present a novel approach that avoids these pitfalls by not relying on strong consistency models or detailed inter-thread dependence tracking. Instead, we only assume that events in the distant past on all threads have become visible; we make no assumptions on (and avoid the overheads of tracking) the relative ordering of more recent events on other threads. To overcome the potential state explosion of considering all the possible orderings among recent events, we adapt two techniques from static dataflow analysis, reaching definitions and reaching expressions, to this new domain of dynamic parallel monitoring. Significant modifications to these techniques are proposed to ensure the correctness and efficiency of our approach. We show how our adapted analysis can be used in two popular memory and security tools. We prove that our approach does not miss errors, and sacrifices precision only due to the lack of a relative ordering among recent events. Moreover, our simulation study on a collection of Splash-2 and Parsec 2.0 benchmarks running a memory-checking tool on a hardware-assisted logging platform demonstrates the potential benefits in trading off a very low false positive rate for (i) reduced overhead and (ii) the ability to run on relaxed consistency models.},
 acmid = {1736050},
 address = {New York, NY, USA},
 author = {Goodstein, Michelle L. and Vlachos, Evangelos and Chen, Shimin and Gibbons, Phillip B. and Kozuch, Michael A. and Mowry, Todd C.},
 booktitle = {Proceedings of the Fifteenth Edition of ASPLOS on Architectural Support for Programming Languages and Operating Systems},
 doi = {10.1145/1736020.1736050},
 isbn = {978-1-60558-839-1},
 keyword = {data flow analysis, dynamic program monitoring, parallel programming, static analysis},
 link = {http://doi.acm.org/10.1145/1736020.1736050},
 location = {Pittsburgh, Pennsylvania, USA},
 numpages = {14},
 pages = {257--270},
 publisher = {ACM},
 series = {ASPLOS XV},
 title = {Butterfly Analysis: Adapting Dataflow Analysis to Dynamic Parallel Monitoring},
 year = {2010}
}


@inproceedings{Hormati:2010:MMS:1736020.1736053,
 abstract = {SIMD (Single Instruction, Multiple Data) engines are an essential part of the processors in various computing markets, from servers to the embedded domain. Although SIMD-enabled architectures have the capability of boosting the performance of many application domains by exploiting data-level parallelism, it is very challenging for compilers and also programmers to identify and transform parts of a program that will benefit from a particular SIMD engine. The focus of this paper is on the problem of SIMDization for the growing application domain of streaming. Streaming applications are an ideal solution for targeting multi-core architectures, such as shared/distributed memory systems, tiled architectures, and single-core systems. Since these architectures, in most cases, provide SIMD acceleration units as well, it is highly beneficial to generate SIMD code from streaming programs. Specifically, we introduce MacroSS, which is capable of performing macro-SIMDization on high-level streaming graphs. Macro-SIMDization uses high-level information such as execution rates of actors and communication patterns between them to transform the graph structure, vectorize actors of a streaming program, and generate intermediate code. We also propose low-overhead architectural modifications that accelerate shuffling of data elements between the scalar and vectorized parts of a streaming program. Our experiments show that MacroSS is capable of generating code that, on average, outperforms scalar code compiled with the current state-of-art auto-vectorizing compilers by 54%. Using the low-overhead data shuffling hardware, performance is improved by an additional 8% with less than 1% area overhead.},
 acmid = {1736053},
 address = {New York, NY, USA},
 author = {Hormati, Amir H. and Choi, Yoonseo and Woh, Mark and Kudlur, Manjunath and Rabbah, Rodric and Mudge, Trevor and Mahlke, Scott},
 booktitle = {Proceedings of the Fifteenth Edition of ASPLOS on Architectural Support for Programming Languages and Operating Systems},
 doi = {10.1145/1736020.1736053},
 isbn = {978-1-60558-839-1},
 keyword = {SIMD architecture, compiler, optimization, streaming},
 link = {http://doi.acm.org/10.1145/1736020.1736053},
 location = {Pittsburgh, Pennsylvania, USA},
 numpages = {12},
 pages = {285--296},
 publisher = {ACM},
 series = {ASPLOS XV},
 title = {MacroSS: Macro-SIMDization of Streaming Applications},
 year = {2010}
}


@article{Raman:2010:SPU:1735970.1736030,
 abstract = {With the right techniques, multicore architectures may be able to continue the exponential performance trend that elevated the performance of applications of all types for decades. While many scientific programs can be parallelized without speculative techniques, speculative parallelism appears to be the key to continuing this trend for general-purpose applications. Recently-proposed code parallelization techniques, such as those by Bridges et al. and by Thies et al., demonstrate scalable performance on multiple cores by using speculation to divide code into atomic units (transactions) that span multiple threads in order to expose data parallelism. Unfortunately, most software and hardware Thread-Level Speculation (TLS) memory systems and transactional memories are not sufficient because they only support single-threaded atomic units. Multi-threaded Transactions (MTXs) address this problem, but they require expensive hardware support as currently proposed in the literature. This paper proposes a Software MTX (SMTX) system that captures the applicability and performance of hardware MTX, but on existing multicore machines. The SMTX system yields a harmonic mean speedup of 13.36x on native hardware with four 6-core processors (24 cores in total) running speculatively parallelized applications.},
 acmid = {1736030},
 address = {New York, NY, USA},
 author = {Raman, Arun and Kim, Hanjun and Mason, Thomas R. and Jablin, Thomas B. and August, David I.},
 doi = {10.1145/1735970.1736030},
 issn = {0163-5964},
 issue_date = {March 2010},
 journal = {SIGARCH Comput. Archit. News},
 keyword = {automatic parallelization, loop-level parallelism, multi-threaded transactions, pipelined parallelism, software transactional memory, thread-level speculation},
 link = {http://doi.acm.org/10.1145/1735970.1736030},
 month = {mar},
 number = {1},
 numpages = {12},
 pages = {65--76},
 publisher = {ACM},
 title = {Speculative Parallelization Using Software Multi-threaded Transactions},
 volume = {38},
 year = {2010}
}


@article{Neelakantam:2010:RSE:1735971.1736026,
 abstract = {In this paper we evaluate the atomic region compiler abstraction by incorporating it into a commercial system. We find that atomic regions are simple and intuitive to integrate into an x86 binary-translation system. Furthermore, doing so trivially enables additional optimization opportunities beyond that achievable by a high-performance dynamic optimizer, which already implements superblocks. We show that atomic regions can suffer from severe performance penalties if misspeculations are left uncontrolled, but that a simple software control mechanism is sufficient to reign in all detrimental side-effects. We evaluate using full reference runs of the SPEC CPU2000 integer benchmarks and find that atomic regions enable up to a 9% (3% on average) improvement beyond the performance of a tuned product. These performance improvements are achieved without any negative side effects. Performance side effects such as code bloat are absent with atomic regions; in fact, static code size is reduced. The hardware necessary is synergistic with other needs and was already available on the commercial product used in our evaluation. Finally, the software complexity is minimal as a single developer was able to incorporate atomic regions into a sophisticated 300,000 line code base in three months, despite never having seen the translator source code beforehand.},
 acmid = {1736026},
 address = {New York, NY, USA},
 author = {Neelakantam, Naveen and Ditzel, David R. and Zilles, Craig},
 doi = {10.1145/1735971.1736026},
 issn = {0362-1340},
 issue_date = {March 2010},
 journal = {SIGPLAN Not.},
 keyword = {atomicity, checkpoint, dynamic translation, optimization, speculation},
 link = {http://doi.acm.org/10.1145/1735971.1736026},
 month = {mar},
 number = {3},
 numpages = {10},
 pages = {29--38},
 publisher = {ACM},
 title = {A Real System Evaluation of Hardware Atomicity for Software Speculation},
 volume = {45},
 year = {2010}
}


@inproceedings{Vlachos:2010:PEA:1736020.1736051,
 abstract = {Instruction-grain lifeguards monitor the events of a running application at the level of individual instructions in order to identify and help mitigate application bugs and security exploits. Because such lifeguards impose a 10-100X slowdown on existing platforms, previous studies have proposed hardware designs to accelerate lifeguard processing. However, these accelerators are either tailored to a specific class of lifeguards or suitable only for monitoring singlethreaded programs. We present ParaLog, the first design of a system enabling fast online parallel monitoring of multithreaded parallel applications. ParaLog supports a broad class of software-defined lifeguards. We show how three existing accelerators can be enhanced to support online multithreaded monitoring, dramatically reducing lifeguard overheads. We identify and solve several challenges in monitoring parallel applications and/or parallelizing these accelerators, including (i) enforcing inter-thread data dependences, (ii) dealing with inter-thread effects that are not reflected in coherence traffic, (iii) dealing with unmonitored operating system activity, and (iv) ensuring lifeguards can access shared metadata with negligible synchronization overheads. We present our system design for both Sequentially Consistent and Total Store Ordering processors. We implement and evaluate our design on a 16 core simulated CMP, using benchmarks from SPLASH-2 and PARSEC and two lifeguards: a data-flow tracking lifeguard and a memory-access checker lifeguard. Our results show that (i) our parallel accelerators improve performance by 2-9X and 1.13-3.4X for our two lifeguards, respectively, (ii) we are 5-126X faster than the time-slicing approach required by existing techniques, and (iii) our average overheads for applications with eight threads are 51% and 28% for the two lifeguards, respectively.},
 acmid = {1736051},
 address = {New York, NY, USA},
 author = {Vlachos, Evangelos and Goodstein, Michelle L. and Kozuch, Michael A. and Chen, Shimin and Falsafi, Babak and Gibbons, Phillip B. and Mowry, Todd C.},
 booktitle = {Proceedings of the Fifteenth Edition of ASPLOS on Architectural Support for Programming Languages and Operating Systems},
 doi = {10.1145/1736020.1736051},
 isbn = {978-1-60558-839-1},
 keyword = {hardware support for debugging, instruction-grain lifeguards, online parallel monitoring},
 link = {http://doi.acm.org/10.1145/1736020.1736051},
 location = {Pittsburgh, Pennsylvania, USA},
 numpages = {14},
 pages = {271--284},
 publisher = {ACM},
 series = {ASPLOS XV},
 title = {ParaLog: Enabling and Accelerating Online Parallel Monitoring of Multithreaded Applications},
 year = {2010}
}


@article{Ipek:2010:DRM:1735971.1736023,
 abstract = {DRAM is facing severe scalability challenges in sub-45nm tech- nology nodes due to precise charge placement and sensing hur- dles in deep-submicron geometries. Resistive memories, such as phase-change memory (PCM), already scale well beyond DRAM and are a promising DRAM replacement. Unfortunately, PCM is write-limited, and current approaches to managing writes must de- commission pages of PCM when the first bit fails. This paper presents dynamically replicated memory (DRM), the first hardware and operating system interface designed for PCM that allows continued operation through graceful degradation when hard faults occur. DRM reuses memory pages that con- tain hard faults by dynamically forming pairs of complementary pages that act as a single page of storage. No changes are required to the processor cores, the cache hierarchy, or the operating sys- tem's page tables. By changing the memory controller, the TLBs, and the operating system to be DRM-aware, we can improve the lifetime of PCM by up to 40x over conventional error-detection techniques.},
 acmid = {1736023},
 address = {New York, NY, USA},
 author = {Ipek, Engin and Condit, Jeremy and Nightingale, Edmund B. and Burger, Doug and Moscibroda, Thomas},
 doi = {10.1145/1735971.1736023},
 issn = {0362-1340},
 issue_date = {March 2010},
 journal = {SIGPLAN Not.},
 keyword = {phase-change memory, write endurance},
 link = {http://doi.acm.org/10.1145/1735971.1736023},
 month = {mar},
 number = {3},
 numpages = {12},
 pages = {3--14},
 publisher = {ACM},
 title = {Dynamically Replicated Memory: Building Reliable Systems from Nanoscale Resistive Memories},
 volume = {45},
 year = {2010}
}


@article{Bhattacharjee:2010:ICT:1735970.1736060,
 abstract = {Translation Lookaside Buffers (TLBs) are commonly employed in modern processor designs and have considerable impact on overall system performance. A number of past works have studied TLB designs to lower access times and miss rates, specifically for uniprocessors. With the growing dominance of chip multiprocessors (CMPs), it is necessary to examine TLB performance in the context of parallel workloads. This work is the first to present TLB prefetchers that exploit commonality in TLB miss patterns across cores in CMPs. We propose and evaluate two Inter-Core Cooperative (ICC) TLB prefetching mechanisms, assessing their effectiveness at eliminating TLB misses both individually and together. Our results show these approaches require at most modest hardware and can collectively eliminate 19% to 90% of data TLB (D-TLB) misses across the surveyed parallel workloads. We also compare performance improvements across a range of hardware and software implementation possibilities. We find that while a fully-hardware implementation results in average performance improvements of 8-46% for a range of TLB sizes, a hardware/software approach yields improvements of 4-32%. Overall, our work shows that TLB prefetchers exploiting inter-core correlations can effectively eliminate TLB misses.},
 acmid = {1736060},
 address = {New York, NY, USA},
 author = {Bhattacharjee, Abhishek and Martonosi, Margaret},
 doi = {10.1145/1735970.1736060},
 issn = {0163-5964},
 issue_date = {March 2010},
 journal = {SIGARCH Comput. Archit. News},
 keyword = {parallelism, prefetching, translation lookaside buffer},
 link = {http://doi.acm.org/10.1145/1735970.1736060},
 month = {mar},
 number = {1},
 numpages = {12},
 pages = {359--370},
 publisher = {ACM},
 title = {Inter-core Cooperative TLB for Chip Multiprocessors},
 volume = {38},
 year = {2010}
}


@inproceedings{Ahmad:2010:JOI:1736020.1736048,
 abstract = {Server power and cooling power amount to a significant fraction of modern data centers' recurring costs. While data centers provision enough servers to guarantee response times under the maximum loading, data centers operate under much less loading most of the times (e.g., 30-70% of the maximum loading). Previous server-power proposals exploit this under-utilization to reduce the server idle power by keeping active only as many servers as necessary and putting the rest into low-power standby modes. However, these proposals incur higher cooling power due to hot spots created by concentrating the data center loading on fewer active servers, or degrade response times due to standby-to-active transition delays, or both. Other proposals optimize the cooling power but incur considerable idle power. To address the first issue of power, we propose PowerTrade, which trades-off idle power and cooling power for each other, thereby reducing the total power. To address the second issue of response time, we propose SurgeGuard to overprovision the number of active servers beyond that needed by the current loading so as to absorb future increases in the loading. SurgeGuard is a two-tier scheme which uses well-known over-provisioning at coarse time granularities (e.g., one hour) to absorb the common, smooth increases in the loading, and a novel fine-grain replenishment of the over-provisioned reserves at fine time granularities (e.g., five minutes) to handle the uncommon, abrupt loading surges. Using real-world traces, we show that combining PowerTrade and SurgeGuard reduces total power by 30% compared to previous low-power schemes while maintaining response times within 1.7%.},
 acmid = {1736048},
 address = {New York, NY, USA},
 author = {Ahmad, Faraz and Vijaykumar, T. N.},
 booktitle = {Proceedings of the Fifteenth Edition of ASPLOS on Architectural Support for Programming Languages and Operating Systems},
 doi = {10.1145/1736020.1736048},
 isbn = {978-1-60558-839-1},
 keyword = {cooling power, data center, idle power, power management, response time},
 link = {http://doi.acm.org/10.1145/1736020.1736048},
 location = {Pittsburgh, Pennsylvania, USA},
 numpages = {14},
 pages = {243--256},
 publisher = {ACM},
 series = {ASPLOS XV},
 title = {Joint Optimization of Idle and Cooling Power in Data Centers While Maintaining Response Time},
 year = {2010}
}


@article{Burckhardt:2010:RSP:1735971.1736040,
 abstract = {This paper presents a randomized scheduler for finding concurrency bugs. Like current stress-testing methods, it repeatedly runs a given test program with supplied inputs. However, it improves on stress-testing by finding buggy schedules more effectively and by quantifying the probability of missing concurrency bugs. Key to its design is the characterization of the depth of a concurrency bug as the minimum number of scheduling constraints required to find it. In a single run of a program with n threads and k steps, our scheduler detects a concurrency bug of depth d with probability at least 1/nkd-1. We hypothesize that in practice, many concurrency bugs (including well-known types such as ordering errors, atomicity violations, and deadlocks) have small bug-depths, and we confirm the efficiency of our schedule randomization by detecting previously unknown and known concurrency bugs in several production-scale concurrent programs.},
 acmid = {1736040},
 address = {New York, NY, USA},
 author = {Burckhardt, Sebastian and Kothari, Pravesh and Musuvathi, Madanlal and Nagarakatte, Santosh},
 doi = {10.1145/1735971.1736040},
 issn = {0362-1340},
 issue_date = {March 2010},
 journal = {SIGPLAN Not.},
 keyword = {concurrency, race conditions, randomized algorithms, testing},
 link = {http://doi.acm.org/10.1145/1735971.1736040},
 month = {mar},
 number = {3},
 numpages = {12},
 pages = {167--178},
 publisher = {ACM},
 title = {A Randomized Scheduler with Probabilistic Guarantees of Finding Bugs},
 volume = {45},
 year = {2010}
}


@article{Gelado:2010:ADS:1735971.1736059,
 abstract = {Heterogeneous computing combines general purpose CPUs with accelerators to efficiently execute both sequential control-intensive and data-parallel phases of applications. Existing programming models for heterogeneous computing rely on programmers to explicitly manage data transfers between the CPU system memory and accelerator memory. This paper presents a new programming model for heterogeneous computing, called Asymmetric Distributed Shared Memory (ADSM), that maintains a shared logical memory space for CPUs to access objects in the accelerator physical memory but not vice versa. The asymmetry allows light-weight implementations that avoid common pitfalls of symmetrical distributed shared memory systems. ADSM allows programmers to assign data objects to performance critical methods. When a method is selected for accelerator execution, its associated data objects are allocated within the shared logical memory space, which is hosted in the accelerator physical memory and transparently accessible by the methods executed on CPUs. We argue that ADSM reduces programming efforts for heterogeneous computing systems and enhances application portability. We present a software implementation of ADSM, called GMAC, on top of CUDA in a GNU/Linux environment. We show that applications written in ADSM and running on top of GMAC achieve performance comparable to their counterparts using programmer-managed data transfers. This paper presents the GMAC system and evaluates different design choices. We further suggest additional architectural support that will likely allow GMAC to achieve higher application performance than the current CUDA model.},
 acmid = {1736059},
 address = {New York, NY, USA},
 author = {Gelado, Isaac and Stone, John E. and Cabezas, Javier and Patel, Sanjay and Navarro, Nacho and Hwu, Wen-mei W.},
 doi = {10.1145/1735971.1736059},
 issn = {0362-1340},
 issue_date = {March 2010},
 journal = {SIGPLAN Not.},
 keyword = {asymmetric distributed shared memory, data-centric programming models, heterogeneous systems},
 link = {http://doi.acm.org/10.1145/1735971.1736059},
 month = {mar},
 number = {3},
 numpages = {12},
 pages = {347--358},
 publisher = {ACM},
 title = {An Asymmetric Distributed Shared Memory Model for Heterogeneous Parallel Systems},
 volume = {45},
 year = {2010}
}


@article{Huang:2010:OES:1735971.1736062,
 abstract = {This paper proposes an efficient hardware/software system that significantly enhances software security through diversified replication on multi-cores. Recent studies show that a large class of software attacks can be detected by running multiple versions of a program simultaneously and checking the consistency of their behaviors. However, execution of multiple replicas incurs significant overheads on today's computing platforms, especially with fine-grained comparisons necessary for high security. Orthrus exploits similarities in automatically generated replicas to enable simultaneous execution of those replicas with minimal overheads; the architecture reduces memory and bandwidth overheads by compressing multiple memory spaces together, and additional power consumption and silicon area by eliminating redundant computations. Utilizing the hardware architecture, Orthrus implements a fine-grained memory layout diversification with the LLVM compiler and can detect corruptions in both pointers and critical data. Experiments indicate that the Orthrus architecture incurs minimal overheads and provides a protection against a broad range of attacks.},
 acmid = {1736062},
 address = {New York, NY, USA},
 author = {Huang, Ruirui and Deng, Daniel Y. and Suh, G. Edward},
 doi = {10.1145/1735971.1736062},
 issn = {0362-1340},
 issue_date = {March 2010},
 journal = {SIGPLAN Not.},
 keyword = {memory protection, multi-core architecture, replication-aware architecture, software diversity and redundancy, software security},
 link = {http://doi.acm.org/10.1145/1735971.1736062},
 month = {mar},
 number = {3},
 numpages = {14},
 pages = {371--384},
 publisher = {ACM},
 title = {Orthrus: Efficient Software Integrity Protection on Multi-cores},
 volume = {45},
 year = {2010}
}


@article{Shen:2010:RBV:1735971.1736034,
 abstract = {A large number of user requests execute (often concurrently) within a server system. A single request may exhibit fluctuating hardware characteristics (such as instruction completion rate and on-chip resource usage) over the course of its execution, due to inherent variations in application execution semantics as well as dynamic resource competition on resource-sharing processors like multicores. Understanding such behavior variations can assist fine-grained request modeling and adaptive resource management. This paper presents operating system management to track request behavior variations online. In addition to metric sample collection during periodic interrupts, we exploit the frequent system calls in server applications to perform low-cost in-kernel sampling. We utilize identified behavior variations to support or enhance request modeling in request classification, anomaly analysis, and online request signature construction. A foundation of our request modeling is the ability to quantify the difference between two requests' time series behaviors. We evaluate several differencing measures and enhance the classic dynamic time warping technique with additional penalties for asynchronous warp steps. Finally, motivated by fluctuating request resource usage and the resulting contention, we implement contention-easing CPU scheduling on multicore platforms and demonstrate its effectiveness in improving the worst-case request performance. Experiments in this paper are based on five server applications -- Apache web server, TPCC, TPCH, RUBiS online auction benchmark, and a user-content-driven online teaching application called WeBWorK.},
 acmid = {1736034},
 address = {New York, NY, USA},
 author = {Shen, Kai},
 doi = {10.1145/1735971.1736034},
 issn = {0362-1340},
 issue_date = {March 2010},
 journal = {SIGPLAN Not.},
 keyword = {hardware counter, multicore, operating system adaptation, request modeling, server system},
 link = {http://doi.acm.org/10.1145/1735971.1736034},
 month = {mar},
 number = {3},
 numpages = {14},
 pages = {103--116},
 publisher = {ACM},
 title = {Request Behavior Variations},
 volume = {45},
 year = {2010}
}


@inproceedings{Sanchez:2010:FAS:1736020.1736055,
 abstract = {To make efficient use of CMPs with tens to hundreds of cores, it is often necessary to exploit fine-grain parallelism. However, managing tasks of a few thousand instructions is particularly challenging, as the runtime must ensure load balance without compromising locality and introducing small overheads. Software-only schedulers can implement various scheduling algorithms that match the characteristics of different applications and programming models, but suffer significant overheads as they synchronize and communicate task information over the deep cache hierarchy of a large-scale CMP. To reduce these costs, hardware-only schedulers like Carbon, which implement task queuing and scheduling in hardware, have been proposed. However, a hardware-only solution fixes the scheduling algorithm and leaves no room for other uses of the custom hardware. This paper presents a combined hardware-software approach to build fine-grain schedulers that retain the flexibility of software schedulers while being as fast and scalable as hardware ones. We propose asynchronous direct messages (ADM), a simple architectural extension that provides direct exchange of asynchronous, short messages between threads in the CMP without going through the memory hierarchy. ADM is sufficient to implement a family of novel, software-mostly schedulers that rely on low-overhead messaging to efficiently coordinate scheduling and transfer task information. These schedulers match and often exceed the performance and scalability of Carbon when using the same scheduling algorithm. When the ADM runtime tailors its scheduling algorithm to application characteristics, it outperforms Carbon by up to 70%.},
 acmid = {1736055},
 address = {New York, NY, USA},
 author = {Sanchez, Daniel and Yoo, Richard M. and Kozyrakis, Christos},
 booktitle = {Proceedings of the Fifteenth Edition of ASPLOS on Architectural Support for Programming Languages and Operating Systems},
 doi = {10.1145/1736020.1736055},
 isbn = {978-1-60558-839-1},
 keyword = {chip-multiprocessors, fine-grain scheduling, many-core, messaging, scheduling, work-stealing},
 link = {http://doi.acm.org/10.1145/1736020.1736055},
 location = {Pittsburgh, Pennsylvania, USA},
 numpages = {12},
 pages = {311--322},
 publisher = {ACM},
 series = {ASPLOS XV},
 title = {Flexible Architectural Support for Fine-grain Scheduling},
 year = {2010}
}


@article{Brewer:2010:TDR:1735970.1736021,
 abstract = {The historic focus of development has rightfully been on macroeconomics and good governance, but technology has an increasingly large role to play. In this talk, I review several novel technologies that we have deployed in India and Africa, and discuss the challenges and opportunities of this new subfield of EECS research. Working with the Aravind Eye Hospital, we are currently supporting doctor / patient videoconferencing in 30+ rural villages; more than 25,000 people have had their blindness cured due to these exams. Although Moore's Law has led to great cost reductions and thus enabled new technologies, we have reached essentially the low point for cost: the computing is essentially free compared to the rest of the system. The premium is thus on a combination of 1) deeper integration (fewer compo-nents), 2) shared usage models (even phones are shared), and 3) lower operating costs in terms of power and connectivity.},
 acmid = {1736021},
 address = {New York, NY, USA},
 author = {Brewer, Eric A.},
 doi = {10.1145/1735970.1736021},
 issn = {0163-5964},
 issue_date = {March 2010},
 journal = {SIGARCH Comput. Archit. News},
 keyword = {developing regions, ictd, it for development.},
 link = {http://doi.acm.org/10.1145/1735970.1736021},
 month = {mar},
 number = {1},
 numpages = {2},
 pages = {1--2},
 publisher = {ACM},
 title = {Technology for Developing Regions: Moore's Law is Not Enough},
 volume = {38},
 year = {2010}
}


@proceedings{Hoe:2010:1736020,
 abstract = {It is our pleasure to introduce the technical program for the fifteenth edition of ASPLOS. The conference this year continues and reinforces the ASPLOS tradition of publishing innovative research spanning the boundaries of hardware, computer architecture, compilers, programming languages, operating systems, and distributed computing. We received 181 submissions this year, which was a record and a significant reversal of a trend from the last 3 years (158, 127 and 113 in 2006, 2008 and 2009, respectively). 19 submissions had PC members as co-authors. The most dominant theme, by far, was parallel processing: over half of the submissions were directly related to this topic, most of which mentioned multicore, manycore, SIMD, or heterogeneous architectures as a target. There were two key changes in the ASPLOS paper reviewing process this year, both of which have been used in other systems conferences: two rounds of reviews, and the use of an External Review Committee preselected by the Program Chair. Each paper was initially reviewed by two PC members and one ERC member. Based on these reviews, 91 papers were selected for a second round of reviews; see the Program Chair's Report on the ASPLOS Web site for the details on how these were selected. These papers received one or two more PC reviews plus one more ERC review. Together, these two changes held the average reviewing load per PC member to under 18 papers, while allowing them to spend more time on papers with a significant chance of being accepted. The PC met in person at the Chicago O'Hare Hilton for a one-day meeting on Oct. 30, 2009. 73 papers were discussed at the meeting. The PC chose 32 papers to be presented at the conference, for an overall acceptance rate of 17.7%. 6 of the 32 have PC members as co-authors. In another departure from previous years, all papers were assigned a shepherd to ensure that the final papers adequately addressed the concerns of the reviewers. The PC also awarded a Best Paper Award and nominated three papers for the CACM Research Highlights. If ASPLOS XV is successful, as we hope, numerous people should share the credit. Most of all, the technical program directly reflects the high quality of the submitted papers; we thank all the authors of the submissions for their effort. The members of the PC and ERC put in a very substantial effort for reviewing and shepherding, and for selecting the award papers. Mike Hind (IBM Research), in particular, also handled the review process and discussions for papers where the PC chair had conflicts, and provided valuable advice during the reviewing process. Seth Goldstein (CMU) once again organized and put his inimitable stamp on the ASPLOS Wild and Crazy Ideas session. We owe them all a debt of gratitude. The extensive logistics behind organizing this conference have been handled by the tireless effort of a large team of Pittsburgh volunteers: Shimin Chen (Intel), Allen Cheng (Pitt), Sangyeun Cho (Pitt), Franz Franchetti (CMU), Ken Mai (CMU), Todd Mowry (CMU), Onur Mutlu (CMU), Jun Yang (Pitt) and Youtao Zhang (Pitt). Each member of this team played an important role in making ASPLOS in Pittsburgh a success. In addition, the ASPLOS Steering Committee gave us timely and valuable guidance over the last year. We want to thank all members of the organizing committee and the steering committee for the time and attention they have invested in this conference.},
 address = {New York, NY, USA},
 isbn = {978-1-60558-839-1},
 location = {Pittsburgh, Pennsylvania, USA},
 note = {415105},
 publisher = {ACM},
 title = {ASPLOS XV: Proceedings of the Fifteenth Edition of ASPLOS on Architectural Support for Programming Languages and Operating Systems},
 year = {2010}
}


@article{Feng:2010:SPS:1735971.1736063,
 abstract = {Aggressive technology scaling provides designers with an ever increasing budget of cheaper and faster transistors. Unfortunately, this trend is accompanied by a decline in individual device reliability as transistors become increasingly susceptible to soft errors. We are quickly approaching a new era where resilience to soft errors is no longer a luxury that can be reserved for just processors in high-reliability, mission-critical domains. Even processors used in mainstream computing will soon require protection. However, due to tighter profit margins, reliable operation for these devices must come at little or no cost. This paper presents Shoestring, a minimally invasive software solution that provides high soft error coverage with very little overhead, enabling its deployment even in commodity processors with "shoestring" reliability budgets. Leveraging intelligent analysis at compile time, and exploiting low-cost, symptom-based error detection, Shoestring is able to focus its efforts on protecting statistically-vulnerable portions of program code. Shoestring effectively applies instruction duplication to protect only those segments of code that, when subjected to a soft error, are likely to result in user-visible faults without first exhibiting symptomatic behavior. Shoestring is able to recover from an additional 33.9% of soft errors that are undetected by a symptom-only approach, achieving an overall user-visible failure rate of 1.6%. This reliability improvement comes at a modest performance overhead of 15.8%.},
 acmid = {1736063},
 address = {New York, NY, USA},
 author = {Feng, Shuguang and Gupta, Shantanu and Ansari, Amin and Mahlke, Scott},
 doi = {10.1145/1735971.1736063},
 issn = {0362-1340},
 issue_date = {March 2010},
 journal = {SIGPLAN Not.},
 keyword = {compiler analysis, error detection, fault injection},
 link = {http://doi.acm.org/10.1145/1735971.1736063},
 month = {mar},
 number = {3},
 numpages = {12},
 pages = {385--396},
 publisher = {ACM},
 title = {Shoestring: Probabilistic Soft Error Reliability on the Cheap},
 volume = {45},
 year = {2010}
}


@article{Lee:2010:REO:1735971.1736031,
 abstract = {Deterministic replay systems record and reproduce the execution of a hardware or software system. While it is well known how to replay uniprocessor systems, replaying shared memory multiprocessor systems at low overhead on commodity hardware is still an open problem. This paper presents Respec, a new way to support deterministic replay of shared memory multithreaded programs on commodity multiprocessor hardware. Respec targets online replay in which the recorded and replayed processes execute concurrently. Respec uses two strategies to reduce overhead while still ensuring correctness: speculative logging and externally deterministic replay. Speculative logging optimistically logs less information about shared memory dependencies than is needed to guarantee deterministic replay, then recovers and retries if the replayed process diverges from the recorded process. Externally deterministic replay relaxes the degree to which the two executions must match by requiring only their system output and final program states match. We show that the combination of these two techniques results in low recording and replay overhead for the common case of data-race-free execution intervals and still ensures correct replay for execution intervals that have data races. We modified the Linux kernel to implement our techniques. Our software system adds on average about 18% overhead to the execution time for recording and replaying programs with two threads and 55% overhead for programs with four threads.},
 acmid = {1736031},
 address = {New York, NY, USA},
 author = {Lee, Dongyoon and Wester, Benjamin and Veeraraghavan, Kaushik and Narayanasamy, Satish and Chen, Peter M. and Flinn, Jason},
 doi = {10.1145/1735971.1736031},
 issn = {0362-1340},
 issue_date = {March 2010},
 journal = {SIGPLAN Not.},
 keyword = {external determinism, replay, speculative execution},
 link = {http://doi.acm.org/10.1145/1735971.1736031},
 month = {mar},
 number = {3},
 numpages = {14},
 pages = {77--90},
 publisher = {ACM},
 title = {Respec: Efficient Online Multiprocessor Replayvia Speculation and External Determinism},
 volume = {45},
 year = {2010}
}


@article{Zhuravlev:2010:ASR:1735971.1736036,
 abstract = {Contention for shared resources on multicore processors remains an unsolved problem in existing systems despite significant research efforts dedicated to this problem in the past. Previous solutions focused primarily on hardware techniques and software page coloring to mitigate this problem. Our goal is to investigate how and to what extent contention for shared resource can be mitigated via thread scheduling. Scheduling is an attractive tool, because it does not require extra hardware and is relatively easy to integrate into the system. Our study is the first to provide a comprehensive analysis of contention-mitigating techniques that use only scheduling. The most difficult part of the problem is to find a classification scheme for threads, which would determine how they affect each other when competing for shared resources. We provide a comprehensive analysis of such classification schemes using a newly proposed methodology that enables to evaluate these schemes separately from the scheduling algorithm itself and to compare them to the optimal. As a result of this analysis we discovered a classification scheme that addresses not only contention for cache space, but contention for other shared resources, such as the memory controller, memory bus and prefetching hardware. To show the applicability of our analysis we design a new scheduling algorithm, which we prototype at user level, and demonstrate that it performs within 2\% of the optimal. We also conclude that the highest impact of contention-aware scheduling techniques is not in improving performance of a workload as a whole but in improving quality of service or performance isolation for individual applications.},
 acmid = {1736036},
 address = {New York, NY, USA},
 author = {Zhuravlev, Sergey and Blagodurov, Sergey and Fedorova, Alexandra},
 doi = {10.1145/1735971.1736036},
 issn = {0362-1340},
 issue_date = {March 2010},
 journal = {SIGPLAN Not.},
 keyword = {multicore processors, scheduling, shared resource contention},
 link = {http://doi.acm.org/10.1145/1735971.1736036},
 month = {mar},
 number = {3},
 numpages = {14},
 pages = {129--142},
 publisher = {ACM},
 title = {Addressing Shared Resource Contention in Multicore Processors via Scheduling},
 volume = {45},
 year = {2010}
}


@article{Johnson:2010:DCM:1735970.1736035,
 abstract = {Many parallel applications exhibit unpredictable communication between threads, leading to contention for shared objects. The choice of contention management strategy impacts strongly the performance and scalability of these applications: spinning provides maximum performance but wastes significant processor resources, while blocking-based approaches conserve processor resources but introduce high overheads on the critical path of computation. Under situations of high or changing load, the operating system complicates matters further with arbitrary scheduling decisions which often preempt lock holders, leading to long serialization delays until the preempted thread resumes execution. We observe that contention management is orthogonal to the problems of scheduling and load management and propose to decouple them so each may be solved independently and effectively. To this end, we propose a load control mechanism which manages the number of active threads in the system separately from any contention which may exist. By isolating contention management from damaging interactions with the OS scheduler, we combine the efficiency of spinning with the robustness of blocking. The proposed load control mechanism results in stable, high performance for both lightly and heavily loaded systems, requires no special privileges or modifications at the OS level, and can be implemented as a library which benefits existing code.},
 acmid = {1736035},
 address = {New York, NY, USA},
 author = {Johnson, F. Ryan and Stoica, Radu and Ailamaki, Anastasia and Mowry, Todd C.},
 doi = {10.1145/1735970.1736035},
 issn = {0163-5964},
 issue_date = {March 2010},
 journal = {SIGARCH Comput. Archit. News},
 keyword = {blocking, concurrency control, contention, load management, multicore, scheduling, spinning, threads},
 link = {http://doi.acm.org/10.1145/1735970.1736035},
 month = {mar},
 number = {1},
 numpages = {12},
 pages = {117--128},
 publisher = {ACM},
 title = {Decoupling Contention Management from Scheduling},
 volume = {38},
 year = {2010}
}


@article{Mesa-Martinez:2010:CPT:1735971.1736043,
 abstract = {Temperature is a dominant factor in the performance, reliability, and leakage power consumption of modern processors. As a result, increasing numbers of researchers evaluate thermal characteristics in their proposals. In this paper, we measure a real processor focusing on its thermal characterization executing diverse workloads. Our results show that in real designs, thermal transients operate at larger scales than their performance and power counterparts. Conventional thermal simulation methodologies based on profile-based simulation or statistical sampling, such as Simpoint, tend to explore very limited execution spans. Short simulation times can lead to reduced matchings between performance and thermal phases. To illustrate these issues we characterize and classify from a thermal standpoint SPEC00 and SPEC06 applications, which are traditionally used in the evaluation of architectural proposals. This paper concludes with a list of recommendations regarding thermal modeling considerations based on our experimental insights.},
 acmid = {1736043},
 address = {New York, NY, USA},
 author = {Mesa-Martinez, Francisco Javier and Ardestani, Ehsan K. and Renau, Jose},
 doi = {10.1145/1735971.1736043},
 issn = {0362-1340},
 issue_date = {March 2010},
 journal = {SIGPLAN Not.},
 keyword = {microarchitecture, temperature, thermal simulation},
 link = {http://doi.acm.org/10.1145/1735971.1736043},
 month = {mar},
 number = {3},
 numpages = {12},
 pages = {193--204},
 publisher = {ACM},
 title = {Characterizing Processor Thermal Behavior},
 volume = {45},
 year = {2010}
}


@inproceedings{Venkatesh:2010:CCR:1736020.1736044,
 abstract = {Growing transistor counts, limited power budgets, and the breakdown of voltage scaling are currently conspiring to create a utilization wall that limits the fraction of a chip that can run at full speed at one time. In this regime, specialized, energy-efficient processors can increase parallelism by reducing the per-computation power requirements and allowing more computations to execute under the same power budget. To pursue this goal, this paper introduces conservation cores. Conservation cores, or c-cores, are specialized processors that focus on reducing energy and energy-delay instead of increasing performance. This focus on energy makes c-cores an excellent match for many applications that would be poor candidates for hardware acceleration (e.g., irregular integer codes). We present a toolchain for automatically synthesizing c-cores from application source code and demonstrate that they can significantly reduce energy and energy-delay for a wide range of applications. The c-cores support patching, a form of targeted reconfigurability, that allows them to adapt to new versions of the software they target. Our results show that conservation cores can reduce energy consumption by up to 16.0x for functions and by up to 2.1x for whole applications, while patching can extend the useful lifetime of individual c-cores to match that of conventional processors.},
 acmid = {1736044},
 address = {New York, NY, USA},
 author = {Venkatesh, Ganesh and Sampson, Jack and Goulding, Nathan and Garcia, Saturnino and Bryksin, Vladyslav and Lugo-Martinez, Jose and Swanson, Steven and Taylor, Michael Bedford},
 booktitle = {Proceedings of the Fifteenth Edition of ASPLOS on Architectural Support for Programming Languages and Operating Systems},
 doi = {10.1145/1736020.1736044},
 isbn = {978-1-60558-839-1},
 keyword = {conservation core, heterogeneous many-core, patching, utilization wall},
 link = {http://doi.acm.org/10.1145/1736020.1736044},
 location = {Pittsburgh, Pennsylvania, USA},
 numpages = {14},
 pages = {205--218},
 publisher = {ACM},
 series = {ASPLOS XV},
 title = {Conservation Cores: Reducing the Energy of Mature Computations},
 year = {2010}
}


@article{Eyerman:2010:PJS:1735971.1736033,
 abstract = {Symbiotic job scheduling boosts simultaneous multithreading (SMT) processor performance by co-scheduling jobs that have `compatible' demands on the processor's shared resources. Existing approaches however require a sampling phase, evaluate a limited number of possible co-schedules, use heuristics to gauge symbiosis, are rigid in their optimization target, and do not preserve system-level priorities/shares. This paper proposes probabilistic job symbiosis modeling, which predicts whether jobs will create positive or negative symbiosis when co-scheduled without requiring the co-schedule to be evaluated. The model, which uses per-thread cycle stacks computed through a previously proposed cycle accounting architecture, is simple enough to be used in system software. Probabilistic job symbiosis modeling provides six key innovations over prior work in symbiotic job scheduling: (i) it does not require a sampling phase, (ii) it readjusts the job co-schedule continuously, (iii) it evaluates a large number of possible co-schedules at very low overhead, (iv) it is not driven by heuristics, (v) it can optimize a performance target of interest (e.g., system throughput or job turnaround time), and (vi) it preserves system-level priorities/shares. These innovations make symbiotic job scheduling both practical and effective. Our experimental evaluation, which assumes a realistic scenario in which jobs come and go, reports an average 16% (and up to 35%) reduction in job turnaround time compared to the previously proposed SOS (sample, optimize, symbios) approach for a two-thread SMT processor, and an average 19% (and up to 45%) reduction in job turnaround time for a four-thread SMT processor.},
 acmid = {1736033},
 address = {New York, NY, USA},
 author = {Eyerman, Stijn and Eeckhout, Lieven},
 doi = {10.1145/1735971.1736033},
 issn = {0362-1340},
 issue_date = {March 2010},
 journal = {SIGPLAN Not.},
 keyword = {performance modeling, simultaneous multi-threading (smt), symbiotic job scheduling},
 link = {http://doi.acm.org/10.1145/1735971.1736033},
 month = {mar},
 number = {3},
 numpages = {12},
 pages = {91--102},
 publisher = {ACM},
 title = {Probabilistic Job Symbiosis Modeling for SMT Processor Scheduling},
 volume = {45},
 year = {2010}
}


@inproceedings{Harris:2010:DFM:1736020.1736027,
 abstract = {This paper introduces a new abstraction to accelerate the read-barriers and write-barriers used by language runtime systems. We exploit the fact that, dynamically, many barrier executions perform checks but no real work -- e.g., in generational garbage collection (GC), frequent checks are needed to detect the creation of inter-generational references, even though such references occur rarely in many workloads. We introduce a form of dynamic filtering that identifies redundant checks by (i) recording checks that have recently been executed, and (ii) detecting when a barrier is repeating one of these checks. We show how this technique can be applied to a variety of algorithms for GC, transactional memory, and language-based security. By supporting dynamic filtering in the instruction set, we show that the fast-paths of these barriers can be streamlined, reducing the impact on the quality of surrounding code. We show how we accelerate the barriers used for generational GC and transactional memory in the Bartok research compiler. With a 2048-entry filter, dynamic filtering eliminates almost all the overhead of the GC write-barriers. Dynamic filtering eliminates around half the overhead of STM over a non-synchronized baseline -- even when used with an STM that is already designed for low overhead, and which employs static analyses to avoid redundant operations.},
 acmid = {1736027},
 address = {New York, NY, USA},
 author = {Harris, Tim and Tomic, Sa\v{s}a and Cristal, Adri\'{a}n and Unsal, Osman},
 booktitle = {Proceedings of the Fifteenth Edition of ASPLOS on Architectural Support for Programming Languages and Operating Systems},
 doi = {10.1145/1736020.1736027},
 isbn = {978-1-60558-839-1},
 keyword = {garbage collection, runtime systems, transactional memory},
 link = {http://doi.acm.org/10.1145/1736020.1736027},
 location = {Pittsburgh, Pennsylvania, USA},
 numpages = {14},
 pages = {39--52},
 publisher = {ACM},
 series = {ASPLOS XV},
 title = {Dynamic Filtering: Multi-purpose Architecture Support for Language Runtime Systems},
 year = {2010}
}


@article{Sudan:2010:MID:1735970.1736045,
 abstract = {Power consumption and DRAM latencies are serious concerns in modern chip-multiprocessor (CMP or multi-core) based compute systems. The management of the DRAM row buffer can significantly impact both power consumption and latency. Modern DRAM systems read data from cell arrays and populate a row buffer as large as 8 KB on a memory request. But only a small fraction of these bits are ever returned back to the CPU. This ends up wasting energy and time to read (and subsequently write back) bits which are used rarely. Traditionally, an open-page policy has been used for uni-processor systems and it has worked well because of spatial and temporal locality in the access stream. In future multi-core processors, the possibly independent access streams of each core are interleaved, thus destroying the available locality and significantly under-utilizing the contents of the row buffer. In this work, we attempt to improve row-buffer utilization for future multi-core systems. The schemes presented here are motivated by our observations that a large number of accesses within heavily accessed OS pages are to small, contiguous "chunks" of cache blocks. Thus, the co-location of chunks (from different OS pages) in a row-buffer will improve the overall utilization of the row buffer contents, and consequently reduce memory energy consumption and access time. Such co-location can be achieved in many ways, notably involving a reduction in OS page size and software or hardware assisted migration of data within DRAM. We explore these mechanisms and discuss the trade-offs involved along with energy and performance improvements from each scheme. On average, for applications with room for improvement, our best performing scheme increases performance by 9% (max. 18%) and reduces memory energy consumption by 15% (max. 70%).},
 acmid = {1736045},
 address = {New York, NY, USA},
 author = {Sudan, Kshitij and Chatterjee, Niladrish and Nellans, David and Awasthi, Manu and Balasubramonian, Rajeev and Davis, Al},
 doi = {10.1145/1735970.1736045},
 issn = {0163-5964},
 issue_date = {March 2010},
 journal = {SIGARCH Comput. Archit. News},
 keyword = {data placement, dram row-buffer management},
 link = {http://doi.acm.org/10.1145/1735970.1736045},
 month = {mar},
 number = {1},
 numpages = {12},
 pages = {219--230},
 publisher = {ACM},
 title = {Micro-pages: Increasing DRAM Efficiency with Locality-aware Data Placement},
 volume = {38},
 year = {2010}
}


@inproceedings{Romanescu:2010:SDV:1736020.1736057,
 abstract = {Computer systems with virtual memory are susceptible to design bugs and runtime faults in their address translation (AT) systems. Detecting bugs and faults requires a clear specification of correct behavior. To address this need, we develop a framework for AT-aware memory consistency models. We expand and divide memory consistency into the physical address memory consistency (PAMC) model that defines the behavior of operations on physical addresses and the virtual address memory consistency (VAMC) model that defines the behavior of operations on virtual addresses. As part of this expansion, we show what AT features are required to bridge the gap between PAMC and VAMC. Based on our AT-aware memory consistency specifications, we design efficient dynamic verification hardware that can detect violations of VAMC and thus detect the effects of design bugs and runtime faults, including most AT related bugs in published errata.},
 acmid = {1736057},
 address = {New York, NY, USA},
 author = {Romanescu, Bogdan F. and Lebeck, Alvin R. and Sorin, Daniel J.},
 booktitle = {Proceedings of the Fifteenth Edition of ASPLOS on Architectural Support for Programming Languages and Operating Systems},
 doi = {10.1145/1736020.1736057},
 isbn = {978-1-60558-839-1},
 keyword = {address translation, dynamic verification, memory consistency, virtual memory},
 link = {http://doi.acm.org/10.1145/1736020.1736057},
 location = {Pittsburgh, Pennsylvania, USA},
 numpages = {12},
 pages = {323--334},
 publisher = {ACM},
 series = {ASPLOS XV},
 title = {Specifying and Dynamically Verifying Address Translation-aware Memory Consistency},
 year = {2010}
}


@article{Kirman:2010:PAO:1735970.1736024,
 abstract = {We present an all-optical approach to constructing data networks on chip that combines the following key features: (1) Wavelength-based routing, where the route followed by a packet depends solely on the wavelength of its carrier signal, and not on information either contained in the packet or traveling along with it. (2) Oblivious routing, by which the wavelength (and thus the route) employed to connect a source-destination pair is invariant for that pair, and does not depend on ongoing transmissions by other nodes, thereby simplifying design and operation. And (3) passive optical wavelength routers, whose routing pattern is set at design time, which allows for area and power optimizations not generally available to solutions that use dynamic routing. Compared to prior proposals, our evaluation shows that our solution is significantly more power efficient at a similar level of performance.},
 acmid = {1736024},
 address = {New York, NY, USA},
 author = {Kirman, Nevin and Mart\'{\i}nez, Jos{\'e} F.},
 doi = {10.1145/1735970.1736024},
 issn = {0163-5964},
 issue_date = {March 2010},
 journal = {SIGARCH Comput. Archit. News},
 keyword = {nanophotonics, on-chip network, optical network, wavelength-based oblivious routing},
 link = {http://doi.acm.org/10.1145/1735970.1736024},
 month = {mar},
 number = {1},
 numpages = {14},
 pages = {15--28},
 publisher = {ACM},
 title = {A Power-efficient All-optical On-chip Interconnect Using Wavelength-based Oblivious Routing},
 volume = {38},
 year = {2010}
}


@article{Yuan:2010:SED:1735970.1736038,
 abstract = {Computer systems often fail due to many factors such as software bugs or administrator errors. Diagnosing such production run failures is an important but challenging task since it is difficult to reproduce them in house due to various reasons: (1) unavailability of users' inputs and file content due to privacy concerns; (2) difficulty in building the exact same execution environment; and (3) non-determinism of concurrent executions on multi-processors. Therefore, programmers often have to diagnose a production run failure based on logs collected back from customers and the corresponding source code. Such diagnosis requires expert knowledge and is also too time-consuming, tedious to narrow down root causes. To address this problem, we propose a tool, called SherLog, that analyzes source code by leveraging information provided by run-time logs to infer what must or may have happened during the failed production run. It requires neither re-execution of the program nor knowledge on the log's semantics. It infers both control and data value information regarding to the failed execution. We evaluate SherLog with 8 representative real world software failures (6 software bugs and 2 configuration errors) from 7 applications including 3 servers. Information inferred by SherLog are very useful for programmers to diagnose these evaluated failures. Our results also show that SherLog can analyze large server applications such as Apache with thousands of logging messages within only 40 minutes.},
 acmid = {1736038},
 address = {New York, NY, USA},
 author = {Yuan, Ding and Mai, Haohui and Xiong, Weiwei and Tan, Lin and Zhou, Yuanyuan and Pasupathy, Shankar},
 doi = {10.1145/1735970.1736038},
 issn = {0163-5964},
 issue_date = {March 2010},
 journal = {SIGARCH Comput. Archit. News},
 keyword = {failure diagnostics, log, static analysis},
 link = {http://doi.acm.org/10.1145/1735970.1736038},
 month = {mar},
 number = {1},
 numpages = {12},
 pages = {143--154},
 publisher = {ACM},
 title = {SherLog: Error Diagnosis by Connecting Clues from Run-time Logs},
 volume = {38},
 year = {2010}
}


