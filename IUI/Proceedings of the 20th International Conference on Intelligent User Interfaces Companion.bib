@inproceedings{Irwin:2015:PHE:2732158.2732163,
 abstract = {Much discussion has taken place regarding environmental sustainability, fossil fuels and other efforts to reverse the trend of global climate change. Unfortunately, individuals often choose the path of least resistance when making home energy decisions. Thus, it is imperative to consider all of the underlying causes that influence home energy consumption in an effort to build a more perceptive interface that addresses the variety of household occupant needs. This research seeks to explore the dynamic nature of household occupancy, individual comfort and situational variants that impact home energy consumption in an effort to discover critical design factors for building novel interfaces for home energy systems.},
 acmid = {2732163},
 address = {New York, NY, USA},
 author = {Irwin, Germaine},
 booktitle = {Proceedings of the 20th International Conference on Intelligent User Interfaces Companion},
 doi = {10.1145/2732158.2732163},
 isbn = {978-1-4503-3308-5},
 keyword = {consumption, context, design, design kit, energy, guides, hci, instructions},
 link = {http://doi.acm.org/10.1145/2732158.2732163},
 location = {Atlanta, Georgia, USA},
 numpages = {4},
 pages = {117--120},
 publisher = {ACM},
 series = {IUI Companion '15},
 title = {Perceptive Home Energy Interfaces: Navigating the Dynamic Household Structure},
 year = {2015}
}


@proceedings{Brdiczka:2015:2732158,
 abstract = {It is with great pleasure that we welcome you to the 2015 ACM International Conference on Intelligent User Interfaces -- ACM IUI 2015 in Atlanta, Georgia, USA. Starting in 1993, ACM IUI is now in its 20th edition and serves as a premier international forum for reporting outstanding research and development on intelligent user interfaces. ACM IUI is where the Human-Computer Interaction (HCI) community meets the Artificial Intelligence (AI) community. We are also very interested in contributions from related fields, such as psychology, behavioral science, cognitive science, computer graphics, design, the arts, etc. At ACM IUI, we focus on the interaction between machine intelligence and human intelligence. While other conferences focus on one side or the other, we address the complex interaction between the two. We welcome research that explores how to make the interaction between computers and people smarter, which may leverage solutions from data mining, knowledge representation, novel interaction paradigms, and emerging technologies. For the 20th edition of the conference, the call for papers attracted a record number of 205 long and short paper submissions, 41 poster paper submissions, 18 demo paper submissions, and 24 student consortium submissions. The final program of the conference includes 3 keynotes, 47 long and short papers (22.9% acceptance rate), 17 posters, 8 demos, 4 workshops, 3 tutorials, and 12 student consortium papers. The program opens with a keynote by Professor Dan Weld on "Intelligent Control of Crowdsourcing", continues with a keynote by Ed Chi on "Blurring of the Boundary Between Interactive Search and Recommendation" on the second day, and ends with a keynote by Rosalind Picard on "Recognizing Stress, Engagement, and Positive Emotion". Four workshops (Personalized Access to Cultural Heritage, Intelligent Digital Games for Empowerment and Inclusion, Interacting with Smart Objects, and Visual Text Analytics) and three tutorials (Speech-based Interaction, Personalization for Behavior Change, and User Affect and Sentiment Modeling) complement the main conference program.},
 address = {New York, NY, USA},
 isbn = {978-1-4503-3308-5},
 location = {Atlanta, Georgia, USA},
 publisher = {ACM},
 title = {IUI Companion '15: Proceedings of the 20th International Conference on Intelligent User Interfaces Companion},
 year = {2015}
}


@inproceedings{Cassani:2015:MOS:2732158.2732193,
 abstract = {The past few years have seen the availability of consumer electroencephalography (EEG) devices increase significantly. These devices, usually with a compact form factor and affordable price, now allow researchers and enthusiasts to use EEG in various new contexts and environments. However, the many consumer headsets often require extensive programming experience to interface with their respective drivers; moreover, standardization of the access and recording of EEG data across the devices still remains to be done. Consequently, a tool is needed to facilitate the recording and streaming of EEG data from consumer headsets that can easily be interfaced with different programming languages and software, and that allows interchangeability between devices. This paper describes the open source MuSAE Lab EEG Server (MuLES), an EEG acquisition and streaming server that aims at creating a standard interface for portable EEG headsets, in order to accelerate the development of brain-computer interfaces (BCIs) and of general EEG use in novel contexts. In addition to the EEG server interface which currently supports five different consumer devices and session playback, clients are developed for use on different platforms and in various programming languages, making prototyping and recording a quick and simple task. To validate the functionality and usability of the EEG server, a use case highlighting its main features is presented. The developed tool simplifies the acquisition and recording of EEG data from portable consumer devices by providing a single efficient interface, with applications in areas such as basic and behavioural research, prototyping, neurogaming, live performance and arts.},
 acmid = {2732193},
 address = {New York, NY, USA},
 author = {Cassani, Raymundo and Banville, Hubert and Falk, Tiago H.},
 booktitle = {Proceedings of the 20th International Conference on Intelligent User Interfaces Companion},
 doi = {10.1145/2732158.2732193},
 isbn = {978-1-4503-3308-5},
 keyword = {brain-computer interface (bci), electroencephalography (eeg), neurofeedback, neurogaming, portable eeg},
 link = {http://doi.acm.org/10.1145/2732158.2732193},
 location = {Atlanta, Georgia, USA},
 numpages = {4},
 pages = {9--12},
 publisher = {ACM},
 series = {IUI Companion '15},
 title = {MuLES: An Open Source EEG Acquisition and Streaming Server for Quick and Simple Prototyping and Recording},
 year = {2015}
}


@inproceedings{Pollmann:2015:RED:2732158.2732161,
 abstract = {Our research explores possibilities to apply neurophysiological methods for real-time emotion detection during human-technology interaction. The present paper outlines our scientific approach, research plan and methodology.},
 acmid = {2732161},
 address = {New York, NY, USA},
 author = {Pollmann, Kathrin},
 booktitle = {Proceedings of the 20th International Conference on Intelligent User Interfaces Companion},
 doi = {10.1145/2732158.2732161},
 isbn = {978-1-4503-3308-5},
 keyword = {adaptive systems, affective computing, brain-computer interface, eeg, emotions, fnirs, neurophysiological methods},
 link = {http://doi.acm.org/10.1145/2732158.2732161},
 location = {Atlanta, Georgia, USA},
 numpages = {4},
 pages = {109--112},
 publisher = {ACM},
 series = {IUI Companion '15},
 title = {Real-Time Emotion Detection for Neuro-Adaptive Systems},
 year = {2015}
}


@inproceedings{Zheng:2015:RIC:2732158.2732167,
 abstract = {In contrast to traditional recommender systems (RS), context-aware recommender systems (CARS) emerged to adapt to users' preferences in various contextual situations. During those years, different context-aware recommendation algorithms have been developed and they are able to demonstrate the effectiveness of CARS. However, this field has yet to agree on the definition of context, where researchers may incorporate diversified variables (e.g., user profiles or item features), which further creates confusions between content-based RS and context-based RS, and positions the problem of context identification in CARS. In this paper, we revisit the definition of contexts in recommender systems, and propose a context identification framework to clarify the preliminary selection of contextual variables, which may further assist interpretation of contextual effects in RS.},
 acmid = {2732167},
 address = {New York, NY, USA},
 author = {Zheng, Yong},
 booktitle = {Proceedings of the 20th International Conference on Intelligent User Interfaces Companion},
 doi = {10.1145/2732158.2732167},
 isbn = {978-1-4503-3308-5},
 keyword = {context, context-aware recommendation, contextual},
 link = {http://doi.acm.org/10.1145/2732158.2732167},
 location = {Atlanta, Georgia, USA},
 numpages = {4},
 pages = {133--136},
 publisher = {ACM},
 series = {IUI Companion '15},
 title = {A Revisit to The Identification of Contexts in Recommender Systems},
 year = {2015}
}


@inproceedings{Dey:2015:KYS:2732158.2732169,
 abstract = {The advancement of mobile technology inspired research communities to achieve centimeter level accuracy in indoor positioning systems [2]. But to get the best out of it, we need assisting navigation applications that will not only help us to reach the destination quickly but will also make us familiar with the surroundings. To address this concern, we propose a two stage approach which can help pedestrians navigate in an indoor location and simultaneously enhance their spatial awareness. In the first stage, we will conduct a behavioral user study to identify the prominent behavioral patterns during different navigational challenges. Once the behavioral state model is prepared, we need to analyze the sensor data in multiple dimensions and build a dynamic sensor state model. This model will enable us to map the behavioral state model to the sensor states and draw a direct one to one relation between the two.},
 acmid = {2732169},
 address = {New York, NY, USA},
 author = {Dey, Sanorita},
 booktitle = {Proceedings of the 20th International Conference on Intelligent User Interfaces Companion},
 doi = {10.1145/2732158.2732169},
 isbn = {978-1-4503-3308-5},
 keyword = {behavioral state model, navigation applications, sensors, spatial knowledge},
 link = {http://doi.acm.org/10.1145/2732158.2732169},
 location = {Atlanta, Georgia, USA},
 numpages = {4},
 pages = {141--144},
 publisher = {ACM},
 series = {IUI Companion '15},
 title = {Know Your Surroundings with an Interactive Map},
 year = {2015}
}


@inproceedings{Wolf:2015:MDS:2732158.2732188,
 abstract = {A sonification is a rendering of audio in response to data, and is used in instances where visual representations of data are impossible, difficult, or unwanted. Designing sonifications often requires knowledge in multiple areas as well as an understanding of how the end users will use the system. This makes it an ideal candidate for end-user development where the user plays a role in the creation of the design. We present a model for sonification that utilizes user-specified examples and data to generate cross-domain mappings from data to sound. As a novel contribution we utilize soundscapes (acoustic scenes) for these user-selected examples to define a structure for the sonification. We demonstrate a proof of concept of our model using sound examples and discuss how we plan to build on this work in the future.},
 acmid = {2732188},
 address = {New York, NY, USA},
 author = {Wolf, KatieAnna E. and Gliner, Genna and Fiebrink, Rebecca},
 booktitle = {Proceedings of the 20th International Conference on Intelligent User Interfaces Companion},
 doi = {10.1145/2732158.2732188},
 isbn = {978-1-4503-3308-5},
 keyword = {cross-domain mappings, end-user development, hci, mutltimedia uis, soundscapes},
 link = {http://doi.acm.org/10.1145/2732158.2732188},
 location = {Atlanta, Georgia, USA},
 numpages = {4},
 pages = {97--100},
 publisher = {ACM},
 series = {IUI Companion '15},
 title = {A Model for Data-Driven Sonification Using Soundscapes},
 year = {2015}
}


@inproceedings{Orlosky:2015:IPE:2732158.2732175,
 abstract = {Recent advances in virtual and augmented reality have led to the development of a number of simulations for different applications. In particular, simulations for monitoring, evaluation, training, and education have started to emerge for the consumer market due to the availability and affordability of immersive display technology. In this work, we introduce a virtual reality environment that provides an immersive traffic simulation designed to observe behavior and monitor relevant skills and abilities of pedestrians who may be at risk, such as elderly persons with cognitive impairments. The system provides basic reactive functionality, such as display of navigation instructions and notifications of dangerous obstacles during navigation tasks. Methods for interaction using hand and arm gestures are also implemented to allow users explore the environment in a more natural manner.},
 acmid = {2732175},
 address = {New York, NY, USA},
 author = {Orlosky, Jason and Weber, Markus and Gu, Yecheng and Sonntag, Daniel and Sosnovsky, Sergey},
 booktitle = {Proceedings of the 20th International Conference on Intelligent User Interfaces Companion},
 doi = {10.1145/2732158.2732175},
 isbn = {978-1-4503-3308-5},
 keyword = {cognitive monitoring, evaluation, interaction, simulation, virtual reality},
 link = {http://doi.acm.org/10.1145/2732158.2732175},
 location = {Atlanta, Georgia, USA},
 numpages = {4},
 pages = {57--60},
 publisher = {ACM},
 series = {IUI Companion '15},
 title = {An Interactive Pedestrian Environment Simulator for Cognitive Monitoring and Evaluation},
 year = {2015}
}


@inproceedings{Mutlu:2015:VTR:2732158.2732190,
 abstract = {Identifying and using the information from distributed and heterogeneous information sources is a challenging task in many application fields. Even with services that offer well-defined structured content, such as digital libraries, it becomes increasingly difficult for a user to find the desired information. To cope with an overloaded information space, we propose a novel approach - VizRec - combining recommender systems (RS) and visualizations. VizRec suggests personalized visual representations for recommended data. One important aspect of our contribution and a prerequisite for VizRec are user preferences that build a personalization model. We present a crowd based evaluation and show how such a model of preferences can be elicited.},
 acmid = {2732190},
 address = {New York, NY, USA},
 author = {Mutlu, Belgin and Veas, Eduardo and Trattner, Christoph and Sabol, Vedran},
 booktitle = {Proceedings of the 20th International Conference on Intelligent User Interfaces Companion},
 doi = {10.1145/2732158.2732190},
 isbn = {978-1-4503-3308-5},
 keyword = {collaborative filtering, crowd-based experiment, recommender systems, visualization recommendation},
 link = {http://doi.acm.org/10.1145/2732158.2732190},
 location = {Atlanta, Georgia, USA},
 numpages = {4},
 pages = {49--52},
 publisher = {ACM},
 series = {IUI Companion '15},
 title = {VizRec: A Two-Stage Recommender System for Personalized Visualizations},
 year = {2015}
}


@inproceedings{Long:2015:ICV:2732158.2732177,
 abstract = {Recently, there has been research on inferring user emotions. Like other inference research, it requires an iterative process in which what-if scenarios are played with different features and algorithms. Traditional, general-purpose data mining tools such as Weka have played an important part in promoting this process. We have augmented this toolset with an additional interactive test-bed designed for prediction and communication of programmer difficulties from user-interface commands. It provides end-user interfaces for communicating, correcting, and reacting to the predictions. In addition, it offers researchers user-interfaces for interacting with the prediction process as it is executed rather than, as in traditional mining tools, after it has generated data for a set of experimental subjects. These user-interfaces can be used to determine key elements of the prediction process, why certain wrong or right predictions have been made, and change parameters of the process. A video demonstration this work is available at http://youtu.be/09LpDIPG5h8.},
 acmid = {2732177},
 address = {New York, NY, USA},
 author = {Long, Duri and Dillon, Nicholas and Wang, Kun and Carter, Jason and Dewan, Prasun},
 booktitle = {Proceedings of the 20th International Conference on Intelligent User Interfaces Companion},
 doi = {10.1145/2732158.2732177},
 isbn = {978-1-4503-3308-5},
 keyword = {recommender systems},
 link = {http://doi.acm.org/10.1145/2732158.2732177},
 location = {Atlanta, Georgia, USA},
 numpages = {4},
 pages = {25--28},
 publisher = {ACM},
 series = {IUI Companion '15},
 title = {Interactive Control and Visualization of Difficulty Inferences from User-Interface Commands},
 year = {2015}
}


@inproceedings{Maus:2015:SOA:2732158.2732185,
 abstract = {We present results from a survey of adults, 63 and older, about the potential implementation of a recommender system within a digital library of health-related content. We studied how these older adults perceive the idea of a recommender system and different aspects of its design. We presented four different types of recommender systems in the survey and our results indicate that this group would prefer a system based on explicit feedback in the form of ratings that measure the helpfulness of content. Reinforcing previous research, we learned this group is interested in a system that explains why it recommended content and they do not want to spend much time creating a profile of interests to warm the system. We discuss where we would use this recommender system, how we designed the survey for our audience, and plans for future studies on this subject.},
 acmid = {2732185},
 address = {New York, NY, USA},
 author = {Maus, Adam N. and Atwood, Amy K.},
 booktitle = {Proceedings of the 20th International Conference on Intelligent User Interfaces Companion},
 doi = {10.1145/2732158.2732185},
 isbn = {978-1-4503-3308-5},
 keyword = {digital library recommender systems, older adults, recommender interfaces, user study},
 link = {http://doi.acm.org/10.1145/2732158.2732185},
 location = {Atlanta, Georgia, USA},
 numpages = {4},
 pages = {41--44},
 publisher = {ACM},
 series = {IUI Companion '15},
 title = {Surveying Older Adults About a Recommender System for a Digital Library},
 year = {2015}
}


@inproceedings{Spiliopoulou:2015:ISB:2732158.2732182,
 abstract = {In Biologically Inspired Design (BID), engineers use biology as a source of ideas for solving engineering problems. However, locating relevant literature is difficult due to vocabulary differences and lack of domain knowledge. IBID is an intelligent search mechanism that uses a functional taxonomy to direct search and a formal modeling notation for annotating relevant search targets.},
 acmid = {2732182},
 address = {New York, NY, USA},
 author = {Spiliopoulou, Evangelia and Rugaber, Spencer and Goel, Ashok and Chen, Lianghao and Wiltgen, Bryan and Jagannathan, Arvind Krishnaa},
 booktitle = {Proceedings of the 20th International Conference on Intelligent User Interfaces Companion},
 doi = {10.1145/2732158.2732182},
 isbn = {978-1-4503-3308-5},
 keyword = {biologically inspired design, intelligent search, natural language processing},
 link = {http://doi.acm.org/10.1145/2732158.2732182},
 location = {Atlanta, Georgia, USA},
 numpages = {4},
 pages = {77--80},
 publisher = {ACM},
 series = {IUI Companion '15},
 title = {Intelligent Search for Biologically Inspired Design},
 year = {2015}
}


@inproceedings{Pienta:2015:IQO:2732158.2732192,
 abstract = {Given the explosive growth of modern graph data, new methods are needed that allow for the querying of complex graph structures without the need of a complicated querying languages; in short, interactive graph querying is desirable. We describe our work towards achieving our overall research goal of designing and developing an interactive querying system for large network data. We focus on three critical aspects: scalable data mining algorithms, graph visualization, and interaction design. We have already completed an approximate subgraph matching system called MAGE in our previous work that fulfills the algorithmic foundation allowing us to query a graph with hundreds of millions of edges. Our preliminary work on visual graph querying, Graphite, was the first step in the process to making an interactive graph querying system. We are in the process of designing the graph visualization and robust interaction needed to make truly interactive graph querying a reality.},
 acmid = {2732192},
 address = {New York, NY, USA},
 author = {Pienta, Robert and Tamersoy, Acar and Tong, Hanghang and Endert, Alex and Chau, Duen Horng (Polo)},
 booktitle = {Proceedings of the 20th International Conference on Intelligent User Interfaces Companion},
 doi = {10.1145/2732158.2732192},
 isbn = {978-1-4503-3308-5},
 keyword = {graph querying and mining, interaction design, visualization},
 link = {http://doi.acm.org/10.1145/2732158.2732192},
 location = {Atlanta, Georgia, USA},
 numpages = {4},
 pages = {61--64},
 publisher = {ACM},
 series = {IUI Companion '15},
 title = {Interactive Querying over Large Network Data: Scalability, Visualization, and Interaction Design},
 year = {2015}
}


@inproceedings{Costa:2015:OIO:2732158.2732164,
 abstract = {This thesis addresses the suitability of body interaction techniques, when used by persons with different levels of visual impairment, to improve the accessibility of mobile devices. The research will focus on: understanding how on-body interaction can surpass the current accessibility levels of mobile devices, characterizing the different complexities of skin mapping for different levels of visual impairment, perceiving for what types of input tasks it is best suited, and studying how it can complement other input modalities. Results will include an on-body interaction model, and several prototypes and studies characterizing body interaction.},
 acmid = {2732164},
 address = {New York, NY, USA},
 author = {Costa, David},
 booktitle = {Proceedings of the 20th International Conference on Intelligent User Interfaces Companion},
 doi = {10.1145/2732158.2732164},
 isbn = {978-1-4503-3308-5},
 keyword = {accessibility, body interaction model, mobile devices, visual impairments},
 link = {http://doi.acm.org/10.1145/2732158.2732164},
 location = {Atlanta, Georgia, USA},
 numpages = {4},
 pages = {121--124},
 publisher = {ACM},
 series = {IUI Companion '15},
 title = {On-Body Interaction for Optimized Accessibility},
 year = {2015}
}


@inproceedings{Kim:2015:ODE:2732158.2732186,
 abstract = {This study explores the contextual transparency of information presented on public transparent user interfaces through the maintenance of adequate legibility. To address this issue, we investigate the relationship between the information and transparency in a shop context. In this paper, we present an experiment which examines the effects of transparency, changing user's proximity, and information types on legibility with a public transparent information system. We report significant effects on performance and legibility, and the results indicate the different contextual transparency related to the user's proximity, depending on whether the user focuses on the information or the environment. In addition, under the 50% transparency (25% and 50% levels) fit into the closer proximity while the 50% transparency offers more harmonious view in a distant context. The implications of these results to the usability of public transparent user interfaces and design recommendations are also discussed.},
 acmid = {2732186},
 address = {New York, NY, USA},
 author = {Kim, Heesun and Huh, Bo Kyung and Im, Seung Hyen and Joung, Hae Youn and Kwon, Gyu Hyun and Park, Ji-Hyung},
 booktitle = {Proceedings of the 20th International Conference on Intelligent User Interfaces Companion},
 doi = {10.1145/2732158.2732186},
 isbn = {978-1-4503-3308-5},
 keyword = {contextual transparency, legibility, public transparent interfaces, transparent displays},
 link = {http://doi.acm.org/10.1145/2732158.2732186},
 location = {Atlanta, Georgia, USA},
 numpages = {4},
 pages = {33--36},
 publisher = {ACM},
 series = {IUI Companion '15},
 title = {From "Overview" to "Detail": An Exploration of Contextual Transparency for Public Transparent Interfaces},
 year = {2015}
}


@inproceedings{delaCruz:2015:TIR:2732158.2732180,
 abstract = {Reinforcement learning is a powerful machine learning paradigm that allows agents to autonomously learn to maximize a scalar reward. However, it often suffers from poor initial performance and long learning times. This paper discusses how collecting on-line human feedback, both in real time and post hoc, can potentially improve the performance of such learning systems. We use the game Pac-Man to simulate a navigation setting and show that workers are able to accurately identify both when a sub-optimal action is executed, and what action should have been performed instead. Demonstrating that the crowd is capable of generating this input, and discussing the types of errors that occur, serves as a critical first step in designing systems that use this real-time feedback to improve systems' learning performance on-the-fly.},
 acmid = {2732180},
 address = {New York, NY, USA},
 author = {de la Cruz, Gabriel V. and Peng, Bei and Lasecki, Walter S. and Taylor, Matthew E.},
 booktitle = {Proceedings of the 20th International Conference on Intelligent User Interfaces Companion},
 doi = {10.1145/2732158.2732180},
 isbn = {978-1-4503-3308-5},
 link = {http://doi.acm.org/10.1145/2732158.2732180},
 location = {Atlanta, Georgia, USA},
 numpages = {4},
 pages = {17--20},
 publisher = {ACM},
 series = {IUI Companion '15},
 title = {Towards Integrating Real-Time Crowd Advice with Reinforcement Learning},
 year = {2015}
}


@inproceedings{Jung:2015:EVP:2732158.2732170,
 abstract = {
                  An abstract is not available.
              },
 acmid = {2732170},
 address = {New York, NY, USA},
 author = {Jung, Hee-Tae},
 booktitle = {Proceedings of the 20th International Conference on Intelligent User Interfaces Companion},
 doi = {10.1145/2732158.2732170},
 isbn = {978-1-4503-3308-5},
 keyword = {customized therapy, latent dirichlet allocation, learning from demonstration, programming by demonstration, robot-mediated therapy},
 link = {http://doi.acm.org/10.1145/2732158.2732170},
 location = {Atlanta, Georgia, USA},
 numpages = {4},
 pages = {145--148},
 publisher = {ACM},
 series = {IUI Companion '15},
 title = {Extended Virtual Presence of Therapists Through Home Service Robots},
 year = {2015}
}


@inproceedings{Hoque:2015:VTA:2732158.2732160,
 abstract = {In the last decade, there has been an exponential growth of online conversations thanks to the rise of social media. Analyzing and gaining insights from such conversations can be quite challenging for a user, especially when the discussions become very long. During my doctoral research, I aim to investigate how to integrate Information Visualization with Natural Language Processing techniques to better support the user's task of exploring and analyzing conversations. For this purpose, I consider the following approaches: apply design study methodology in InfoVis to uncover data and task abstractions; apply NLP methods for extracting the identified data to support those tasks; and incorporate human feedback in the text analysis process when the extracted data is noisy and/or may not match the user's mental model, and current tasks. Through a set of design studies, I aim to evaluate the effectiveness of our approaches.},
 acmid = {2732160},
 address = {New York, NY, USA},
 author = {Hoque, Enamul},
 booktitle = {Proceedings of the 20th International Conference on Intelligent User Interfaces Companion},
 doi = {10.1145/2732158.2732160},
 isbn = {978-1-4503-3308-5},
 keyword = {asynchronous conversation, computer mediated communication, interactive topic modeling, text visualization},
 link = {http://doi.acm.org/10.1145/2732158.2732160},
 location = {Atlanta, Georgia, USA},
 numpages = {4},
 pages = {105--108},
 publisher = {ACM},
 series = {IUI Companion '15},
 title = {Visual Text Analytics for Asynchronous Online Conversations},
 year = {2015}
}


@inproceedings{Cunha:2015:ACI:2732158.2732168,
 abstract = {Ambient Assisted Living (AAL) applications aim to allow elderly, sick and disabled people to stay safely at home while collaboratively assisted by their family, friends and medical staff. In principle, AAL amalgamated with Internet of Things (IoT) introduces a new healthcare connectivity paradigm that interconnects mobile apps and sensors allowing constant monitoring of the patient. By hiding technology into light fixtures, in this thesis proposal we present AmbLEDs, a ambient light sensing system, as an alternative to spreading sensors that are perceived as invasive, such as cameras, microphones, microcontrollers, tags or wearables, in order to create a crowdware ubiquitous context-aware interface for recognizing, informing and alerting home environmental changes and human activities to support continuous proactive care.},
 acmid = {2732168},
 address = {New York, NY, USA},
 author = {Cunha, Marcio},
 booktitle = {Proceedings of the 20th International Conference on Intelligent User Interfaces Companion},
 doi = {10.1145/2732158.2732168},
 isbn = {978-1-4503-3308-5},
 keyword = {ambient assisted living, collaborative systems, collective intelligence, crowdware, internet of things, smart light},
 link = {http://doi.acm.org/10.1145/2732158.2732168},
 location = {Atlanta, Georgia, USA},
 numpages = {4},
 pages = {137--140},
 publisher = {ACM},
 series = {IUI Companion '15},
 title = {AmbLEDs: Context-Aware I/O for AAL Systems},
 year = {2015}
}


@inproceedings{Rao:2015:TCP:2732158.2732159,
 abstract = {Picture schematization has shown to be a very important step for a wide range of applications and remains challenging as an active research area. Many automation methods have been proposed to solve this problem but yet far from expectation. Since crowdsourcing has been successfully applied to fill the gap that AI can not reach yet, we are interested to explore how crowdsourcing can be utilized into this area. This paper briefly summarizes our recent efforts towards building a feasible crowd-based system to schematise pictures in a reliable and cost-effective way.},
 acmid = {2732159},
 address = {New York, NY, USA},
 author = {Rao, Huaming},
 booktitle = {Proceedings of the 20th International Conference on Intelligent User Interfaces Companion},
 doi = {10.1145/2732158.2732159},
 isbn = {978-1-4503-3308-5},
 keyword = {crowdsourcing, picture schematization, remote collaboration, spatial},
 link = {http://doi.acm.org/10.1145/2732158.2732159},
 location = {Atlanta, Georgia, USA},
 numpages = {4},
 pages = {101--104},
 publisher = {ACM},
 series = {IUI Companion '15},
 title = {Towards a Crowd-based Picture Schematization System},
 year = {2015}
}


@inproceedings{Fan:2015:MQE:2732158.2732173,
 abstract = {We present MindMiner, a mixed-initiative interface for capturing subjective similarity measurements via a combination of new interaction techniques and machine learning algorithms. MindMiner collects qualitative, hard to express similarity measurements from users via active polling with uncertainty and example based visual constraint creation. MindMiner also formulates human prior knowledge into a set of inequalities and learns a quantitative similarity distance metric via convex optimization. In a 12-participant peer-review understanding task, we found MindMiner was easy to learn and use, and could capture users' implicit knowledge about writing performance and cluster target entities into groups that match subjects' mental models.},
 acmid = {2732173},
 address = {New York, NY, USA},
 author = {Fan, Xiangmin and Liu, Youming and Cao, Nan and Hong, Jason and Wang, Jingtao},
 booktitle = {Proceedings of the 20th International Conference on Intelligent User Interfaces Companion},
 doi = {10.1145/2732158.2732173},
 isbn = {978-1-4503-3308-5},
 keyword = {clustering, convex optimization, machine learning, mixed-initiative interface, visualization},
 link = {http://doi.acm.org/10.1145/2732158.2732173},
 location = {Atlanta, Georgia, USA},
 numpages = {4},
 pages = {93--96},
 publisher = {ACM},
 series = {IUI Companion '15},
 title = {MindMiner: Quantifying Entity Similarity via Interactive Distance Metric Learning},
 year = {2015}
}


@inproceedings{Raybourn:2015:DPS:2732158.2732195,
 abstract = {A hypothetical scenario is utilized to explore privacy and security considerations for intelligent systems, such as a Personal Assistant for Learning (PAL). Two categories of potential concerns are addressed: factors facilitated by user models, and factors facilitated by systems. Among the strategies presented for risk mitigation is a call for ongoing, iterative dialog among privacy, security, and personalization researchers during all stages of development, testing, and deployment.},
 acmid = {2732195},
 address = {New York, NY, USA},
 author = {Raybourn, Elaine M. and Fabian, Nathan and Davis, Warren and Parks, Raymond C. and McClain, Jonathan and Trumbo, Derek and Regan, Damon and Durlach, Paula},
 booktitle = {Proceedings of the 20th International Conference on Intelligent User Interfaces Companion},
 doi = {10.1145/2732158.2732195},
 isbn = {978-1-4503-3308-5},
 keyword = {personalized learning, privacy, security},
 link = {http://doi.acm.org/10.1145/2732158.2732195},
 location = {Atlanta, Georgia, USA},
 numpages = {4},
 pages = {69--72},
 publisher = {ACM},
 series = {IUI Companion '15},
 title = {Data Privacy and Security Considerations for Personal Assistants for Learning (PAL)},
 year = {2015}
}


@inproceedings{Wolf:2015:AEU:2732158.2732165,
 abstract = {In my dissertation I plan to explore the design of digital systems and how we can support users in the design process. Specifically, I focus on the design of sonifications, which are the representation of data using sound. Creating the algorithm that maps data to sound is not an easy task as there are many things to consider: an individual's aesthetic preferences, multiple dimensions of sound, complexities of the data to be represented, and previously developed theories for how to convey information using sound. This makes it an ideal domain for end-user development and data-driven design creation.},
 acmid = {2732165},
 address = {New York, NY, USA},
 author = {Wolf, KatieAnna E.},
 booktitle = {Proceedings of the 20th International Conference on Intelligent User Interfaces Companion},
 doi = {10.1145/2732158.2732165},
 isbn = {978-1-4503-3308-5},
 keyword = {cross-domain mappings, end-user development, hci, mutlimedia uis, sonification, soundscapes},
 link = {http://doi.acm.org/10.1145/2732158.2732165},
 location = {Atlanta, Georgia, USA},
 numpages = {4},
 pages = {125--128},
 publisher = {ACM},
 series = {IUI Companion '15},
 title = {Assisting End Users in the Design of Sonification Systems},
 year = {2015}
}


@inproceedings{Michel:2015:TIO:2732158.2732181,
 abstract = {Although collaborative activities are paramount in science, little attention has been devoted to supporting on-line scientific collaborations. Our work focuses on scientific collaborations that revolve around complex science questions that require significant coordination to synthesize multi-disciplinary findings, enticing contributors to remain engaged for extended periods of time, and continuous growth to accommodate new contributors as needed as the work evolves over time. This paper presents the interface of the Organic Data Science Wiki to address these challenges. Our solution is based on the Semantic MediaWiki and extends it with new features for scientific collaboration. We present preliminary results from the usage of the interface in a pilot research project.},
 acmid = {2732181},
 address = {New York, NY, USA},
 author = {Michel, Felix and Gil, Yolanda and Ratnakar, Varun and Hauder, Matheus},
 booktitle = {Proceedings of the 20th International Conference on Intelligent User Interfaces Companion},
 doi = {10.1145/2732158.2732181},
 isbn = {978-1-4503-3308-5},
 keyword = {collaboration interfaces, organic data science, scientific collaboration, semantic mediawiki},
 link = {http://doi.acm.org/10.1145/2732158.2732181},
 location = {Atlanta, Georgia, USA},
 numpages = {4},
 pages = {45--48},
 publisher = {ACM},
 series = {IUI Companion '15},
 title = {A Task-Centered Interface for On-Line Collaboration in Science},
 year = {2015}
}


@inproceedings{Guo:2015:MIM:2732158.2732166,
 abstract = {Designing intelligent computer interfaces requires human intelligence, which can be captured through multimodal sensors during human-computer interactions. These data modalities may involve users' language, vision, and body signals, which shed light on different aspects of human cognition and behaviors. I propose to integrate multimodal data to more effectively understand users during interactions. Since users' manipulation of big data (e.g., texts, images, videos) through interfaces can be computationally intensive, an interactive machine learning framework will be constructed in an unsupervised manner.},
 acmid = {2732166},
 address = {New York, NY, USA},
 author = {Guo, Xuan},
 booktitle = {Proceedings of the 20th International Conference on Intelligent User Interfaces Companion},
 doi = {10.1145/2732158.2732166},
 isbn = {978-1-4503-3308-5},
 keyword = {interactive machine learning, multimodal data fusion, unsupervised knowledge discovery},
 link = {http://doi.acm.org/10.1145/2732158.2732166},
 location = {Atlanta, Georgia, USA},
 numpages = {4},
 pages = {129--132},
 publisher = {ACM},
 series = {IUI Companion '15},
 title = {Multimodal Interactive Machine Learning for User Understanding},
 year = {2015}
}


@inproceedings{Birnbaum:2015:NCP:2732158.2732178,
 abstract = {We describe intelligent information technologies designed to automatically provide both journalists and ordinary newsreaders with a broad range of the contextual information they need in order to better understand news stories, presented in an immediate and compelling fashion. These systems automatically identify, select, and present appropriate contextual information based on the story a user is currently viewing. Our experiences in building a number of specific systems of this kind have led to the creation of a general architecture and platform for developing such applications. These systems interact with news consumers directly through mechanisms such as browser extensions.},
 acmid = {2732178},
 address = {New York, NY, USA},
 author = {Birnbaum, Larry and Boon, Miriam and Bradley, Scott and Wilson, Jennifer},
 booktitle = {Proceedings of the 20th International Conference on Intelligent User Interfaces Companion},
 doi = {10.1145/2732158.2732178},
 isbn = {978-1-4503-3308-5},
 keyword = {computational journalism, contextual search, intelligent information systems},
 link = {http://doi.acm.org/10.1145/2732158.2732178},
 location = {Atlanta, Georgia, USA},
 numpages = {4},
 pages = {5--8},
 publisher = {ACM},
 series = {IUI Companion '15},
 title = {The News Context Project},
 year = {2015}
}


@proceedings{Brdiczka:2015:2678025,
 abstract = {It is with great pleasure that we welcome you to the 2015 ACM International Conference on Intelligent User Interfaces -- ACM IUI 2015 in Atlanta, Georgia, USA. Starting in 1993, ACM IUI is now in its 20th edition and serves as a premier international forum for reporting outstanding research and development on intelligent user interfaces. ACM IUI is where the Human-Computer Interaction (HCI) community meets the Artificial Intelligence (AI) community. We are also very interested in contributions from related fields, such as psychology, behavioral science, cognitive science, computer graphics, design, the arts, etc. At ACM IUI, we focus on the interaction between machine intelligence and human intelligence. While other conferences focus on one side or the other, we address the complex interaction between the two. We welcome research that explores how to make the interaction between computers and people smarter, which may leverage solutions from data mining, knowledge representation, novel interaction paradigms, and emerging technologies. For the 20th edition of the conference, the call for papers attracted a record number of 205 long and short paper submissions, 41 poster paper submissions, 18 demo paper submissions, and 24 student consortium submissions. The final program of the conference includes 3 keynotes, 47 long and short papers (22.9% acceptance rate), 17 posters, 8 demos, 4 workshops, 3 tutorials, and 12 student consortium papers. The program opens with a keynote by Professor Dan Weld on "Intelligent Control of Crowdsourcing", continues with a keynote by Ed Chi on "Blurring of the Boundary Between Interactive Search and Recommendation" on the second day, and ends with a keynote by Rosalind Picard on "Recognizing Stress, Engagement, and Positive Emotion". Four workshops (Personalized Access to Cultural Heritage, Intelligent Digital Games for Empowerment and Inclusion, Interacting with Smart Objects, and Visual Text Analytics) and three tutorials (Speech-based Interaction, Personalization for Behavior Change, and User Affect and Sentiment Modeling) complement the main conference program.},
 address = {New York, NY, USA},
 isbn = {978-1-4503-3306-1},
 location = {Atlanta, Georgia, USA},
 publisher = {ACM},
 title = {IUI '15: Proceedings of the 20th International Conference on Intelligent User Interfaces},
 year = {2015}
}


@inproceedings{Nelligan:2015:MSE:2732158.2732194,
 abstract = {At the university level, high enrollment numbers in classes can be overwhelming for professors and teaching assistants to manage. Grading assignments and tests for hundreds of students is time consuming and has led towards a push for software-based learning in large university classes. Unfortunately, traditional quantitative question-and-answer mechanisms are often not sufficient for STEM courses, where there is a focus on problem-solving techniques over finding the "right" answers. Working through problems by hand can be important in memory retention, so in order for software learning systems to be effective in STEM courses, they should be able to intelligently understand students' sketches. Mechanix is a sketch-based system that allows students to step through problems designed by their instructors with personalized feedback and optimized interface controls. Optimizations like color-coding, menu bar simplification, and tool consolidation are recent improvements in Mechanix that further the aim to engage and motivate students in learning.},
 acmid = {2732194},
 address = {New York, NY, USA},
 author = {Nelligan, Trevor and Polsley, Seth and Ray, Jaideep and Helms, Michael and Linsey, Julie and Hammond, Tracy},
 booktitle = {Proceedings of the 20th International Conference on Intelligent User Interfaces Companion},
 doi = {10.1145/2732158.2732194},
 isbn = {978-1-4503-3308-5},
 keyword = {computer-assisted instruction (cai), graphical user interfaces (gui), guides, interaction styles, user-centered design},
 link = {http://doi.acm.org/10.1145/2732158.2732194},
 location = {Atlanta, Georgia, USA},
 numpages = {4},
 pages = {53--56},
 publisher = {ACM},
 series = {IUI Companion '15},
 title = {Mechanix: A Sketch-Based Educational Interface},
 year = {2015}
}


@inproceedings{Trivedi:2015:CTA:2732158.2732162,
 abstract = {Natural Language Processing (NLP) systems are typically developed by informaticists skilled in machine learning techniques that are unfamiliar to end-users. Although NLP has been widely used in extracting information from clinical text, current systems generally do not provide any provisions for incorporating feedback and revising models based on input from domain experts. The goal of this research is to close this gap by building highly-usable tools suitable for the analysis of free text reports.},
 acmid = {2732162},
 address = {New York, NY, USA},
 author = {Trivedi, Gaurav},
 booktitle = {Proceedings of the 20th International Conference on Intelligent User Interfaces Companion},
 doi = {10.1145/2732158.2732162},
 isbn = {978-1-4503-3308-5},
 keyword = {electronic medical records, interactive machine learning, visualization},
 link = {http://doi.acm.org/10.1145/2732158.2732162},
 location = {Atlanta, Georgia, USA},
 numpages = {4},
 pages = {113--116},
 publisher = {ACM},
 series = {IUI Companion '15},
 title = {Clinical Text Analysis Using Interactive Natural Language Processing},
 year = {2015}
}


@proceedings{Nichols:2016:2876456,
 abstract = {It is with great pleasure that we welcome you to beautiful Sonoma, California and the 21st edition of ACM International Conference on Intelligent User Interfaces -- ACM IUI 2016. ACM IUI is "where HCI meets AI," or where the academic research communities of Human-Computer Interaction (HCI) and Artificial Intelligence (AI) intersect. As the premier international forum for reporting outstanding research and development on intelligent user interfaces, the conference welcomes submissions describing work at the cross-section of these two fields and other related fields, such as psychology, behavioral science, cognitive science, computer graphics, design, the arts, and many others. Members of the ACM IUI community are interested in improving the symbiosis between humans and computers, increasing the intelligence of both in the process. The call for papers attracted 194 long and short paper submissions, 31 poster paper submissions, 11 demo paper submissions, and 17 student consortium submissions. The final program of the conference includes 2 keynotes, 49 long and short papers (25.3% acceptance rate), 15 posters, 6 demos, 2 workshops, 1 tutorial, and 12 student consortium papers. We are particularly excited for the keynotes by two distinguished speakers that will open the first and third days of the conference. The opening keynote will be given by Xavier Amatriain, VP of Engineering at Quora, on the topic of the "Past, Present, and Future of Recommender Systems: An Industry Perspective." The closing keynote will be given by Professor Elisabeth André, from Augsburg University and a long-time member of the ACM IUI community, on the topic of "Socially-Sensitive Interfaces: From Offline Studies to Interactive Experiences." The conference could not be organized without the help of a large number of individuals who generously volunteered much of their own time. Their names can be found on the following pages. All of the members of the organizing committee have done a fantastic job of coordinating the many moving parts that go into putting on a great conference. We must also particularly thank our 34 senior program committee members for coordinating the papers review process and the 91 members of the program committee for providing high quality reviews. And, most important, we must thank the authors for their diligent work that resulted in so many great submissions. These have allowed us to develop the excellent program that is the enduring heart of the conference. Another key for any great conference is a collection of strong sponsor organizations and generous corporate supporters. Our sponsors ACM, SIGAI, and SIGCHI are instrumental in making the conference happen year in and year out. This year, SIGCHI and SIGAI were particularly generous in providing financial support for our student travel grants, which have enabled 19 students to attend the conference that might not have otherwise. Our corporate supporters, Microsoft, IBM, Google, and Tableau have been supremely generous and the conference would be weaker without their contributions. We hope you will find the program engaging and the mix of academic disciplines broadens your perspective on computing. We also hope the conference will provide you with a valuable opportunity to share ideas with other researchers and practitioners from around the world, whether through presenting your own work formally or through informal discussions during the banquet or coffee breaks. With luck, those shared ideas will manifest themselves in exciting papers at next year's conference and ultimately have impact far beyond the conference! If you have any suggestions for how to improve the conference either this year or in the future, please do not hesitate to let us know.},
 address = {New York, NY, USA},
 isbn = {978-1-4503-4140-0},
 location = {Sonoma, California, USA},
 publisher = {ACM},
 title = {IUI '16 Companion: Companion Publication of the 21st International Conference on Intelligent User Interfaces},
 year = {2016}
}


@inproceedings{Vega:2015:HCH:2732158.2732176,
 abstract = {Our aim is to use our own bodies as an interactive platform. We are trying to move away from traditional wearable devices worn on clothes and accessories where gestures are noticeable and remind cyborg looking. We follow Beauty Technology paradigm that uses the body's surface as an interactive platform by integrating technology into beauty products applied directly to one's skin, fingernails and hair. Thus, we propose Hairware, a Beauty Technology Prototype that connects chemically metalized hair extensions to a microcontroller turning it into an input device for triggering different objects. Hairware acts as a capacitive touch sensor that detects touch variations on hair and uses machine learning algorithms in order to recognize user's intention. In this way, we add a new functionality to hair extensions, becoming a seamless device that recognizes auto-contact behaviors that no observers would identify. This work presents the design of Hairware's hardware and software implementation. In this demo, we show Hairware acting as a controller for smartphones and computers.},
 acmid = {2732176},
 address = {New York, NY, USA},
 author = {Vega, Katia and Cunha, Marcio and Fuks, Hugo},
 booktitle = {Proceedings of the 20th International Conference on Intelligent User Interfaces Companion},
 doi = {10.1145/2732158.2732176},
 isbn = {978-1-4503-3308-5},
 keyword = {beauty technology, conductive hair, gesture recognition, hairware, wearable computing},
 link = {http://doi.acm.org/10.1145/2732158.2732176},
 location = {Atlanta, Georgia, USA},
 numpages = {4},
 pages = {89--92},
 publisher = {ACM},
 series = {IUI Companion '15},
 title = {Hairware: Conductive Hair Extensions As a Capacitive Touch Input Device},
 year = {2015}
}


@inproceedings{Toshniwal:2015:UIT:2732158.2732187,
 abstract = {Audio Guides have been the prevalent mode of information delivery in public spaces such as Museums and Art Galleries. These devices are programmed to render static information to their users about the collections and artworks present and require human input to operate. The inability to automatically deliver contextual messages and the lack of interactivity are major hurdles to ensuring a rich and seamless user experience. Ubiquitous smartphones can be leveraged to create pervasive audio guides that provide rich and personalized user experience. In this paper, we present the design and implementation of "Usher", an intelligent tour companion. Usher provides three distinct advantages over traditional audio guides. First, Usher uses smartphone sensors to infer user context such as his physical location, locomotive state and orientation to deliver relevant information to the user. Second, Usher also provides interface to a cognitive Question Answer(QA) service for the inquisitive users and answers contextual queries. Finally, Usher notifies users if any of their social media friends are present in the vicinity. The ability to seamlessly track user context to provide rich semantic information and the cognitive capability to answer contextual queries means that Usher can enhance the user experience in a museum by multitudes.},
 acmid = {2732187},
 address = {New York, NY, USA},
 author = {Toshniwal, Shubham and Sharma, Parikshit and Srivastava, Saurabh and Sehgal, Richa},
 booktitle = {Proceedings of the 20th International Conference on Intelligent User Interfaces Companion},
 doi = {10.1145/2732158.2732187},
 isbn = {978-1-4503-3308-5},
 keyword = {context based services, museum tour companion, natural language and speech processing, social media},
 link = {http://doi.acm.org/10.1145/2732158.2732187},
 location = {Atlanta, Georgia, USA},
 numpages = {4},
 pages = {81--84},
 publisher = {ACM},
 series = {IUI Companion '15},
 title = {USHER: An Intelligent Tour Companion},
 year = {2015}
}


@inproceedings{Sakai:2015:FRF:2732158.2732184,
 abstract = {Various eye-trackers have recently become commercially available, but studies on more high-spec eye-tracking system have been conducted. Especially, studies have shown that conventional eye-trackers are rather inflexible in layout. The cameras employed in these eye-trackers, as well as the light sources and user's position are fixed, and only a predefined plane can be the target of the eye-tracking. In this study, we propose a new framework that we call a Free-Target Eye-tracking System, which consists of eye-tracking hardware and a hardware layout solver. We developed a prototype of a hardware layout solver and demonstrated its effectiveness.},
 acmid = {2732184},
 address = {New York, NY, USA},
 author = {Sakai, Daiki and Yamamoto, Michiya and Nagamatsu, Takashi},
 booktitle = {Proceedings of the 20th International Conference on Intelligent User Interfaces Companion},
 doi = {10.1145/2732158.2732184},
 isbn = {978-1-4503-3308-5},
 keyword = {eye-tracking, free target, gaze cone},
 link = {http://doi.acm.org/10.1145/2732158.2732184},
 location = {Atlanta, Georgia, USA},
 numpages = {4},
 pages = {73--76},
 publisher = {ACM},
 series = {IUI Companion '15},
 title = {Framework for Realizing a Free-Target Eye-tracking System},
 year = {2015}
}


@inproceedings{Tomiyasu:2015:HCV:2732158.2732171,
 abstract = {Wide-angle multi-view video, which provides viewers with a realistic experience, has received increasing attention in recent years. Users want to watch such videos interactively, switching viewpoints freely, but without the burdens of consecutive viewpoint selection or complex operation. Viewing systems should therefore satisfy these conflicting needs simultaneously. In this paper, we take the novel approach of confronting multi-view videos as a cooperative work. We also introduce a human-machine cooperative viewing system for wide-angle multi-view videos exploiting target-centered viewing. Our system consists of a manual viewpoint selection function and an automatic viewpoint selection function based on our concept.},
 acmid = {2732171},
 address = {New York, NY, USA},
 author = {Tomiyasu, Fumiharu and Mase, Kenji},
 booktitle = {Proceedings of the 20th International Conference on Intelligent User Interfaces Companion},
 doi = {10.1145/2732158.2732171},
 isbn = {978-1-4503-3308-5},
 keyword = {cooperative work, target-centered viewing, video viewing interface, wide-angle multi-view video},
 link = {http://doi.acm.org/10.1145/2732158.2732171},
 location = {Atlanta, Georgia, USA},
 numpages = {4},
 pages = {85--88},
 publisher = {ACM},
 series = {IUI Companion '15},
 title = {Human-Machine Cooperative Viewing System for Wide-angle Multi-view Videos},
 year = {2015}
}


@inproceedings{Krishna:2015:AGI:2732158.2732183,
 abstract = {In this paper, we propose a prototype system for automatic generation and insertion of assessment items in online video courses. The proposed system analyzes text transcript of a requested video lecture to suggest self-assessment items in runtime through automatic discourse segmentation and question generation. To deal with the problem of question generation from noisy transcription, the system relies on semantically similar Wikipedia text segments. We base our study on a popular video lecture portal - National Programme on Technology Enhanced Learning (NPTEL). However, it can be adapted to other portals as well.},
 acmid = {2732183},
 address = {New York, NY, USA},
 author = {Krishna, Amrith and Bhowmick, Plaban and Ghosh, Krishnendu and Sahu, Archana and Roy, Subhayan},
 booktitle = {Proceedings of the 20th International Conference on Intelligent User Interfaces Companion},
 doi = {10.1145/2732158.2732183},
 isbn = {978-1-4503-3308-5},
 keyword = {latent semantic analysis, moocs, online video courses, question generation, rhetorical structure theory (rst)},
 link = {http://doi.acm.org/10.1145/2732158.2732183},
 location = {Atlanta, Georgia, USA},
 numpages = {4},
 pages = {1--4},
 publisher = {ACM},
 series = {IUI Companion '15},
 title = {Automatic Generation and Insertion of Assessment Items in Online Video Courses},
 year = {2015}
}


@inproceedings{Hamlet:2015:UIR:2732158.2732172,
 abstract = {We have created a new set of existing and novel predictive user-interfaces for exchanging messages in asynchronous collaborative systems such as email and internet communities. These interfaces support predictions of tags, hierarchical recipients, and message response times. The predictions are made incrementally, as messages are composed, and are offered to both senders and receivers of messages. The user interfaces are implemented by a test-bed that also supports experiments to evaluate them. It can automate the actions of the collaborators with whom a subject exchanges messages, replay user actions, and gather and display effort and correctness metrics related to these predictions. The collaborator actions and predictions are specified using a declarative mechanism. A video demonstration of this work is available at http://youtu.be/NJt9Rfqb1ko.},
 acmid = {2732172},
 address = {New York, NY, USA},
 author = {Hamlet, Connor and Korn, Daniel and Prasad, Nikhil and Siedlecki, Volodymyr and Encarnacion, Eliezer and Bartel, Jacob and Dewan, Prasun},
 booktitle = {Proceedings of the 20th International Conference on Intelligent User Interfaces Companion},
 doi = {10.1145/2732158.2732172},
 isbn = {978-1-4503-3308-5},
 keyword = {recommender systems},
 link = {http://doi.acm.org/10.1145/2732158.2732172},
 location = {Atlanta, Georgia, USA},
 numpages = {4},
 pages = {21--24},
 publisher = {ACM},
 series = {IUI Companion '15},
 title = {User-Interfaces for Incremental Recipient and Response Time Predictions in Asynchronous Messaging},
 year = {2015}
}


@inproceedings{Prange:2015:RCS:2732158.2732174,
 abstract = {In this demo paper we describe how a digital pen and a humanoid robot companion can improve the social communication of a dementia patient. We propose the use of NAO, a humanoid robot, as a companion to the dementia patient in order to continuously monitor his or her activities and provide cognitive assistance in daily life situations. For example, patients can communicate with NAO through natural language by the speech dialogue functionality we integrated. Most importantly, to improve communication, i.e., sending digital messages (texting, emails), we propose the usage of a smartpen, where the patients write messages on normal paper with an invisible dot pattern to initiate hand-writing and sketch recognition in real-time. The smartpen application is embedded into the human-robot speech dialogue.},
 acmid = {2732174},
 address = {New York, NY, USA},
 author = {Prange, Alexander and Sandrala, Indra Praveen and Weber, Markus and Sonntag, Daniel},
 booktitle = {Proceedings of the 20th International Conference on Intelligent User Interfaces Companion},
 doi = {10.1145/2732158.2732174},
 isbn = {978-1-4503-3308-5},
 keyword = {design, healthcare, pen/ink interface, reality orientation dialogue, realtime interaction, speech dialogue},
 link = {http://doi.acm.org/10.1145/2732158.2732174},
 location = {Atlanta, Georgia, USA},
 numpages = {4},
 pages = {65--68},
 publisher = {ACM},
 series = {IUI Companion '15},
 title = {Robot Companions and Smartpens for Improved Social Communication of Dementia Patients},
 year = {2015}
}


@inproceedings{Masuko:2015:WMI:2732158.2732179,
 abstract = {We propose WallSHOP, a novel interactive shopping experience that extends content sharing between publicly available digital signage and mobile devices. Multiple users can freely access and browse the content of public digital signage through a public network. Furthermore, users can interact with this content using a personalized cursor that can be controlled with a touch-screen mobile device. WallSHOP also supports pulling content from the digital signage to a user's device, allowing users to browse through available products and privately perform checkouts by utilizing the advantages of both public and private displays. WallSHOP focuses on feasibility and scalability; therefore, it is implemented using only web-based components and does not require the installation of additional software.},
 acmid = {2732179},
 address = {New York, NY, USA},
 author = {Masuko, Soh and Muta, Masafumi and Shinzato, Keiji and Mujibiya, Adiyan},
 booktitle = {Proceedings of the 20th International Conference on Intelligent User Interfaces Companion},
 doi = {10.1145/2732158.2732179},
 isbn = {978-1-4503-3308-5},
 keyword = {contents sharing, digital signage, e-commerce, personal devices, public display, web-based application},
 link = {http://doi.acm.org/10.1145/2732158.2732179},
 location = {Atlanta, Georgia, USA},
 numpages = {4},
 pages = {37--40},
 publisher = {ACM},
 series = {IUI Companion '15},
 title = {WallSHOP: Multiuser Interaction with Public Digital Signage Using Mobile Devices for Personalized Shopping},
 year = {2015}
}


@inproceedings{Gao:2015:OSS:2732158.2732189,
 abstract = {We describe OfficeHours, a recommender system that assists students in finding potential supervisors for their dissertation projects. OfficeHours is an interactive recommender system that combines reinforcement learning techniques with a novel interface that assists the student in formulating their query and allows active engagement in directing their search. Students can directly manipulate document features (keywords) extracted from scientific articles written by faculty members to indicate their interests and reinforcement learning is used to model the student's interests by allowing the system to trade off between exploration and exploitation. The goal of system is to give the student the opportunity to more effectively search for possible project supervisors in a situation where the student may have difficulties formulating their query or when very little information may be available on faculty members' websites about their research interests.},
 acmid = {2732189},
 address = {New York, NY, USA},
 author = {Gao, Yuan and Ilves, Kalle and G\lowacka, Dorota},
 booktitle = {Proceedings of the 20th International Conference on Intelligent User Interfaces Companion},
 doi = {10.1145/2732158.2732189},
 isbn = {978-1-4503-3308-5},
 keyword = {exploratory search, student support systems},
 link = {http://doi.acm.org/10.1145/2732158.2732189},
 location = {Atlanta, Georgia, USA},
 numpages = {4},
 pages = {29--32},
 publisher = {ACM},
 series = {IUI Companion '15},
 title = {OfficeHours: A System for Student Supervisor Matching Through Reinforcement Learning},
 year = {2015}
}


