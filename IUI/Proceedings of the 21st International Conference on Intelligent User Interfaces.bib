@inproceedings{Sousa:2016:SAR:2856767.2856773,
 abstract = {We present an intelligent user interface that allows people to perform rehabilitation exercises by themselves under the offline supervision of a therapist. Every year, many people suffer injuries that require rehabilitation. This entails considerable time overheads since it requires people to perform specified exercises under the direct supervision of a therapist. Therefore it is desirable that patients continue performing exercises outside the clinic (for instance at home, thus without direct supervision), to complement in-clinic physical therapy. However, to perform rehabilitation tasks accurately, patients need appropriate feedback, as otherwise provided by a physical therapist, to ensure that these unsupervised exercises are correctly executed. Different approaches address this problem, providing feedback mechanisms to aid rehabilitation. Unfortunately, test subjects frequently report having trouble to completely understand the feedback thus provided, which makes it hard to correctly execute the prescribed movements. Worse, injuries may occur due to incorrect performance of the prescribed exercises, which severely hinders recovery. SleeveAR is a novel approach to provide real-time, active feedback, using multiple projection surfaces to provide effective visualizations. Empirical evaluation shows the effectiveness of our approach as compared to traditional video-based feedback. Our experimental results show that our intelligent UI can successfully guide subjects through an exercise prescribed (and demonstrated) by a physical therapist, with performance improvements between consecutive executions, a desirable goal to successful rehabilitation.},
 acmid = {2856773},
 address = {New York, NY, USA},
 author = {Sousa, Maur\'{\i}cio and Vieira, Jo\~{a}o and Medeiros, Daniel and Arsenio, Artur and Jorge, Joaquim},
 booktitle = {Proceedings of the 21st International Conference on Intelligent User Interfaces},
 doi = {10.1145/2856767.2856773},
 isbn = {978-1-4503-4137-0},
 keyword = {augmented reality, projection-based systems, rehabilitation},
 link = {http://doi.acm.org/10.1145/2856767.2856773},
 location = {Sonoma, California, USA},
 numpages = {11},
 pages = {175--185},
 publisher = {ACM},
 series = {IUI '16},
 title = {SleeveAR: Augmented Reality for Rehabilitation Using Realtime Feedback},
 year = {2016}
}


@inproceedings{Altmeyer:2016:ECG:2856767.2856790,
 abstract = {We investigate a crowd-based approach to enhance the outcome of optical character recognition in the domain of receipt capturing to keep track of expenses. In contrast to existing work, our approach is capable of extracting single products and provides categorizations for both articles and expenses, through the use of microtasks which are delegated to an unpaid crowd. To evaluate our approach, we developed a smartphone application based on a receipt analysis and an online questionnaire in which users are able to track expenses by taking photos of receipts, and solve microtasks to enhance the recognition. To provide additional motivation to solve these tasks, we make use of gamification. In a three-week-long user study (N=12), we found that our system is appreciated, that our approach reduces the error rate of captured receipts significantly, and that the gamification provided additional motivation to contribute more and thereby enrich the database.},
 acmid = {2856790},
 address = {New York, NY, USA},
 author = {Altmeyer, Maximilian and Lessel, Pascal and Kr\"{u}ger, Antonio},
 booktitle = {Proceedings of the 21st International Conference on Intelligent User Interfaces},
 doi = {10.1145/2856767.2856790},
 isbn = {978-1-4503-4137-0},
 keyword = {crowdsourcing, digital household accounting book, gamification, ocr, wisdom of crowds},
 link = {http://doi.acm.org/10.1145/2856767.2856790},
 location = {Sonoma, California, USA},
 numpages = {12},
 pages = {31--42},
 publisher = {ACM},
 series = {IUI '16},
 title = {Expense Control: A Gamified, Semi-Automated, Crowd-Based Approach For Receipt Capturing},
 year = {2016}
}


@proceedings{Papadopoulos:2017:3030024,
 abstract = {It is our great pleasure to welcome you to the 2017 International Conference on Intelligent User Interfaces (IUI'17). It is the twenty-second IUI conference, continuing its tradition of being the principal international forum for reporting outstanding research at the intersection of Human Computer Interaction (HCI) and Artificial Intelligence (AI). The work that appears at IUI bridges these two fields and delves also into related fields, such as psychology, cognitive science, computer graphics, the arts, and many others. Members of the IUI community are interested in improving the symbiosis between humans and computers, and in making systems adapt to humans rather than the other way around. The call for papers attracted 272 submissions from Africa, America, Asia, Australia and Europe, a record number for the IUI series, 58 submissions of posters and demos and 25 submissions to the students' consortium. The program committee accepted 63 papers, covering a diverse set of topics, reflected in the session titles "Recommender Systems", "Multimodal and Augmented Interaction", "Understanding Users", "Information Visualization", "Personalization", "Interactive Programming and Automation", "Trust", "Intelligent Training and Educational Interfaces", "Intelligent Systems", "Gestural and Haptic Interaction", "Analytics", "Interactive Machine Learning and Explanation", "Machine Learning", and "Information Retrieval". We also have 19 posters and 13 demos and 16 doctoral consortium papers. Following the tradition of collaboration with TiiS journal, 5 papers that were published during 2016 are presented at IUI 2017 and selected papers will be invited to submit extended versions to the journal. In addition, the conference provides six workshops and three tutorials. One of the main attractions of the conference is provided by the scientific keynotes: Dr. Shumin Zhai opens the conference program with a keynote on "Modern Touchscreen Keyboards as Intelligent User Interfaces: A Research Review", Professor George Samaras' second day keynote is on "Utilizing Human Cognitive and Emotional Factors for User-Centered Computing" and the last day keynote, by Professor Panos Markopoulos, is on "Interaction Design for Rehabilitation". The conference program is available for IUI participants on Conference Navigator. IUI 2017 has a few novelties: For the first time, IUI introduced parallel sessions throughout the conference. The doctoral consortium papers were integrated into the main program for the first time in IUI. Another novelty of IUI 2017 is the introduction of the "most impactful IUI paper" award.},
 address = {New York, NY, USA},
 isbn = {978-1-4503-4893-5},
 location = {Limassol, Cyprus},
 publisher = {ACM},
 title = {IUI '17 Companion: Proceedings of the 22Nd International Conference on Intelligent User Interfaces Companion},
 year = {2017}
}


@inproceedings{Kamalzadeh:2016:TAM:2856767.2856780,
 abstract = {We report on the design and evaluation of TagFlip, a novel interface for active music discovery based on social tags of music. The tool, which was built for phone-sized screens, couples high user control on the recommended music with minimal interaction effort. Contrary to conventional recommenders, which only allow the specification of seed attributes and the subsequent like/dislike of songs, we put the users in the centre of the recommendation process. With a library of 100,000 songs, TagFlip describes each played song to the user through its most popular tags on Last.fm and allows the user to easily specify which of the tags should be considered for the next song, or the next stream of songs. In a lab user study where we compared it to Spotify's mobile application, TagFlip came out on top in both subjective user experience (control, transparency, and trust) and our objective measure of number of interactions per liked song. Our users found TagFlip to be an important complementary experience to that of Spotify, enabling more active and directed discovery sessions as opposed to the mostly passive experience that traditional recommenders offer.},
 acmid = {2856780},
 address = {New York, NY, USA},
 author = {Kamalzadeh, Mohsen and Kralj, Christoph and M\"{o}ller, Torsten and Sedlmair, Michael},
 booktitle = {Proceedings of the 21st International Conference on Intelligent User Interfaces},
 doi = {10.1145/2856767.2856780},
 isbn = {978-1-4503-4137-0},
 keyword = {exploration, fine tuning, folksonomies, minimal effort, music discovery, recommendation, social tags, transparency, user controlled, user interface, user-centred design},
 link = {http://doi.acm.org/10.1145/2856767.2856780},
 location = {Sonoma, California, USA},
 numpages = {12},
 pages = {19--30},
 publisher = {ACM},
 series = {IUI '16},
 title = {TagFlip: Active Mobile Music Discovery with Social Tags},
 year = {2016}
}


@inproceedings{Athukorala:2016:BRA:2856767.2856786,
 abstract = {We present a novel adaptation technique for search engines to better support information-seeking activities that include both lookup and exploratory tasks. Building on previous findings, we describe (1) a classifier that recognizes task type (lookup vs. exploratory) as a user is searching and (2) a reinforcement learning based search engine that adapts accordingly the balance of exploration/exploitation in ranking the documents. This allows supporting both task types surreptitiously without changing the familiar list-based interface. Search results include more diverse results when users are exploring and more precise results for lookup tasks. Users found more useful results in exploratory tasks when compared to a base-line system, which is specifically tuned for lookup tasks.},
 acmid = {2856786},
 address = {New York, NY, USA},
 author = {Athukorala, Kumaripaba and Medlar, Alan and Oulasvirta, Antti and Jacucci, Giulio and Glowacka, Dorota},
 booktitle = {Proceedings of the 21st International Conference on Intelligent User Interfaces},
 doi = {10.1145/2856767.2856786},
 isbn = {978-1-4503-4137-0},
 keyword = {adaptive systems., exploratory search, lookup search, models of search behavior, reinforcement learning},
 link = {http://doi.acm.org/10.1145/2856767.2856786},
 location = {Sonoma, California, USA},
 numpages = {11},
 pages = {359--369},
 publisher = {ACM},
 series = {IUI '16},
 title = {Beyond Relevance: Adapting Exploration/Exploitation in Information Retrieval},
 year = {2016}
}


@inproceedings{SuYin:2016:TIT:2856767.2856810,
 abstract = {Numerous VR simulators have been developed as a means of addressing limitations of the traditional apprenticeship approach to dental surgical skill training. Most existing simulators support intra- and extra-coronal procedures such as carries removal. In this paper we address the problem of automated outcome assessment for endodontic surgery. Outcome assessment is an essential component of any system that provides formative feedback, which requires assessing the outcome, relating it to the procedure, and communicating in a language natural to dental students. This paper takes a first step toward automated generation of such comprehensive feedback. Our system automatically computes reference templates based on tooth anatomy, which provides flexibility to adjust parameters such as tolerance and to create new templates on demand. Detailed scores are transformed into the standard scoring language used by dental schools. Preliminary evaluation of our system on fifteen outcome samples with three expert endodontists shows a high degree of agreement with expert scores.},
 acmid = {2856810},
 address = {New York, NY, USA},
 author = {Su Yin, Myat and Haddawy, Peter and Suebnukarn, Siriwan and Rhienmora, Phattanapon},
 booktitle = {Proceedings of the 21st International Conference on Intelligent User Interfaces},
 doi = {10.1145/2856767.2856810},
 isbn = {978-1-4503-4137-0},
 keyword = {automated outcome assessment, formative assessment, intelligent tutorings, surgical simulation, virtual reality},
 link = {http://doi.acm.org/10.1145/2856767.2856810},
 location = {Sonoma, California, USA},
 numpages = {5},
 pages = {402--406},
 publisher = {ACM},
 series = {IUI '16},
 title = {Toward Intelligent Tutorial Feedback in Surgical Simulation: Robust Outcome Scoring for Endodontic Surgery},
 year = {2016}
}


@inproceedings{Chang:2016:AKI:2856767.2856783,
 abstract = {A relatively new feature in Google Play Store presents mobile app search results grouped by topic, helping users to quickly navigate and explore. The underlying Search Results Clustering (SRC) system faces several challenges, including grouping search results in topical coherent clusters as well as finding the appropriate level of granularity for clustering. We present AppGrouper, an alternative approach to algorithmic-only solutions, incorporating human input in a knowledge-graph-based clustering process. AppGrouper provides an interactive interface that lets domain experts steer the clustering process in early, mid, and late stages. We deployed and evaluated AppGrouper with internal experts. We found that AppGroup improved quality of algorithm-generated app clusters on 56 out of 82 search queries. We also found that the internal experts made more changes in early and mid stages for lower-quality algorithmic results, focusing more on narrow queries. Our result suggests, in some contexts, machine learning systems can greatly benefit from steering from human experts, creating a symbiotic working relationship.},
 acmid = {2856783},
 address = {New York, NY, USA},
 author = {Chang, Shuo and Dai, Peng and Hong, Lichan and Sheng, Cheng and Zhang, Tianjiao and Chi, Ed H.},
 booktitle = {Proceedings of the 21st International Conference on Intelligent User Interfaces},
 doi = {10.1145/2856767.2856783},
 isbn = {978-1-4503-4137-0},
 keyword = {clustering, interactive machine learning, search result clustering},
 link = {http://doi.acm.org/10.1145/2856767.2856783},
 location = {Sonoma, California, USA},
 numpages = {11},
 pages = {348--358},
 publisher = {ACM},
 series = {IUI '16},
 title = {AppGrouper: Knowledge-based Interactive Clustering Tool for App Search Results},
 year = {2016}
}


@inproceedings{Yan:2016:EAE:2856767.2856768,
 abstract = {Audience engagement is an important indicator of the quality of the performing arts but hard to measure. Psychophysiological measurements are promising research methods for perceiving and understanding audience's responses in real-time. Currently, such research are conducted by collecting biometric data from audience when they are watching a performance. In this paper, we draw on techniques from brain-computer interfaces (BCI) and knowledge from quality of performing arts to develop a system that monitor audience engagement in real time using electroencephalography (EEG) measurement and seek to improve it by triggering the adaptive performing cues when the engagement level decreased. We simulated the immersive theatre performances to provide audience a high-fidelity visual-audio experience. An experimental evaluation is conducted with 48 participants during two different performance studies. The results showed that our system could successfully detect the decreases in audience engagement and the performing cues had positive effects on regain audience engagement. Our research offers the guidelines for designing theatre performances from the audience's perception.},
 acmid = {2856768},
 address = {New York, NY, USA},
 author = {Yan, Shuo and Ding, GangYi and Li, Hongsong and Sun, Ningxiao and Wu, Yufeng and Guan, Zheng and Zhang, Longfei and Huang, Tianyu},
 booktitle = {Proceedings of the 21st International Conference on Intelligent User Interfaces},
 doi = {10.1145/2856767.2856768},
 isbn = {978-1-4503-4137-0},
 keyword = {adaptive user interface, audience engagement, brain-computer interface (bci), electroencephalography (eeg)},
 link = {http://doi.acm.org/10.1145/2856767.2856768},
 location = {Sonoma, California, USA},
 numpages = {11},
 pages = {306--316},
 publisher = {ACM},
 series = {IUI '16},
 title = {Enhancing Audience Engagement in Performing Arts Through an Adaptive Virtual Environment with a Brain-Computer Interface},
 year = {2016}
}


@inproceedings{Vannaprathip:2016:DST:2856767.2856807,
 abstract = {Use of simulation to teach decision making in surgery is challenging partly due to the situated nature of the decisions, with situation awareness playing a critical role in making high quality decisions. Thus simulation systems need to be able to provide the key cues needed in making decisions with high fidelity. In this paper we present the first version of Desitra, a simulation environment for teaching decision making in dental surgery. System design was driven by an observational study of teaching sessions for endodontic surgery in the operating room which identified perceptual cues used in decision making as well as tutorial intervention strategies used by surgeons. Desitra provides an open environment for learning decision making -- students carry out dental procedures and are free to make mistakes. The pedagogical module monitors the student actions and intervenes when students make mistakes, providing as little guidance as necessary to keep students on a productive learning path. The system is implemented to run on Android tablets to be maximally accessible. Preliminary evaluation of the system shows that Desitra effectively captures key perceptual cues.},
 acmid = {2856807},
 address = {New York, NY, USA},
 author = {Vannaprathip, Narumol and Haddawy, Peter and Suebnukarn, Siriwan and Sangsartra, Patcharapon and Sasikhant, Nunnapin and Sangutai, Sornram},
 booktitle = {Proceedings of the 21st International Conference on Intelligent User Interfaces},
 doi = {10.1145/2856767.2856807},
 isbn = {978-1-4503-4137-0},
 keyword = {intelligent tutoring systems, serious games, surgical decision making, surgical simulation},
 link = {http://doi.acm.org/10.1145/2856767.2856807},
 location = {Sonoma, California, USA},
 numpages = {5},
 pages = {397--401},
 publisher = {ACM},
 series = {IUI '16},
 title = {Desitra: A Simulator for Teaching Situated Decision Making in Dental Surgery},
 year = {2016}
}


@inproceedings{Graells-Garrido:2016:EDR:2856767.2856775,
 abstract = {In centralized countries, not only population, media and economic power are concentrated, but people give more attention to central locations. While this is not inherently bad, this behavior extends to micro-blogging platforms: central locations get more attention in terms of information flow. In this paper we study the effects of an information filtering algorithm that decentralizes content in such platforms. Particularly, we found that users from non-central locations were not able to identify the geographical diversity on timelines generated by the algorithm, which were diverse by construction. To make users see the inherent diversity, we define a design rationale to approach this problem, focused on the treemap visualization technique. Then, we deployed an" in the wild" implementation of our proposed system. On one hand, we found that there are effects of centralization in exploratory user behavior. On the other hand, we found that the treemap was able to make users see the inherent geographical diversity of timelines. We measured these effects based on how users engaged with content filtered by the algorithm. With these results in mind, we propose practical actions for micro-blogging platforms to account for the differences and biased behavior induced by centralization.},
 acmid = {2856775},
 address = {New York, NY, USA},
 author = {Graells-Garrido, Eduardo and Lalmas, Mounia and Baeza-Yates, Ricardo},
 booktitle = {Proceedings of the 21st International Conference on Intelligent User Interfaces},
 doi = {10.1145/2856767.2856775},
 isbn = {978-1-4503-4137-0},
 keyword = {centralization, information filtering, information visualization, location bias},
 link = {http://doi.acm.org/10.1145/2856767.2856775},
 location = {Sonoma, California, USA},
 numpages = {12},
 pages = {7--18},
 publisher = {ACM},
 series = {IUI '16},
 title = {Encouraging Diversity- and Representation-Awareness in Geographically Centralized Content},
 year = {2016}
}


@inproceedings{Gotz:2016:ACC:2856767.2856779,
 abstract = {Large and high-dimensional real-world datasets are being gathered across a wide range of application disciplines to enable data-driven decision making. Interactive data visualization can play a critical role in allowing domain experts to select and analyze data from these large collections. However, there is a critical mismatch between the very large number of dimensions in complex real-world datasets and the much smaller number of dimensions that can be concurrently visualized using modern techniques. This gap in dimensionality can result in high levels of selection bias that go unnoticed by users. The bias can in turn threaten the very validity of any subsequent insights. In this paper, we present Adaptive Contextualization (AC), a novel approach to interactive visual data selection that is specifically designed to combat the invisible introduction of selection bias. Our approach (1) monitors and models a user's visual data selection activity, (2) computes metrics over that model to quantify the amount of selection bias after each step, (3) visualizes the metric results, and (4) provides interactive tools that help users assess and avoid bias-related problems. We also share results from a user study which demonstrate the effectiveness of our technique.},
 acmid = {2856779},
 address = {New York, NY, USA},
 author = {Gotz, David and Sun, Shun and Cao, Nan},
 booktitle = {Proceedings of the 21st International Conference on Intelligent User Interfaces},
 doi = {10.1145/2856767.2856779},
 isbn = {978-1-4503-4137-0},
 keyword = {exploratory analysis, intelligent visual interfaces, visual analytics, visualization},
 link = {http://doi.acm.org/10.1145/2856767.2856779},
 location = {Sonoma, California, USA},
 numpages = {11},
 pages = {85--95},
 publisher = {ACM},
 series = {IUI '16},
 title = {Adaptive Contextualization: Combating Bias During High-Dimensional Visualization and Data Selection},
 year = {2016}
}


@inproceedings{Tanveer:2016:AAI:2856767.2856785,
 abstract = {Many individuals exhibit unconscious body movements called mannerisms while speaking. These repeated changes often distract the audience when not relevant to the verbal context. We present an intelligent interface that can automatically extract human gestures using Microsoft Kinect to make speakers aware of their mannerisms. We use a sparsity-based algorithm, Shift Invariant Sparse Coding, to automatically extract the patterns of body movements. These patterns are displayed in an interface with subtle question and answer-based feedback scheme that draws attention to the speaker's body language. Our formal evaluation with 27 participants shows that the users became aware of their body language after using the system. In addition, when independent observers annotated the accuracy of the algorithm for every extracted pattern, we find that the patterns extracted by our algorithm is significantly (p<0.001) more accurate than just random selection. This represents a strong evidence that the algorithm is able to extract human-interpretable body movement patterns. An interactive demo of AutoManner is available at http://tinyurl.com/AutoManner.},
 acmid = {2856785},
 address = {New York, NY, USA},
 author = {Tanveer, M. Iftekhar and Zhao, Ru and Chen, Kezhen and Tiet, Zoe and Hoque, Mohammed Ehsan},
 booktitle = {Proceedings of the 21st International Conference on Intelligent User Interfaces},
 doi = {10.1145/2856767.2856785},
 isbn = {978-1-4503-4137-0},
 keyword = {body language, interface design, public speaking},
 link = {http://doi.acm.org/10.1145/2856767.2856785},
 location = {Sonoma, California, USA},
 numpages = {12},
 pages = {385--396},
 publisher = {ACM},
 series = {IUI '16},
 title = {AutoManner: An Automated Interface for Making Public Speakers Aware of Their Mannerisms},
 year = {2016}
}


@inproceedings{Andre:2016:SIO:2856767.2856799,
 abstract = {Recent years have initiated a paradigm shift from pure taskbased human-machine interfaces towards socially-sensitive interaction. In addition to what users explicitly say or gesture at, socially-sensitive interfaces are able to sense more subtle human cues, such as head postures and movements, to infer psychological user states, such as attention and affect, and also to enrich system responses with social signals. However, most approaches focus on offline analysis of previously recorded data limiting the investigation to prototypical behaviors in laboratory-like settings. In my presentation, I will focus on challenges that arise when integrating social signal processing techniques into interactive systems designed for real-world applications. From a technical perspective, this requires effective tools able to synchronize, process, and analyze relevant signals in online mode. From a user perspective, appropriate strategies need to be defined to respond to social signals at the right moment in time without disturbing the flow of interaction. I will discuss two interaction styles for socially-sensitive interfaces. In the area of information retrieval, the concept of empathic stimulation has been used to optimize the selection and presentation of data. The basic idea is to exploit sensory data on the users' emotional state to provide them with cues that inspire their curiosity during the data exploration task. In the domain of social coaching, the concept of social augmentation has been employed to give people ambient feedback on their behavior while being engaged in a social interaction. The presentation will be illustrated by examples from various national and international projects following these two interaction styles.},
 acmid = {2856799},
 address = {New York, NY, USA},
 author = {Andr{\'e}, Elisabeth},
 booktitle = {Proceedings of the 21st International Conference on Intelligent User Interfaces},
 doi = {10.1145/2856767.2856799},
 isbn = {978-1-4503-4137-0},
 keyword = {affective computing, empathic stimulation, social augmentation, social signal processing},
 link = {http://doi.acm.org/10.1145/2856767.2856799},
 location = {Sonoma, California, USA},
 numpages = {1},
 pages = {305--305},
 publisher = {ACM},
 series = {IUI '16},
 title = {Socially-Sensitive Interfaces: From Offline Studies to Interactive Experiences},
 year = {2016}
}


@inproceedings{Chen:2016:TMD:2856767.2856787,
 abstract = {We describe methods for analyzing and visualizing document metadata to provide insights about collaborations over time. We investigate the use of Latent Dirichlet Allocation (LDA) based topic modeling to compute areas of interest on which people collaborate. The topics are represented in a node-link force directed graph by persistent fixed nodes laid out with multidimensional scaling (MDS), and the people by transient movable nodes. The topics are also analyzed to detect bursts to highlight "hot" topics during a time interval. As the user manipulates a time interval slider, the people nodes and links are dynamically updated. We evaluate the results of LDA topic modeling for the visualization by comparing topic keywords against the submitted keywords from the InfoVis 2004 Contest, and we found that the additional terms provided by LDA-based keyword sets result in improved similarity between a topic keyword set and the documents in a corpus. We extended the InfoVis dataset from 8 to 20 years and collected publication metadata from our lab over a period of 21 years, and created interactive visualizations for exploring these larger datasets.},
 acmid = {2856787},
 address = {New York, NY, USA},
 author = {Chen, Francine and Chiu, Patrick and Lim, Seongtaek},
 booktitle = {Proceedings of the 21st International Conference on Intelligent User Interfaces},
 doi = {10.1145/2856767.2856787},
 isbn = {978-1-4503-4137-0},
 keyword = {interactive visualization, small group collaboration, temporal dynamics, topic modeling},
 link = {http://doi.acm.org/10.1145/2856767.2856787},
 location = {Sonoma, California, USA},
 numpages = {10},
 pages = {108--117},
 publisher = {ACM},
 series = {IUI '16},
 title = {Topic Modeling of Document Metadata for Visualizing Collaborations over Time},
 year = {2016}
}


@inproceedings{Laput:2016:SAH:2856767.2856812,
 abstract = {Devices can be made more intelligent if they have the ability to sense their surroundings and physical configuration. However, adding extra, special purpose sensors increases size, price and build complexity. Instead, we use speakers and microphones already present in a wide variety of devices to open new sensing opportunities. Our technique sweeps through a range of inaudible frequencies and measures the intensity of reflected sound to deduce information about the immediate environment, chiefly the materials and geometry of proximate surfaces. We offer several example uses, two of which we implemented as self-contained demos, and conclude with an evaluation that quantifies their performance and demonstrates high accuracy.},
 acmid = {2856812},
 address = {New York, NY, USA},
 author = {Laput, Gierad and Chen, Xiang 'Anthony' and Harrison, Chris},
 booktitle = {Proceedings of the 21st International Conference on Intelligent User Interfaces},
 doi = {10.1145/2856767.2856812},
 isbn = {978-1-4503-4137-0},
 keyword = {acoustic sensing, interaction techniques, mobile devices, novel input.},
 link = {http://doi.acm.org/10.1145/2856767.2856812},
 location = {Sonoma, California, USA},
 numpages = {4},
 pages = {332--335},
 publisher = {ACM},
 series = {IUI '16},
 title = {SweepSense: Ad Hoc Configuration Sensing Using Reflected Swept-Frequency Ultrasonics},
 year = {2016}
}


@inproceedings{Masai:2016:FER:2856767.2856770,
 abstract = {This paper presents a novel smart eyewear that uses embedded photo reflective sensors and machine learning to recognize a wearer's facial expressions in daily life. We leverage the skin deformation when wearers change their facial expressions. With small photo reflective sensors, we measure the proximity between the skin surface on a face and the eyewear frame where 17 sensors are integrated. A Support Vector Machine (SVM) algorithm was applied for the sensor information. The sensors can cover various facial muscle movements and can be integrated into everyday glasses. The main contributions of our work are as follows. (1) The eyewear recognizes eight facial expressions (92.8% accuracy for one time use and 78.1% for use on 3 different days). (2) It is designed and implemented considering social acceptability. The device looks like normal eyewear, so users can wear it anytime, anywhere. (3) Initial field trials in daily life were undertaken. Our work is one of the first attempts to recognize and evaluate a variety of facial expressions in the form of an unobtrusive wearable device.},
 acmid = {2856770},
 address = {New York, NY, USA},
 author = {Masai, Katsutoshi and Sugiura, Yuta and Ogata, Masa and Kunze, Kai and Inami, Masahiko and Sugimoto, Maki},
 booktitle = {Proceedings of the 21st International Conference on Intelligent User Interfaces},
 doi = {10.1145/2856767.2856770},
 isbn = {978-1-4503-4137-0},
 keyword = {eyewear computing, facial expression recognition, smart eye glasses, wearable},
 link = {http://doi.acm.org/10.1145/2856767.2856770},
 location = {Sonoma, California, USA},
 numpages = {10},
 pages = {317--326},
 publisher = {ACM},
 series = {IUI '16},
 title = {Facial Expression Recognition in Daily Life by Embedded Photo Reflective Sensors on Smart Eyewear},
 year = {2016}
}


@inproceedings{Holliday:2016:UTI:2856767.2856811,
 abstract = {Trust is a significant factor in user adoption of new systems. However, although trust is a dynamic attitude of the user towards the system and changes over time, trust in intelligent systems is typically captured as a single quantitative measure at the conclusion of a task. This paper challenges this approach. We report a case study that employed a combination of repeated quantitative and qualitative measures to examine how trust in an intelligent system evolved over time and whether this varied depending on whether the system offered explanations. We discovered different patterns in participants' trust journeys. When provided with explanations, participants' trust levels initially increased, before returning to their original level. Without explanations, participants' trust reduced over time. The qualitative data showed that perceived system ability was more important in determining trust amongst with-explanation participants and perceived transparency was a greater influence on the trust of participants who did not receive explanations. The findings provide a deeper understanding of the development of user trust in intelligent systems and indicate the value of the approach adopted.},
 acmid = {2856811},
 address = {New York, NY, USA},
 author = {Holliday, Daniel and Wilson, Stephanie and Stumpf, Simone},
 booktitle = {Proceedings of the 21st International Conference on Intelligent User Interfaces},
 doi = {10.1145/2856767.2856811},
 isbn = {978-1-4503-4137-0},
 keyword = {case study, intelligent systems, qualitative evaluations, quantitative evaluations, trust},
 link = {http://doi.acm.org/10.1145/2856767.2856811},
 location = {Sonoma, California, USA},
 numpages = {5},
 pages = {164--168},
 publisher = {ACM},
 series = {IUI '16},
 title = {User Trust in Intelligent Systems: A Journey Over Time},
 year = {2016}
}


@inproceedings{Hodhod:2016:CCG:2856767.2856774,
 abstract = {This paper proposes a new formal approach for negotiating shared mental models between humans and computational improvisational agents (improv agents) based on our sociocognitive studies of human improvisers. Negotiation of shared mental models serves as a core mechanism for improv agents to co-create stories with each other and with human interactors. The model aims to narrow the gap between human and machine intelligence by providing AI agents that, in the presence of incomplete knowledge about an improv scene, can use procedural representations not only to understand human parties but also to negotiate their mental models with them. The described approach allows flexible modeling of ambiguous, non-Boolean knowledge through the use of fuzzy logic and situation calculus that allows reasoning under uncertainty in a dynamic improvisational setting.},
 acmid = {2856774},
 address = {New York, NY, USA},
 author = {Hodhod, Rania and Magerko, Brian},
 booktitle = {Proceedings of the 21st International Conference on Intelligent User Interfaces},
 doi = {10.1145/2856767.2856774},
 isbn = {978-1-4503-4137-0},
 keyword = {cognitive consensus, computational creativity, fuzzy logic, improvisational agents, shared mental models},
 link = {http://doi.acm.org/10.1145/2856767.2856774},
 location = {Sonoma, California, USA},
 numpages = {12},
 pages = {135--146},
 publisher = {ACM},
 series = {IUI '16},
 title = {Closing the Cognitive Gap Between Humans and Interactive Narrative Agents Using Shared Mental Models},
 year = {2016}
}


@inproceedings{Huang:2016:CRC:2856767.2856792,
 abstract = {Novice composers often find it difficult to go beyond common chord progressions. To make it easier for composers to experiment with radical chord choices, we built a creativity support tool, ChordRipple, which makes chord recommendations that aim to be both diverse and appropriate to the current context. Composers can use it to help select the next chord, or to replace sequences of chords in an internally consistent manner. To make such recommendations, we adapt a neural network model from natural language processing known as Word2Vec to the music domain. This model learns chord embeddings from a corpus of chord sequences, placing chords nearby when they are used in similar contexts. The learned embeddings support creative substitutions between chords, and also exhibit topological properties that correspond to musical structure. For example, the major and minor chords are both arranged in the latent space in shapes corresponding to the circle-of-fifths. Our structured observations with 14 music students show that the tool helped them explore a wider palette of chords, and to make "big jumps in just a few chords". It gave them "new ideas of ways to move forward in the piece", not just on a chord-to-chord level but also between phrases. Our controlled studies with 9 more music students show that more adventurous chords are adopted when composing with ChordRipple.},
 acmid = {2856792},
 address = {New York, NY, USA},
 author = {Huang, Cheng-Zhi Anna and Duvenaud, David and Gajos, Krzysztof Z.},
 booktitle = {Proceedings of the 21st International Conference on Intelligent User Interfaces},
 doi = {10.1145/2856767.2856792},
 isbn = {978-1-4503-4137-0},
 keyword = {chords, creativity support tools, recommender systems, neural language models, embeddings, harmony, music, songwriting},
 link = {http://doi.acm.org/10.1145/2856767.2856792},
 location = {Sonoma, California, USA},
 numpages = {10},
 pages = {241--250},
 publisher = {ACM},
 series = {IUI '16},
 title = {ChordRipple: Recommending Chords to Help Novice Composers Go Beyond the Ordinary},
 year = {2016}
}


@inproceedings{Avrahami:2016:SMV:2856767.2856801,
 abstract = {The use of videoconferencing in the workplace has been steadily growing. While multitasking during video conferencing is often necessary, it is also viewed as impolite and sometimes unacceptable. One potential contributor to negative attitudes towards such multitasking is the disrupted sense of eye contact that occurs when an individual shifts their gaze away to another screen, for example, in a dual-monitor setup, common in office settings. We present an approach to improve a sense of eye contact over videoconferencing in dual-monitor setups. Our approach uses computer vision and desktop activity detection to dynamically choose the camera with the best view of a user's face. We describe two alternative implementations of our solution (RGB-only, and a combination of RGB and RGB-D cameras). We then describe results from an online experiment that shows the potential of our approach to significantly improve perceptions of a person's politeness and engagement in the meeting.},
 acmid = {2856801},
 address = {New York, NY, USA},
 author = {Avrahami, Daniel and van Everdingen, Eveline and Marlow, Jennifer},
 booktitle = {Proceedings of the 21st International Conference on Intelligent User Interfaces},
 doi = {10.1145/2856767.2856801},
 isbn = {978-1-4503-4137-0},
 keyword = {gaze tracking, head-pose, intelligent camera switching, multitasking, video conferencing},
 link = {http://doi.acm.org/10.1145/2856767.2856801},
 location = {Sonoma, California, USA},
 numpages = {5},
 pages = {130--134},
 publisher = {ACM},
 series = {IUI '16},
 title = {Supporting Multitasking in Video Conferencing Using Gaze Tracking and On-Screen Activity Detection},
 year = {2016}
}


@inproceedings{Davis:2016:ESP:2856767.2856795,
 abstract = {This paper reports on the design and evaluation of a co-creative drawing partner called the Drawing Apprentice, which was designed to improvise and collaborate on abstract sketches with users in real time. The system qualifies as a new genre of creative technologies termed "casual creators" that are meant to creatively engage users and provide enjoyable creative experiences rather than necessarily helping users make a higher quality creative product. We introduce the conceptual framework of participatory sense-making and describe how it can help model and understand open-ended collaboration. We report the results of a user study comparing human-human collaboration to human-computer collaboration using the Drawing Apprentice system. Based on insights from the user study, we present a set of design recommendations for co-creative agents.},
 acmid = {2856795},
 address = {New York, NY, USA},
 author = {Davis, Nicholas and Hsiao, Chih-PIn and Yashraj Singh, Kunwar and Li, Lisa and Magerko, Brian},
 booktitle = {Proceedings of the 21st International Conference on Intelligent User Interfaces},
 doi = {10.1145/2856767.2856795},
 isbn = {978-1-4503-4137-0},
 keyword = {collaboration, computational creativity, creativity support tools},
 link = {http://doi.acm.org/10.1145/2856767.2856795},
 location = {Sonoma, California, USA},
 numpages = {12},
 pages = {196--207},
 publisher = {ACM},
 series = {IUI '16},
 title = {Empirically Studying Participatory Sense-Making in Abstract Drawing with a Co-Creative Cognitive Agent},
 year = {2016}
}


@inproceedings{diSciascio:2016:RYG:2856767.2856797,
 abstract = {Whenever users engage in gathering and organizing new information, searching and browsing activities emerge at the core of the exploration process. As the process unfolds and new knowledge is acquired, interest drifts occur inevitably and need to be accounted for. Despite the advances in retrieval and recommender algorithms, real-world interfaces have remained largely unchanged: results are delivered in a relevance-ranked list. However, it quickly becomes cumbersome to reorganize resources along new interests, as any new search brings new results. We introduce uRank and investigate interactive methods for understanding, refining and reorganizing documents on-the-fly as information needs evolve. uRank includes views summarizing the contents of a recommendation set and interactive methods conveying the role of users' interests through a recommendation ranking. A formal evaluation showed that gathering items relevant to a particular topic of interest with uRank incurs in lower cognitive load compared to a traditional ranked list. A second study consisting in an ecological validation reports on usage patterns and usability of the various interaction techniques within a free, more natural setting.},
 acmid = {2856797},
 address = {New York, NY, USA},
 author = {di Sciascio, Cecilia and Sabol, Vedran and Veas, Eduardo E.},
 booktitle = {Proceedings of the 21st International Conference on Intelligent User Interfaces},
 doi = {10.1145/2856767.2856797},
 isbn = {978-1-4503-4137-0},
 keyword = {interaction, search interface, user study, visual analytics},
 link = {http://doi.acm.org/10.1145/2856767.2856797},
 location = {Sonoma, California, USA},
 numpages = {12},
 pages = {118--129},
 publisher = {ACM},
 series = {IUI '16},
 title = {Rank As You Go: User-Driven Exploration of Search Results},
 year = {2016}
}


@inproceedings{Daee:2016:IIM:2856767.2856803,
 abstract = {In exploratory search, the user starts with an uncertain information need and provides relevance feedback to the system's suggestions to direct the search. The search system learns the user intent based on this feedback and employs it to recommend novel results. However, the amount of user feedback is very limited compared to the size of the information space to be explored. To tackle this problem, we take into account user feedback on both the retrieved items (documents) and their features (keywords). In order to combine feedback from multiple domains, we introduce a coupled multi-armed bandits algorithm, which employs a probabilistic model of the relationship between the domains. Simulation results show that with multi-domain feedback, the search system can find the relevant items in fewer iterations than with only one domain. A preliminary user study indicates improvement in user satisfaction and quality of retrieved information.},
 acmid = {2856803},
 address = {New York, NY, USA},
 author = {Daee, Pedram and Pyykk\"{o}, Joel and Glowacka, Dorota and Kaski, Samuel},
 booktitle = {Proceedings of the 21st International Conference on Intelligent User Interfaces},
 doi = {10.1145/2856767.2856803},
 isbn = {978-1-4503-4137-0},
 keyword = {exploratory search, intent modeling, multi-armed bandits, probabilistic user models, relevance feedback},
 link = {http://doi.acm.org/10.1145/2856767.2856803},
 location = {Sonoma, California, USA},
 numpages = {5},
 pages = {71--75},
 publisher = {ACM},
 series = {IUI '16},
 title = {Interactive Intent Modeling from Multiple Feedback Domains},
 year = {2016}
}


@inproceedings{Paudyal:2016:SPN:2856767.2856794,
 abstract = {Communication and collaboration between deaf people and hearing people is hindered by lack of a common language. Although there has been a lot of research in this domain, there is room for work towards a system that is ubiquitous, non-invasive, works in real-time and can be trained interactively by the user. Such a system will be powerful enough to translate gestures performed in real-time, while also being flexible enough to be fully personalized to be used as a platform for gesture based HCI. We propose SCEPTRE which utilizes two non-invasive wrist-worn devices to decipher gesture-based communication. The system uses a multi-tiered template based comparison system for classification on input data from accelerometer, gyroscope and electromyography (EMG) sensors. This work demonstrates that the system is very easily trained using just one to three training instances each for twenty randomly chosen signs from the American Sign Language(ASL) dictionary and also for user-generated custom gestures. The system is able to achieve an accuracy of 97.72% for ASL gestures.},
 acmid = {2856794},
 address = {New York, NY, USA},
 author = {Paudyal, Prajwal and Banerjee, Ayan and Gupta, Sandeep K.S.},
 booktitle = {Proceedings of the 21st International Conference on Intelligent User Interfaces},
 doi = {10.1145/2856767.2856794},
 isbn = {978-1-4503-4137-0},
 keyword = {accelerometer, emg, gesture-based interfaces, gyroscope, pervasive computing, sign language processing, wearable computing},
 link = {http://doi.acm.org/10.1145/2856767.2856794},
 location = {Sonoma, California, USA},
 numpages = {12},
 pages = {282--293},
 publisher = {ACM},
 series = {IUI '16},
 title = {SCEPTRE: A Pervasive, Non-Invasive, and Programmable Gesture Recognition Technology},
 year = {2016}
}


@inproceedings{Sun:2016:IAH:2856767.2856818,
 abstract = {People are able to interact with domain-specific intelligent assistants (IAs) and get help with tasks. But sometimes user goals are complex and may require interactions with multiple applications. However current IAs are limited to specific applications and users have to directly manage execution spanning multiple applications in order to engage in more complex activities. An ideal personal agent would be able to learn, over time, about tasks that span different resources. This paper addresses the problem of cross-domain task assistance in the context of spoken dialogue systems. We propose approaches to discover users' high-level intentions and using this information to assist users in their task. We collected real-life smartphone usage data from 14 participants and investigated how to extract high-level intents from users' descriptions of their activities. Our experiments show that understanding high-level tasks allows the agent to actively suggest apps relevant to pursuing particular user goals and reduce the cost of users' self-management.},
 acmid = {2856818},
 address = {New York, NY, USA},
 author = {Sun, Ming and Chen, Yun-Nung and Rudnicky, Alexander I.},
 booktitle = {Proceedings of the 21st International Conference on Intelligent User Interfaces},
 doi = {10.1145/2856767.2856818},
 isbn = {978-1-4503-4137-0},
 keyword = {language understanding, multi-domain, spoken dialog system (sds), user intention},
 link = {http://doi.acm.org/10.1145/2856767.2856818},
 location = {Sonoma, California, USA},
 numpages = {6},
 pages = {169--174},
 publisher = {ACM},
 series = {IUI '16},
 title = {An Intelligent Assistant for High-Level Task Understanding},
 year = {2016}
}


@inproceedings{Graells-Garrido:2016:DPI:2856767.2856776,
 abstract = {In micro-blogging platforms, people connect and interact with others. However, due to cognitive biases, they tend to interact with like-minded people and read agreeable information only. Many efforts to make people connect with those who think differently have not worked well. In this paper, we hypothesize, first, that previous approaches have not worked because they have been direct -- they have tried to explicitly connect people with those having opposing views on sensitive issues. Second, that neither recommendation or presentation of information by themselves are enough to encourage behavioral change. We propose a platform that mixes a recommender algorithm and a visualization-based user interface to explore recommendations. It recommends politically diverse profiles in terms of distance of latent topics, and displays those recommendations in a visual representation of each user's personal content. We performed an" in the wild" evaluation of this platform, and found that people explored more recommendations when using a biased algorithm instead of ours. In line with our hypothesis, we also found that the mixture of our recommender algorithm and our user interface, allowed politically interested users to exhibit an unbiased exploration of the recommended profiles. Finally, our results contribute insights in two aspects: first, which individual differences are important when designing platforms aimed at behavioral change; and second, which algorithms and user interfaces should be mixed to help users avoid cognitive mechanisms that lead to biased behavior.},
 acmid = {2856776},
 address = {New York, NY, USA},
 author = {Graells-Garrido, Eduardo and Lalmas, Mounia and Baeza-Yates, Ricardo},
 booktitle = {Proceedings of the 21st International Conference on Intelligent User Interfaces},
 doi = {10.1145/2856767.2856776},
 isbn = {978-1-4503-4137-0},
 keyword = {homophily, information visualization., recommender systems, selective exposure},
 link = {http://doi.acm.org/10.1145/2856767.2856776},
 location = {Sonoma, California, USA},
 numpages = {13},
 pages = {228--240},
 publisher = {ACM},
 series = {IUI '16},
 title = {Data Portraits and Intermediary Topics: Encouraging Exploration of Politically Diverse Profiles},
 year = {2016}
}


@inproceedings{Roemmele:2016:RHA:2856767.2856793,
 abstract = {People naturally anthropomorphize the movement of nonliving objects, as social psychologists Fritz Heider and Marianne Simmel demonstrated in their influential 1944 research study. When they asked participants to narrate an animated film of two triangles and a circle moving in and around a box, participants described the shapes' movement in terms of human actions. Using a framework for authoring and annotating animations in the style of Heider and Simmel, we established new crowdsourced datasets where the motion trajectories of animated shapes are labeled according to the actions they depict. We applied two machine learning approaches, a spatial-temporal bag-of-words model and a recurrent neural network, to the task of automatically recognizing actions in these datasets. Our best results outperformed a majority baseline and showed similarity to human performance, which encourages further use of these datasets for modeling perception from motion trajectories. Future progress on simulating human-like motion perception will require models that integrate motion information with top-down contextual knowledge.},
 acmid = {2856793},
 address = {New York, NY, USA},
 author = {Roemmele, Melissa and Morgens, Soja-Marie and Gordon, Andrew S. and Morency, Louis-Philippe},
 booktitle = {Proceedings of the 21st International Conference on Intelligent User Interfaces},
 doi = {10.1145/2856767.2856793},
 isbn = {978-1-4503-4137-0},
 keyword = {animation, crowdsourcing, machine learning, touch/haptic/pointing/gesture},
 link = {http://doi.acm.org/10.1145/2856767.2856793},
 location = {Sonoma, California, USA},
 numpages = {11},
 pages = {271--281},
 publisher = {ACM},
 series = {IUI '16},
 title = {Recognizing Human Actions in the Motion Trajectories of Shapes},
 year = {2016}
}


@inproceedings{Kim:2016:LMA:2856767.2856815,
 abstract = {Current pedestrian collision warning systems use either auditory alarms or visual symbols to inform drivers. These traditional approaches cannot tell the driver where the detected pedestrians are located, which is critical for the driver to respond appropriately. To address this problem, we introduce a new driver interface taking advantage of a volumetric head-up display (HUD). In our experimental user study, sixteen participants drove a test vehicle in a parking lot while braking for crossing pedestrians using different interface designs on the HUD. Our results showed that spatial information provided by conformal graphics on the HUD resulted in not only better driver performance but also smoother braking behavior as compared to the baseline.},
 acmid = {2856815},
 address = {New York, NY, USA},
 author = {Kim, Hyungil and Miranda Anon, Alexandre and Misu, Teruhisa and Li, Nanxiang and Tawari, Ashish and Fujimura, Kikuo},
 booktitle = {Proceedings of the 21st International Conference on Intelligent User Interfaces},
 doi = {10.1145/2856767.2856815},
 isbn = {978-1-4503-4137-0},
 keyword = {augmented reality, depth perception, driver vehicle interface, head up display, pedestrian collision warning},
 link = {http://doi.acm.org/10.1145/2856767.2856815},
 location = {Sonoma, California, USA},
 numpages = {5},
 pages = {294--298},
 publisher = {ACM},
 series = {IUI '16},
 title = {Look at Me: Augmented Reality Pedestrian Warning System Using an In-Vehicle Volumetric Head Up Display},
 year = {2016}
}


@inproceedings{Nakano:2016:PIU:2856767.2856809,
 abstract = {We propose a novel interface that allows the user to interactively change the playback order of multiple songs by choosing one or more criteria. The criteria include not only the song's title and artist name but also its content automatically estimated by music/singing signal processing and artist-level social analysis. The artist-level social information is discovered from Wikipedia and DBpedia. With regard to manipulating playback order, existing interfaces typically allow the user to change it manually or automatically by choosing one of a few types of criteria. The proposed interface, on the other hand, deals with nine properties and multiple integrations of them (e.g., vocal gender and beats per minute). To realize the ordering by multiple criteria, a distance matrix is computed from the criteria vectors and is then used to estimate paths for ascending, descending, and random orders by applying principle component analysis or to estimate a path for a smooth order by solving the travelling salesman problem.},
 acmid = {2856809},
 address = {New York, NY, USA},
 author = {Nakano, Tomoyasu and Kato, Jun and Hamasaki, Masahiro and Goto, Masataka},
 booktitle = {Proceedings of the 21st International Conference on Intelligent User Interfaces},
 doi = {10.1145/2856767.2856809},
 isbn = {978-1-4503-4137-0},
 keyword = {music playlist, musical commonness, playback order, social commonness, visualization, vocal gender},
 link = {http://doi.acm.org/10.1145/2856767.2856809},
 location = {Sonoma, California, USA},
 numpages = {5},
 pages = {186--190},
 publisher = {ACM},
 series = {IUI '16},
 title = {PlaylistPlayer: An Interface Using Multiple Criteria to Change the Playback Order of a Music Playlist},
 year = {2016}
}


@inproceedings{Rajan:2016:TLE:2856767.2856769,
 abstract = {Human performance falls off predictably with excessive task difficulty. This paper reports on a search for a task load estimation metric. Of the five physiological signals analyzed from a multitasking study, only pupil dilation measures correlated well with real-time task load. The paper introduces a novel task load estimation model based on pupil dilation measures. We demonstrate its effectiveness in a multitasking driving scenario. Autonomous mediation of notifications using this model significantly improved user task performance compared to no mediation. The model showed promise even when used outside in a car. Results were achieved using low-cost cameras and open-source measurement tools lending to its potential to be used broadly.},
 acmid = {2856769},
 address = {New York, NY, USA},
 author = {Rajan, Rahul and Selker, Ted and Lane, Ian},
 booktitle = {Proceedings of the 21st International Conference on Intelligent User Interfaces},
 doi = {10.1145/2856767.2856769},
 isbn = {978-1-4503-4137-0},
 keyword = {cognitive load, considerate systems, divided attention, psychophysiological measures, pupil dilation},
 link = {http://doi.acm.org/10.1145/2856767.2856769},
 location = {Sonoma, California, USA},
 numpages = {12},
 pages = {48--59},
 publisher = {ACM},
 series = {IUI '16},
 title = {Task Load Estimation and Mediation Using Psycho-physiological Measures},
 year = {2016}
}


@inproceedings{Seeliger:2016:BTC:2856767.2856777,
 abstract = {Multitasking and interruptions in information work make frequent activity switches necessary. Individuals need to recall and restore earlier states of work which generally involves retrieval of information objects. To avoid resulting tooling time an activity-centric organization of information objects has been proposed. For each activity a collection with related information objects (like documents, websites etc.) is created to improve information access and serve as a memory aid. While the manual maintenance of such information collections is a tedious task and becomes an interruption on its own, the automatic maintenance of such collections using activity mining is promising. Activity mining utilizes interaction histories to extract unique activities based on the stream of interaction with information objects. For activity mining, existing work shows varying success in limited study setups. In this paper, we present a method for activity mining to generate activity-centric information object collections automatically from interaction histories. The technique is a hybrid approach considering all information types used in previous work -- activity stream and accessed content related information. Method performance is evaluated based on interaction histories collected during real work data from eight information workers collected over several weeks. For the dataset our hybrid approach shows on average a performance of 0.53 ARI up to 0.77 ARI, outperforming single metric-based approaches.},
 acmid = {2856777},
 address = {New York, NY, USA},
 author = {Seeliger, Alexander and Schmidt, Benedikt and Schweizer, Immanuel and M\"{u}hlh\"{a}user, Max},
 booktitle = {Proceedings of the 21st International Conference on Intelligent User Interfaces},
 doi = {10.1145/2856767.2856777},
 isbn = {978-1-4503-4137-0},
 keyword = {activity mining, document organization, information work},
 link = {http://doi.acm.org/10.1145/2856767.2856777},
 location = {Sonoma, California, USA},
 numpages = {11},
 pages = {60--70},
 publisher = {ACM},
 series = {IUI '16},
 title = {What Belongs Together Comes Together: Activity-centric Document Clustering for Information Work},
 year = {2016}
}


@inproceedings{Bogina:2016:LIT:2856767.2856781,
 abstract = {Predicting whether a session is a buying session (e.g. will end with buying an item) is an ongoing research task. Drawing from recent experience in Web search and movie recommenders, we explore the effect of temporal trends and characteristics on the ability to predict buying sessions. We suggest a new approach, based on items' temporal dynamics, together with sessions' temporal aspects for predicting whether a session is going to end up with a purchase. We suggest a model for estimating the probability of a session to end with a purchase, according to the purchase history of items clicked on during the session over the past few days. The predictions can be used by recommender systems, enabling them to take relevant actions, thus improving shoppers experience as well as increasing sales for e-commerce companies. Our findings shed light on the importance of considering temporal dynamics in items recommendations in e-commerce sites. Empirical results on imbalanced e-commerce dataset with more than nine million sessions demonstrate that we achieve high Precision, Recall and ROC in predicting whether session ends up with a purchase or not.},
 acmid = {2856781},
 address = {New York, NY, USA},
 author = {Bogina, Veronika and Kuflik, Tsvi and Mokryn, Osnat},
 booktitle = {Proceedings of the 21st International Conference on Intelligent User Interfaces},
 doi = {10.1145/2856767.2856781},
 isbn = {978-1-4503-4137-0},
 keyword = {electronic commerce, imbalanced data set, machine learning, recommender systems, temporal dynamics},
 link = {http://doi.acm.org/10.1145/2856767.2856781},
 location = {Sonoma, California, USA},
 numpages = {5},
 pages = {251--255},
 publisher = {ACM},
 series = {IUI '16},
 title = {Learning Item Temporal Dynamics for Predicting Buying Sessions},
 year = {2016}
}


@inproceedings{Kang:2016:AIL:2856767.2856789,
 abstract = {We present AnalyticalInk, a novel math learning environment prototype that uses a semantic graph as the knowledge representation of algebraic and geometric word problems. The system solves math problems by reasoning upon the semantic graph and automatically generates conceptual and procedural scaffoldings in sequence. We further introduces a step-wise tutoring framework, which can check students' input steps and provide the adaptive scaffolding feedback. Based on the knowledge representation, AnalyticalInk highlights keywords that allow users to further drag them onto the workspace to gather insight into the problem's initial conditions. The system simulates a pen-and-paper environment to let users input both in algebraic and geometric workspaces. We conducted an usability evaluation to measure the effectiveness of AnalyticalInk. We found that keyword highlighting and dragging is useful and effective toward math problem solving. Answer checking in the tutoring component is useful. In general, our prototype shows the promise in helping users to understand geometrical concepts and master algebraic procedures under the problem solving.},
 acmid = {2856789},
 address = {New York, NY, USA},
 author = {Kang, Bo and Kulshreshth, Arun and LaViola,Jr., Joseph J.},
 booktitle = {Proceedings of the 21st International Conference on Intelligent User Interfaces},
 doi = {10.1145/2856767.2856789},
 isbn = {978-1-4503-4137-0},
 keyword = {inference and reasoning, intelligent tutoring system, learning techniques., scaffolding generation, semantic representation, user modeling},
 link = {http://doi.acm.org/10.1145/2856767.2856789},
 location = {Sonoma, California, USA},
 numpages = {12},
 pages = {419--430},
 publisher = {ACM},
 series = {IUI '16},
 title = {AnalyticalInk: An Interactive Learning Environment for Math Word Problem Solving},
 year = {2016}
}


@inproceedings{Pittman:2016:FFA:2856767.2856808,
 abstract = {We explore the benefits of intelligent prototype selection for $-family recognizers. Currently, the state of the art is to randomly select a subset of prototypes from a dataset without any processing. This results in reduced computation time for the recognizer, but also increases error rates. We propose applying optimization algorithms, specifically random mutation hill climb and a genetic algorithm, to search for reduced sets of prototypes that minimize recognition error. After an evaluation, we found that error rates could be reduced compared to random selection and rapidly approached the baseline accuracies for a number of different $-family recognizers.},
 acmid = {2856808},
 address = {New York, NY, USA},
 author = {Pittman, Corey and Taranta II, Eugene M. and LaViola,Jr., Joseph J.},
 booktitle = {Proceedings of the 21st International Conference on Intelligent User Interfaces},
 doi = {10.1145/2856767.2856808},
 isbn = {978-1-4503-4137-0},
 keyword = {classifier, gesture recognition, rapid prototyping, user interfaces},
 link = {http://doi.acm.org/10.1145/2856767.2856808},
 location = {Sonoma, California, USA},
 numpages = {5},
 pages = {370--374},
 publisher = {ACM},
 series = {IUI '16},
 title = {A \$-Family Friendly Approach to Prototype Selection},
 year = {2016}
}


@inproceedings{Amatriain:2016:PPF:2856767.2856798,
 abstract = {In 2006, Netflix announced a $1M prize competition to advance recommendation algorithms. The recommendation problem was simplified as the accuracy in predicting a user rating measured by the Root Mean Squared Error. While that formulation helped get the attention of the research community, it put the focus on the wrong approach and metric while leaving many important factors out, in particular the UI. In this talk I will talk of the Netflix Prize as the past of recommendations. I will then describe the present from an industry perspective based on my personal experience at Netflix first and Quora now . I will describe the different components of modern recommender systems such as: personalized ranking, similarity, explanations, context-awareness, or search as recommendation. I will also review the usage of novel algorithmic approaches such as Factorization Machines, Restricted Boltzmann Machines, SimRank, Deep Neural Networks, or Listwise Learning-to-rank. We will see how those components and algorithmic approaches can be used to recommend not only movies, but also questions, answers, topics, or users. But, most importantly, I will give many examples of prototypical industrial-scale recommender systems with special focus on the user interface and its interaction with the algorithms. It is clearly in the interface of the UI and the novel algorithms where the future of recommender systems lays.},
 acmid = {2856798},
 address = {New York, NY, USA},
 author = {Amatriain, Xavier},
 booktitle = {Proceedings of the 21st International Conference on Intelligent User Interfaces},
 doi = {10.1145/2856767.2856798},
 isbn = {978-1-4503-4137-0},
 keyword = {recommender systems, user interfaces},
 link = {http://doi.acm.org/10.1145/2856767.2856798},
 location = {Sonoma, California, USA},
 numpages = {1},
 pages = {1--1},
 publisher = {ACM},
 series = {IUI '16},
 title = {Past, Present, and Future of Recommender Systems: An Industry Perspective},
 year = {2016}
}


@inproceedings{Mahmud:2016:PAA:2856767.2856800,
 abstract = {In this paper, we present computational models to predict Twitter users' attitude towards a specific brand through their personal and social characteristics. We also predict their likelihood of taking different actions based on their attitudes. In order to operationalize our research on users' attitude and actions, we collected ground-truth data through surveys of Twitter users. We have conducted experiments using two real world datasets to validate the effectiveness of our attitude and action prediction framework. Finally, we show how our models can be integrated with a visual analytics system for customer intervention.},
 acmid = {2856800},
 address = {New York, NY, USA},
 author = {Mahmud, Jalal and Fei, Geli and Xu, Anbang and Pal, Aditya and Zhou, Michelle},
 booktitle = {Proceedings of the 21st International Conference on Intelligent User Interfaces},
 doi = {10.1145/2856767.2856800},
 isbn = {978-1-4503-4137-0},
 keyword = {attitude, brand, social media, twitter},
 link = {http://doi.acm.org/10.1145/2856767.2856800},
 location = {Sonoma, California, USA},
 numpages = {5},
 pages = {2--6},
 publisher = {ACM},
 series = {IUI '16},
 title = {Predicting Attitude and Actions of Twitter Users},
 year = {2016}
}


@inproceedings{Muhammad:2016:LSO:2856767.2856813,
 abstract = {This paper describes an approach for generating rich and compelling explanations in recommender systems, based on opinions mined from user-generated reviews. The explanations highlight the features of a recommended item that matter most to the user and also relate them to other recommendation alternatives and the user's past activities to provide a context.},
 acmid = {2856813},
 address = {New York, NY, USA},
 author = {Muhammad, Khalil Ibrahim and Lawlor, Aonghus and Smyth, Barry},
 booktitle = {Proceedings of the 21st International Conference on Intelligent User Interfaces},
 doi = {10.1145/2856767.2856813},
 isbn = {978-1-4503-4137-0},
 keyword = {explanations, opinion mining, recommender systems},
 link = {http://doi.acm.org/10.1145/2856767.2856813},
 location = {Sonoma, California, USA},
 numpages = {5},
 pages = {256--260},
 publisher = {ACM},
 series = {IUI '16},
 title = {A Live-User Study of Opinionated Explanations for Recommender Systems},
 year = {2016}
}


@inproceedings{Kosunen:2016:RNI:2856767.2856796,
 abstract = {Meditation in general and mindfulness in particular have been shown to be useful techniques in the treatment of a plethora of ailments, yet they can be challenging for novices. We present RelaWorld: a neuroadaptive virtual reality meditation system that combines virtual reality with neurofeedback to provide a tool that is easy for novices to use yet provides added value even for experienced meditators. Using a head-mounted display, users can levitate in a virtual world by doing meditation exercises. The system measures users' brain activity in real time via EEG and calculates estimates for the level of conCentration and relaxation. These values are then mapped into the virtual reality. In a user study of 43 subjects, we were able to show that the RelaWorld system elicits deeper relaxation, feeling of presence and a deeper level of meditation when compared to a similar setup without head-mounted display or neurofeedback.},
 acmid = {2856796},
 address = {New York, NY, USA},
 author = {Kosunen, Ilkka and Salminen, Mikko and J\"{a}rvel\"{a}, Simo and Ruonala, Antti and Ravaja, Niklas and Jacucci, Giulio},
 booktitle = {Proceedings of the 21st International Conference on Intelligent User Interfaces},
 doi = {10.1145/2856767.2856796},
 isbn = {978-1-4503-4137-0},
 keyword = {meditation, mindfulness, neurofeedback, virtual reality},
 link = {http://doi.acm.org/10.1145/2856767.2856796},
 location = {Sonoma, California, USA},
 numpages = {10},
 pages = {208--217},
 publisher = {ACM},
 series = {IUI '16},
 title = {RelaWorld: Neuroadaptive and Immersive Virtual Reality Meditation System},
 year = {2016}
}


@inproceedings{Wang:2016:CMA:2856767.2856817,
 abstract = {Recent advancement of smart devices and wearable tech-nologies greatly enlarges the variety of personal data people can track. Applications and services can leverage such data to provide better life support, but also impose privacy and security threats. Obfuscation schemes, consequently, have been developed to retain data access while mitigate risks. Compared to offering choices of releasing raw data and not releasing at all, we examine the effect of adding a data obfuscation option on users' disclosure decisions when configuring applications' access, and how that effect varies with data types and application contexts. Our online user experiment shows that users are less likely to block data access when the obfuscation option is available except for locations. This effect significantly differs between applications for domain-specific dynamic tracking data, but not for generic personal traits. We further unpack the role of context and discuss the design opportunities.},
 acmid = {2856817},
 address = {New York, NY, USA},
 author = {Wang, Jing and Wang, Na and Jin, Hongxia},
 booktitle = {Proceedings of the 21st International Conference on Intelligent User Interfaces},
 doi = {10.1145/2856767.2856817},
 isbn = {978-1-4503-4137-0},
 keyword = {data obfuscation, mobile, privacy, tracking, user behavior},
 link = {http://doi.acm.org/10.1145/2856767.2856817},
 location = {Sonoma, California, USA},
 numpages = {6},
 pages = {299--304},
 publisher = {ACM},
 series = {IUI '16},
 title = {Context Matters?: How Adding the Obfuscation Option Affects End Users' Data Disclosure Decisions},
 year = {2016}
}


@inproceedings{Sha:2016:CNS:2856767.2856772,
 abstract = {The recent explosion of sports tracking data has dramatically increased the interest in effective data processing and access of sports plays (i.e., short trajectory sequences of players and the ball). And while there exist systems that offer improved categorizations of sports plays (e.g., into relatively coarse clusters), to the best of our knowledge there does not exist any retrieval system that can effectively search for the most relevant plays given a specific input query. One significant design challenge is how best to phrase queries for multi-agent spatiotemporal trajectories such as sports plays.We have developed a novel query paradigm and retrieval system, which we call Chalkboarding, that allows the user to issue queries by drawing a play of interest (similar to how coaches draw up plays). Our system utilizes effective alignment, templating, and hashing techniques tailored to multi-agent trajectories, and achieves accurate play retrieval at interactive speeds.We showcase the efficacy of our approach in a user study, where we demonstrate orders-of-magnitude improvements in search quality compared to baseline systems.},
 acmid = {2856772},
 address = {New York, NY, USA},
 author = {Sha, Long and Lucey, Patrick and Yue, Yisong and Carr, Peter and Rohlf, Charlie and Matthews, Iain},
 booktitle = {Proceedings of the 21st International Conference on Intelligent User Interfaces},
 doi = {10.1145/2856767.2856772},
 isbn = {978-1-4503-4137-0},
 keyword = {spatiotemporal retrieval, sports analytics, user interfaces},
 link = {http://doi.acm.org/10.1145/2856767.2856772},
 location = {Sonoma, California, USA},
 numpages = {12},
 pages = {336--347},
 publisher = {ACM},
 series = {IUI '16},
 title = {Chalkboarding: A New Spatiotemporal Query Paradigm for Sports Play Retrieval},
 year = {2016}
}


@inproceedings{Hoque:2016:MVT:2856767.2856782,
 abstract = {Online conversations, such as blogs, provide rich amount of information and opinions about popular queries. Given a query, traditional blog sites return a set of conversations often consisting of thousands of comments with complex thread structure. Since the interfaces of these blog sites do not provide any overview of the data, it becomes very difficult for the user to explore and analyze such a large amount of conversational data. In this paper, we present MultiConVis, a visual text analytics system designed to support the exploration of a collection of online conversations. Our system tightly integrates NLP techniques for topic modeling and sentiment analysis with information visualizations, by considering the unique characteristics of online conversations. The resulting interface supports the user exploration, starting from a possibly large set of conversations, then narrowing down to the subset of conversations, and eventually drilling-down to the set of comments of one conversation. Our evaluations through case studies with domain experts and a formal user study with regular blog readers illustrate the potential benefits of our approach, when compared to a traditional blog reading interface.},
 acmid = {2856782},
 address = {New York, NY, USA},
 author = {Hoque, Enamul and Carenini, Giuseppe},
 booktitle = {Proceedings of the 21st International Conference on Intelligent User Interfaces},
 doi = {10.1145/2856767.2856782},
 isbn = {978-1-4503-4137-0},
 keyword = {asynchronous conversations, computer mediated communication., text visualization, visual text analytics},
 link = {http://doi.acm.org/10.1145/2856767.2856782},
 location = {Sonoma, California, USA},
 numpages = {12},
 pages = {96--107},
 publisher = {ACM},
 series = {IUI '16},
 title = {MultiConVis: A Visual Text Analytics System for Exploring a Collection of Online Conversations},
 year = {2016}
}


@inproceedings{Gall:2016:RMA:2856767.2856816,
 abstract = {Classical reminiscence therapy has been shown to effectively enhance the stability of memory and identity in people with dementia. Typically, reminiscence therapy uses biography artifacts like photos and personal items and objects. Today, many of these artifacts are from the digital realm providing new options to adapt or even improve the purely analog therapy. In this work we propose a method to enhance reminiscence therapy by computer simulated biographic associations. Our approach provides assistance for associative reasoning on affective stimuli and thus enables access to biographic content so that no deliberate search is required. We develop a recommender model for mapping mental states to biographic content based on similarity. The system dynamically adapts its state and the depicted digital artifacts to the responses of the user. It is a first step towards an immersive reminiscence therapy which will incorporate associated stimuli on multiple channels to increase effectiveness. A preliminary study showed encouraging results concerning the usability of the system.},
 acmid = {2856816},
 address = {New York, NY, USA},
 author = {Gall, Dominik and Lugrin, Jean-Luc and Wiebusch, Dennis and Latoschik, Marc Erich},
 booktitle = {Proceedings of the 21st International Conference on Intelligent User Interfaces},
 doi = {10.1145/2856767.2856816},
 isbn = {978-1-4503-4137-0},
 keyword = {computer-mediated health promotion, dementia, elderly people, immersion, reminiscence therapy},
 link = {http://doi.acm.org/10.1145/2856767.2856816},
 location = {Sonoma, California, USA},
 numpages = {5},
 pages = {191--195},
 publisher = {ACM},
 series = {IUI '16},
 title = {Remind Me: An Adaptive Recommendation-Based Simulation of Biographic Associations},
 year = {2016}
}


@inproceedings{Wang:2016:PCS:2856767.2856778,
 abstract = {Taking a picture has been traditionally a one-person task. In this paper we present a novel system that allows multiple mobile devices to work collaboratively in a synchronized fashion to capture a panorama of a highly dynamic scene, creating an entirely new photography experience that encourages social interactions and teamwork. Our system contains two components: a client app that runs on all participating devices, and a server program that monitors and communicates with each device. In a capturing session, the server collects in realtime the viewfinder images of all devices and stitches them on-the-fly to create a panorama preview, which is then streamed to all devices as visual guidance. The system also allows one camera to be the host and send direct visual instructions to others to guide camera adjustment. When ready, all devices take pictures at the same time for panorama stitching. Our preliminary study suggests that the proposed system can help users capture high quality panoramas with an enjoyable teamwork experience.},
 acmid = {2856778},
 address = {New York, NY, USA},
 author = {Wang, Yan and Cho, Sunghyun and Wang, Jue and Chang, Shih-Fu},
 booktitle = {Proceedings of the 21st International Conference on Intelligent User Interfaces},
 doi = {10.1145/2856767.2856778},
 isbn = {978-1-4503-4137-0},
 keyword = {collaborative, multi-device, panorama, photography, synchronized},
 link = {http://doi.acm.org/10.1145/2856767.2856778},
 location = {Sonoma, California, USA},
 numpages = {10},
 pages = {261--270},
 publisher = {ACM},
 series = {IUI '16},
 title = {PanoSwarm: Collaborative and Synchronized Multi-Device Panoramic Photography},
 year = {2016}
}


@proceedings{Nichols:2016:2876456,
 abstract = {It is with great pleasure that we welcome you to beautiful Sonoma, California and the 21st edition of ACM International Conference on Intelligent User Interfaces -- ACM IUI 2016. ACM IUI is "where HCI meets AI," or where the academic research communities of Human-Computer Interaction (HCI) and Artificial Intelligence (AI) intersect. As the premier international forum for reporting outstanding research and development on intelligent user interfaces, the conference welcomes submissions describing work at the cross-section of these two fields and other related fields, such as psychology, behavioral science, cognitive science, computer graphics, design, the arts, and many others. Members of the ACM IUI community are interested in improving the symbiosis between humans and computers, increasing the intelligence of both in the process. The call for papers attracted 194 long and short paper submissions, 31 poster paper submissions, 11 demo paper submissions, and 17 student consortium submissions. The final program of the conference includes 2 keynotes, 49 long and short papers (25.3% acceptance rate), 15 posters, 6 demos, 2 workshops, 1 tutorial, and 12 student consortium papers. We are particularly excited for the keynotes by two distinguished speakers that will open the first and third days of the conference. The opening keynote will be given by Xavier Amatriain, VP of Engineering at Quora, on the topic of the "Past, Present, and Future of Recommender Systems: An Industry Perspective." The closing keynote will be given by Professor Elisabeth André, from Augsburg University and a long-time member of the ACM IUI community, on the topic of "Socially-Sensitive Interfaces: From Offline Studies to Interactive Experiences." The conference could not be organized without the help of a large number of individuals who generously volunteered much of their own time. Their names can be found on the following pages. All of the members of the organizing committee have done a fantastic job of coordinating the many moving parts that go into putting on a great conference. We must also particularly thank our 34 senior program committee members for coordinating the papers review process and the 91 members of the program committee for providing high quality reviews. And, most important, we must thank the authors for their diligent work that resulted in so many great submissions. These have allowed us to develop the excellent program that is the enduring heart of the conference. Another key for any great conference is a collection of strong sponsor organizations and generous corporate supporters. Our sponsors ACM, SIGAI, and SIGCHI are instrumental in making the conference happen year in and year out. This year, SIGCHI and SIGAI were particularly generous in providing financial support for our student travel grants, which have enabled 19 students to attend the conference that might not have otherwise. Our corporate supporters, Microsoft, IBM, Google, and Tableau have been supremely generous and the conference would be weaker without their contributions. We hope you will find the program engaging and the mix of academic disciplines broadens your perspective on computing. We also hope the conference will provide you with a valuable opportunity to share ideas with other researchers and practitioners from around the world, whether through presenting your own work formally or through informal discussions during the banquet or coffee breaks. With luck, those shared ideas will manifest themselves in exciting papers at next year's conference and ultimately have impact far beyond the conference! If you have any suggestions for how to improve the conference either this year or in the future, please do not hesitate to let us know.},
 address = {New York, NY, USA},
 isbn = {978-1-4503-4140-0},
 location = {Sonoma, California, USA},
 publisher = {ACM},
 title = {IUI '16 Companion: Companion Publication of the 21st International Conference on Intelligent User Interfaces},
 year = {2016}
}


@inproceedings{Runge:2016:NMA:2856767.2856804,
 abstract = {Navigation systems allow drivers to find the shortest or fastest path between two or multiple locations mostly using time or distance as input parameters. Various researchers extended traditional route planning approaches by taking into account the user's preferences, such as enjoying a coastal view or alpine landscapes during a drive. Current approaches mainly rely on volunteered geographic information (VGI), such as point of interest (POI) data from OpenStreetMap, or social media data, such as geotagged photos from Flickr, to generate scenic routes. While these approaches use proximity, distribution or other spatial relationships of the data sets, they do not take into account the actual view on specific route segments. In this paper, we propose Autobahn: a system for generating scenic routes using Google Street View images to classify route segments based on their visual characteristics enhancing the driving experience. We show that this vision-based approach can complement other approaches for scenic route planning and introduce a personalized scenic route by aligning the characteristics of the route to the preferences of the user.},
 acmid = {2856804},
 address = {New York, NY, USA},
 author = {Runge, Nina and Samsonov, Pavel and Degraen, Donald and Sch\"{o}ning, Johannes},
 booktitle = {Proceedings of the 21st International Conference on Intelligent User Interfaces},
 doi = {10.1145/2856767.2856804},
 isbn = {978-1-4503-4137-0},
 keyword = {deep learning, google street view, intelligent user interfaces, scenic routes},
 link = {http://doi.acm.org/10.1145/2856767.2856804},
 location = {Sonoma, California, USA},
 numpages = {5},
 pages = {147--151},
 publisher = {ACM},
 series = {IUI '16},
 title = {No More Autobahn!: Scenic Route Generation Using Googles Street View},
 year = {2016}
}


@inproceedings{Wiebe:2016:EUA:2856767.2856814,
 abstract = {Feature-rich software applications offer users hundreds of commands, yet most people use only a very small fraction of the available command set. Command recommenders aim to increase awareness of an application's capabilities by generating personalized recommendations for new commands. A primary distinguishing characteristic of these recommenders concerns the manner in which they determine command relevance. Social approaches do so by analyzing community usage logs, whereas, task-based approaches mine web documentation for logical command clusters. Through a qualitative study with sixteen participants, in this work we explored user attitudes towards these different approaches and the supplemental information they enable.},
 acmid = {2856814},
 address = {New York, NY, USA},
 author = {Wiebe, Michelle and Geiskkovitch, Denise Y. and Bunt, Andrea},
 booktitle = {Proceedings of the 21st International Conference on Intelligent User Interfaces},
 doi = {10.1145/2856767.2856814},
 isbn = {978-1-4503-4137-0},
 keyword = {explanations, recommender systems, software learnability},
 link = {http://doi.acm.org/10.1145/2856767.2856814},
 location = {Sonoma, California, USA},
 numpages = {5},
 pages = {43--47},
 publisher = {ACM},
 series = {IUI '16},
 title = {Exploring User Attitudes Towards Different Approaches to Command Recommendation in Feature-Rich Software},
 year = {2016}
}


@inproceedings{Yadav:2016:VAP:2856767.2856788,
 abstract = {Instructional videos are one of the most popular ways of teaching and learning in an online setting. However, navigation in videos is linear as compared to other instructional resources such as textbooks, where a table of topics and a multi-faceted index of different anchor points i.e., list of figures, tables aid in efficiently navigating to a desired point of interest. There is a lack of appropriate techniques and interfaces which can support such textbook-style navigation in instructional videos. This paper presents a novel approach to automatically localize and classify different anchor points in a video including figures, tables, equations, flowcharts, code snippets and charts. Our approach uses a deep convolution neural network in a semi-supervised fashion where the training data is obtained from the unconstrained Internet images. On an anchor point dataset of about 10K images, the proposed algorithm leads to a classification accuracy of 86%. Further, we designed a system ViZig that uses these localized anchor points along with a automatically generated list of topics for non-linear video navigation and studied its effectiveness in real-world. Our user studies with 18 participants establish that the proposed video navigation mechanism provides statistically significant time savings as compared to the popularly used time-synched transcript along with youtube-style timeline scrubbing.},
 acmid = {2856788},
 address = {New York, NY, USA},
 author = {Yadav, Kuldeep and Gandhi, Ankit and Biswas, Arijit and Shrivastava, Kundan and Srivastava, Saurabh and Deshmukh, Om},
 booktitle = {Proceedings of the 21st International Conference on Intelligent User Interfaces},
 doi = {10.1145/2856767.2856788},
 isbn = {978-1-4503-4137-0},
 keyword = {convolutional neural network, deep learning, education, moocs, video indexing, video navigation, video summarization},
 link = {http://doi.acm.org/10.1145/2856767.2856788},
 location = {Sonoma, California, USA},
 numpages = {12},
 pages = {407--418},
 publisher = {ACM},
 series = {IUI '16},
 title = {ViZig: Anchor Points Based Non-Linear Navigation and Summarization in Educational Videos},
 year = {2016}
}


@inproceedings{Guerra:2016:IIL:2856767.2856784,
 abstract = {We present the Mastery Grids system, an intelligent interface for online learning content that combines open learner modeling (OLM) and social comparison features. We grounded the design of Mastery Grids in self-regulated learning and learning motivation theories, as well as in our past work in social comparison, OLM, and adaptive navigation support. The force behind the interface is the combination of adaptive navigation functionality with the mastery-oriented aspects of OLM and the performance-oriented aspects of social comparison. We examined different configurations of Mastery Grids in two classroom studies and report the results of analysis of log data and survey responses. The results show how Mastery Grids interacts with different factors, like gender and achievement-goal orientation, and ultimately, its impact on student engagement, performance, and motivation.},
 acmid = {2856784},
 address = {New York, NY, USA},
 author = {Guerra, Julio and Hosseini, Roya and Somyurek, Sibel and Brusilovsky, Peter},
 booktitle = {Proceedings of the 21st International Conference on Intelligent User Interfaces},
 doi = {10.1145/2856767.2856784},
 isbn = {978-1-4503-4137-0},
 keyword = {achievement-goal orientation, open learner model, self-regulated learning, social comparison},
 link = {http://doi.acm.org/10.1145/2856767.2856784},
 location = {Sonoma, California, USA},
 numpages = {12},
 pages = {152--163},
 publisher = {ACM},
 series = {IUI '16},
 title = {An Intelligent Interface for Learning Content: Combining an Open Learner Model and Social Comparison to Support Self-Regulated Learning and Engagement},
 year = {2016}
}


@inproceedings{PereiraSantos:2016:IPN:2856767.2856805,
 abstract = {Player behavior during game play can be used to construct player models that help adapt the game and make it more fun for the player involved. Similarly in-game behavior could help model personality traits that describe people's attitudes in a fashion that can be stable over time and over different domains, e.g., to support health coaching, or other behavior change approaches. This paper demonstrates the feasibility of this approach by relating Need for Cognition (NfC) a personality trait that can predict the effectiveness of different persuasion strategies upon users to a commonly used game mechanic -- hints. An experiment with N=188 participants confirmed our hypothesis that NfC has a negative correlation with the number of hints players follow during the game. Future work should confirm if adherence to hints can be used as a predictor of behavior in different games, and to find other game mechanics than hints, that help predict user traits.},
 acmid = {2856805},
 address = {New York, NY, USA},
 author = {Pereira Santos, Carlos and Khan, Vassilis-Javed and Markopoulos, Panos},
 booktitle = {Proceedings of the 21st International Conference on Intelligent User Interfaces},
 doi = {10.1145/2856767.2856805},
 isbn = {978-1-4503-4137-0},
 keyword = {game design., games with a purpose, need for cognition, player modeling, player profiling},
 link = {http://doi.acm.org/10.1145/2856767.2856805},
 location = {Sonoma, California, USA},
 numpages = {4},
 pages = {76--79},
 publisher = {ACM},
 series = {IUI '16},
 title = {Inferring A Player's Need For Cognition From Hints},
 year = {2016}
}


@inproceedings{Zhao:2016:EPC:2856767.2856771,
 abstract = {Location-sharing services such as Facebook and Foursquare/Swarm have become increasingly popular, due to the ease at which users can share their locations, and participate in services, games and other applications that leverage these locations. But it is important for people who use these services to configure appropriate location-privacy preferences so that they can control to whom they want to share their location information. Manually configuring these preferences may be burdensome and confusing, and so location-privacy preference recommenders based on crowdsourcing preferences from other users have been proposed. Whether people will accept the recommended preferences acquired from other users, who they may not know or trust, has not, however, been investigated. In this paper, we present a user experiment (n=99) to explore what factors influence people's acceptance of location-privacy preference recommenders. We find that 44% of our participants have privacy concerns about such recommenders. These concerns are shown to have a negative effect (p<0.001) on their acceptance of the recommendations and their satisfaction about their choices. Furthermore, users' acceptance of recommenders varies according to both context and recommendations being made. Our findings are potentially useful to designers of location-sharing services and privacy recommenders.},
 acmid = {2856771},
 address = {New York, NY, USA},
 author = {Zhao, Yuchen and Ye, Juan and Henderson, Tristan},
 booktitle = {Proceedings of the 21st International Conference on Intelligent User Interfaces},
 doi = {10.1145/2856767.2856771},
 isbn = {978-1-4503-4137-0},
 keyword = {location-based services, location-sharing services, privacy preferences, recommender systems, user acceptance},
 link = {http://doi.acm.org/10.1145/2856767.2856771},
 location = {Sonoma, California, USA},
 numpages = {10},
 pages = {218--227},
 publisher = {ACM},
 series = {IUI '16},
 title = {The Effect of Privacy Concerns on Privacy Recommenders},
 year = {2016}
}


@inproceedings{Vainstein:2016:TUM:2856767.2856802,
 abstract = {Augmented reality (AR) technology has the potential to enrich our daily lives in many aspects. One of them is the museum visit experience. Nowadays, state of the art mobile museum visitor guides provide us with rich personalized, context aware information. However, these systems have one major drawback -- they force the visitor to hold the guide and to look at its screen. The utilization of smart-glasses technology allows providing a wearable AR display without the necessity to hold the guide and to look at it and without distracting the user from the real object. This paper describes initial steps towards the implementation of a head-worn display (HWD) museum visitor guide -- the results of users' requirements elicitation process, the implementation of a first research prototype and initial insights gathered during the process.},
 acmid = {2856802},
 address = {New York, NY, USA},
 author = {Vainstein, Natalia and Kuflik, Tsvi and Lanir, Joel},
 booktitle = {Proceedings of the 21st International Conference on Intelligent User Interfaces},
 doi = {10.1145/2856767.2856802},
 isbn = {978-1-4503-4137-0},
 keyword = {augmented reality, head-worn display, museum visitors' guide},
 link = {http://doi.acm.org/10.1145/2856767.2856802},
 location = {Sonoma, California, USA},
 numpages = {5},
 pages = {327--331},
 publisher = {ACM},
 series = {IUI '16},
 title = {Towards Using Mobile, Head-Worn Displays in Cultural Heritage: User Requirements and a Research Agenda},
 year = {2016}
}


@inproceedings{Zhang:2016:DCB:2856767.2856806,
 abstract = {In this paper we develop a model capable of classifying drivers from their driving behaviors sensed by only low level sensors. The sensing platform consists of data available from the diagnostic outlet (OBD) of the car and smartphone sensors. We develop a window based support vector machine model to classify drivers. We test our model with two datasets collected under both controlled and naturalistic conditions. Furthermore, we evaluate the model using each sensor source (car and phone) independently and combining both the sensors. The average classification accuracies attained with data collected from three different cars shared between couples in a naturalistic environment were 75.83%, 85.83% and 86.67% using only phone sensors, only cars sensors and combined car and phone sensors respectively.},
 acmid = {2856806},
 address = {New York, NY, USA},
 author = {Zhang, Cheng and Patel, Mitesh and Buthpitiya, Senaka and Lyons, Kent and Harrison, Beverly and Abowd, Gregory D.},
 booktitle = {Proceedings of the 21st International Conference on Intelligent User Interfaces},
 doi = {10.1145/2856767.2856806},
 isbn = {978-1-4503-4137-0},
 keyword = {driver behavior modeling, driver identification, sensor fusion, smartphones},
 link = {http://doi.acm.org/10.1145/2856767.2856806},
 location = {Sonoma, California, USA},
 numpages = {5},
 pages = {80--84},
 publisher = {ACM},
 series = {IUI '16},
 title = {Driver Classification Based on Driving Behaviors},
 year = {2016}
}


@proceedings{Nichols:2016:2856767,
 abstract = {It is with great pleasure that we welcome you to beautiful Sonoma, California and the 21st edition of ACM International Conference on Intelligent User Interfaces -- ACM IUI 2016. ACM IUI is "where HCI meets AI," or where the academic research communities of Human-Computer Interaction (HCI) and Artificial Intelligence (AI) intersect. As the premier international forum for reporting outstanding research and development on intelligent user interfaces, the conference welcomes submissions describing work at the cross-section of these two fields and other related fields, such as psychology, behavioral science, cognitive science, computer graphics, design, the arts, and many others. Members of the ACM IUI community are interested in improving the symbiosis between humans and computers, increasing the intelligence of both in the process. The call for papers attracted 194 long and short paper submissions, 31 poster paper submissions, 11 demo paper submissions, and 17 student consortium submissions. The final program of the conference includes 2 keynotes, 49 long and short papers (25.3% acceptance rate), 15 posters, 6 demos, 2 workshops, 1 tutorial, and 12 student consortium papers. We are particularly excited for the keynotes by two distinguished speakers that will open the first and third days of the conference. The opening keynote will be given by Xavier Amatriain, VP of Engineering at Quora, on the topic of the "Past, Present, and Future of Recommender Systems: An Industry Perspective." The closing keynote will be given by Professor Elisabeth André, from Augsburg University and a long-time member of the ACM IUI community, on the topic of "Socially-Sensitive Interfaces: From Offline Studies to Interactive Experiences." The conference could not be organized without the help of a large number of individuals who generously volunteered much of their own time. Their names can be found on the following pages. All of the members of the organizing committee have done a fantastic job of coordinating the many moving parts that go into putting on a great conference. We must also particularly thank our 34 senior program committee members for coordinating the papers review process and the 91 members of the program committee for providing high quality reviews. And, most important, we must thank the authors for their diligent work that resulted in so many great submissions. These have allowed us to develop the excellent program that is the enduring heart of the conference. Another key for any great conference is a collection of strong sponsor organizations and generous corporate supporters. Our sponsors ACM, SIGAI, and SIGCHI are instrumental in making the conference happen year in and year out. This year, SIGCHI and SIGAI were particularly generous in providing financial support for our student travel grants, which have enabled 19 students to attend the conference that might not have otherwise. Our corporate supporters, Microsoft, IBM, Google, and Tableau have been supremely generous and the conference would be weaker without their contributions. We hope you will find the program engaging and the mix of academic disciplines broadens your perspective on computing. We also hope the conference will provide you with a valuable opportunity to share ideas with other researchers and practitioners from around the world, whether through presenting your own work formally or through informal discussions during the banquet or coffee breaks. With luck, those shared ideas will manifest themselves in exciting papers at next year's conference and ultimately have impact far beyond the conference! If you have any suggestions for how to improve the conference either this year or in the future, please do not hesitate to let us know.},
 address = {New York, NY, USA},
 isbn = {978-1-4503-4137-0},
 location = {Sonoma, California, USA},
 publisher = {ACM},
 title = {IUI '16: Proceedings of the 21st International Conference on Intelligent User Interfaces},
 year = {2016}
}


@inproceedings{Wu:2016:MCM:2856767.2856791,
 abstract = {Data transformation often requires users to write many trivial and task-dependent programs to transform thousands of records. Recently, programming-by-example (PBE) approaches enable users to transform data without coding. A key challenge of these PBE approaches is to deliver correctly transformed results on large datasets, since these transformation programs are likely to be generated by non-expert users. To address this challenge, existing approaches aim to identify a small set of potentially incorrect records and ask users to examine these records instead of the entire dataset. However, because the transformation scenarios are highly task-dependent, existing approaches cannot capture the incorrect records for various scenarios. We present a approach that learns from past transformation scenarios to generate a meta-classifier to identify the incorrect records. Our approach color-codes these transformed records and then presents them for users to examine. The method allows users to either enter an example for a record transformed incorrectly or confirm the correctness of a transformed record. And our approach can learn from the users' labels to refine the meta-classifier to accurately identify the incorrect records. Simulation results and a user study show that our method can identify the incorrectly transformed records and reduce the user efforts in examining the results.},
 acmid = {2856791},
 address = {New York, NY, USA},
 author = {Wu, Bo and Knoblock, Craig A.},
 booktitle = {Proceedings of the 21st International Conference on Intelligent User Interfaces},
 doi = {10.1145/2856767.2856791},
 isbn = {978-1-4503-4137-0},
 keyword = {data transformation, program synthesis, programming by example},
 link = {http://doi.acm.org/10.1145/2856767.2856791},
 location = {Sonoma, California, USA},
 numpages = {10},
 pages = {375--384},
 publisher = {ACM},
 series = {IUI '16},
 title = {Maximizing Correctness with Minimal User Effort to Learn Data Transformations},
 year = {2016}
}


