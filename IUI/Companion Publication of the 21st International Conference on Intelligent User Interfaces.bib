@inproceedings{Gatti:2016:HCG:2876456.2879469,
 abstract = {In this paper we present Heady-Lines, a creative system that produces news headlines based on well-known expressions. The algorithm is composed of several steps that identify keywords from a news article, select an appropriate well-known expression and modify it to produce a novel one, using state-of-the-art natural language processing and linguistic creativity techniques. The system has a simple web-interface that abstracts the technical details from users and lets them concentrate on the task of producing creative headlines.},
 acmid = {2879469},
 address = {New York, NY, USA},
 author = {Gatti, Lorenzo and Ozbal, Gozde and Guerini, Marco and Stock, Oliviero and Strapparava, Carlo},
 booktitle = {Companion Publication of the 21st International Conference on Intelligent User Interfaces},
 doi = {10.1145/2876456.2879469},
 isbn = {978-1-4503-4140-0},
 keyword = {computational creativity, nlp and journalism},
 link = {http://doi.acm.org/10.1145/2876456.2879469},
 location = {Sonoma, California, USA},
 numpages = {5},
 pages = {79--83},
 publisher = {ACM},
 series = {IUI '16 Companion},
 title = {Heady-Lines: A Creative Generator Of Newspaper Headlines},
 year = {2016}
}


@proceedings{Brdiczka:2015:2732158,
 abstract = {It is with great pleasure that we welcome you to the 2015 ACM International Conference on Intelligent User Interfaces -- ACM IUI 2015 in Atlanta, Georgia, USA. Starting in 1993, ACM IUI is now in its 20th edition and serves as a premier international forum for reporting outstanding research and development on intelligent user interfaces. ACM IUI is where the Human-Computer Interaction (HCI) community meets the Artificial Intelligence (AI) community. We are also very interested in contributions from related fields, such as psychology, behavioral science, cognitive science, computer graphics, design, the arts, etc. At ACM IUI, we focus on the interaction between machine intelligence and human intelligence. While other conferences focus on one side or the other, we address the complex interaction between the two. We welcome research that explores how to make the interaction between computers and people smarter, which may leverage solutions from data mining, knowledge representation, novel interaction paradigms, and emerging technologies. For the 20th edition of the conference, the call for papers attracted a record number of 205 long and short paper submissions, 41 poster paper submissions, 18 demo paper submissions, and 24 student consortium submissions. The final program of the conference includes 3 keynotes, 47 long and short papers (22.9% acceptance rate), 17 posters, 8 demos, 4 workshops, 3 tutorials, and 12 student consortium papers. The program opens with a keynote by Professor Dan Weld on "Intelligent Control of Crowdsourcing", continues with a keynote by Ed Chi on "Blurring of the Boundary Between Interactive Search and Recommendation" on the second day, and ends with a keynote by Rosalind Picard on "Recognizing Stress, Engagement, and Positive Emotion". Four workshops (Personalized Access to Cultural Heritage, Intelligent Digital Games for Empowerment and Inclusion, Interacting with Smart Objects, and Visual Text Analytics) and three tutorials (Speech-based Interaction, Personalization for Behavior Change, and User Affect and Sentiment Modeling) complement the main conference program.},
 address = {New York, NY, USA},
 isbn = {978-1-4503-3308-5},
 location = {Atlanta, Georgia, USA},
 publisher = {ACM},
 title = {IUI Companion '15: Proceedings of the 20th International Conference on Intelligent User Interfaces Companion},
 year = {2015}
}


@inproceedings{Vogl:2016:IMR:2876456.2879471,
 abstract = {The drum tracks of electronic dance music are a central and style-defining element. Yet, creating them can be a cumbersome task, mostly due to lack of appropriate tools and input devices. In this work we present an artificial-intelligence-powered software prototype, which supports musicians composing the rhythmic patterns for drum tracks. Starting with a basic pattern (seed pattern), which is provided by the user, a list of variations with varying degree of similarity to the seed pattern is generated. The variations are created using a generative stochastic neural network. The interface visualizes the patterns and provides an intuitive way to browse through them. A user study with ten experts in electronic music production was conducted to evaluate five aspects of the presented prototype. For four of these aspects the feedback was generally positive. Only regarding the use case in live environments some participants showed concerns and requested safety features.},
 acmid = {2879471},
 address = {New York, NY, USA},
 author = {Vogl, Richard and Knees, Peter},
 booktitle = {Companion Publication of the 21st International Conference on Intelligent User Interfaces},
 doi = {10.1145/2876456.2879471},
 isbn = {978-1-4503-4140-0},
 keyword = {generative stochastic models., machine learning, neural networks, restricted boltzmann machines, rhythm pattern generation},
 link = {http://doi.acm.org/10.1145/2876456.2879471},
 location = {Sonoma, California, USA},
 numpages = {4},
 pages = {88--91},
 publisher = {ACM},
 series = {IUI '16 Companion},
 title = {An Intelligent Musical Rhythm Variation Interface},
 year = {2016}
}


@inproceedings{Wan:2016:EIT:2876456.2879482,
 abstract = {Transcranial Direct Current Stimulation (tDCS) is a non-invasive type of neural stimulation known for modulation of cortical excitability leading to positive effects on working memory and attention. The availability of low-cost and consumer grade tDCS devices has democratized access to such technology allowing us to explore its applicability to HCI. We review the relevant literature and identify potential avenues for exploration within the context of enhancing interactivity and use of tDCS in the context of HCI.},
 acmid = {2879482},
 address = {New York, NY, USA},
 author = {Wan, Bo and Vi, Chi and Subramanian, Sriram and Martinez Plasencia, Diego},
 booktitle = {Companion Publication of the 21st International Conference on Intelligent User Interfaces},
 doi = {10.1145/2876456.2879482},
 isbn = {978-1-4503-4140-0},
 keyword = {hci, neurostimulation, tdcs},
 link = {http://doi.acm.org/10.1145/2876456.2879482},
 location = {Sonoma, California, USA},
 numpages = {4},
 pages = {41--44},
 publisher = {ACM},
 series = {IUI '16 Companion},
 title = {Enhancing Interactivity with Transcranial Direct Current Stimulation},
 year = {2016}
}


@inproceedings{Tanveer:2016:UIC:2876456.2876468,
 abstract = {Portable and inexpensive technologies have the potential to capture a huge variety of signals about human being. Systematic analysis of these signals can provide deep understanding on the basic nature of interpersonal communication. I am interested in taking a machine learning approach for analyzing human behaviors---atleast in a formal, well-established setting (e.g. in public speaking, job interview etc.). Understanding human behavior will enable us to design systems capable to make people self-aware. In many cases they might be useful for behavior modification as well.},
 acmid = {2876468},
 address = {New York, NY, USA},
 author = {Tanveer, M. Iftekhar},
 booktitle = {Companion Publication of the 21st International Conference on Intelligent User Interfaces},
 doi = {10.1145/2876456.2876468},
 isbn = {978-1-4503-4140-0},
 keyword = {affective computing, behavior analysis, machine learning, user interface design},
 link = {http://doi.acm.org/10.1145/2876456.2876468},
 location = {Sonoma, California, USA},
 numpages = {4},
 pages = {150--153},
 publisher = {ACM},
 series = {IUI '16 Companion},
 title = {Understanding and Intervening Communicational Behavior Using Artificial Intelligence},
 year = {2016}
}


@inproceedings{Chuan:2016:DSI:2876456.2879483,
 abstract = {This paper describes an interactive mobile application that aims to assist children who are deaf or hard of hearing (D/HH) and their families to learn and practice American Sign Language (ASL). Approximately 95% of D/HH children are born to hearing parents. Research indicates that the lack of common communication tools between the parent and child often results in delayed development in the child's language and social skills. Benefiting from the interactive advantages and popularity of touchscreen mobile devices, we created SmartSignPlay, an app to teach D/HH children and their families everyday ASL vocabulary and phrases. Vocabulary is arranged into context-based lessons where the vocabulary is frequently used. After watching the sign demonstrated by an animated avatar, the user performed the sign by drawing the trajectory of the hand movement and selecting the correct handshape. While the app is still under iterative development, preliminary results on the usability are provided.},
 acmid = {2879483},
 address = {New York, NY, USA},
 author = {Chuan, Ching-Hua and Guardino, Caroline Anne},
 booktitle = {Companion Publication of the 21st International Conference on Intelligent User Interfaces},
 doi = {10.1145/2876456.2879483},
 isbn = {978-1-4503-4140-0},
 keyword = {american sign language, assistive technology, context-aware learning, feedback, touchscreen},
 link = {http://doi.acm.org/10.1145/2876456.2879483},
 location = {Sonoma, California, USA},
 numpages = {4},
 pages = {45--48},
 publisher = {ACM},
 series = {IUI '16 Companion},
 title = {Designing SmartSignPlay: An Interactive and Intelligent American Sign Language App for Children Who Are Deaf or Hard of Hearing and Their Families},
 year = {2016}
}


@inproceedings{Gandhi:2016:ENT:2876456.2879472,
 abstract = {The amount of instructional videos available online, already in tens of thousands of hours, is growing steadily. A major bottleneck in their wide spread usage is the lack of tools for easy consumption of these videos. In this demonstration, we present MMToC: Multimodal Method for Table of Content, a technique that automatically generates a table of content for a given instructional video and enables text-book-like efficient navigation through the video. MMToC quantifies word saliency for visual words extracted from the slides and spoken words obtained from the lecture transcript. These saliency scores are combined using a dynamic programming based segmentation algorithm to identify likely points in the video where the topic has changed. MMToC is a web-based modular solution that can be used as a stand alone video navigation solution or can be integrated with any e-platform for multimedia content management. MMToC can be seen in action on a sample video at http://104.130.241.45:8080/TopicTransitionV2/index.html.},
 acmid = {2879472},
 address = {New York, NY, USA},
 author = {Gandhi, Ankit and Biswas, Arijit and Shrivastava, Kundan and Kumar, Ranjeet and Loomba, Sahil and Deshmukh, Om},
 booktitle = {Companion Publication of the 21st International Conference on Intelligent User Interfaces},
 doi = {10.1145/2876456.2879472},
 isbn = {978-1-4503-4140-0},
 keyword = {efficient indexing, instructional videos, non-linear navigation, table of content, temporal segmentation, user interface for navigation},
 link = {http://doi.acm.org/10.1145/2876456.2879472},
 location = {Sonoma, California, USA},
 numpages = {5},
 pages = {92--96},
 publisher = {ACM},
 series = {IUI '16 Companion},
 title = {Easy Navigation Through Instructional Videos Using Automatically Generated Table of Content},
 year = {2016}
}


@inproceedings{Knijnenburg:2016:EIU:2876456.2882846,
 abstract = {User experiments are an essential tool to evaluate the user experience of intelligent user interfaces. This tutorial teaches the practical aspects of designing and setting up user experiments, as well as state-of-the-art methods to statistically evaluate the outcomes of such experiments.},
 acmid = {2882846},
 address = {New York, NY, USA},
 author = {Knijnenburg, Bart P.},
 booktitle = {Companion Publication of the 21st International Conference on Intelligent User Interfaces},
 doi = {10.1145/2876456.2882846},
 isbn = {978-1-4503-4140-0},
 keyword = {intelligent user interfaces, user experience, user experiments, user-centric evaluation},
 link = {http://doi.acm.org/10.1145/2876456.2882846},
 location = {Sonoma, California, USA},
 numpages = {3},
 pages = {6--8},
 publisher = {ACM},
 series = {IUI '16 Companion},
 title = {Evaluating Intelligent User Interfaces with User Experiments},
 year = {2016}
}


@inproceedings{Gao:2016:IIO:2876456.2876464,
 abstract = {Lots of posts containing social opinions are published on Reddit in a messy and staggered format with sub-Reddit labels summarizing their contents only. It's hard for users to have a global insight across different positions and opinions for a specific topic in a short time, especially for a controversial topic. We propose an intelligent mechanism which combines social opinion clustering and information visualization together. First, we cluster the Reddit posts into different categories based on crowd position and opinions, and generate informative clustering labels using the human computation technique. Second, we create an intelligent user interface with Reddit posts category visualization. This would expose categorized posts of different positions and opinions to users, and motivate users to hunt for posts supported by unlike-minded people.},
 acmid = {2876464},
 address = {New York, NY, USA},
 author = {Gao, Mingkun},
 booktitle = {Companion Publication of the 21st International Conference on Intelligent User Interfaces},
 doi = {10.1145/2876456.2876464},
 isbn = {978-1-4503-4140-0},
 keyword = {clustering, human computation., intelligent user interface, social opinion},
 link = {http://doi.acm.org/10.1145/2876456.2876464},
 location = {Sonoma, California, USA},
 numpages = {4},
 pages = {134--137},
 publisher = {ACM},
 series = {IUI '16 Companion},
 title = {Intelligent Interface for Organizing Online Social Opinions on Reddit},
 year = {2016}
}


@inproceedings{Kerren:2016:WEV:2876456.2882847,
 abstract = {
                  An abstract is not available.
              },
 acmid = {2882847},
 address = {New York, NY, USA},
 author = {Kerren, Andreas and Cernea, Daniel and Pohl, Margit},
 booktitle = {Companion Publication of the 21st International Conference on Intelligent User Interfaces},
 doi = {10.1145/2876456.2882847},
 isbn = {978-1-4503-4140-0},
 keyword = {affective computing, emotion detection, emotion measurement, emotion-driven adaptation, emotion-enhanced interaction, emotion-enhanced visualization, visualization},
 link = {http://doi.acm.org/10.1145/2876456.2882847},
 location = {Sonoma, California, USA},
 numpages = {2},
 pages = {1--2},
 publisher = {ACM},
 series = {IUI '16 Companion},
 title = {Workshop on Emotion and Visualization: EmoVis 2016},
 year = {2016}
}


@inproceedings{Zhao:2016:UPL:2876456.2876458,
 abstract = {Location-sharing services such as Facebook and Foursquare have become increasingly popular. These services can be helpful for us but can also pose threats to people's privacy. Usability issues in existing location-privacy protection mechanisms are one of the main reasons why people fail to protect their location privacy properly. Most people are not capable and find it cumbersome to configure location-privacy preferences by themselves. My PhD research aims to address these usability issues by using recommenders, understand people's acceptance of, and concerns about, such recommenders, and to alleviate their concerns.},
 acmid = {2876458},
 address = {New York, NY, USA},
 author = {Zhao, Yuchen},
 booktitle = {Companion Publication of the 21st International Conference on Intelligent User Interfaces},
 doi = {10.1145/2876456.2876458},
 isbn = {978-1-4503-4140-0},
 keyword = {location-based services, recommender systems, usable privacy},
 link = {http://doi.acm.org/10.1145/2876456.2876458},
 location = {Sonoma, California, USA},
 numpages = {4},
 pages = {110--113},
 publisher = {ACM},
 series = {IUI '16 Companion},
 title = {Usable Privacy in Location-Sharing Services},
 year = {2016}
}


@inproceedings{Rajanna:2016:GFI:2876456.2876462,
 abstract = {Transforming gaze input into a rich and assistive interaction modality is one of the primary interests in eye tracking research. Gaze input in conjunction with traditional solutions to the "Midas Touch" problem, dwell time or a blink, is not matured enough to be widely adopted. In this regard, we present our preliminary work, a framework that achieves precise "point and click" interactions in a desktop environment through combining the gaze and foot interaction modalities. The framework comprises of an eye tracker and a foot-operated quasi-mouse that is wearable. The system evaluation shows that our gaze and foot interaction framework performs as good as a mouse (time and precision) in the majority of tasks. Furthermore, this dissertation work focuses on the goal of realizing gaze-assisted interaction as a primary interaction modality to substitute conventional mouse and keyboard-based interaction methods. In addition, we consider some of the challenges that need to be addressed, and also present the possible solutions toward achieving our goal.},
 acmid = {2876462},
 address = {New York, NY, USA},
 author = {Rajanna, Vijay Dandur},
 booktitle = {Companion Publication of the 21st International Conference on Intelligent User Interfaces},
 doi = {10.1145/2876456.2876462},
 isbn = {978-1-4503-4140-0},
 keyword = {authentication, eye tracking, foot input, gaze and foot interaction, tabletop interaction},
 link = {http://doi.acm.org/10.1145/2876456.2876462},
 location = {Sonoma, California, USA},
 numpages = {4},
 pages = {126--129},
 publisher = {ACM},
 series = {IUI '16 Companion},
 title = {Gaze and Foot Input: Toward a Rich and Assistive Interaction Modality},
 year = {2016}
}


@inproceedings{Kangasraasio:2016:DCD:2876456.2879487,
 abstract = {In exploratory search, when the user formulates a query iteratively through relevance feedback, it is likely that the feedback given earlier requires adjustment later on. The main reason for this is that the user learns while searching, which causes changes in the relevance of items and features as estimated by the user -- a phenomenon known as {it concept drift}. It might be helpful for the user to see the recent history of her feedback and get suggestions from the system about the accuracy of that feedback. In this paper we present a timeline interface that visualizes the feedback history, and a Bayesian regression model that can estimate jointly the user's current interests and the accuracy of each user feedback. We demonstrate that the user model can improve retrieval performance over a baseline model that does not estimate accuracy of user feedback. Furthermore, we show that the new interface provides usability improvements, which leads to the users interacting more with it.},
 acmid = {2879487},
 address = {New York, NY, USA},
 author = {Kangasr\"{a}\"{a}si\"{o}, Antti and Chen, Yi and Glowacka, Dorota and Kaski, Samuel},
 booktitle = {Companion Publication of the 21st International Conference on Intelligent User Interfaces},
 doi = {10.1145/2876456.2879487},
 isbn = {978-1-4503-4140-0},
 keyword = {concept drift, exploratory search, interactive user modeling, probabilistic user models, user interfaces},
 link = {http://doi.acm.org/10.1145/2876456.2879487},
 location = {Sonoma, California, USA},
 numpages = {5},
 pages = {62--66},
 publisher = {ACM},
 series = {IUI '16 Companion},
 title = {Dealing with Concept Drift in Exploratory Search: An Interactive Bayesian Approach},
 year = {2016}
}


@inproceedings{Yordanova:2016:TIS:2876456.2879488,
 abstract = {There are various activity recognition approaches that rely on manual definition of precondition-effect rules to describe user behaviour. These rules are later used to generate computational models of human behaviour that are able to reason about the user behaviour based on sensor observations. One problem with these approaches is that the manual rule definition is time consuming and error prone process. To address this problem, in this paper we outline an approach that extracts the rules from textual instructions. It then learns the optimal model structure based on observations in the form of manually created plans and sensor data. The learned model can then be used to recognise the behaviour of users during their daily activities.},
 acmid = {2879488},
 address = {New York, NY, USA},
 author = {Yordanova, Kristina},
 booktitle = {Companion Publication of the 21st International Conference on Intelligent User Interfaces},
 doi = {10.1145/2876456.2879488},
 isbn = {978-1-4503-4140-0},
 keyword = {activity recognition, model learning, natural language processing, ontology learning, user behaviour models},
 link = {http://doi.acm.org/10.1145/2876456.2879488},
 location = {Sonoma, California, USA},
 numpages = {7},
 pages = {67--73},
 publisher = {ACM},
 series = {IUI '16 Companion},
 title = {From Textual Instructions to Sensor-based Recognition of User Behaviour},
 year = {2016}
}


@inproceedings{Wauck:2016:EDS:2876456.2876467,
 abstract = {This document gives an overview of my current research project investigating how children develop spatial reasoning skills through video game training. I describe the motivation and goals of the project and the progress made so far.},
 acmid = {2876467},
 address = {New York, NY, USA},
 author = {Wauck, Helen},
 booktitle = {Companion Publication of the 21st International Conference on Intelligent User Interfaces},
 doi = {10.1145/2876456.2876467},
 isbn = {978-1-4503-4140-0},
 keyword = {children, cognitive science, education, learning, spatial reasoning, video games},
 link = {http://doi.acm.org/10.1145/2876456.2876467},
 location = {Sonoma, California, USA},
 numpages = {4},
 pages = {146--149},
 publisher = {ACM},
 series = {IUI '16 Companion},
 title = {Exploring the Development of Spatial Skills in a Video Game},
 year = {2016}
}


@inproceedings{Srinivasan:2016:ESC:2876456.2879477,
 abstract = {The evolution of digital technology has resulted in the consumption of content on a multitude of environments (desktop, mobile, etc). Content now needs to be appropriately delivered to all these environments. This calls for a mechanism to automate the process for rendering the content in its appropriate form on a targeted environment. In this paper, we propose an algorithm that takes the content along with the a set of environment-specific layouts where it has to be rendered to automatically decide the mapping and transformation of the content for the right rendition. Metrics to measure the `goodness' of the resulting rendition is also proposed to choose the right layout for the given content.},
 acmid = {2879477},
 address = {New York, NY, USA},
 author = {Srinivasan, Balaji Vasan and Goyal, Tanya and Syal, Varun and Suman Singh, Shubhankar and Sharma, Vineet},
 booktitle = {Companion Publication of the 21st International Conference on Intelligent User Interfaces},
 doi = {10.1145/2876456.2879477},
 isbn = {978-1-4503-4140-0},
 keyword = {content distribution, content transformation, layout selection},
 link = {http://doi.acm.org/10.1145/2876456.2879477},
 location = {Sonoma, California, USA},
 numpages = {5},
 pages = {18--22},
 publisher = {ACM},
 series = {IUI '16 Companion},
 title = {Environment Specific Content Rendering \&\#38; Transformation},
 year = {2016}
}


@inproceedings{Tsiakas:2016:FSA:2876456.2876457,
 abstract = {In this paper, we propose a learning framework for the adaptation of an interactive agent to a new user. We focus on applications where safety and personalization are essential, as Rehabilitation Systems and Robot Assisted Therapy. We argue that interactive learning methods can be utilised and combined into the Reinforcement Learning framework, aiming at a safe and tailored interaction.},
 acmid = {2876457},
 address = {New York, NY, USA},
 author = {Tsiakas, Konstantinos},
 booktitle = {Companion Publication of the 21st International Conference on Intelligent User Interfaces},
 doi = {10.1145/2876456.2876457},
 isbn = {978-1-4503-4140-0},
 keyword = {adaptation, interaction management, interactive reinforcement learning, learning agents},
 link = {http://doi.acm.org/10.1145/2876456.2876457},
 location = {Sonoma, California, USA},
 numpages = {4},
 pages = {106--109},
 publisher = {ACM},
 series = {IUI '16 Companion},
 title = {Facilitating Safe Adaptation of Interactive Agents Using Interactive Reinforcement Learning},
 year = {2016}
}


@inproceedings{Chander:2016:LIO:2876456.2879478,
 abstract = {We introduce the Lifeboard: a dynamic information interface designed to render personal data so as to positively influence wellness outcomes. We report on the results of an experiment that compares the effect of presenting clinically significant data to subjects on their activity levels, with the effect of presenting the same data using the Lifeboard. The statistically significant increase in this wellness outcome in the Lifeboard group vs. the Data-only group suggests that the Lifeboard effectively leverages the scarcity response [4] in the service of improved wellness outcomes. Moreover, the significant week-on-week decrease in this wellness outcome in the Data-only group points to the need for care when exposing clinical data to users.},
 acmid = {2879478},
 address = {New York, NY, USA},
 author = {Chander, Ajay and Mirzazad Barijough, Sanam},
 booktitle = {Companion Publication of the 21st International Conference on Intelligent User Interfaces},
 doi = {10.1145/2876456.2879478},
 isbn = {978-1-4503-4140-0},
 keyword = {information interface, lifeboard, mindset, scarcity, wellness},
 link = {http://doi.acm.org/10.1145/2876456.2879478},
 location = {Sonoma, California, USA},
 numpages = {5},
 pages = {23--27},
 publisher = {ACM},
 series = {IUI '16 Companion},
 title = {The Lifeboard: Improving Outcomes via Scarcity Priming},
 year = {2016}
}


@inproceedings{Itoh:2016:SEV:2876456.2879486,
 abstract = {We devised a method of visualizing spatio-temporal events extracted from a geo-parsed microblog stream by using a multi-layered geo-locational word-cloud representation. In our method, real-time geo-parsing geo-locates posts in the stream, in order to recognize words appearing on a user-specified location and time grid as temporal local events. The recognized temporal local events (textit{e.g.}, sports games) are then displayed on a map as multi-layered word-clouds and are then used for finding global events (textit{e.g.}, earthquakes), in order to avoid occlusions among the local and global events. We showed the effectiveness of our method by testing it on real events extracted from our archive of five years worth of Twitter posts.},
 acmid = {2879486},
 address = {New York, NY, USA},
 author = {Itoh, Masahiko and Yoshinaga, Naoki and Toyoda, Masashi},
 booktitle = {Companion Publication of the 21st International Conference on Intelligent User Interfaces},
 doi = {10.1145/2876456.2879486},
 isbn = {978-1-4503-4140-0},
 keyword = {social media, spatio-temporal visualization, text analysis},
 link = {http://doi.acm.org/10.1145/2876456.2879486},
 location = {Sonoma, California, USA},
 numpages = {4},
 pages = {58--61},
 publisher = {ACM},
 series = {IUI '16 Companion},
 title = {Spatio-temporal Event Visualization from a Geo-parsed Microblog Stream},
 year = {2016}
}


@inproceedings{Salvado:2016:SST:2876456.2879489,
 abstract = {The world population is aging rapidly. There is an increasing need for health assistance personnel, such as nurses and physiotherapeutic experts, in developed countries. On the other hand, there is a need to improve health care assistance to the population, and especially to elderly people. This will mostly benefit specific user groups, such as elderly, patients recovering from physical injury, or athletes. This paper describes a wearable sleeve being developed under the scope of the Augmented Human Assistance (AHA) project for assisting people. It proposes a new architecture for providing haptic feedback through patterns created by multiple actuators. Different sensing technologies are analyzed and discussed.},
 acmid = {2879489},
 address = {New York, NY, USA},
 author = {Salvado, Luis Miguel and Arsenio, Artur},
 booktitle = {Companion Publication of the 21st International Conference on Intelligent User Interfaces},
 doi = {10.1145/2876456.2879489},
 isbn = {978-1-4503-4140-0},
 keyword = {augmented health assistance, haptic feedback, posture sensing, wearable computing},
 link = {http://doi.acm.org/10.1145/2876456.2879489},
 location = {Sonoma, California, USA},
 numpages = {5},
 pages = {74--78},
 publisher = {ACM},
 series = {IUI '16 Companion},
 title = {Sleeve Sensing Technologies and Haptic Feedback Patterns for Posture Sensing and Correction},
 year = {2016}
}


@inproceedings{Schnelle-Walka:2016:SJW:2876456.2882849,
 abstract = {The increasing number of smart objects in our everyday life shapes how we interact beyond the desktop. In this workshop we discuss how advanced interactions with smart objects in the context of the Internet-of-Thingsshould be designed from various perspectives, such as HCI and AI as well as industry and academia.},
 acmid = {2882849},
 address = {New York, NY, USA},
 author = {Schnelle-Walka, Dirk and Limonad, Lior and Grosse-Puppendahl, Tobias and Lanir, Joel and M\"{u}ller, Florian and Mecella, Massimo and Luyten, Kris and Kuflik, Tsvi and Brdiczka, Oliver and M\"{u}hlh\"{a}user, Max},
 booktitle = {Companion Publication of the 21st International Conference on Intelligent User Interfaces},
 doi = {10.1145/2876456.2882849},
 isbn = {978-1-4503-4140-0},
 keyword = {context-awareness, embodied interaction, enabling techologies, hci, multimodal and adapter interaction, novel interaction, smart objects, tangible interaction},
 link = {http://doi.acm.org/10.1145/2876456.2882849},
 location = {Sonoma, California, USA},
 numpages = {3},
 pages = {3--5},
 publisher = {ACM},
 series = {IUI '16 Companion},
 title = {SCWT: A Joint Workshop on Smart Connected and Wearable Things},
 year = {2016}
}


@inproceedings{Samsonov:2016:IIS:2876456.2876459,
 abstract = {We have seen a recent rise of context- as well as location-based-mobile services. Finally, those services entering applications and adding features to mobile operating system to make everyday user interactions handier. Nevertheless, those services still have certain limitations, such as lack of certain data types that limit them to exploit their full potentials. My research is situated in the area of human-computer interaction with strong links to the field of intelligent user interfaces and aims to improve interactions with spatial context-aware services by combining methods from computer vision and artificial intelligence.},
 acmid = {2876459},
 address = {New York, NY, USA},
 author = {Samsonov, Pavel Andreevich},
 booktitle = {Companion Publication of the 21st International Conference on Intelligent User Interfaces},
 doi = {10.1145/2876456.2876459},
 isbn = {978-1-4503-4140-0},
 keyword = {context-aware applications, geohci, geospatial information, location-based services, scenic routes, space usage rules},
 link = {http://doi.acm.org/10.1145/2876456.2876459},
 location = {Sonoma, California, USA},
 numpages = {4},
 pages = {114--117},
 publisher = {ACM},
 series = {IUI '16 Companion},
 title = {Improving Interactions with Spatial Context-aware Services},
 year = {2016}
}


@inproceedings{Suzuki:2016:PRE:2876456.2879476,
 abstract = {Expert manual workers in factories assemble more efficiently than novices because their movements are optimized for the tasks. In this paper, we present an approach to projecting the hand movements of experts at real size, and real speed and onto real objects in order to match the manual work movements of novices to those of experts. We prototyped a projector-camera system, which projects the virtual hands of experts. We conducted a user study in which users worked after watching experts work under two conditions: using a display and using our prototype system. The results show our prototype users worked more precisely and felt the tasks were easier. User ratings also show our prototype users watched videos of experts more fixedly, memorized them more clearly and distinctly tried to work in the same way shown in the videos as compared with display users.},
 acmid = {2879476},
 address = {New York, NY, USA},
 author = {Suzuki, Genta and Murase, Taichi and Fujii, Yusaku},
 booktitle = {Companion Publication of the 21st International Conference on Intelligent User Interfaces},
 doi = {10.1145/2876456.2879476},
 isbn = {978-1-4503-4140-0},
 keyword = {augmented reality, projector-camera system},
 link = {http://doi.acm.org/10.1145/2876456.2879476},
 location = {Sonoma, California, USA},
 numpages = {5},
 pages = {13--17},
 publisher = {ACM},
 series = {IUI '16 Companion},
 title = {Projecting Recorded Expert Hands at Real Size, at Real Speed, and Onto Real Objects for Manual Work},
 year = {2016}
}


@inproceedings{Huang:2016:CAR:2876456.2876465,
 abstract = {Songwriting is the interplay of a composer's creative intent and an idiom's language. This language both facilitates and poses stylistic constraints on a composer's expressivity. Novice composers often find it difficult to go beyond common chord progressions, to find the chords that realize their intentions. To make it easier for composers to experiment with radical chord choices and to prototype "what-if" ideas, we are building a creativity support tool, ChordRipple, which (1) makes chord recommendations that aim to be both diverse and appropriate to the current context, (2) infers a composer's intention to help her more quickly prototype ideas. Composers can use it to help select the next chord, to replace sequences of chords in an internally consist manner, or to edit one part of a sequence and see the whole sequence change in that direction. To make such recommendations, we adapt neural-network models such as Word2Vec to the music domain as Chord2Vec. This model learns chord embeddings from a corpus of chord sequences, placing chords nearby when they are used in similar contexts. The learned embeddings support creative substitutions between chords, and also exhibit topological properties that correspond to musical structure. For example, the major and minor chords are both arranged in the latent space in shapes corresponding to the circle-of-fifths. To support the dynamic nature of the creative process, we propose to infer a composer's intentions for adaptive recommendation. As a composer makes chord changes, she is moving in the embedding space. We can infer a composer's intention from the gradient of her edits' trace and use this gradient to help her fine-tune her current changes or to project the sequence into the future to give recommendations on how the sequence could look like if more edits in that direction were performed.},
 acmid = {2876465},
 address = {New York, NY, USA},
 author = {Huang, Cheng-Zhi Anna},
 booktitle = {Companion Publication of the 21st International Conference on Intelligent User Interfaces},
 doi = {10.1145/2876456.2876465},
 isbn = {978-1-4503-4140-0},
 keyword = {creativity support tools, harmony, intelligent user interfaces, music, neural language models, recommender systems, songwriting},
 link = {http://doi.acm.org/10.1145/2876456.2876465},
 location = {Sonoma, California, USA},
 numpages = {4},
 pages = {138--141},
 publisher = {ACM},
 series = {IUI '16 Companion},
 title = {ChordRipple: Adaptively Recommending and Propagating Chord Changes for Songwriters},
 year = {2016}
}


@inproceedings{Garvey:2016:PTS:2876456.2879470,
 abstract = {Atlanta has consistently ranked as one of the most dangerous cities in America with over 2.5 million crime events recorded within the past six years. People who commute by walking are highly susceptible to crime here. To address this problem, our group has developed a mobile application, PASSAGE, that integrates Atlanta-based crime data to find "safe paths" between any given start and end locations in Atlanta. It also provides security features in a convenient user interface to further enhance safety while walking.},
 acmid = {2879470},
 address = {New York, NY, USA},
 author = {Garvey, Matthew and Das, Nilaksh and Su, Jiaxing and Natraj, Meghna and Verma, Bhanu},
 booktitle = {Companion Publication of the 21st International Conference on Intelligent User Interfaces},
 doi = {10.1145/2876456.2879470},
 isbn = {978-1-4503-4140-0},
 keyword = {bi-objective path optimization, pulse algorithm, safe navigation, safe paths},
 link = {http://doi.acm.org/10.1145/2876456.2879470},
 location = {Sonoma, California, USA},
 numpages = {4},
 pages = {84--87},
 publisher = {ACM},
 series = {IUI '16 Companion},
 title = {PASSAGE: A Travel Safety Assistant with Safe Path Recommendations for Pedestrians},
 year = {2016}
}


@proceedings{Nichols:2016:2876456,
 abstract = {It is with great pleasure that we welcome you to beautiful Sonoma, California and the 21st edition of ACM International Conference on Intelligent User Interfaces -- ACM IUI 2016. ACM IUI is "where HCI meets AI," or where the academic research communities of Human-Computer Interaction (HCI) and Artificial Intelligence (AI) intersect. As the premier international forum for reporting outstanding research and development on intelligent user interfaces, the conference welcomes submissions describing work at the cross-section of these two fields and other related fields, such as psychology, behavioral science, cognitive science, computer graphics, design, the arts, and many others. Members of the ACM IUI community are interested in improving the symbiosis between humans and computers, increasing the intelligence of both in the process. The call for papers attracted 194 long and short paper submissions, 31 poster paper submissions, 11 demo paper submissions, and 17 student consortium submissions. The final program of the conference includes 2 keynotes, 49 long and short papers (25.3% acceptance rate), 15 posters, 6 demos, 2 workshops, 1 tutorial, and 12 student consortium papers. We are particularly excited for the keynotes by two distinguished speakers that will open the first and third days of the conference. The opening keynote will be given by Xavier Amatriain, VP of Engineering at Quora, on the topic of the "Past, Present, and Future of Recommender Systems: An Industry Perspective." The closing keynote will be given by Professor Elisabeth Andr√©, from Augsburg University and a long-time member of the ACM IUI community, on the topic of "Socially-Sensitive Interfaces: From Offline Studies to Interactive Experiences." The conference could not be organized without the help of a large number of individuals who generously volunteered much of their own time. Their names can be found on the following pages. All of the members of the organizing committee have done a fantastic job of coordinating the many moving parts that go into putting on a great conference. We must also particularly thank our 34 senior program committee members for coordinating the papers review process and the 91 members of the program committee for providing high quality reviews. And, most important, we must thank the authors for their diligent work that resulted in so many great submissions. These have allowed us to develop the excellent program that is the enduring heart of the conference. Another key for any great conference is a collection of strong sponsor organizations and generous corporate supporters. Our sponsors ACM, SIGAI, and SIGCHI are instrumental in making the conference happen year in and year out. This year, SIGCHI and SIGAI were particularly generous in providing financial support for our student travel grants, which have enabled 19 students to attend the conference that might not have otherwise. Our corporate supporters, Microsoft, IBM, Google, and Tableau have been supremely generous and the conference would be weaker without their contributions. We hope you will find the program engaging and the mix of academic disciplines broadens your perspective on computing. We also hope the conference will provide you with a valuable opportunity to share ideas with other researchers and practitioners from around the world, whether through presenting your own work formally or through informal discussions during the banquet or coffee breaks. With luck, those shared ideas will manifest themselves in exciting papers at next year's conference and ultimately have impact far beyond the conference! If you have any suggestions for how to improve the conference either this year or in the future, please do not hesitate to let us know.},
 address = {New York, NY, USA},
 isbn = {978-1-4503-4140-0},
 location = {Sonoma, California, USA},
 publisher = {ACM},
 title = {IUI '16 Companion: Companion Publication of the 21st International Conference on Intelligent User Interfaces},
 year = {2016}
}


@inproceedings{Lioulemes:2016:AUH:2876456.2876463,
 abstract = {My research is focusing on developing smart robotic rehabilitation interfaces that use machine intelligence to adjust the level of difficulty, assess physical and mental obstacles on the part of the user, and provide analysis of the multi-sensing data collected in real time as the user exercises. The main goal of the interfaces is to engage the patient in repetitive exercise sessions and to provide better data visualization to the therapist for the patient's recovery progress. In this doctoral consortium, I will present three prototype user interfaces that can be applied in assistive environments and enhance the productivity and interaction among therapist and patient. The data processing and the decision making algorithms compose the core components of this study.},
 acmid = {2876463},
 address = {New York, NY, USA},
 author = {Lioulemes, Alexandros},
 booktitle = {Companion Publication of the 21st International Conference on Intelligent User Interfaces},
 doi = {10.1145/2876456.2876463},
 isbn = {978-1-4503-4140-0},
 keyword = {adaptive serious games, computer vision, force/torque control, haptic interfaces, physical human-robot interaction, reinforcement learning, user behavior modeling},
 link = {http://doi.acm.org/10.1145/2876456.2876463},
 location = {Sonoma, California, USA},
 numpages = {4},
 pages = {130--133},
 publisher = {ACM},
 series = {IUI '16 Companion},
 title = {Adaptive User and Haptic Interfaces for Smart Assessment and Training},
 year = {2016}
}


@inproceedings{Hoque:2016:VTA:2876456.2876461,
 abstract = {Analyzing and gaining insights from a large amount of textual conversations can be quite challenging for a user, especially when the discussions become very long. During my doctoral research, I have focused on integrating Information Visualization (InfoVis) with Natural Language Processing (NLP) techniques to better support the user's task of exploring and analyzing conversations. For this purpose, I have designed a visual text analytics system that supports the user exploration, starting from a possibly large set of conversations, then narrowing down to a subset of conversations, and eventually drilling-down to a set of comments of one conversation. While so far our approach is evaluated mainly based on lab studies, in my on-going and future work I plan to evaluate our approach via online longitudinal studies.},
 acmid = {2876461},
 address = {New York, NY, USA},
 author = {Hoque, Enamul},
 booktitle = {Companion Publication of the 21st International Conference on Intelligent User Interfaces},
 doi = {10.1145/2876456.2876461},
 isbn = {978-1-4503-4140-0},
 keyword = {asynchronous conversation, computer-mediated communication, interactive topic modeling, text visualization},
 link = {http://doi.acm.org/10.1145/2876456.2876461},
 location = {Sonoma, California, USA},
 numpages = {4},
 pages = {122--125},
 publisher = {ACM},
 series = {IUI '16 Companion},
 title = {Visual Text Analytics for Online Conversations: Design, Evaluation, and Applications},
 year = {2016}
}


@inproceedings{Ocegueda-Hernandez:2016:CMN:2876456.2879485,
 abstract = {Modern medical image technologies are capable of providing meaningful structural and functional information in the form of volumetric digital data. However current standard systems for the visualization and interaction with such data fail to provide a natural-intuitive way to interact with these data. In this paper, we present our advances towards the development of computational methods for the natural and intuitive visualization of volumetric medical data.},
 acmid = {2879485},
 address = {New York, NY, USA},
 author = {Ocegueda-Hern\'{a}ndez, Vladimir and Mendizabal-Ruiz, Gerardo},
 booktitle = {Companion Publication of the 21st International Conference on Intelligent User Interfaces},
 doi = {10.1145/2876456.2879485},
 isbn = {978-1-4503-4140-0},
 keyword = {human computer interaction, intuitive, medical image, natural, volume visualizer.},
 link = {http://doi.acm.org/10.1145/2876456.2879485},
 location = {Sonoma, California, USA},
 numpages = {4},
 pages = {54--57},
 publisher = {ACM},
 series = {IUI '16 Companion},
 title = {Computational Methods for the Natural and Intuitive Visualization of Volumetric Medical Data},
 year = {2016}
}


@inproceedings{Shimonishi:2016:TTC:2876456.2879475,
 abstract = {To design interactive systems that proactively assist users' decision making, the users' gaze information is an important cue for the system to estimate users' selection criteria. Users sometimes change selection criteria while browsing content. Therefore, temporal changes of those criteria need to be traced from gaze data in short time scales. In this paper, we propose an approach to detecting users' distinctive browsing periods with its appropriate time-scale by leveraging multiscale exact tests so that the system can trace temporal changes of selection criteria. We demonstrate the applicability of the proposed method through a toy example and experiments.},
 acmid = {2879475},
 address = {New York, NY, USA},
 author = {Shimonishi, Kei and Kawashima, Hiroaki and Schaffer, Erina and Matsuyama, Takashi},
 booktitle = {Companion Publication of the 21st International Conference on Intelligent User Interfaces},
 doi = {10.1145/2876456.2879475},
 isbn = {978-1-4503-4140-0},
 keyword = {decision making, gaze information, multiscale exact test},
 link = {http://doi.acm.org/10.1145/2876456.2879475},
 location = {Sonoma, California, USA},
 numpages = {4},
 pages = {9--12},
 publisher = {ACM},
 series = {IUI '16 Companion},
 title = {Tracing Temporal Changes of Selection Criteria from Gaze Information},
 year = {2016}
}


@inproceedings{Smith:2016:DOC:2876456.2876460,
 abstract = {Traditional cognitive testing for detecting cognitive impairment (CI) can be inaccessible, expensive, and time consuming. This dissertation aims to develop an automated online computerized neuropsychological testing system for rapidly tracking an individual's cognitive performance throughout the user's daily or weekly schedule in an unobtrusive way. By utilizing embedded microsensors within tablet devices, the proposed context-aware system will capture ambient and behavioral data pertinent to the real-world contexts and times of testing to compliment psychometric results, by providing insight into the contextual factors relevant to the user's testing efficacy and performance.},
 acmid = {2876460},
 address = {New York, NY, USA},
 author = {Smith, Sean-Ryan},
 booktitle = {Companion Publication of the 21st International Conference on Intelligent User Interfaces},
 doi = {10.1145/2876456.2876460},
 isbn = {978-1-4503-4140-0},
 keyword = {assistive technology, cognitive test, context-aware, mobile sensors, older persons},
 link = {http://doi.acm.org/10.1145/2876456.2876460},
 location = {Sonoma, California, USA},
 numpages = {4},
 pages = {118--121},
 publisher = {ACM},
 series = {IUI '16 Companion},
 title = {Dynamic Online Computerized Neuropsychological Testing System},
 year = {2016}
}


@inproceedings{Deloatch:2016:SMA:2876456.2879474,
 abstract = {The language people use on social media has been shown to provide insight into their personality characteristics. We developed a mobile system that aids the exploration and comparison of personal personality profiles with those of others. We conducted a user study to evaluate system usability, gauge user interaction of interest, and the system's performance in completing exploration and comparison tasks. Our study shows that the system is easy to use and enables users effectively explore and compare personality profiles, and users were interested in comparing their personality traits with the personality traits of friends, role models, and celebrities.},
 acmid = {2879474},
 address = {New York, NY, USA},
 author = {Deloatch, Robert and Gou, Liang and Kau, Chris and Mahmud, Jalal and Zhou, Michelle},
 booktitle = {Companion Publication of the 21st International Conference on Intelligent User Interfaces},
 doi = {10.1145/2876456.2879474},
 isbn = {978-1-4503-4140-0},
 keyword = {mobile system, personality, visualization and comparison},
 link = {http://doi.acm.org/10.1145/2876456.2879474},
 location = {Sonoma, California, USA},
 numpages = {4},
 pages = {102--105},
 publisher = {ACM},
 series = {IUI '16 Companion},
 title = {ScopeG: A Mobile Application for Exploration and Comparison of Personality Traits},
 year = {2016}
}


@inproceedings{Chen:2016:HTA:2876456.2879479,
 abstract = {We developed the user interfaces for two Human-Robot Interaction (HRI) tasking environments: dismounted infantry interacting with a ground robot (Autonomous Squad Member) and human interaction with an intelligent agent to manage a team of heterogeneous robotic vehicles (IMPACT). These user interfaces were developed based on the Situation awareness-based Agent Transparency (SAT) model. User testing showed that as agent transparency increased, so did overall human-agent team performance. Participants were able to calibrate their trust in the agent more appropriately as agent transparency increased.},
 acmid = {2879479},
 address = {New York, NY, USA},
 author = {Chen, Jessie Y.C. and Barnes, Michael J. and Selkowitz, Anthony R. and Stowers, Kimberly and Lakhmani, Shan G. and Kasdaglis, Nicholas},
 booktitle = {Companion Publication of the 21st International Conference on Intelligent User Interfaces},
 doi = {10.1145/2876456.2879479},
 isbn = {978-1-4503-4140-0},
 keyword = {agent transparency, autonomy, human-agent teaming, human-robot interaction, situation awareness},
 link = {http://doi.acm.org/10.1145/2876456.2879479},
 location = {Sonoma, California, USA},
 numpages = {4},
 pages = {28--31},
 publisher = {ACM},
 series = {IUI '16 Companion},
 title = {Human-Autonomy Teaming and Agent Transparency},
 year = {2016}
}


@inproceedings{Oduola:2016:AET:2876456.2876466,
 abstract = {This research seeks to produce a new way of assessing empathy in individuals. The current widely used diagnostic tools are questionnaires. These questionnaires are easy to "pass" if the individual simply lies and chooses the answers that would be most beneficial to them. Furthermore, it is shown, assessing empathy is harder in a clinical setting because it is not the natural world, a person may purposely inhibit their behavior to seem more "normal". Finding methods that would assess affect while interacting with a computer could yield higher accuracy in diagnosis},
 acmid = {2876466},
 address = {New York, NY, USA},
 author = {Oduola, Cassandra},
 booktitle = {Companion Publication of the 21st International Conference on Intelligent User Interfaces},
 doi = {10.1145/2876456.2876466},
 isbn = {978-1-4503-4140-0},
 keyword = {affective computing, mixed reality, virtual reality},
 link = {http://doi.acm.org/10.1145/2876456.2876466},
 location = {Sonoma, California, USA},
 numpages = {4},
 pages = {142--145},
 publisher = {ACM},
 series = {IUI '16 Companion},
 title = {Assessing Empathy Through Mixed Reality},
 year = {2016}
}


@inproceedings{Pienta:2016:SSE:2876456.2879480,
 abstract = {As the bulk electric grid becomes more complex, power system operators and engineers have more information to process and interpret than ever before. The information overload they experience can be mitigated by effective visualizations that facilitate rapid and intuitive assessment of the system state. With the introduction of non-dispatchable renewable energy, flexible loads, and energy storage, the ability to temporally explore system states becomes critical. This paper introduces STEPS, a new 3D Spatio-temporal Electric Power Systems visualization tool suitable for steady-state operational applications.},
 acmid = {2879480},
 address = {New York, NY, USA},
 author = {Pienta, Robert and Xiong, Leilei and Grijalva, Santiago and Chau, Duen Horng (Polo) and Kahng, Minsuk},
 booktitle = {Companion Publication of the 21st International Conference on Intelligent User Interfaces},
 doi = {10.1145/2876456.2879480},
 isbn = {978-1-4503-4140-0},
 keyword = {power system visualization, spatio-temporal visualization},
 link = {http://doi.acm.org/10.1145/2876456.2879480},
 location = {Sonoma, California, USA},
 numpages = {4},
 pages = {32--35},
 publisher = {ACM},
 series = {IUI '16 Companion},
 title = {STEPS: A Spatio-temporal Electric Power Systems Visualization},
 year = {2016}
}


@proceedings{Nichols:2016:2856767,
 abstract = {It is with great pleasure that we welcome you to beautiful Sonoma, California and the 21st edition of ACM International Conference on Intelligent User Interfaces -- ACM IUI 2016. ACM IUI is "where HCI meets AI," or where the academic research communities of Human-Computer Interaction (HCI) and Artificial Intelligence (AI) intersect. As the premier international forum for reporting outstanding research and development on intelligent user interfaces, the conference welcomes submissions describing work at the cross-section of these two fields and other related fields, such as psychology, behavioral science, cognitive science, computer graphics, design, the arts, and many others. Members of the ACM IUI community are interested in improving the symbiosis between humans and computers, increasing the intelligence of both in the process. The call for papers attracted 194 long and short paper submissions, 31 poster paper submissions, 11 demo paper submissions, and 17 student consortium submissions. The final program of the conference includes 2 keynotes, 49 long and short papers (25.3% acceptance rate), 15 posters, 6 demos, 2 workshops, 1 tutorial, and 12 student consortium papers. We are particularly excited for the keynotes by two distinguished speakers that will open the first and third days of the conference. The opening keynote will be given by Xavier Amatriain, VP of Engineering at Quora, on the topic of the "Past, Present, and Future of Recommender Systems: An Industry Perspective." The closing keynote will be given by Professor Elisabeth Andr√©, from Augsburg University and a long-time member of the ACM IUI community, on the topic of "Socially-Sensitive Interfaces: From Offline Studies to Interactive Experiences." The conference could not be organized without the help of a large number of individuals who generously volunteered much of their own time. Their names can be found on the following pages. All of the members of the organizing committee have done a fantastic job of coordinating the many moving parts that go into putting on a great conference. We must also particularly thank our 34 senior program committee members for coordinating the papers review process and the 91 members of the program committee for providing high quality reviews. And, most important, we must thank the authors for their diligent work that resulted in so many great submissions. These have allowed us to develop the excellent program that is the enduring heart of the conference. Another key for any great conference is a collection of strong sponsor organizations and generous corporate supporters. Our sponsors ACM, SIGAI, and SIGCHI are instrumental in making the conference happen year in and year out. This year, SIGCHI and SIGAI were particularly generous in providing financial support for our student travel grants, which have enabled 19 students to attend the conference that might not have otherwise. Our corporate supporters, Microsoft, IBM, Google, and Tableau have been supremely generous and the conference would be weaker without their contributions. We hope you will find the program engaging and the mix of academic disciplines broadens your perspective on computing. We also hope the conference will provide you with a valuable opportunity to share ideas with other researchers and practitioners from around the world, whether through presenting your own work formally or through informal discussions during the banquet or coffee breaks. With luck, those shared ideas will manifest themselves in exciting papers at next year's conference and ultimately have impact far beyond the conference! If you have any suggestions for how to improve the conference either this year or in the future, please do not hesitate to let us know.},
 address = {New York, NY, USA},
 isbn = {978-1-4503-4137-0},
 location = {Sonoma, California, USA},
 publisher = {ACM},
 title = {IUI '16: Proceedings of the 21st International Conference on Intelligent User Interfaces},
 year = {2016}
}


@inproceedings{Yamaya:2016:FMC:2876456.2879481,
 abstract = {Eye movement is expected to provide important clues for analyzing the human reading process. However, the noisy tracking environment makes it difficult to map the gaze data captured by eye-trackers to the user's intended word. In this paper, we propose an effective approach for accurately mapping a fixation to a word in the text. Our method regards consecutive horizontally progressive fixations as a sequential reading segment. We first classify transitions between segments according to six classes, and then identify the set of segments associated with each line of the document. Our experiments demonstrate that the proposed method achieves 87% mapping accuracy (15% higher than our previous work) with a classification performance of 84%. We also confirmed that manual annotation time can be reduced by using our approach as a reference. We believe that our method provides sufficiently good accuracy to warrant future analysis.},
 acmid = {2879481},
 address = {New York, NY, USA},
 author = {Yamaya, Akito and Topi\'{c}, Goran and Aizawa, Akiko},
 booktitle = {Companion Publication of the 21st International Conference on Intelligent User Interfaces},
 doi = {10.1145/2876456.2879481},
 isbn = {978-1-4503-4140-0},
 keyword = {eye-tracking, fixation-to-word mapping, reading},
 link = {http://doi.acm.org/10.1145/2876456.2879481},
 location = {Sonoma, California, USA},
 numpages = {5},
 pages = {36--40},
 publisher = {ACM},
 series = {IUI '16 Companion},
 title = {Fixation-to-Word Mapping with Classification of Saccades},
 year = {2016}
}


@inproceedings{Tanase:2016:SSV:2876456.2879473,
 abstract = {The IMOTION system is a content-based video search engine that provides fast and intuitive known item search in large video collections. User interaction consists mainly of sketching, which the system recognizes in real-time and makes suggestions based on both visual appearance of the sketch (what does the sketch look like in terms of colors, edge distribution, etc.) and semantic content (what object is the user sketching). The latter is enabled by a predictive sketch-based UI that identifies likely candidates for the sketched object via state-of-the-art sketch recognition techniques and offers on-screen completion suggestions. In this demo, we show how the sketch-based video retrieval of the IMOTION system is used in a collection of roughly 30,000 video shots. The system indexes collection data with over 30 visual features describing color, edge, motion, and semantic information. Resulting feature data is stored in ADAM, an efficient database system optimized for fast retrieval.},
 acmid = {2879473},
 address = {New York, NY, USA},
 author = {Tanase, Claudiu and Giangreco, Ivan and Rossetto, Luca and Schuldt, Heiko and Seddati, Omar and Dupont, Stephane and Altiok, Ozan Can and Sezgin, Metin},
 booktitle = {Companion Publication of the 21st International Conference on Intelligent User Interfaces},
 doi = {10.1145/2876456.2879473},
 isbn = {978-1-4503-4140-0},
 keyword = {content-based video retrieval, sketch interface},
 link = {http://doi.acm.org/10.1145/2876456.2879473},
 location = {Sonoma, California, USA},
 numpages = {5},
 pages = {97--101},
 publisher = {ACM},
 series = {IUI '16 Companion},
 title = {Semantic Sketch-Based Video Retrieval with Autocompletion},
 year = {2016}
}


@inproceedings{Araujo:2016:LOA:2876456.2879484,
 abstract = {Learning objects authoring is still a complex and time-consuming task for instructors, which requires attention to technical and pedagogical aspects. However, one can take advantage of the Ubiquitous Learning Environments characteristics to make it a mild process by means of automatic or semi-automatic processes. In this way, this paper presents an approach for creating learning objects and their metadata in such environments considering collaborative interactions among users. The proposed approach is being integrated to a real multimedia capture system used as a complementary tool in a university.},
 acmid = {2879484},
 address = {New York, NY, USA},
 author = {Ara\'{u}jo, Rafael D. and Ferreira, Hiran N.M. and Dor\c{c}a, Fabiano A. and Cattelan, Renan G.},
 booktitle = {Companion Publication of the 21st International Conference on Intelligent User Interfaces},
 doi = {10.1145/2876456.2879484},
 isbn = {978-1-4503-4140-0},
 keyword = {collaborative authoring, learning object metadata, learning objects authoring, ubiquitous learning environments},
 link = {http://doi.acm.org/10.1145/2876456.2879484},
 location = {Sonoma, California, USA},
 numpages = {5},
 pages = {49--53},
 publisher = {ACM},
 series = {IUI '16 Companion},
 title = {Learning Objects Authoring Supported by Ubiquitous Learning Environments},
 year = {2016}
}


