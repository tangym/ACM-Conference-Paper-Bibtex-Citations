@inproceedings{Martino:2010:OIF:1719970.1720030,
 abstract = {In this paper, we propose a tool to explore human movement dynamics in a Metropolitan Area. By analyzing a mass of individual cell phone traces, we build a Human-City Interaction System for understanding urban mobility patterns at different user-controlled temporal and geographic scales. We solve the problems that are found in available tools for spatio-temporal analysis, by allowing seamless manipulability and introducing a simultaneous\multi-scale visualization of individual and aggregate flows. Our tool is built to support the exploration and discovery of urban mobility patterns and the daily interactions of millions of people. Moreover, we implement an intelligent algorithm to evaluate the level of mobility homophily of people moving from place to place.},
 acmid = {1720030},
 address = {New York, NY, USA},
 author = {Martino, Mauro and Calabrese, Francesco and Di Lorenzo, Giusy and Andris, Clio and Liang, Liu and Ratti, Carlo},
 booktitle = {Proceedings of the 15th International Conference on Intelligent User Interfaces},
 doi = {10.1145/1719970.1720030},
 isbn = {978-1-60558-515-4},
 keyword = {cellphone data analysi, exploratory spatial data analysis, graph visualization, intelligent human information interaction, visual analysis},
 link = {http://doi.acm.org/10.1145/1719970.1720030},
 location = {Hong Kong, China},
 numpages = {4},
 pages = {357--360},
 publisher = {ACM},
 series = {IUI '10},
 title = {Ocean of Information: Fusing Aggregate \&\#38; Individual Dynamics for Metropolitan Analysis},
 year = {2010}
}


@inproceedings{Faulring:2010:ATM:1719970.1719980,
 abstract = {RADAR is a multiagent system with a mixed-initiative user interface designed to help office workers cope with email overload. RADAR agents observe experts to learn models of their strategies and then use the models to assist other people who are working on similar tasks. The agents' assistance helps a person to transition from the normal email-centric workflow to a more efficient task-centric workflow. The Email Classifier learns to identify tasks contained within emails and then inspects new emails for similar tasks. A novel task-management user interface displays the found tasks in a to-do list, which has integrated support for performing the tasks. The Multitask Coordination Assistant learns a model of the order in which experts perform tasks and then suggests a schedule to other people who are working on similar tasks. A novel Progress Bar displays the suggested schedule of incomplete tasks as well as the completed tasks. A large evaluation demonstrated that novice users confronted with an email overload test performed significantly better (a 37% better overall score with a factor of four fewer errors) when assisted by the RADAR agents.},
 acmid = {1719980},
 address = {New York, NY, USA},
 author = {Faulring, Andrew and Myers, Brad and Mohnkern, Ken and Schmerl, Bradley and Steinfeld, Aaron and Zimmerman, John and Smailagic, Asim and Hansen, Jeffery and Siewiorek, Daniel},
 booktitle = {Proceedings of the 15th International Conference on Intelligent User Interfaces},
 doi = {10.1145/1719970.1719980},
 isbn = {978-1-60558-515-4},
 keyword = {agents, email classification, email overload, intelligent planning, learning, radar, task management},
 link = {http://doi.acm.org/10.1145/1719970.1719980},
 location = {Hong Kong, China},
 numpages = {10},
 pages = {61--70},
 publisher = {ACM},
 series = {IUI '10},
 title = {Agent-assisted Task Management That Reduces Email Overload},
 year = {2010}
}


@inproceedings{Hussein:2010:IWS:1719970.1720061,
 abstract = {The International Workshop on Semantic Models for Adaptive Interactive Systems (SEMAIS 2010) aims to identify emerging trends in interactive system design using semantic models.},
 acmid = {1720061},
 address = {New York, NY, USA},
 author = {Hussein, Tim and Lukosch, Stephan G. and Ziegler, Juergen and Dix, Alan},
 booktitle = {Proceedings of the 15th International Conference on Intelligent User Interfaces},
 doi = {10.1145/1719970.1720061},
 isbn = {978-1-60558-515-4},
 keyword = {adaptive interactive systems, interface design, model-driven user interfaces, semantic models, usability},
 link = {http://doi.acm.org/10.1145/1719970.1720061},
 location = {Hong Kong, China},
 numpages = {2},
 pages = {437--438},
 publisher = {ACM},
 series = {IUI '10},
 title = {1st International Workshop on Semantic Models for Adaptive Interactive Systems (SEMAIS 2010)},
 year = {2010}
}


@inproceedings{Liu:2010:WIV:1719970.1720062,
 abstract = {This workshop brought together researchers and practitioners from both text analytics and interactive visualization communities to explore, define, and develop intelligent visual interfaces that help enhance the consumption and quality of complex text analysis results. Using this workshop as a starting point, we aim to foster closer, interdisciplinary relationships among researchers from text analytics and interactive visualization communities, so they can combine their expertise together to better tackle the difficult problems that face the text analytics community today.},
 acmid = {1720062},
 address = {New York, NY, USA},
 author = {Liu, Shixia and Zhou, Michelle X. and Carenini, Giuseppe and Qu, Huamin},
 booktitle = {Proceedings of the 15th International Conference on Intelligent User Interfaces},
 doi = {10.1145/1719970.1720062},
 isbn = {978-1-60558-515-4},
 keyword = {interactie visualization, text analytics, visual analytics},
 link = {http://doi.acm.org/10.1145/1719970.1720062},
 location = {Hong Kong, China},
 numpages = {2},
 pages = {439--440},
 publisher = {ACM},
 series = {IUI '10},
 title = {Workshop on Intelligent Visual Interfaces for Text Analysis},
 year = {2010}
}


@proceedings{Pu:2011:1943403,
 abstract = {It is our great pleasure to welcome you to the 2011 ACM International Conference on Intelligent User Interfaces -- IUI'11. Intelligent User Interfaces (IUI) is the premier conference for reporting on the study of user interfaces with intelligent devices. This topic is of increasing importance as the consumer is interfacing with a wide variety of devices with embedded computation and connectivity and the computer is fading into the background. IUI is where the community of people interested in Human-Computer Interaction (HCI) meets the Artificial Intelligence (AI) community. We have retained the successful format of the conference with Long Papers, Short Papers and Demonstrations. The accepted submissions cover a wide range of topics, including handheld devices, multimodal interfaces, social computing and navigation, intelligent help agents, input technologies, user modeling and personalization, intelligent authoring and information presentation, and pen-based interfaces. Geographically, the accepted work also represents researchers and institutions in many countries across four continents, including Argentina, Australia, Belgium, Canada, China, Denmark, Finland, France, Germany, Ireland, Israel, Italy, Japan, Korea, Netherlands, Pakistan, Spain, Switzerland, Thailand, United Kingdom, and the United States of America. As always, the selection process has been the object of careful consideration and multiple discussions. In order to ensure that all topics were adequately covered more than 150 experts have contributed to the selection of this year's program, and we trust that this had a very positive impact on the relevance and quality of individual reviews. We are grateful to the reviewers, the program committee and the senior program committee, who worked very hard in reviewing papers and providing feedback for authors. Following a trend adopted by several high-quality conferences, we had a rebuttal phase for Long Papers. We have asked the Senior PC moderators and the Demo Chair to formulate recommendations for acceptance and in the vast majority of cases it has been straightforward for us to endorse their choice. It is always down to the Program Chairs to make final decisions, sometimes difficult ones, on the total number of contributions to be accepted. Out of 180 submissions, we selected 28 long papers, 36 short papers and 15 demo papers. We have adopted a continuity policy from previous editions (around 30% for Long Papers); this year's overall acceptance rate achieves the right balance between selectivity and openness to innovative papers. The conference program highlights three invited talks: Andrei Broder from Yahoo! Research, Eric Horvitz from Microsoft Research and Ken Perlin from New York University. We are also glad that we got Anthony Jameson from DFKI GmbH and Joseph Konstan from the University of Minnesota as tutorial speakers. This year's Conference also features ten workshops, covering several hot topics in the area of IUI, with strong emphasis on multimodality of interfaces, smart interaction and personalization including: Sketch recognition, Semantic Models for Adaptive Interactive Systems, Intelligent User Interfaces for Developing Regions, Context-Awareness in Retrieval and Recommendation, Multimodal Interfaces for Automotive Applications, Visual Interfaces to the Social and Semantic Web, Location-Awareness for Mixed and Dual Reality, Eye Gaze in Intelligent Human Machine Interaction, Interacting with Smart Objects and Personalized Access to Cultural Heritage.},
 address = {New York, NY, USA},
 isbn = {978-1-4503-0419-1},
 location = {Palo Alto, CA, USA},
 note = {608110},
 publisher = {ACM},
 title = {IUI '11: Proceedings of the 16th International Conference on Intelligent User Interfaces},
 year = {2011}
}


@proceedings{Conati:2009:1502650,
 abstract = {The 2009 International Conference on Intelligent User Interfaces (IUI 2009) is the 14th such meeting since the first international conference in 1997. This series of annual conferences is the principal international forum for the presentation and discussion of outstanding research and applications involving intelligent user interfaces, a field at the intersection of Human-Computer Interaction and Artificial Intelligence. IUI focuses on interfaces that incorporate machine learning and artificial intelligence techniques. The interests and creative outlets of the community span a wide range of topics reflected in this year's meeting. These include automatic summarization techniques, recommendations and intelligent Web systems, intelligent information and knowledge management, example and demonstration based interfaces, novel approaches to input and output, mobile interaction, intelligent assistants and visualization and designer tools. This year's submitted papers came from all continents. In addition, both the program committee and submitted papers included members from industry and academia. This year's IUI conference counted with 134 long papers and 59 short papers submitted. Of the papers submitted, this program includes 35 long papers (26%) and 21 short papers, including long papers accepted as short papers. The papers program is complemented by the always popular Posters and Demonstrations session where authors of accepted short papers can showcase their work and where many published authors present live demos of their systems. This year's eight demonstrations will be presented during the Poster session. Intelligent User Interfaces are increasingly becoming a reality in our daily interactions with computers. Therefore, this year's invited speakers represent an interesting mix between industry and academia: Professor Trevor Darrell, from ICSI and University of Berkeley, Dr Jun Rekimoto from Sony Labs and University of Tokyo, and Dr. Alon Halvey, from Google. Seven workshops, each a full day, will also take place at IUI this year: Common Sense and Intelligent User Interfaces 2009: Story Understanding and Generation for Context-Aware Interface Design; Multimodal Interfaces for Automotive Applications (MIAA); Human Interaction with Intelligent & Networked Systems; Users' Preferences Regarding Intelligent User Interfaces: Differences Among Users and Changes Over Time; Visual Interfaces to the Social and the Semantic Web; Sketch Recognition and Model Driven Development of Advanced User Interfaces (MDDAUI 2009). Interestingly, this year's IUI has a record number of workshops, counterbalanced by a lack of tutorials. Perhaps this signifies a change of focus in the IUI community?},
 address = {New York, NY, USA},
 isbn = {978-1-60558-168-2},
 location = {Sanibel Island, Florida, USA},
 note = {608090},
 publisher = {ACM},
 title = {IUI '09: Proceedings of the 14th International Conference on Intelligent User Interfaces},
 year = {2009}
}


@inproceedings{Chen:2010:ICS:1719970.1720014,
 abstract = {We present an intelligent photo slideshow system that automatically analyzes thematic information about the photo collection and utilizes such information to generate compositions and transitions in two modes: story-telling mode and person-highlighting mode. In the story-telling mode the system groups photos by a theme-based clustering algorithm and multiple photos in each theme cluster are seamlessly tiled on a slide. Multiple tiling layouts are generated for each theme cluster and the slideshow is animated by intra-cluster transitions. In the person-highlighting mode, the system first recognizes faces from photos and creates photo clusters for individuals. It then uses face areas as ROI (Regions of Interests) and creates various content-based transitions to highlight individuals in a cluster. With an emphasis on photo content, our system creates slideshows with more fluid, dynamic and meaningful structure compared to existing systems.},
 acmid = {1720014},
 address = {New York, NY, USA},
 author = {Chen, Jiajian and Xiao, Jun and Gao, Yuli},
 booktitle = {Proceedings of the 15th International Conference on Intelligent User Interfaces},
 doi = {10.1145/1719970.1720014},
 isbn = {978-1-60558-515-4},
 keyword = {GPU, content-based transition, slideshow, theme clustering},
 link = {http://doi.acm.org/10.1145/1719970.1720014},
 location = {Hong Kong, China},
 numpages = {4},
 pages = {293--296},
 publisher = {ACM},
 series = {IUI '10},
 title = {iSlideShow: A Content-aware Slideshow System},
 year = {2010}
}


@inproceedings{Burel:2010:UWD:1719970.1720044,
 abstract = {The Ozone Browser is a platform independent tool that enables users to visually augment the knowledge presented in a web document in an unobtrusive way. This tool supports the user comprehension of Web documents through the use of Semantic Overlays. This tool uses linked data and lightweight semantics for getting relevant information within a document. The current implementation uses a JavaScript bookmarklet.},
 acmid = {1720044},
 address = {New York, NY, USA},
 author = {Burel, Gr{\'e}goire and Cano, Amparo Elizabeth},
 booktitle = {Proceedings of the 15th International Conference on Intelligent User Interfaces},
 doi = {10.1145/1719970.1720044},
 isbn = {978-1-60558-515-4},
 keyword = {semantic overlays, semantic web, web augmentation},
 link = {http://doi.acm.org/10.1145/1719970.1720044},
 location = {Hong Kong, China},
 numpages = {2},
 pages = {405--406},
 publisher = {ACM},
 series = {IUI '10},
 title = {Understanding Web Documents Using Semantic Overlays},
 year = {2010}
}


@inproceedings{McNally:2010:TRM:1719970.1719996,
 abstract = {While web search tasks are often inherently collaborative in nature, many search engines do not explicitly support collaboration during search. In this paper, we describe HeyStaks (www.heystaks.com), a system that provides a novel approach to collaborative web search. Designed to work with mainstream search engines such as Google, HeyStaks supports searchers by harnessing the experiences of others as the basis for result recommendations. Moreover, a key contribution of our work is to propose a reputation system for HeyStaks to model the value of individual searchers from a result recommendation perspective. In particular, we propose an algorithm to calculate reputation directly from user search activity and we provide encouraging results for our approach based on a preliminary analysis of user activity and reputation scores across a sample of HeyStaks users.},
 acmid = {1719996},
 address = {New York, NY, USA},
 author = {McNally, Kevin and O'Mahony, Michael P. and Smyth, Barry and Coyle, Maurice and Briggs, Peter},
 booktitle = {Proceedings of the 15th International Conference on Intelligent User Interfaces},
 doi = {10.1145/1719970.1719996},
 isbn = {978-1-60558-515-4},
 keyword = {collaborative web search, heystaks, reputation model},
 link = {http://doi.acm.org/10.1145/1719970.1719996},
 location = {Hong Kong, China},
 numpages = {10},
 pages = {179--188},
 publisher = {ACM},
 series = {IUI '10},
 title = {Towards a Reputation-based Model of Social Web Search},
 year = {2010}
}


@inproceedings{Brdiczka:2010:DTD:1719970.1720012,
 abstract = {A typical knowledge worker is involved in multiple tasks and switches frequently between them every work day. These frequent switches become expensive because each task switch requires some recovery time as well as the reconstitution of task context. First task management support systems have been proposed in recent years in order to assist the user during these switches. However, these systems still need a fairly big amount of investment from the user side in order to either learn to use or train such a system. In order to reduce the necessary amount of training, this paper proposes a new approach for automatically estimating a user's tasks from document interactions in an unsupervised manner. While most previous approaches to task detection look at the content of documents or window titles, which might raise confidentiality and privacy issues, our approach only requires document identifiers and the temporal switch history between them as input. Our prototype system monitors a user's desktop activities and logs documents that have focus on the user's desktop by attributing a unique identifier to each of these documents. Retrieved documents are filtered by their dwell times and a document similarity matrix is estimated based on document frequencies and switches. A spectral clustering algorithm then groups documents into tasks using the derived similarity matrix. The described prototype system has been evaluated on user data of 29 days from 10 different subjects in a corporation. Obtained results indicate that the approach is better than previous approaches that use content.},
 acmid = {1720012},
 address = {New York, NY, USA},
 author = {Brdiczka, Oliver},
 booktitle = {Proceedings of the 15th International Conference on Intelligent User Interfaces},
 doi = {10.1145/1719970.1720012},
 isbn = {978-1-60558-515-4},
 keyword = {automatic task identification, document clustering, user task modeling},
 link = {http://doi.acm.org/10.1145/1719970.1720012},
 location = {Hong Kong, China},
 numpages = {4},
 pages = {285--288},
 publisher = {ACM},
 series = {IUI '10},
 title = {From Documents to Tasks: Deriving User Tasks from Document Usage Patterns},
 year = {2010}
}


@inproceedings{Pedersen:2010:AGR:1719970.1720033,
 abstract = {We propose the concept of research trails to help web users create and reestablish context across fragmented research processes without requiring them to explicitly structure and organize the material. A research trail is an ordered sequence of web pages that were accessed as part of a larger investigation; they are automatically constructed by filtering and organizing users' activity history, using a combination of semantic and activity based criteria for grouping similar visited web pages. The design was informed by an ethnographic study of ordinary people doing research on the web, emphasizing a need to support research processes that are fragmented and where the research question is still in formation. This paper motivates and describes our algorithms for generating research trails. Research trails can be applied in several situations: as the underlying mechanism for a research task browser, or as feed to an ambient display of history information while searching. A prototype was built to assess the utility of the first option, a research trail browser.},
 acmid = {1720033},
 address = {New York, NY, USA},
 author = {Pedersen, Elin R{\o}nby and Gyllstrom, Karl and Gu, Shengyin and Hong, Peter Jin},
 booktitle = {Proceedings of the 15th International Conference on Intelligent User Interfaces},
 doi = {10.1145/1719970.1720033},
 isbn = {978-1-60558-515-4},
 keyword = {activity based computing, automatic clustering, ethnography, semantic clustering, task browser, web history},
 link = {http://doi.acm.org/10.1145/1719970.1720033},
 location = {Hong Kong, China},
 numpages = {4},
 pages = {369--372},
 publisher = {ACM},
 series = {IUI '10},
 title = {Automatic Generation of Research Trails in Web History},
 year = {2010}
}


@inproceedings{Bergman:2010:OWP:1719970.1719999,
 abstract = {Presentation material is a commonly-performed task. Yet current tools provide inadequate support - search tools are unable to return individual slides, and the linear model employed by presentation creation tools lacks structure and context. We propose a novel method for presentation creation, implemented in a tool called Outline Wizard, which enables outline-based composition and search. An Outline Wizard user enters a hierarchically-structured outline of a presentation; using that structure, the tool extracts user requests to formulate contextual queries, matches them against presentations within a repository, taking into account both content and structures of the presentations, and presents the user with sets of slides that are appropriate for each outline topic. At the heart of Outline Wizard is an outline-based search technique, which conducts content search within the context derived from the hierarchical structures of both user requests and presentations. We present a heuristic outline-extraction technique, which is used to reverse engineer the structures of presentations, thereby making the structures available for our search engine. Evaluations show that the outline extraction technique and outline-based search both perform well, and that users report a satisfying experience when using Outline Wizard to compose presentations from libraries of existing material.},
 acmid = {1719999},
 address = {New York, NY, USA},
 author = {Bergman, Lawrence and Lu, Jie and Konuru, Ravi and MacNaught, Julie and Yeh, Danny},
 booktitle = {Proceedings of the 15th International Conference on Intelligent User Interfaces},
 doi = {10.1145/1719970.1719999},
 isbn = {978-1-60558-515-4},
 keyword = {context-sensitive information retrieval, outline-based search, presentation composition, presentation search},
 link = {http://doi.acm.org/10.1145/1719970.1719999},
 location = {Hong Kong, China},
 numpages = {10},
 pages = {209--218},
 publisher = {ACM},
 series = {IUI '10},
 title = {Outline Wizard: Presentation Composition and Search},
 year = {2010}
}


@inproceedings{Hui:2010:UPL:1719970.1719989,
 abstract = {This paper describes our work in usage pattern analysis and development of a latent semantic analysis framework for interpreting multimodal user input consisting speech and pen gestures. We have designed and collected a multimodal corpus of navigational inquiries. Each modality carries semantics related to domain-specific task goal. Each inquiry is annotated manually with a task goal based on the semantics. Multimodal input usually has a simpler syntactic structure than unimodal input and the order of semantic constituents is different in multimodal and unimodal inputs. Therefore, we proposed to use semantic analysis to derive the latent semantics from the multimodal inputs using latent semantic modeling (LSM). In order to achieve this, we parse the recognized Chinese spoken input for the spoken locative references (SLR). These SLRs are then aligned with their corresponding pen gesture(s). Then, we characterized the cross-modal integration pattern as 3-tuple multimodal terms with SLR, pen gesture type and their temporal relation. The inquiry-multimodal term matrix is then decomposed using singular value decomposition (SVD) to derive the latent semantics automatically. Task goal inference based on the latent semantics shows that the task goal inference accuracy on a disjoint test set is of 99%.},
 acmid = {1719989},
 address = {New York, NY, USA},
 author = {Hui, Pui-Yu and Lo, Wai-Kit and Meng, Helen},
 booktitle = {Proceedings of the 15th International Conference on Intelligent User Interfaces},
 doi = {10.1145/1719970.1719989},
 isbn = {978-1-60558-515-4},
 keyword = {latent semantic modeling, multimodal input, pen gesture, singular value decomposition, spoken input, task goal inference},
 link = {http://doi.acm.org/10.1145/1719970.1719989},
 location = {Hong Kong, China},
 numpages = {10},
 pages = {129--138},
 publisher = {ACM},
 series = {IUI '10},
 title = {Usage Patterns and Latent Semantic Analyses for Task Goal Inference of Multimodal User Interactions},
 year = {2010}
}


@inproceedings{Waldner:2010:ACS:1719970.1720040,
 abstract = {Multi-display environments combine displays of various form factors into a common interaction space. Cross-display navigation techniques have to provide transitions to move the mouse pointer across display boundaries to reach distant display locations. A spatially consistent description of display relationships thereby supports fluid cross-display navigation. In this paper, we present two spatially consistent navigation techniques for seamless cross-display navigation in multi-user multi-display environments. These navigation techniques are automatically configured from a spatial model of the environment, which is generated in a camera-assisted calibration step. We describe the implementation in a distributed system and present results of a comparative experiment.},
 acmid = {1720040},
 address = {New York, NY, USA},
 author = {Waldner, Manuela and Pirchheim, Christian and Kruijff, Ernst and Schmalstieg, Dieter},
 booktitle = {Proceedings of the 15th International Conference on Intelligent User Interfaces},
 doi = {10.1145/1719970.1720040},
 isbn = {978-1-60558-515-4},
 keyword = {cross-display mouse navigation, multi-display environment},
 link = {http://doi.acm.org/10.1145/1719970.1720040},
 location = {Hong Kong, China},
 numpages = {4},
 pages = {397--400},
 publisher = {ACM},
 series = {IUI '10},
 title = {Automatic Configuration of Spatially Consistent Mouse Pointer Navigation in Multi-display Environments},
 year = {2010}
}


@inproceedings{Liu:2010:PNR:1719970.1719976,
 abstract = {Online news reading has become very popular as the web provides access to news articles from millions of sources around the world. A key challenge of news websites is to help users find the articles that are interesting to read. In this paper, we present our research on developing personalized news recommendation system in Google News. For users who are logged in and have explicitly enabled web history, the recommendation system builds profiles of users' news interests based on their past click behavior. To understand how users' news interests change over time, we first conducted a large-scale analysis of anonymized Google News users click logs. Based on the log analysis, we developed a Bayesian framework for predicting users' current news interests from the activities of that particular user and the news trends demonstrated in the activity of all users. We combine the content-based recommendation mechanism which uses learned user profiles with an existing collaborative filtering mechanism to generate personalized news recommendations. The hybrid recommender system was deployed in Google News. Experiments on the live traffic of Google News website demonstrated that the hybrid method improves the quality of news recommendation and increases traffic to the site.},
 acmid = {1719976},
 address = {New York, NY, USA},
 author = {Liu, Jiahui and Dolan, Peter and Pedersen, Elin R{\o}nby},
 booktitle = {Proceedings of the 15th International Conference on Intelligent User Interfaces},
 doi = {10.1145/1719970.1719976},
 isbn = {978-1-60558-515-4},
 keyword = {news trend, personalization, user modeling},
 link = {http://doi.acm.org/10.1145/1719970.1719976},
 location = {Hong Kong, China},
 numpages = {10},
 pages = {31--40},
 publisher = {ACM},
 series = {IUI '10},
 title = {Personalized News Recommendation Based on Click Behavior},
 year = {2010}
}


@inproceedings{Kimani:2010:AAF:1719970.1720025,
 abstract = {Social relationships and family involvement play an important role in health management, whereas activity awareness is useful in decision-making and stimulating motivation and action. In this paper, we propose a novel activity awareness user interface for family-oriented healthy living social networks. It is intended to increase family members' interaction with healthy living social networks. A user study showed that the activity awareness interface can add value to specific aspects of interaction with family-based healthy living social applications. The interface increased interaction with the underlying healthy living content and led to higher level of learning about healthy living and impact on specific healthy living activities. There was also significant appreciation of and interaction with the activity awareness user interface elements.},
 acmid = {1720025},
 address = {New York, NY, USA},
 author = {Kimani, Stephen and Berkovsky, Shlomo and Smith, Greg and Freyne, Jill and Baghaei, Nilufar and Bhandari, Dipak},
 booktitle = {Proceedings of the 15th International Conference on Intelligent User Interfaces},
 doi = {10.1145/1719970.1720025},
 isbn = {978-1-60558-515-4},
 keyword = {activity awareness, evaluation, families, healthy living, online social networks, user interaction, user interface},
 link = {http://doi.acm.org/10.1145/1719970.1720025},
 location = {Hong Kong, China},
 numpages = {4},
 pages = {337--340},
 publisher = {ACM},
 series = {IUI '10},
 title = {Activity Awareness in Family-based Healthy Living Online Social Networks},
 year = {2010}
}


@inproceedings{Mao:2010:SEI:1719970.1720029,
 abstract = {Formulating proper keywords and evaluating search results are common difficulties in exploratory information seeking. Reusing and refining others' successful searches are pragmatic directions to tackle these difficulties. In this paper, we present a novel epistemology-based social search solution, where search epistemologies are effectively shared, reused, and refined by others with the same or similar search interests through novel user interfaces. We have developed a prototype system Baijia and experimental results show that an epistemology-based social search system outperforms a conventional search engine in supporting exploratory information seeking.},
 acmid = {1720029},
 address = {New York, NY, USA},
 author = {Mao, Yuqing and Shen, Haifeng and Sun, Chengzheng},
 booktitle = {Proceedings of the 15th International Conference on Intelligent User Interfaces},
 doi = {10.1145/1719970.1720029},
 isbn = {978-1-60558-515-4},
 keyword = {epistemology-based social search, exploratory information seeking, world wide web},
 link = {http://doi.acm.org/10.1145/1719970.1720029},
 location = {Hong Kong, China},
 numpages = {4},
 pages = {353--356},
 publisher = {ACM},
 series = {IUI '10},
 title = {Supporting Exploratory Information Seeking by Epistemology-based Social Search},
 year = {2010}
}


@inproceedings{Hurst:2010:AIT:1719970.1719973,
 abstract = {Information about the location and size of the targets that users interact with in real world settings can enable new innovations in human performance assessment and soft-ware usability analysis. Accessibility APIs provide some information about the size and location of targets. How-ever this information is incomplete because it does not sup-port all targets found in modern interfaces and the reported sizes can be inaccurate. These accessibility APIs access the size and location of targets through low-level hooks to the operating system or an application. We have developed an alternative solution for target identification that leverages visual affordances in the interface, and the visual cues produced as users interact with targets. We have used our novel target identification technique in a hybrid solution that combines machine learning, computer vision, and accessibility API data to find the size and location of targets users select with 89% accuracy. Our hybrid approach is superior to the performance of the accessibility API alone: in our dataset of 1355 targets covering 8 popular applications, only 74% of the targets were correctly identified by the API alone.},
 acmid = {1719973},
 address = {New York, NY, USA},
 author = {Hurst, Amy and Hudson, Scott E. and Mankoff, Jennifer},
 booktitle = {Proceedings of the 15th International Conference on Intelligent User Interfaces},
 doi = {10.1145/1719970.1719973},
 isbn = {978-1-60558-515-4},
 keyword = {computer accessibility, pointing input, target identification, usability analysis},
 link = {http://doi.acm.org/10.1145/1719970.1719973},
 location = {Hong Kong, China},
 numpages = {10},
 pages = {11--20},
 publisher = {ACM},
 series = {IUI '10},
 title = {Automatically Identifying Targets Users Interact with During Real World Tasks},
 year = {2010}
}


@inproceedings{Mahmud:2010:ASI:1719970.1720028,
 abstract = {3D virtual world software is becoming a popular medium for entertainment, social interaction and commerce. To the best of our knowledge, there is no system available to facilitate the bridging between Web applications and virtual world systems in the form of information sharing, data collection and control propagation. As a result, user experience in a Web interface is not sensitive to state changes of virtual world avatars or objects. Similarly, a virtual world environment does not provide Web context-rich user experience. We address this issue and propose a bridging and context sharing architecture between the Web and virtual world applications such that Web applications can control, monitor and collect information from artifacts in the virtual worlds, and vice versa. We also implemented this architecture using existing Web and virtual world technologies. Based on this implementation, we illustrate some novel applications and present a user study to illustrate the value of the system.},
 acmid = {1720028},
 address = {New York, NY, USA},
 author = {Mahmud, Jalal and Huang, Yun-Wu and Ponzo, John and Pollak, Roger},
 booktitle = {Proceedings of the 15th International Conference on Intelligent User Interfaces},
 doi = {10.1145/1719970.1720028},
 isbn = {978-1-60558-515-4},
 keyword = {3D web, bridging, second life, virtual world},
 link = {http://doi.acm.org/10.1145/1719970.1720028},
 location = {Hong Kong, China},
 numpages = {4},
 pages = {349--352},
 publisher = {ACM},
 series = {IUI '10},
 title = {Avara: A System to Improve User Experience in Web and Virtual World},
 year = {2010}
}


@inproceedings{Ketabdar:2010:AAL:1719970.1720050,
 abstract = {In this work, we present a system and methodology for using mobile phones for monitoring physical activities of a user, and its applications in assisting elderly and people with need for special care and monitoring. The method is based on processing acceleration data provided by accelerometers integrated in mobile phones. This information is sent to a monitoring server, analyzed and presented as different health related factors for assistance, monitoring and healthcare purposes. A monitoring agent can use a desktop application to observe pattern of physical activities of several users in a live manner, and receive warnings in case of unexpected physical conditions. The data can be also stored offline for longer term analysis of physical behaviour and health. The desktop application also provides different options for managing, browsing, and searching activity related data.},
 acmid = {1720050},
 address = {New York, NY, USA},
 author = {Ketabdar, Hamed and Lyra, Matti},
 booktitle = {Proceedings of the 15th International Conference on Intelligent User Interfaces},
 doi = {10.1145/1719970.1720050},
 isbn = {978-1-60558-515-4},
 keyword = {acceleration sensor, assisted life, desktop application, health related factors, live activity monitoring, mobile phones},
 link = {http://doi.acm.org/10.1145/1719970.1720050},
 location = {Hong Kong, China},
 numpages = {2},
 pages = {417--418},
 publisher = {ACM},
 series = {IUI '10},
 title = {ActivityMonitor: Assisted Life Using Mobile Phones},
 year = {2010}
}


@inproceedings{Rhienmora:2010:HAR:1719970.1720054,
 abstract = {We developed an augmented reality (AR) dental training simulator utilizing a haptic (force feedback) device. A number of dental procedures such as crown preparation and opening access to the pulp can be simulated with various shapes of dental drill. The system allows students to practise surgery in the correct postures as in the actual environment by combining 3D tooth and tool models upon the real-world view and displaying the result through a video see-through head mounted display (HMD). The system monitors the important features such as applied forces and tool movement that characterize the quality of the procedure. Automatic performance assessment is achieved by comparing outcome and process features of a student with the best matching expert. Moreover, we incorporated kinematic feedback and hand guidance by haptic device. The result from an initial evaluation shows that the simulator is promising for supplemental training.},
 acmid = {1720054},
 address = {New York, NY, USA},
 author = {Rhienmora, Phattanapon and Gajananan, Kugamoorthy and Haddawy, Peter and Suebnukarn, Siriwan and Dailey, Matthew N. and Supataratarn, Ekarin and Shrestha, Poonam},
 booktitle = {Proceedings of the 15th International Conference on Intelligent User Interfaces},
 doi = {10.1145/1719970.1720054},
 isbn = {978-1-60558-515-4},
 keyword = {augmented reality, automatic performance assessment, dental surgical training, haptic device},
 link = {http://doi.acm.org/10.1145/1719970.1720054},
 location = {Hong Kong, China},
 numpages = {2},
 pages = {425--426},
 publisher = {ACM},
 series = {IUI '10},
 title = {Haptic Augmented Reality Dental Trainer with Automatic Performance Assessment},
 year = {2010}
}


@inproceedings{Baur:2010:RRR:1719970.1719984,
 abstract = {We present rush as a recommendation-based interaction and visualization technique for repeated item selection from large data sets on mobile touch screen devices. Proposals and choices are intertwined in a continuous finger gesture navigating a two-dimensional canvas of recommended items. This provides users with more flexibility for the resulting selections. Our design is based on a formative user study regarding orientation and occlusion aspects. Subsequently, we implemented a version of rush for music playlist creation. In an experimental evaluation we compared different types of recommendations based on similarity, namely the top 5 most similar items, five random selections from the list of similar items and a hybrid version of the two. Participants had to create playlists using each condition. Our results show that top 5 was too restricting, while random and hybrid suggestions had comparable results.},
 acmid = {1719984},
 address = {New York, NY, USA},
 author = {Baur, Dominikus and Boring, Sebastian and Butz, Andreas},
 booktitle = {Proceedings of the 15th International Conference on Intelligent User Interfaces},
 doi = {10.1145/1719970.1719984},
 isbn = {978-1-60558-515-4},
 keyword = {interaction technique, mobile, recommender systems},
 link = {http://doi.acm.org/10.1145/1719970.1719984},
 location = {Hong Kong, China},
 numpages = {10},
 pages = {91--100},
 publisher = {ACM},
 series = {IUI '10},
 title = {Rush: Repeated Recommendations on Mobile Devices},
 year = {2010}
}


@inproceedings{Sonntag:2010:MDM:1719970.1720036,
 abstract = {This paper presents a multimodal dialogue mashup where different users are involved in the use of different user interfaces for the annotation and retrieval of medical images. Our solution is a mashup that integrates a multimodal interface for speech-based annotation of medical images and dialogue-based image retrieval with a semantic image annotation tool for manual annotations on a desktop computer. A remote RDF repository connects the annotation and querying task into a common framework and serves as the semantic backend system for the advanced multimodal dialogue a radiologist can use.},
 acmid = {1720036},
 address = {New York, NY, USA},
 author = {Sonntag, Daniel and M\"{o}ller, Manuel},
 booktitle = {Proceedings of the 15th International Conference on Intelligent User Interfaces},
 doi = {10.1145/1719970.1720036},
 isbn = {978-1-60558-515-4},
 keyword = {collaborative environments, design, touchscreen interface},
 link = {http://doi.acm.org/10.1145/1719970.1720036},
 location = {Hong Kong, China},
 numpages = {4},
 pages = {381--384},
 publisher = {ACM},
 series = {IUI '10},
 title = {A Multimodal Dialogue Mashup for Medical Image Semantics},
 year = {2010}
}


@inproceedings{Goto:2010:RTP:1719970.1720047,
 abstract = {On-demand services for TV program, which provide users with past programs on demand, are becoming popular. It is therefore necessary to find a means of efficiently searching for programs that users want to view, from huge program archives. This paper proposes an automatic method of retrieving programs related to the one being viewed by the user. To that end, we compute similarity between program summaries and closed captions obtained from broadcasting by weighting significant words such as compound words and named entities. Additionally our method provides inter-program relationship labels to indicate why the results of relevant programs were chosen. The results of an evaluation showed that the method recommended relevant programs with higher accuracy than baseline methods and indicated appropriate relationship labels for related programs.},
 acmid = {1720047},
 address = {New York, NY, USA},
 author = {Goto, Jun and Sumiyoshi, Hideki and Miyazaki, Masaru and Tanaka, Hideki and Shibata, Masahiro and Aizawa, Akiko},
 booktitle = {Proceedings of the 15th International Conference on Intelligent User Interfaces},
 doi = {10.1145/1719970.1720047},
 isbn = {978-1-60558-515-4},
 keyword = {TV program retrieval, n-gram, named entity, relationship},
 link = {http://doi.acm.org/10.1145/1719970.1720047},
 location = {Hong Kong, China},
 numpages = {2},
 pages = {411--412},
 publisher = {ACM},
 series = {IUI '10},
 title = {Relevant TV Program Retrieval Using Broadcast Summaries},
 year = {2010}
}


@inproceedings{Chen:2010:SIP:1719970.1720015,
 abstract = {"Product popularity" is in-depth explored in this paper, regarding its practical role within a consumer's decision process. Specifically, the usability evaluation of a novel product finder service (Flickr Camera Finder) shows that users more frequently consulted it, rather than a standard shopping site, to locate popular products. User comments further revealed their credibility concerns and tendency to trust the "popularity" from social resources. Design implications from the experiment are summarized at the end, indicating suggestive directions to integrate social media data to boost current e-commerce decision tools.},
 acmid = {1720015},
 address = {New York, NY, USA},
 author = {Chen, Li},
 booktitle = {Proceedings of the 15th International Conference on Intelligent User Interfaces},
 doi = {10.1145/1719970.1720015},
 isbn = {978-1-60558-515-4},
 keyword = {Flickr camera finder, consumer decision behavior, e-commerce, product popularity, social influence, usability study},
 link = {http://doi.acm.org/10.1145/1719970.1720015},
 location = {Hong Kong, China},
 numpages = {4},
 pages = {297--300},
 publisher = {ACM},
 series = {IUI '10},
 title = {Social Influence of Product Popularity on Consumer Decisions: Usability Study of Flickr Camera Finder},
 year = {2010}
}


@inproceedings{Kratz:2010:GRS:1719970.1720026,
 abstract = {We present the $3 Gesture Recognizer, a simple but robust gesture recognition system for input devices featuring 3D acceleration sensors. The algorithm is designed to be implemented quickly in prototyping environments, is intended to be device-independent and does not require any special toolkits or frameworks. It relies solely on simple trigonometric and geometric calculations. A user evaluation of our system resulted in a correct gesture recognition rate of 80%, when using a set of 10 unique gestures for classification. Our method requires significantly less training data than other gesture recognizers and is thus suited to be deployed and to deliver results rapidly.},
 acmid = {1720026},
 address = {New York, NY, USA},
 author = {Kratz, Sven and Rohs, Michael},
 booktitle = {Proceedings of the 15th International Conference on Intelligent User Interfaces},
 doi = {10.1145/1719970.1720026},
 isbn = {978-1-60558-515-4},
 keyword = {3D gestures, classifier, gesture recognition, rapid prototyping, recognition rates, user interfaces},
 link = {http://doi.acm.org/10.1145/1719970.1720026},
 location = {Hong Kong, China},
 numpages = {4},
 pages = {341--344},
 publisher = {ACM},
 series = {IUI '10},
 title = {A \$3 Gesture Recognizer: Simple Gesture Recognition for Devices Equipped with 3D Acceleration Sensors},
 year = {2010}
}


@inproceedings{Biswas:2010:EDI:1719970.1720010,
 abstract = {We have developed a simulator to help with the design and evaluation of assistive interfaces. The simulator can predict possible interaction patterns when undertaking a task using a variety of input devices, and estimate the time to complete the task in the presence of different dis-abilities. In this paper, we have presented a study to evaluate the simulator by considering a representative application being used by able-bodied, visually impaired and mobility impaired people. The simulator predicted task completion times for all three groups with statistically significant accuracy. The simulator also predicted the effects of different interface designs on task completion time accurately.},
 acmid = {1720010},
 address = {New York, NY, USA},
 author = {Biswas, Pradipta and Robinson, Peter},
 booktitle = {Proceedings of the 15th International Conference on Intelligent User Interfaces},
 doi = {10.1145/1719970.1720010},
 isbn = {978-1-60558-515-4},
 keyword = {assistive technology, human computer interaction, simulator, usability evaluation, user model},
 link = {http://doi.acm.org/10.1145/1719970.1720010},
 location = {Hong Kong, China},
 numpages = {4},
 pages = {277--280},
 publisher = {ACM},
 series = {IUI '10},
 title = {Evaluating the Design of Inclusive Interfaces by Simulation},
 year = {2010}
}


@inproceedings{Park:2010:ANB:1719970.1719977,
 abstract = {Aspect-level news browsing provides readers with a classified view of news articles with different viewpoints. It facilitates active interactions with which readers easily discover and compare diverse existing biased views over a news event. As such, it effectively helps readers understand the event from a plural of viewpoints and formulate their own, more balanced viewpoints free from specific biased views. Realizing aspect-level browsing raises important challenges, mainly due to the lack of semantic knowledge with which to abstract and classify the intended salient aspects of articles. We first demonstrate the feasibility of aspect-level news browsing through user studies. We then deeply look into the news article production process and develop framing cycle-aware clustering. The evaluation results show that the developed method performs classification more accurately than other methods.},
 acmid = {1719977},
 address = {New York, NY, USA},
 author = {Park, Souneil and Lee, SangJeong and Song, Junehwa},
 booktitle = {Proceedings of the 15th International Conference on Intelligent User Interfaces},
 doi = {10.1145/1719970.1719977},
 isbn = {978-1-60558-515-4},
 keyword = {aspect-level classification, aspect-level news browsing, media bias},
 link = {http://doi.acm.org/10.1145/1719970.1719977},
 location = {Hong Kong, China},
 numpages = {10},
 pages = {41--50},
 publisher = {ACM},
 series = {IUI '10},
 title = {Aspect-level News Browsing: Understanding News Events from Multiple Viewpoints},
 year = {2010}
}


@inproceedings{Li:2010:MLI:1719970.1720027,
 abstract = {Under wearable environments, it is not convenient to label an object with portable keyboards and mice. This paper presents a multimodal labeling interface to solve this problem with natural and efficient operations. Visual and audio modalities cooperate with each other: an object is encircled by visual tracking of a pointing gesture, and meanwhile its name is obtained by speech recognition. In this paper, we propose a concept of virtual touchpad based on stereo vision techniques. With the touchpad, the object encircling task is achieved by drawing a closed curve on a transparent blackboard. The touch events and movements of a pointing gesture are robustly detected for natural gesture interactions. The experimental results demonstrate the efficiency and usability of our multimodal interface.},
 acmid = {1720027},
 address = {New York, NY, USA},
 author = {Li, Shanqing and Jia, Yunde},
 booktitle = {Proceedings of the 15th International Conference on Intelligent User Interfaces},
 doi = {10.1145/1719970.1720027},
 isbn = {978-1-60558-515-4},
 keyword = {multimodal labeling, virtual touchpad, wearable computing},
 link = {http://doi.acm.org/10.1145/1719970.1720027},
 location = {Hong Kong, China},
 numpages = {4},
 pages = {345--348},
 publisher = {ACM},
 series = {IUI '10},
 title = {A Multimodal Labeling Interface for Wearable Computing},
 year = {2010}
}


@inproceedings{Legaspi:2010:APD:1719970.1719974,
 abstract = {Data-centric affect modeling may render itself restrictive in practical applications for three reasons, namely, it falls short of feature optimization, infers discrete affect classes, and deals with relatively small to average sized datasets. Though it seems practical to use the feature combinations already associated to commonly investigated sensors, there may be other potentially optimal features that can lead to new relations. Secondly, although it seems more realistic to view affect as continuous, it requires using continuous labels that will increase the difficulty of modeling. Lastly, although a large scale dataset reflects a more precise range of values for any given feature, it severely hinders computational efficiency. We address these problems when inferring physiology-affect relations from datasets that contain 2-3 million feature vectors, each with 49 features and labelled with continuous affect values. We employ automatic feature selection to acquire near optimal feature subsets and a fast approximate kNN algorithm to solve the regression problem and cope with the challenge of a large scale dataset. Our results show that high estimation accuracy may be achieved even when the selected feature subset is only about 7% of the original features. May the results here motivate the HCI community to pursue affect modeling without being deterred by large datasets and further the discussions on acquiring optimal features for accurate continuous affect approximation.},
 acmid = {1719974},
 address = {New York, NY, USA},
 author = {Legaspi, Roberto and Fukui, Ken-ichi and Moriyama, Koichi and Kurihara, Satoshi and Numao, Masayuki and Suarez, Merlin},
 booktitle = {Proceedings of the 15th International Conference on Intelligent User Interfaces},
 doi = {10.1145/1719970.1719974},
 isbn = {978-1-60558-515-4},
 keyword = {affective computing, machine learning, pattern recognition},
 link = {http://doi.acm.org/10.1145/1719970.1719974},
 location = {Hong Kong, China},
 numpages = {10},
 pages = {21--30},
 publisher = {ACM},
 series = {IUI '10},
 title = {Addressing the Problems of Data-centric Physiology-affect Relations Modeling},
 year = {2010}
}


@inproceedings{Felfernig:2010:PUI:1719970.1720020,
 abstract = {Configuration technologies are well established as a foundation of mass customization which is a production paradigm that supports the manufacturing of highly-variant products under pricing conditions similar to mass production. A side-effect of the high diversity of products offered by a configurator is that the complexity of the alternatives may outstrip a user's capability to explore them and make a buying decision. In order to improve the quality of configuration processes, we combine knowledge-based configuration with collaborative and content-based recommendation algorithms. In this paper we present configuration techniques that recommend personalized default values to users. Results of an empirical study show improvements in terms of, for example, user satisfaction or the quality of the configuration process.},
 acmid = {1720020},
 address = {New York, NY, USA},
 author = {Felfernig, Alexander and Mandl, Monika and Tiihonen, Juha and Schubert, Monika and Leitner, Gerhard},
 booktitle = {Proceedings of the 15th International Conference on Intelligent User Interfaces},
 doi = {10.1145/1719970.1720020},
 isbn = {978-1-60558-515-4},
 keyword = {configuration systems, model-based diagnosis, recommender systems},
 link = {http://doi.acm.org/10.1145/1719970.1720020},
 location = {Hong Kong, China},
 numpages = {4},
 pages = {317--320},
 publisher = {ACM},
 series = {IUI '10},
 title = {Personalized User Interfaces for Product Configuration},
 year = {2010}
}


@inproceedings{Ortiz-Martinez:2010:IMT:1719970.1720053,
 abstract = {In this paper we present a new way of translating documents by using a Web-based system. An interactive approach is proposed as an alternative to post-editing the output of a machine translation system. In this approach, the user's feedback is used to validate or to correct parts of the system output that allow the generation of improved versions of the rest of the output.},
 acmid = {1720053},
 address = {New York, NY, USA},
 author = {Ortiz-Mart\'{\i}nez, Daniel and Leiva, Luis A. and Alabau, Vicent and Casacuberta, Francisco},
 booktitle = {Proceedings of the 15th International Conference on Intelligent User Interfaces},
 doi = {10.1145/1719970.1720053},
 isbn = {978-1-60558-515-4},
 keyword = {computer assisted translation, interactive machine translation, statistical machine translation},
 link = {http://doi.acm.org/10.1145/1719970.1720053},
 location = {Hong Kong, China},
 numpages = {2},
 pages = {423--424},
 publisher = {ACM},
 series = {IUI '10},
 title = {Interactive Machine Translation Using a Web-based Architecture},
 year = {2010}
}


@inproceedings{Yu:2010:AIU:1719970.1720056,
 abstract = {Riding on the back of the rapid expansion of the Internet, online virtual worlds which combine the prowess of interactive digital media and social networks have attained a high level of acceptance among the Net Generation users. This development prompted researchers to look into the potential of embedding learning contents into virtual worlds to create virtual learning environments (VLEs) that suit the need of the Net Generation learners. However, the special characteristics of virtual worlds that make them popular also pose great challenges to educators who wish to leverage their power. These challenges call for more sophisticated human computer interaction (HCI) mechanisms to assist learners to navigate the intriguing landscape of VLEs. In this paper, we demonstrate a teachable remembrance agent which acts as an intelligent user interface to provide innovative ways for students to interact with VLEs.},
 acmid = {1720056},
 address = {New York, NY, USA},
 author = {Yu, Han and Cai, Yundong and Shen, Zhiqi and Tao, Xuehong and Miao, Chunyan},
 booktitle = {Proceedings of the 15th International Conference on Intelligent User Interfaces},
 doi = {10.1145/1719970.1720056},
 isbn = {978-1-60558-515-4},
 keyword = {interface agent, remembrance agent, teachable agent, virtual learning environment, virtual world},
 link = {http://doi.acm.org/10.1145/1719970.1720056},
 location = {Hong Kong, China},
 numpages = {2},
 pages = {429--430},
 publisher = {ACM},
 series = {IUI '10},
 title = {Agents As Intelligent User Interfaces for the Net Generation},
 year = {2010}
}


@inproceedings{Khawaja:2010:ULC:1719970.1720024,
 abstract = {An adaptive interaction system, which is aware of the users' current cognitive load, can change its response, presentation and interaction flow to improve users' experience and their task performance. In this paper, we propose a novel speech content analysis approach for measuring users' cognitive load, based on their language and dialogue complexity. We have analysed the transcribed speech of operators working in computerized incident control rooms and involved in highly complex bushfire management tasks in Australia. The resulting patterns of language complexity show significant differences between the speech from cognitively low load and high load tasks. We also discuss the value of using this approach of cognitive load measurement for user interface evaluation and interaction design improvement.},
 acmid = {1720024},
 address = {New York, NY, USA},
 author = {Khawaja, M. Asif and Chen, Fang and Marcus, Nadine},
 booktitle = {Proceedings of the 15th International Conference on Intelligent User Interfaces},
 doi = {10.1145/1719970.1720024},
 isbn = {978-1-60558-515-4},
 keyword = {cognitive load, interaction design, language complexity measures, measurement},
 link = {http://doi.acm.org/10.1145/1719970.1720024},
 location = {Hong Kong, China},
 numpages = {4},
 pages = {333--336},
 publisher = {ACM},
 series = {IUI '10},
 title = {Using Language Complexity to Measure Cognitive Load for Adaptive Interaction Design},
 year = {2010}
}


@inproceedings{Andre:2010:WEG:1719970.1720058,
 abstract = {This workshop brought researchers from academia and industry together to share recent advances and discuss research directions and opportunities for next generation of intelligent human machine interaction that incorporate eye gaze.},
 acmid = {1720058},
 address = {New York, NY, USA},
 author = {Andr{\'e}, Elisabeth and Chai, Joyce Y.},
 booktitle = {Proceedings of the 15th International Conference on Intelligent User Interfaces},
 doi = {10.1145/1719970.1720058},
 isbn = {978-1-60558-515-4},
 keyword = {eye gaze, intelligent human machine interaction},
 link = {http://doi.acm.org/10.1145/1719970.1720058},
 location = {Hong Kong, China},
 numpages = {2},
 pages = {431--432},
 publisher = {ACM},
 series = {IUI '10},
 title = {Workshop: Eye Gaze in Intelligent Human Machine Interaction},
 year = {2010}
}


@inproceedings{Han:2010:NLI:1719970.1720022,
 abstract = {One of the critical problems in natural language interfaces is the discordance between the expressions covered by the interface and those by the knowledge base. In the graph-based knowledge base such as an ontology, all possible queries can be prepared in advance. As a solution of the discordance problem in natural language interfaces, this paper proposes a method that translates a natural language query into a formal language query such as SPARQL. In this paper, a user query is translated into a formal language by choosing the most appropriate query from the prepared queries. The experimental results show a high accuracy and coverage for the given knowledge base.},
 acmid = {1720022},
 address = {New York, NY, USA},
 author = {Han, Yong-Jin and Noh, Tae-Gil and Park, Seong-Bae and Park, Se Young and Lee, Sang-Jo},
 booktitle = {Proceedings of the 15th International Conference on Intelligent User Interfaces},
 doi = {10.1145/1719970.1720022},
 isbn = {978-1-60558-515-4},
 keyword = {knowledge base, knowledge concordance, natural language interface, ontology},
 link = {http://doi.acm.org/10.1145/1719970.1720022},
 location = {Hong Kong, China},
 numpages = {4},
 pages = {325--328},
 publisher = {ACM},
 series = {IUI '10},
 title = {A Natural Language Interface of Thorough Coverage by Concordance with Knowledge Bases},
 year = {2010}
}


@inproceedings{Berkovsky:2010:IGY:1719970.1720043,
 abstract = {The addictive nature of game playing contributes to an increasingly sedentary lifestyle. In this demonstration we showcase PLAY, MATE!, a novel mixed reality game design that motivates players to perform physical activity as part of playing. According to the PLAY, MATE! design, players gain virtual game rewards in return for the real physical activity they perform. We demonstrate the application of the PLAY, MATE! design to an open source game and allow participants to experience physical activity motivating games in person.},
 acmid = {1720043},
 address = {New York, NY, USA},
 author = {Berkovsky, Shlomo and Coombe, Mac and Freyne, Jill and Bhandari, Dipak},
 booktitle = {Proceedings of the 15th International Conference on Intelligent User Interfaces},
 doi = {10.1145/1719970.1720043},
 isbn = {978-1-60558-515-4},
 keyword = {activity motivation, bodily interface, game interaction},
 link = {http://doi.acm.org/10.1145/1719970.1720043},
 location = {Hong Kong, China},
 numpages = {2},
 pages = {403--404},
 publisher = {ACM},
 series = {IUI '10},
 title = {Isn'T It Great?: You Can PLAY, MATE!},
 year = {2010}
}


@inproceedings{Ehara:2010:PRS:1719970.1719978,
 abstract = {Novel intelligent interface eases the browsing of Web documents written in the second languages of users. It automatically predicts words unfamiliar to the user by collective intelligence and glosses them with their meaning in advance. If the prediction succeeds, the user does not need to consult a dictionary; even if it fails, the user can correct the prediction. The correction data are collected and used to improve the accuracy of further predictions. The prediction is personalized in that every user's language ability is estimated by a state-of-the-art language testing model, which is trained in a practical response time with only a small sacrifice of prediction accuracy. Evaluation results for the system in terms of prediction accuracy are encouraging.},
 acmid = {1719978},
 address = {New York, NY, USA},
 author = {Ehara, Yo and Shimizu, Nobuyuki and Ninomiya, Takashi and Nakagawa, Hiroshi},
 booktitle = {Proceedings of the 15th International Conference on Intelligent User Interfaces},
 doi = {10.1145/1719970.1719978},
 isbn = {978-1-60558-515-4},
 keyword = {collective intelligence, glossing system, item response theory, reading support, web page},
 link = {http://doi.acm.org/10.1145/1719970.1719978},
 location = {Hong Kong, China},
 numpages = {10},
 pages = {51--60},
 publisher = {ACM},
 series = {IUI '10},
 title = {Personalized Reading Support for Second-language Web Documents by Collective Intelligence},
 year = {2010}
}


@inproceedings{Serrano:2010:BES:1719970.1720034,
 abstract = {An effective approach to transcribe handwritten text documents is to follow an interactive-predictive paradigm in which both, the system is guided by the user, and the user is assisted by the system to complete the transcription task as efficiently as possible. This approach has been recently implemented in a system prototype called GIDOC, in which standard speech technology is adapted to handwritten text (line) images: HMM-based text image modeling, n-gram language modeling, and also confidence measures on recognized words. Confidence measures are used to assist the user in locating possible transcription errors, and thus validate system output after only supervising those (few) words for which the system is not highly confident. However, a certain degree of supervision is required for proper model adaptation from partially supervised transcriptions. Here, we propose a simple yet effective method to find an optimal balance between recognition error and supervision effort.},
 acmid = {1720034},
 address = {New York, NY, USA},
 author = {Serrano, Nicol\'{a}s and Sanchis, Albert and Juan, Alfons},
 booktitle = {Proceedings of the 15th International Conference on Intelligent User Interfaces},
 doi = {10.1145/1719970.1720034},
 isbn = {978-1-60558-515-4},
 keyword = {computer-assisted text transcription, confidence measures, document analysis, handwriting recognition},
 link = {http://doi.acm.org/10.1145/1719970.1720034},
 location = {Hong Kong, China},
 numpages = {4},
 pages = {373--376},
 publisher = {ACM},
 series = {IUI '10},
 title = {Balancing Error and Supervision Effort in Interactive-predictive Handwriting Recognition},
 year = {2010}
}


@inproceedings{Jiang:2010:IUH:1719970.1719988,
 abstract = {Computer-based geometry systems have been widely used for teaching and learning, but largely based on mouse-and-keyboard interaction, these systems usually require users to draw figures by following strict task structures defined by menus, buttons, and mouse and keyboard actions. Pen-based designs offer a more natural way to develop geometry theorem proofs with hand-drawn figures and scripts. This paper describes a pen-based geometry theorem proving system that can effectively recognize hand-drawn figures and hand-written proof scripts, and accurately establish the correspondence between geometric components and proof steps. Our system provides dynamic and intelligent visual assistance to help users understand the process of proving and allows users to manipulate geometric components and proof scripts based on structures rather than strokes. The results from evaluation study show that our system is well perceived and users have high satisfaction with the accuracy of sketch recognition, the effectiveness of visual hints, and the efficiency of structure-based manipulation.},
 acmid = {1719988},
 address = {New York, NY, USA},
 author = {Jiang, Yingying and Tian, Feng and Wang, Hongan and Zhang, Xiaolong and Wang, Xugang and Dai, Guozhong},
 booktitle = {Proceedings of the 15th International Conference on Intelligent User Interfaces},
 doi = {10.1145/1719970.1719988},
 isbn = {978-1-60558-515-4},
 keyword = {geometry theorem proving, hand-drawn figures, hand-written proof scripts, recognition, structure based manipulation},
 link = {http://doi.acm.org/10.1145/1719970.1719988},
 location = {Hong Kong, China},
 numpages = {10},
 pages = {119--128},
 publisher = {ACM},
 series = {IUI '10},
 title = {Intelligent Understanding of Handwritten Geometry Theorem Proving},
 year = {2010}
}


@inproceedings{Nikolova:2010:VNM:1719970.1720031,
 abstract = {It is challenging to search a dictionary consisting of thousands of entries in order to select appropriate words for building written communication. This is true both for people trying to communicate in a foreign language who have not developed a full vocabulary, for school children learning to write, for authors who wish to be more precise and expressive, and especially for people with lexical access disorders. We make vocabulary navigation and word finding easier by augmenting a basic vocabulary with links between words based on human judgments of semantic similarity. In this paper, we report the results from a user study evaluating how our system named ViVA performs compared to a widely used assistive vocabulary in which words are organized hierarchically into common categories.},
 acmid = {1720031},
 address = {New York, NY, USA},
 author = {Nikolova, Sonya and Ma, Xiaojuan and Tremaine, Marilyn and Cook, Perry},
 booktitle = {Proceedings of the 15th International Conference on Intelligent User Interfaces},
 doi = {10.1145/1719970.1720031},
 isbn = {978-1-60558-515-4},
 keyword = {adaptive user interfaces, assistive communication, semantic networks, visual vocabularies},
 link = {http://doi.acm.org/10.1145/1719970.1720031},
 location = {Hong Kong, China},
 numpages = {4},
 pages = {361--364},
 publisher = {ACM},
 series = {IUI '10},
 title = {Vocabulary Navigation Made Easier},
 year = {2010}
}


@inproceedings{Vallet:2010:MFR:1719970.1720038,
 abstract = {In this paper we examine the use of multi faceted recommendations to aid users while carrying out exploratory video retrieval tasks. These recommendations are integrated into ViGOR (Video Grouping, Organisation and Retrieval), a system which employs grouping techniques to facilitate video retrieval tasks. Two types of recommendations based on past usage history are utilised, the first attempts to couple the multi-faceted nature of explorative video retrieval tasks with the current user interests in order to provide global recommendations, while the second exploits the organisational features of ViGOR in order to provide recommendations based on a specific aspect of the user's task.},
 acmid = {1720038},
 address = {New York, NY, USA},
 author = {Vallet, David and Halvey, Martin and Hannah, David and Jose, Joemon M.},
 booktitle = {Proceedings of the 15th International Conference on Intelligent User Interfaces},
 doi = {10.1145/1719970.1720038},
 isbn = {978-1-60558-515-4},
 keyword = {collaborative, exploratory, recommendation, search, video},
 link = {http://doi.acm.org/10.1145/1719970.1720038},
 location = {Hong Kong, China},
 numpages = {4},
 pages = {389--392},
 publisher = {ACM},
 series = {IUI '10},
 title = {A Multi Faceted Recommendation Approach for Explorative Video Retrieval Tasks},
 year = {2010}
}


@inproceedings{Elahi:2010:CIR:1719970.1720045,
 abstract = {This demo paper presents a context-aware recommendation system. The system mines data from user's web searches and other sources to improve the presentation of content on visited web pages. While user is browsing the internet, a memory resident agent records and analyzes the content of the webpages that were either searched for or visited in order to identify topic preferences. Then, based on such information, the content of requested web page is ranked and classified with different styles. The demo shows how a music weblog can be modified automatically based on user's affinities.},
 acmid = {1720045},
 address = {New York, NY, USA},
 author = {Elahi, Mehdi},
 booktitle = {Proceedings of the 15th International Conference on Intelligent User Interfaces},
 doi = {10.1145/1719970.1720045},
 isbn = {978-1-60558-515-4},
 keyword = {active learning, classification, context-aware, fuzzy logic, recommendation systems, recommenders},
 link = {http://doi.acm.org/10.1145/1719970.1720045},
 location = {Hong Kong, China},
 numpages = {2},
 pages = {407--408},
 publisher = {ACM},
 series = {IUI '10},
 title = {Context-aware Intelligent Recommender System},
 year = {2010}
}


@inproceedings{Park:2010:PAP:1719970.1719972,
 abstract = {Most of the previous work on non-invasive brain-computer interfaces (BCIs) has been focused on feature extraction and classification algorithms to achieve high performance for the communication between the brain and the computer. While significant progress has been made in the lower layer of the BCI system, the issues in the higher layer have not been sufficiently addressed. Existing P300-based BCI systems, for example the P300 speller, use a random order of stimulus sequence for eliciting P300 signal for identifying users' intentions. This paper is about computing an optimal sequence of stimulus in order to minimize the number of stimuli, hence improving the performance. To accomplish this, we model the problem as a partially observable Markov decision process (POMDP), which is a model for planning in partially observable stochastic environments. Through simulation and human subject experiments, we show that our approach achieves a significant performance improvement in terms of the success rate and the bit rate.},
 acmid = {1719972},
 address = {New York, NY, USA},
 author = {Park, Jaeyoung and Kim, Kee-Eung and Jo, Sungho},
 booktitle = {Proceedings of the 15th International Conference on Intelligent User Interfaces},
 doi = {10.1145/1719970.1719972},
 isbn = {978-1-60558-515-4},
 keyword = {P300, brain-computer interface (bci), partially observable markov decision process (pomdp)},
 link = {http://doi.acm.org/10.1145/1719970.1719972},
 location = {Hong Kong, China},
 numpages = {10},
 pages = {1--10},
 publisher = {ACM},
 series = {IUI '10},
 title = {A POMDP Approach to P300-based Brain-computer Interfaces},
 year = {2010}
}


@inproceedings{Girgensohn:2010:DFS:1719970.1719997,
 abstract = {Browsing and searching for documents in large, online enterprise document repositories are common activities. While internet search produces satisfying results for most user queries, enterprise search has not been as successful because of differences in document types and user requirements. To support users in finding the information they need in their online enterprise repository, we created DocuBrowse, a faceted document browsing and search system. Search results are presented within the user-created document hierarchy, showing only directories and documents matching selected facets and containing text query terms. In addition to file properties such as date and file size, automatically detected document types, or genres, serve as one of the search facets. Highlighting draws the user's attention to the most promising directories and documents while thumbnail images and automatically identified keyphrases help select appropriate documents. DocuBrowse utilizes document similarities, browsing histories, and recommender system techniques to suggest additional promising documents for the current facet and content filters.},
 acmid = {1719997},
 address = {New York, NY, USA},
 author = {Girgensohn, Andreas and Shipman, Frank and Chen, Francine and Wilcox, Lynn},
 booktitle = {Proceedings of the 15th International Conference on Intelligent User Interfaces},
 doi = {10.1145/1719970.1719997},
 isbn = {978-1-60558-515-4},
 keyword = {document management, document recommendation, document retrieval, document visualization, faceted search},
 link = {http://doi.acm.org/10.1145/1719970.1719997},
 location = {Hong Kong, China},
 numpages = {10},
 pages = {189--198},
 publisher = {ACM},
 series = {IUI '10},
 title = {DocuBrowse: Faceted Searching, Browsing, and Recommendations in an Enterprise Context},
 year = {2010}
}


@inproceedings{Dim:2010:SSP:1719970.1720018,
 abstract = {Social Signal Processing of small groups enables detection of their social context. Monitoring of the social context may be based on position proximity (as a pre-condition for conversation), and on voice communication (an evidence for interaction). Understanding of the social context of a group may allow a system to intervene at the right moment and to suggest relevant services/information. This, in turn, may enhance the group members' experience during leisure activity. This study focuses on assessing the possibility of automatic detection of intra group interaction in a museum environment. It presents analysis and tools that intend to set the foundation for computer aided group interaction during leisure activities.},
 acmid = {1720018},
 address = {New York, NY, USA},
 author = {Dim, Eyal and Kuflik, Tsvi},
 booktitle = {Proceedings of the 15th International Conference on Intelligent User Interfaces},
 doi = {10.1145/1719970.1720018},
 isbn = {978-1-60558-515-4},
 keyword = {group modeling, interrupt management, social signal processing, ubiquitous computing},
 link = {http://doi.acm.org/10.1145/1719970.1720018},
 location = {Hong Kong, China},
 numpages = {4},
 pages = {309--312},
 publisher = {ACM},
 series = {IUI '10},
 title = {Social Signal Processing: Detecting Small Group Interaction in Leisure Activity},
 year = {2010}
}


@inproceedings{Cheema:2010:TIM:1719970.1720013,
 abstract = {We present a new approach for creating dynamic illustrations to assist in the understanding of concepts in physics and mathematics using pen-based interaction. Our approach builds upon mathematical sketching by combining the ability to make associations between handwritten mathematics and free-form drawings with an underlying physics engine. This combination lets users create animations without having to directly specify object behavior with position functions through time, yet still supports writing the mathematics needed to formulate a problem. This functionality significantly expands the capabilities of mathematical sketching to support a wider variety of dynamic illustrations. We describe our approach to creating this mathematical sketching/physics engine fusion and discuss how it provides a foundation for using mathematical sketching in intelligent tutoring systems.},
 acmid = {1720013},
 address = {New York, NY, USA},
 author = {Cheema, Salman and LaViola,Jr., Joseph J.},
 booktitle = {Proceedings of the 15th International Conference on Intelligent User Interfaces},
 doi = {10.1145/1719970.1720013},
 isbn = {978-1-60558-515-4},
 keyword = {mathematical sketching, pen-based interfaces, sketch inferencing, sketch parsing},
 link = {http://doi.acm.org/10.1145/1719970.1720013},
 location = {Hong Kong, China},
 numpages = {4},
 pages = {289--292},
 publisher = {ACM},
 series = {IUI '10},
 title = {Towards Intelligent Motion Inferencing in Mathematical Sketching},
 year = {2010}
}


@inproceedings{Liu:2010:WWB:1719970.1719993,
 abstract = {Nowadays the Web and Web browsers have become the most important and universal platform for people to search, view, process, and exchange various kinds of information. Consequently, today's users usually open many Web pages simultaneously in order to perform multiple tasks in parallel, which makes Web browsers crucial in our daily task management. However, no existing Web browser provides users with sufficient support for the management of many tabs or windows of opened pages. On the other hand, wide displays have become more affordable and prevalent, while extra space on those displays is not utilized effectively in Web browsing. In this paper, we propose a new Web browser interface aiming to support efficient task management in Web browsing on wide displays. In order to help users switch between opened Web pages, we show thumbnails of the pages in the extra space around the currently focused page. In the page thumbnails, we emphasize distinctive elements in each page in order to make the selection of the thumbnails easier. In addition, we calculate the relevance between pages based on users' switching history, and emphasize pages relevant to the current page by adjusting the size or opacity of the thumbnails. This further helps users find the thumbnails of needed pages, and also helps users get the overview of the page set related to the current task.},
 acmid = {1719993},
 address = {New York, NY, USA},
 author = {Liu, Shenwei and Tajima, Keishi},
 booktitle = {Proceedings of the 15th International Conference on Intelligent User Interfaces},
 doi = {10.1145/1719970.1719993},
 isbn = {978-1-60558-515-4},
 keyword = {augmented thumbnail, multitask, site logo, tab-browser, task grouping, task switching, window system, working set},
 link = {http://doi.acm.org/10.1145/1719970.1719993},
 location = {Hong Kong, China},
 numpages = {10},
 pages = {159--168},
 publisher = {ACM},
 series = {IUI '10},
 title = {WildThumb: A Web Browser Supporting Efficient Task Management on Wide Displays},
 year = {2010}
}


@inproceedings{Liu:2010:EMM:1719970.1719992,
 abstract = {Embedded Media Markers, or simply EMMs, are nearly transparent iconic marks printed on paper documents that signify the existence of media associated with that part of the document. EMMs also guide users' camera operations for media retrieval. Users take a picture of an EMM-signified document patch using a cell phone, and the media associated with the EMM-signified document location is displayed on the phone. Unlike bar codes, EMMs are nearly transparent and thus do not interfere with the document appearance. Retrieval of media associated with an EMM is based on image local features of the captured EMM-signified document patch. This paper describes a technique for semi-automatically placing an EMM at a location in a document, in such a way that it encompasses sufficient identification features with minimal disturbance to the original document.},
 acmid = {1719992},
 address = {New York, NY, USA},
 author = {Liu, Qiong and Liao, Chunyuan and Wilcox, Lynn and Dunnigan, Anthony and Liew, Bee},
 booktitle = {Proceedings of the 15th International Conference on Intelligent User Interfaces},
 doi = {10.1145/1719970.1719992},
 isbn = {978-1-60558-515-4},
 keyword = {augmented paper, barcode, camera phone, document recognition, marker on paper, vision-based paper interface},
 link = {http://doi.acm.org/10.1145/1719970.1719992},
 location = {Hong Kong, China},
 numpages = {10},
 pages = {149--158},
 publisher = {ACM},
 series = {IUI '10},
 title = {Embedded Media Markers: Marks on Paper That Signify Associated Media},
 year = {2010}
}


@inproceedings{Dong:2010:TCI:1719970.1720019,
 abstract = {Do people from different cultures tag digital images differently? The current study examined the relationship between the position and content of tags for digital images created by participants from two cultural groups (European Americans and Chinese). In line with previous findings on cultural differences in attentional patterns, we found cultural differences in the order of the parts of images people chose to tag. European Americans tended to tag main objects first, and tag background objects and overall properties in the images later; in contrast, Chinese tended to tag the overall properties first, and tag the main and background objects later. Based on findings of the current study, we discuss implications on developing a cultural-sensitive algorithm to facilitate the tagging and search process of digital media and data-mining tools to identify user profiles based on their cultural origins.},
 acmid = {1720019},
 address = {New York, NY, USA},
 author = {Dong, Wei and Fu, Wai-Tat},
 booktitle = {Proceedings of the 15th International Conference on Intelligent User Interfaces},
 doi = {10.1145/1719970.1720019},
 isbn = {978-1-60558-515-4},
 keyword = {algorithm, annotation, attention, cultural difference, image tagging, perception, tagging},
 link = {http://doi.acm.org/10.1145/1719970.1720019},
 location = {Hong Kong, China},
 numpages = {4},
 pages = {313--316},
 publisher = {ACM},
 series = {IUI '10},
 title = {Toward a Cultural-sensitive Image Tagging Interface},
 year = {2010}
}


@inproceedings{Smith:2010:WUU:1719970.1720035,
 abstract = {People interact with interfaces to accomplish goals, and knowledge about human goals can be useful for building intelligent user interfaces. We suggest that modeling high, human-level goals like "repair my credit score", is especially useful for coordinating workflows between interfaces, automated planning, and building introspective applications. We analyzed data from 43Things.com, a website where users share and discuss goals and plans in natural language, and constructed a goal network that relates what goals people have with how people solve them. We then label goals with specific details, such as where the goal typically is met and how long it takes to achieve, facilitating plan and goal recognition. Lastly, we demonstrate a simple application of goal networks, deploying it in a mobile, location-aware to-do list application, ToDoGo, which uses goal networks to help users plan where and when to accomplish their desired goals.},
 acmid = {1720035},
 address = {New York, NY, USA},
 author = {Smith, Dustin A. and Lieberman, Henry},
 booktitle = {Proceedings of the 15th International Conference on Intelligent User Interfaces},
 doi = {10.1145/1719970.1720035},
 isbn = {978-1-60558-515-4},
 keyword = {goal networks, learning goal networks, plan recognition, to-do list},
 link = {http://doi.acm.org/10.1145/1719970.1720035},
 location = {Hong Kong, China},
 numpages = {4},
 pages = {377--380},
 publisher = {ACM},
 series = {IUI '10},
 title = {The Why UI: Using Goal Networks to Improve User Interfaces},
 year = {2010}
}


@inproceedings{Speer:2010:FYW:1719970.1720037,
 abstract = {In AI, we often need to make sense of data that can be measured in many different dimensions -- thousands of dimensions or more -- especially when this data represents natural language semantics. Dimensionality reduction techniques can make this kind of data more understandable and more powerful, by projecting the data into a space of many fewer dimensions, which are suggested by the computer. Still, frequently, these results require more dimensions than the human mind can grasp at once to represent all the meaningful distinctions in the data. We present Luminoso, a tool that helps researchers to visualize and understand a multi-dimensional semantic space by exploring it interactively. It also streamlines the process of creating such a space, by inputting text documents and optionally including common-sense background information. This interface is based on the fundamental operation of "grabbing" a point, which simultaneously allows a user to rotate their view using that data point, view associated text and statistics, and compare it to other data points. This also highlights the point's neighborhood of semantically-associated points, providing clues for reasons as to why the points were classified along the dimensions they were. We show how this interface can be used to discover trends in a text corpus, such as free-text responses to a survey.},
 acmid = {1720037},
 address = {New York, NY, USA},
 author = {Speer, Robert H. and Havasi, Catherine and Treadway, K. Nichole and Lieberman, Henry},
 booktitle = {Proceedings of the 15th International Conference on Intelligent User Interfaces},
 doi = {10.1145/1719970.1720037},
 isbn = {978-1-60558-515-4},
 keyword = {SVD, common sense, n-dimensional visualization, natural language processing},
 link = {http://doi.acm.org/10.1145/1719970.1720037},
 location = {Hong Kong, China},
 numpages = {4},
 pages = {385--388},
 publisher = {ACM},
 series = {IUI '10},
 title = {Finding Your Way in a Multi-dimensional Semantic Space with Luminoso},
 year = {2010}
}


@inproceedings{Mahmud:2010:LBW:1719970.1719994,
 abstract = {In this paper, we present CoTester, a system designed to decrease the difficulty of testing web applications. CoTester allows testers to create test scripts that are represented in an easy-to-understand scripting language rather than a complex programming language, which allows tests to be created rapidly and by non-developers. CoTester improves the management of test scripts by grouping sequences of lowlevel actions into subroutines, such as "log in" or "check out shopping cart", which help testers visualize test structure and make bulk modifications. A key innovation in CoTester is its ability to automatically identify these subroutines using a machine learning algorithm. Our algorithm is able to achieve 91% accuracy at recognizing a set of 7 representative subroutines commonly found in test scripts.},
 acmid = {1719994},
 address = {New York, NY, USA},
 author = {Mahmud, Jalal and Lau, Tessa},
 booktitle = {Proceedings of the 15th International Conference on Intelligent User Interfaces},
 doi = {10.1145/1719970.1719994},
 isbn = {978-1-60558-515-4},
 keyword = {instruction, subroutine, test script, website testing},
 link = {http://doi.acm.org/10.1145/1719970.1719994},
 location = {Hong Kong, China},
 numpages = {10},
 pages = {169--178},
 publisher = {ACM},
 series = {IUI '10},
 title = {Lowering the Barriers to Website Testing with CoTester},
 year = {2010}
}


@inproceedings{Ketabdar:2010:MIM:1719970.1720048,
 abstract = {In this work, we present a new technique for efficient use of 3D space around a mobile device for interaction with the device. Around Device Interaction (ADI) enables extending interaction space of small mobile and tangible devices beyond their physical boundary. Our proposed method is based on using compass (magnetic field) sensor integrated in mobile devices (e.g. iPhone 3GS, G1 Android). In this method, a properly shaped permanent magnet (e.g. in the shape of a rod, pen or a ring) is used for interaction. The user makes coarse gestures in the 3D space around the device using the magnet. Movement of the magnet affects the magnetic field sensed by the compass sensor integrated in the device. The temporal pattern of the gesture is then used as a basis for sending different interaction commands to the mobile device. Zooming, turning pages, accepting/rejecting calls, clicking items, controlling a music player, and game interaction are some example use cases. The proposed method does not impose changes in hardware specifications of the mobile device, and unlike optical methods is not limited by occlusion problems.},
 acmid = {1720048},
 address = {New York, NY, USA},
 author = {Ketabdar, Hamed and Y\"{u}ksel, Kamer Ali and Roshandel, Mehran},
 booktitle = {Proceedings of the 15th International Conference on Intelligent User Interfaces},
 doi = {10.1145/1719970.1720048},
 isbn = {978-1-60558-515-4},
 keyword = {around device interaction, compass (magnetic) sensor, magnet, mobile devices, movement-based gestures},
 link = {http://doi.acm.org/10.1145/1719970.1720048},
 location = {Hong Kong, China},
 numpages = {2},
 pages = {413--414},
 publisher = {ACM},
 series = {IUI '10},
 title = {MagiTact: Interaction with Mobile Devices Based on Compass (Magnetic) Sensor},
 year = {2010}
}


@inproceedings{Berger:2010:UIF:1719970.1720042,
 abstract = {We present a user interface that combines the requirements needed for information search in a professional environment with the possibilities for multimedial queries based on automatically generated fuzzy high level metadata.},
 acmid = {1720042},
 address = {New York, NY, USA},
 author = {Berger, Arne},
 booktitle = {Proceedings of the 15th International Conference on Intelligent User Interfaces},
 doi = {10.1145/1719970.1720042},
 isbn = {978-1-60558-515-4},
 keyword = {filtering, interactive information retrieval, user interface},
 link = {http://doi.acm.org/10.1145/1719970.1720042},
 location = {Hong Kong, China},
 numpages = {2},
 pages = {401--402},
 publisher = {ACM},
 series = {IUI '10},
 title = {User Interface for Filtering Videos Interconnecting High Level and Intellectual Metadata},
 year = {2010}
}


@inproceedings{Gross:2010:CRI:1719970.1720001,
 abstract = {We describe a code reuse tool for use in the Looking Glass IDE, the successor to Storytelling Alice [17], which enables middle school students with little to no programming experience to reuse functionality they find in programs written by others. Users (1) record a feature to reuse, (2) find code responsible for the feature, (3) abstract the code into a reusable Actionscript by describing object "roles," and (4) integrate the Actionscript into another program. An exploratory study with middle school students indicates they can successfully reuse code. Further, 36 of the 47 users appropriated new programming constructs through the process of reuse.},
 acmid = {1720001},
 address = {New York, NY, USA},
 author = {Gross, Paul A. and Herstand, Micah S. and Hodges, Jordana W. and Kelleher, Caitlin L.},
 booktitle = {Proceedings of the 15th International Conference on Intelligent User Interfaces},
 doi = {10.1145/1719970.1720001},
 isbn = {978-1-60558-515-4},
 keyword = {code reuse, end user, looking glass, middle school, non-programmer, storytelling alice},
 link = {http://doi.acm.org/10.1145/1719970.1720001},
 location = {Hong Kong, China},
 numpages = {10},
 pages = {219--228},
 publisher = {ACM},
 series = {IUI '10},
 title = {A Code Reuse Interface for Non-programmer Middle School Students},
 year = {2010}
}


@inproceedings{Kratz:2010:RSG:1719970.1720051,
 abstract = {We present the $3 Gesture Recognizer, a simple but robust gesture recognition system for input devices featuring 3D acceleration sensors. The algorithm is designed to be implemented quickly in prototyping environments, is intended to be device-independent and does not require any special toolkits or frameworks, but relies solely on simple trigonometric and geometric calculations. Our method requires significantly less training data than other gesture recognizers and is thus suited to be deployed and to deliver results rapidly.},
 acmid = {1720051},
 address = {New York, NY, USA},
 author = {Kratz, Sven and Rohs, Michael},
 booktitle = {Proceedings of the 15th International Conference on Intelligent User Interfaces},
 doi = {10.1145/1719970.1720051},
 isbn = {978-1-60558-515-4},
 keyword = {3D gestures, classifier, gesture recognition, rapid prototyping, recognition rates, user interfaces},
 link = {http://doi.acm.org/10.1145/1719970.1720051},
 location = {Hong Kong, China},
 numpages = {2},
 pages = {419--420},
 publisher = {ACM},
 series = {IUI '10},
 title = {The \$3 Recognizer: Simple 3D Gesture Recognition on Mobile Devices},
 year = {2010}
}


@inproceedings{Brdiczka:2010:TTF:1719970.1720011,
 abstract = {This paper introduces a new representation for describing routine tasks, called temporal task footprints. Routines are characterized by their temporal regularity or rhythm. Temporal pattern analysis (T-patterns) can be used to isolate frequent recurrent patterns in routine tasks that appear repeatedly in the same temporal configuration. Using tf-idf statistics, each task can then be defined in terms of its temporal task footprint, a ranked list of temporal patterns along with their typical frequencies. Experimental evaluations using data of 29 days observing and logging 10 subjects showed that temporal task footprints of application windows, email and document usage outperform decision tree and SVMs in recognizing the subjects' tasks.},
 acmid = {1720011},
 address = {New York, NY, USA},
 author = {Brdiczka, Oliver and Su, Norman Makoto and Begole, James Bo},
 booktitle = {Proceedings of the 15th International Conference on Intelligent User Interfaces},
 doi = {10.1145/1719970.1720011},
 isbn = {978-1-60558-515-4},
 keyword = {routine task representation, t-patterns, task footprint, temporal patterns},
 link = {http://doi.acm.org/10.1145/1719970.1720011},
 location = {Hong Kong, China},
 numpages = {4},
 pages = {281--284},
 publisher = {ACM},
 series = {IUI '10},
 title = {Temporal Task Footprinting: Identifying Routine Tasks by Their Temporal Patterns},
 year = {2010}
}


@inproceedings{Chi:2010:RIS:1719970.1720016,
 abstract = {When editing a story from a large collection of media, such as photos and video clips captured from daily life, it is not always easy to understand how particular scenes fit into the intent for the overall story. Especially for novice editors, there is often a lack of coherent connections between scenes, making it difficult for the viewers to follow the story. In this paper, we present Raconteur, a story editing system that helps users assemble coherent stories from media elements, each annotated with a sentence or two in unrestricted natural language. It uses a Commonsense knowledge base, and the AnalogySpace Commonsense reasoning technique. Raconteur focuses on finding story analogies - different elements illustrating the same overall "point", or independent stories exhibiting similar narrative structures.},
 acmid = {1720016},
 address = {New York, NY, USA},
 author = {Chi, Pei-Yu and Lieberman, Henry},
 booktitle = {Proceedings of the 15th International Conference on Intelligent User Interfaces},
 doi = {10.1145/1719970.1720016},
 isbn = {978-1-60558-515-4},
 keyword = {commonsense computing, media editing, photograph, story analogy, story goal, storytelling, video},
 link = {http://doi.acm.org/10.1145/1719970.1720016},
 location = {Hong Kong, China},
 numpages = {4},
 pages = {301--304},
 publisher = {ACM},
 series = {IUI '10},
 title = {Raconteur: From Intent to Stories},
 year = {2010}
}


@inproceedings{Cao:2010:LDW:1719970.1720004,
 abstract = {Local danger warning is an important function of Advanced Driver Assistance Systems (ADAS) to improve the safety of driving. The user interface (the warning presentation) is particularly crucial to a successful danger avoidance. We present a user study investigating various warning presentations using a scenario of emergent road obstacles. Two presentation factors were selected: modality and level of assistance. The modality factor had 4 variants: speech warning, visual and speech warning, visual warning with blinking cue, and visual warning with sound cue. The level of assistance varied between with or without action suggestions (AS). In accordance with the ISO usability model, a total of 6 measurements were derived to assess the effectiveness and efficiency of the warnings and the drivers' satisfaction. Results indicate that the combination of speech and visual modality leads to the best performance as well as the highest satisfaction. In contrast, purely auditory and purely visual modalities were both insufficient for presenting high-priority warnings. AS generally improved the usability of the warnings especially when they were accompanied by supporting information so that drivers could validate the suggestions.},
 acmid = {1720004},
 address = {New York, NY, USA},
 author = {Cao, Yujia and Mahr, Angela and Castronovo, Sandro and Theune, Mari\"{e}t and Stahl, Christoph and M\"{u}ller, Christian A.},
 booktitle = {Proceedings of the 15th International Conference on Intelligent User Interfaces},
 doi = {10.1145/1719970.1720004},
 isbn = {978-1-60558-515-4},
 keyword = {automotive, car2car communication, multimodal intefaces},
 link = {http://doi.acm.org/10.1145/1719970.1720004},
 location = {Hong Kong, China},
 numpages = {10},
 pages = {239--248},
 publisher = {ACM},
 series = {IUI '10},
 title = {Local Danger Warnings for Drivers: The Effect of Modality and Level of Assistance on Driver Reaction},
 year = {2010}
}


@inproceedings{Kang:2010:EIS:1719970.1720023,
 abstract = {The arising popularity of social tagging system has the potential to transform traditional web search into a new era of social search. Based on the finding that domain expertise could influence search behavior in traditional search engines, we hypothesized and tested the idea that domain expertise would have similar influence on search behavior in a social tagging system. We conducted an experiment comparing search behavior of experts and novices when they searched using a tradition search engine and a social tagging system. Results from our experiment showed that experts relied more on their own domain knowledge to generate search queries, while novices were influenced more by social cues in the social tagging system. Experts were also found to conform to each other more than novices in their choice of bookmarks and tags. Implications on the design of future social information systems are discussed.},
 acmid = {1720023},
 address = {New York, NY, USA},
 author = {Kang, Ruogu and Fu, Wai-Tat},
 booktitle = {Proceedings of the 15th International Conference on Intelligent User Interfaces},
 doi = {10.1145/1719970.1720023},
 isbn = {978-1-60558-515-4},
 keyword = {domain expertise, exploratory search, social search},
 link = {http://doi.acm.org/10.1145/1719970.1720023},
 location = {Hong Kong, China},
 numpages = {4},
 pages = {329--332},
 publisher = {ACM},
 series = {IUI '10},
 title = {Exploratory Information Search by Domain Experts and Novices},
 year = {2010}
}


@inproceedings{Balagtas-Fernandez:2010:MME:1719970.1720008,
 abstract = {The development of mobile applications has now extended from mobile network providers into the hands of ordinary people as organizations and companies encourage people to come up with their own software masterpieces by opening up APIs and tools. However, as of the moment, these APIs and tools are only usable by people with programming skills. There is a scarcity of tools that enable users without programming experience to easily build customized mobile applications. We present in this paper a tool and its underlying framework that would enable non-technical people to create their own domain-specific mobile applications. As a proof of concept, we focus on the creation of applications in the domain of mobile health monitoring. In the future, we would like to extend our work to cover other domains as well.},
 acmid = {1720008},
 address = {New York, NY, USA},
 author = {Balagtas-Fernandez, Florence and Tafelmayer, Max and Hussmann, Heinrich},
 booktitle = {Proceedings of the 15th International Conference on Intelligent User Interfaces},
 doi = {10.1145/1719970.1720008},
 isbn = {978-1-60558-515-4},
 keyword = {domain-specific modeling, mobile application, modeling tools, user-centered design},
 link = {http://doi.acm.org/10.1145/1719970.1720008},
 location = {Hong Kong, China},
 numpages = {4},
 pages = {269--272},
 publisher = {ACM},
 series = {IUI '10},
 title = {Mobia Modeler: Easing the Creation Process of Mobile Applications for Non-technical Users},
 year = {2010}
}


@proceedings{Rich:2010:1719970,
 abstract = {IUI has successfully established itself as a unique, interdisciplinary conference, at the intersection of Artificial Intelligence and Human-Computer Interaction. The Conference receives contributions from many traditional as well as hot topics in the field, ranging from Multimodal Interfaces and Recommender Systems, to Affective Computing and Brain-Computing Interfaces. Because of this ever-increasing diversity of topics, this edition has seen further changes to our reviewing process. We have introduced a rebuttal phase for Long Papers, following a trend adopted by several high-quality conferences. The objective of the rebuttal is to ensure greater transparency and fairness, as the authors' responses should influence the discussion phase moderated by a Senior Program Committee (SPC) member. In order to ensure that all topics were adequately covered over 200 reviewers have contributed to the selection of this year's program and we trust that this had a very positive impact on the relevance and quality of individual reviews. We have retained the successful format of the conference with Long Papers, Short Papers and Demonstrations. In addition, we have addressed the issue of conversion of Long Papers into Short Papers by explicitly requesting authors' approval at submission time. The accepted submissions cover a wide range of topics, including personalized information systems, intelligent user interaction for information search and browsing, affective computing, gesture-based systems, and multimodal user interfaces. Geographically, the accepted work also represents researchers and institutions in many countries across four continents, including China, Japan, Korea, Singapore, Canada, Netherlands, Ireland, France, Germany, Australia, New Zealand, United Kingdom, and the United States of America. As always, the selection process has been the object of careful consideration and multiple discussions. We have asked the SPC moderators to formulate recommendations for acceptance and in the vast majority of cases it has been straightforward for us to endorse their choice. It is always down to the Program Chairs to make final decisions, sometimes difficult ones, on the total number of papers to be accepted. We have adopted a continuity policy from previous editions (around 22% for Long Papers), this year's overall acceptance rate achieves the right balance between selectivity and openness to innovative papers. The conference program highlights two invited talks: Paul Sajda, from Columbia University and Kazuo Yano, from Hitachi's Advanced Research Laboratory. This year's Conference will also feature five full-day and one half-day workshops, covering several hot topics in the area of IUI, with strong emphasis on semantics, social aspects and multimodality of interfaces, including:  Social Recommender Systems  Intelligent Visual Interfaces for Text Analysis  Multimodal Interfaces for Automotive Applications  Interoperability and Interaction on the Social and Semantic Web  Eye Gaze in Intelligent Human Machine Interaction  Semantic Models for Adaptive Interactive Systems The demonstration program accepted fifteen regular submissions. We would like to thank Tyler Baldwin and Brian Romanowski for their help in organizing the demo session.},
 address = {New York, NY, USA},
 isbn = {978-1-60558-515-4},
 location = {Hong Kong, China},
 note = {608010},
 publisher = {ACM},
 title = {IUI '10: Proceedings of the 15th International Conference on Intelligent User Interfaces},
 year = {2010}
}


@inproceedings{Gunawardana:2010:UGK:1719970.1719986,
 abstract = {Soft keyboards offer touch-capable mobile and tabletop devices many advantages such as multiple language support and room for larger displays. On the other hand, because soft keyboards lack haptic feedback, users often produce more typing errors. In order to make soft keyboards more robust to noisy input, researchers have developed key-target resizing algorithms, where underlying target areas for keys are dynamically resized based on their probabilities. In this paper, we describe how overly aggressive key-target resizing can sometimes prevent users from typing their desired text, violating basic user expectations about keyboard functionality. We propose an anchored key-target method which incorporates usability principles so that soft keyboards can remain robust to errors while respecting usability principles. In an empirical evaluation, we found that using anchored dynamic key-targets significantly reduce keystroke errors as compared to the state-of-the-art.},
 acmid = {1719986},
 address = {New York, NY, USA},
 author = {Gunawardana, Asela and Paek, Tim and Meek, Christopher},
 booktitle = {Proceedings of the 15th International Conference on Intelligent User Interfaces},
 doi = {10.1145/1719970.1719986},
 isbn = {978-1-60558-515-4},
 keyword = {language model, source-channel key-target resizing, touch model},
 link = {http://doi.acm.org/10.1145/1719970.1719986},
 location = {Hong Kong, China},
 numpages = {8},
 pages = {111--118},
 publisher = {ACM},
 series = {IUI '10},
 title = {Usability Guided Key-target Resizing for Soft Keyboards},
 year = {2010}
}


@inproceedings{Freyne:2010:MMW:1719970.1720046,
 abstract = {In recent years health care professionals have been investigating the use of ICT technologies in order to influence the general public to change their attitude and behaviour toward a healthier lifestyle. We present Mobile Mentor, a platform aimed at supporting individuals on goal driven programs through personalized mobile technology. This demonstration focuses on a weight loss prototype, Weight Management Mentor, which supports self regulation through the collection of real time diet and exercise data, self reflection and awareness through its graphical feedback mechanisms, and interaction with a health practitioner or advisor through a central server.},
 acmid = {1720046},
 address = {New York, NY, USA},
 author = {Freyne, Jill and Bhandari, Dipak and Berkovsky, Shlomo and Borlyse, Lyle and Campbell, Chris and Chau, Steve},
 booktitle = {Proceedings of the 15th International Conference on Intelligent User Interfaces},
 doi = {10.1145/1719970.1720046},
 isbn = {978-1-60558-515-4},
 keyword = {diet, health, mobile},
 link = {http://doi.acm.org/10.1145/1719970.1720046},
 location = {Hong Kong, China},
 numpages = {2},
 pages = {409--410},
 publisher = {ACM},
 series = {IUI '10},
 title = {Mobile Mentor: Weight Management Platform},
 year = {2010}
}


@inproceedings{Rosenthal:2010:TMA:1719970.1720006,
 abstract = {We present two studies that evaluate the accuracy of human responses to an intelligent agent's data classification questions. Prior work has shown that agents can elicit accurate human responses, but the applications vary widely in the data features and prediction information they provide to the labelers when asking for help. In an initial analysis of this work, we found the five most popular features, namely uncertainty, amount and level of context, prediction of an answer, and request for user feedback. We propose that there is a set of these data features and prediction information that maximizes the accuracy of labeler responses. In our first study, we compare accuracy of users of an activity recognizer labeling their own data across the dimensions. In the second study, participants were asked to classify a stranger's emails into folders and strangers' work activities by interruptibility. We compared the accuracy of the responses to the users' self-reports across the same five dimensions. We found very similar combinations of information (for users and strangers) that led to very accurate responses as well as more feedback that the agents could use to refine their predictions. We use these results for insight into the information that help labelers the most.},
 acmid = {1720006},
 address = {New York, NY, USA},
 author = {Rosenthal, Stephanie L. and Dey, Anind K.},
 booktitle = {Proceedings of the 15th International Conference on Intelligent User Interfaces},
 doi = {10.1145/1719970.1720006},
 isbn = {978-1-60558-515-4},
 keyword = {active learning, labeling sensor data},
 link = {http://doi.acm.org/10.1145/1719970.1720006},
 location = {Hong Kong, China},
 numpages = {10},
 pages = {259--268},
 publisher = {ACM},
 series = {IUI '10},
 title = {Towards Maximizing the Accuracy of Human-labeled Sensor Data},
 year = {2010}
}


@inproceedings{Berkovsky:2010:AIP:1719970.1720009,
 abstract = {Contemporary lifestyle is becoming increasingly sedentary with no or little physical activity. We propose a novel design for physical activity motivating games that leverages engagement with games in order to motivate users to perform physical activity as part of traditionally sedentary playing. This paper focuses on the wearable activity interface for physical activity motivating games. We discuss the activity interface design considerations, present physical activity processing details, and analyse some observations of user interaction with the activity interface.},
 acmid = {1720009},
 address = {New York, NY, USA},
 author = {Berkovsky, Shlomo and Coombe, Mac and Helmer, Richard},
 booktitle = {Proceedings of the 15th International Conference on Intelligent User Interfaces},
 doi = {10.1145/1719970.1720009},
 isbn = {978-1-60558-515-4},
 keyword = {physical activity, serious games, wearable interface},
 link = {http://doi.acm.org/10.1145/1719970.1720009},
 location = {Hong Kong, China},
 numpages = {4},
 pages = {273--276},
 publisher = {ACM},
 series = {IUI '10},
 title = {Activity Interface for Physical Activity Motivating Games},
 year = {2010}
}


@inproceedings{vanSchooten:2010:EAW:1719970.1720039,
 abstract = {Visual search is a task that is performed in various application areas. Search can be aided by an automatic warning system, which highlights the sections that may contain targets and require the user's attention. The effect of imperfect automatic warnings on overall performance ultimately depends on the interplay between the user and the automatic warning system. While various user studies exist, the different studies differ in several experimental variables including the nature of the visualisation itself. Studies in the medical area remain relatively rare, even though there is a growing interest in medical screening systems. We describe an experiment where users had to perform a visual search on a vascular structure, traversing a particular vessel linearly in search of possible errors made in an automatic segmentation. We find that only the case in which the warning system generates only false positives improves user time and error performance. We discuss this finding in relation to the findings of other studies.},
 acmid = {1720039},
 address = {New York, NY, USA},
 author = {van Schooten, Boris W. and van Dijk, Betsy M.A.G. and Nijholt, Anton and Reiber, Johan H.C.},
 booktitle = {Proceedings of the 15th International Conference on Intelligent User Interfaces},
 doi = {10.1145/1719970.1720039},
 isbn = {978-1-60558-515-4},
 keyword = {automatic warning system, image segmentation, magnetic resonance angiography, visual search},
 link = {http://doi.acm.org/10.1145/1719970.1720039},
 location = {Hong Kong, China},
 numpages = {4},
 pages = {393--396},
 publisher = {ACM},
 series = {IUI '10},
 title = {Evaluating Automatic Warning Cues for Visual Search in Vascular Images},
 year = {2010}
}


@inproceedings{Ruiz:2010:SPT:1719970.1720002,
 abstract = {Target expansion is a pointing facilitation technique where the user's target, typically an interface widget, is dynamically enlarged to speed pointing in interfaces. However, with densely packed (tiled) arrangements of widgets, interfaces cannot expand all potential targets; they must, instead, predict the user's desired target. As a result, mispredictions will occur which may disrupt the pointing task. In this paper, we present a model describing the cost/benefit of expanding multiple targets using the probability distribution of a given predictor. Using our model, we demonstrate how the model can be used to infer the accuracy required by target prediction techniques. The results of this work are another step toward pointing facilitation techniques that allow users to outperform Fitts' Law in realistic pointing tasks.},
 acmid = {1720002},
 address = {New York, NY, USA},
 author = {Ruiz, Jaime and Lank, Edward},
 booktitle = {Proceedings of the 15th International Conference on Intelligent User Interfaces},
 doi = {10.1145/1719970.1720002},
 isbn = {978-1-60558-515-4},
 keyword = {Fitts' law, human performance, pointing, target expansion, tiled targets},
 link = {http://doi.acm.org/10.1145/1719970.1720002},
 location = {Hong Kong, China},
 numpages = {10},
 pages = {229--238},
 publisher = {ACM},
 series = {IUI '10},
 title = {Speeding Pointing in Tiled Widgets: Understanding the Effects of Target Expansion and Misprediction},
 year = {2010}
}


@inproceedings{Lohmann:2010:RUI:1719970.1720052,
 abstract = {Being aware of the relationships that exist between objects of interest is crucial in many situations. The RelFinder user interface helps to get an overview: Even large amounts of relationships can be visualized, filtered, and analyzed by the user. Common concepts of knowledge representation are exploited in order to support interactive exploration both on the level of global filters and single relationships. The RelFinder is easy-to-use and works on every RDF knowledge base that provides standardized SPARQL access},
 acmid = {1720052},
 address = {New York, NY, USA},
 author = {Lohmann, Steffen and Heim, Philipp and Stegemann, Timo and Ziegler, J\"{u}rgen},
 booktitle = {Proceedings of the 15th International Conference on Intelligent User Interfaces},
 doi = {10.1145/1719970.1720052},
 isbn = {978-1-60558-515-4},
 keyword = {dbpedia, decision support, graph visualization, linked data, relationship discovery, relationship web, semantic user interfaces, semantic web, sparql, visual exploration},
 link = {http://doi.acm.org/10.1145/1719970.1720052},
 location = {Hong Kong, China},
 numpages = {2},
 pages = {421--422},
 publisher = {ACM},
 series = {IUI '10},
 title = {The RelFinder User Interface: Interactive Exploration of Relationships Between Objects of Interest},
 year = {2010}
}


@inproceedings{Amin:2010:DTC:1719970.1720005,
 abstract = {Comparison search is an information seeking task where a user examines individual items or sets of items for similarities and differences. While this is a known information need among experts and knowledge workers, appropriate tools are not available. In this paper, we discuss comparison search in the cultural heritage domain, a domain characterized by large, rich and heterogeneous data sets, where different organizations deploy different schemata and terminologies to describe their artifacts. This diversity makes meaningful comparison difficult. We developed a thesaurus-based comparison search application called LISA, a tool that allows a user to search, select and compare sets of artifacts. Different visualizations allow users to use different comparison strategies to cope with the underlying heterogeneous data and the complexity of the search tasks. We conducted two user studies. A preliminary study identifies the problems experts face while performing comparison search tasks. A second user study examines the effectiveness of LISA in helping to solve comparison search tasks. The main contribution of this paper is to establish design guidelines for the data and interface of a comparison search application. Moreover, we offer insights into when thesauri and metadata are appropriate for use in such applications.},
 acmid = {1720005},
 address = {New York, NY, USA},
 author = {Amin, Alia and Hildebrand, Michiel and van Ossenbruggen, Jacco and Hardman, Lynda},
 booktitle = {Proceedings of the 15th International Conference on Intelligent User Interfaces},
 doi = {10.1145/1719970.1720005},
 isbn = {978-1-60558-515-4},
 keyword = {comparison search, cultural heritage, thesauri},
 link = {http://doi.acm.org/10.1145/1719970.1720005},
 location = {Hong Kong, China},
 numpages = {10},
 pages = {249--258},
 publisher = {ACM},
 series = {IUI '10},
 title = {Designing a Thesaurus-based Comparison Search Interface for Linked Cultural Heritage Sources},
 year = {2010}
}


@inproceedings{Guy:2010:WSR:1719970.1720059,
 abstract = {This workshop brought researchers from academia and industry together to share recent advances and discuss research directions for recommender systems in social media and Web 2.0. With social media sites becoming ubiquitous, the challenges and opportunities for recommendation technologies become greater, setting the grounds for new research and innovation.},
 acmid = {1720059},
 address = {New York, NY, USA},
 author = {Guy, Ido and Chen, Li and Zhou, Michelle X.},
 booktitle = {Proceedings of the 15th International Conference on Intelligent User Interfaces},
 doi = {10.1145/1719970.1720059},
 isbn = {978-1-60558-515-4},
 keyword = {recommender systems, social media, social web, web 2.0},
 link = {http://doi.acm.org/10.1145/1719970.1720059},
 location = {Hong Kong, China},
 numpages = {2},
 pages = {433--434},
 publisher = {ACM},
 series = {IUI '10},
 title = {Workshop on Social Recommender Systems},
 year = {2010}
}


@inproceedings{Creswick:2010:EVS:1719970.1720017,
 abstract = {Application customization has been extensively researched in the field of Programming by Demonstration (PBD), and Version Space Algebra has proven itself to be a viable means of quickly learning precise action sequences from user demonstrations. However, this technique is not capable of handling user error in domains with actions that depend on parameters that accept myriad values. Activities such as image, audio and video editing require user actions that are difficult for users to precisely replicate in different circumstances. Demonstrations that are off by a single pixel or a split-second cause traditional composite Version Spaces to collapse. We present a method of incorporating error tolerance into Version Space algebra. This approach, termed Error-Tolerant Version Spaces, adapts Version Space Algebra to domains where the tactile capabilities of the user have a much greater chance of prematurely collapsing the hypothesis space that is being learned. The resulting framework is capable of quickly learning in domains where perfectly consistent user input can not be expected. We have successfully applied our technique in the domain of image redaction, allowing our users to quickly specify redactions that can be reliably applied to many images without the entry of explicit parameters.},
 acmid = {1720017},
 address = {New York, NY, USA},
 author = {Creswick, Eugene R. and Novstrup, Aaron M.},
 booktitle = {Proceedings of the 15th International Conference on Intelligent User Interfaces},
 doi = {10.1145/1719970.1720017},
 isbn = {978-1-60558-515-4},
 keyword = {error tolerance, programming by demonstration, smart environments, version spaces},
 link = {http://doi.acm.org/10.1145/1719970.1720017},
 location = {Hong Kong, China},
 numpages = {4},
 pages = {305--308},
 publisher = {ACM},
 series = {IUI '10},
 title = {Error-tolerant Version Space Algebra},
 year = {2010}
}


@inproceedings{Handschuh:2010:VIS:1719970.1720060,
 abstract = {Recent innovations in the Social and Semantic Web fields have resulted in large amounts of data created, published and consumed by users of the Web. This vast amount of data exists in a variety of formats, from the traditional ones such as text, image, video to the more recent additions such as streams of status information from Twitter and Facebook. The ability to easily integrate such vast amounts of data raises significant and exciting research challenges, not least of which how to provide effective access to and navigation across heterogeneous data sources on different platforms (e.g. computers, mobile devices, set-top boxes). Building on the success of the VISSW2009 workshop, the IUI2010 workshop on Visual Interfaces to the Social and Semantic Web aims to bring together researchers and practitioners from different fields to discuss the latest research results and challenges in designing, implementing, and evaluating intelligent interfaces supporting access, navigation and publishing of different types of contents on the Social and Semantic Web. This paper outlines the context of the workshop and provides an overview of the research to be presented at the event.},
 acmid = {1720060},
 address = {New York, NY, USA},
 author = {Handschuh, Siegfried and Heath, Tom and Thai, VinhTuan and Dickinson, Ian and Aroyo, Lora and Presutti, Valentina},
 booktitle = {Proceedings of the 15th International Conference on Intelligent User Interfaces},
 doi = {10.1145/1719970.1720060},
 isbn = {978-1-60558-515-4},
 keyword = {social and semantic web, visual interfaces},
 link = {http://doi.acm.org/10.1145/1719970.1720060},
 location = {Hong Kong, China},
 numpages = {2},
 pages = {435--436},
 publisher = {ACM},
 series = {IUI '10},
 title = {Visual Interfaces to the Social and Semantic Web (VISSW 2010)},
 year = {2010}
}


@inproceedings{Feld:2010:MIA:1719970.1720063,
 abstract = {This paper summarizes the main objectives of the 2nd IUI workshop on multimodal interfaces for automotive applications (MIAA 2010).},
 acmid = {1720063},
 address = {New York, NY, USA},
 author = {Feld, Michael and M\"{u}ller, Christian A. and Schwartz, Tim},
 booktitle = {Proceedings of the 15th International Conference on Intelligent User Interfaces},
 doi = {10.1145/1719970.1720063},
 isbn = {978-1-60558-515-4},
 keyword = {automotive applications, human-machine-interaction, multimodal interfaces},
 link = {http://doi.acm.org/10.1145/1719970.1720063},
 location = {Hong Kong, China},
 numpages = {2},
 pages = {441--442},
 publisher = {ACM},
 series = {IUI '10},
 title = {2Nd Multimodal Interfaces for Automotive Applications (MIAA 2010)},
 year = {2010}
}


@inproceedings{Krzywicki:2010:ACA:1719970.1719981,
 abstract = {In this paper, we present SmartCal, a calendar assistant that suggests appointment attributes, such as time, day, duration, etc., given any combination of initial user input attributes. SmartCal uses closed pattern mining to discover patterns in past appointment data in order to represent user preferences and adapt to changing user preferences over time. The SmartCal interface is designed to be minimally intrusive: users are free to choose or ignore suggestions, which are dynamically updated as users enter new information. The user model as a collection of patterns is intuitive and transparent: users can view and edit existing patterns or create new patterns based on existing appointments. SmartCal was evaluated in a user study with four users over a four week period. The user study shows that pattern mining makes appointment creation more efficient and users regarded the appointment suggestion feature favourably.},
 acmid = {1719981},
 address = {New York, NY, USA},
 author = {Krzywicki, Alfred and Wobcke, Wayne and Wong, Anna},
 booktitle = {Proceedings of the 15th International Conference on Intelligent User Interfaces},
 doi = {10.1145/1719970.1719981},
 isbn = {978-1-60558-515-4},
 keyword = {calendar management, data mining, personal assistants},
 link = {http://doi.acm.org/10.1145/1719970.1719981},
 location = {Hong Kong, China},
 numpages = {10},
 pages = {71--80},
 publisher = {ACM},
 series = {IUI '10},
 title = {An Adaptive Calendar Assistant Using Pattern Mining for User Preference Modelling},
 year = {2010}
}


@inproceedings{Nakano:2010:EUE:1719970.1719990,
 abstract = {In face-to-face conversations, speakers are continuously checking whether the listener is engaged in the conversation and change the conversational strategy if the listener is not fully engaged in the conversation. With the goal of building a conversational agent that can adaptively control conversations with the user, this study analyzes the user's gaze behaviors and proposes a method for estimating whether the user is engaged in the conversation based on gaze transition 3-gram patterns. First, we conduct a Wizard-of-Oz experiment to collect the user's gaze behaviors. Based on the analysis of the gaze data, we propose an engagement estimation method that detects the user's disengagement gaze patterns. The algorithm is implemented as a real-time engagement-judgment mechanism and is incorporated into a multimodal dialogue manager in a conversational agent. The agent estimates the user's conversational engagement and generates probing questions when the user is distracted from the conversation. Finally, we conduct an evaluation experiment using the proposed engagement-sensitive agent and demonstrate that the engagement estimation function improves the user's impression of the agent and the interaction with the agent. In addition, probing performed with proper timing was also found to have a positive effect on user's verbal/nonverbal behaviors in communication with the conversational agent.},
 acmid = {1719990},
 address = {New York, NY, USA},
 author = {Nakano, Yukiko I. and Ishii, Ryo},
 booktitle = {Proceedings of the 15th International Conference on Intelligent User Interfaces},
 doi = {10.1145/1719970.1719990},
 isbn = {978-1-60558-515-4},
 keyword = {conversational agent, conversational engagement, dialogue management, eye-gaze},
 link = {http://doi.acm.org/10.1145/1719970.1719990},
 location = {Hong Kong, China},
 numpages = {10},
 pages = {139--148},
 publisher = {ACM},
 series = {IUI '10},
 title = {Estimating User's Engagement from Eye-gaze Behaviors in Human-agent Conversations},
 year = {2010}
}


@inproceedings{Pang:2010:ITP:1719970.1720032,
 abstract = {Color and texture are basic elements in digital graphics. Selection of color with a picker is convenient in many of the image editing softwares. However, more organized and intelligent GUI for texture pattern selection is still missing. In this paper, we attempt to fill this gap with the introduction of several robust techniques in building an intuitive texture picking GUI. By arranging patterns according to their visual similarities, texture picker with plane and circular layout are presented. Additional functionality include content-based texture searching which can quickly find similar patterns of given sample. Preliminary response to the proposed interface is positive in general, while further improvements are required, for example, on building a hierarchy to facilitate high to low level selection for huge amount of texture patterns.},
 acmid = {1720032},
 address = {New York, NY, USA},
 author = {Pang, Wai-Man},
 booktitle = {Proceedings of the 15th International Conference on Intelligent User Interfaces},
 doi = {10.1145/1719970.1720032},
 isbn = {978-1-60558-515-4},
 keyword = {multidimensional scaling, texture pattern picker, texture selection GUI, texture similarity},
 link = {http://doi.acm.org/10.1145/1719970.1720032},
 location = {Hong Kong, China},
 numpages = {4},
 pages = {365--368},
 publisher = {ACM},
 series = {IUI '10},
 title = {An Intuitive Texture Picker},
 year = {2010}
}


@inproceedings{Church:2010:SNM:1719970.1719985,
 abstract = {The mobile Internet offers anytime, anywhere access to a wealth of information to billions of users across the globe. However, the mobile Internet represents a challenging information access platform due to the inherent limitations of mobile environments, limitations that go beyond simple screen size and network issues. Mobile users often have information needs which are impacted by contexts such as location and time. Furthermore, human beings are social creatures that often seek out new strategies for sharing knowledge and information in mobile settings. To investigate the social aspect of mobile search, we have developed SocialSearchBrowser (SSB), a novel proof-of-concept interface that incorporates social networking capabilities with key mobile contexts to improve the search and information discovery experience of mobile users. In this paper, we present the results of an exploratory field study of SSB and outline key implications for the design of next generation mobile information access services.},
 acmid = {1719985},
 address = {New York, NY, USA},
 author = {Church, Karen and Neumann, Joachim and Cherubini, Mauro and Oliver, Nuria},
 booktitle = {Proceedings of the 15th International Conference on Intelligent User Interfaces},
 doi = {10.1145/1719970.1719985},
 isbn = {978-1-60558-515-4},
 keyword = {context, field study, location-based services, mobile search, social networks, social search, user evaluation},
 link = {http://doi.acm.org/10.1145/1719970.1719985},
 location = {Hong Kong, China},
 numpages = {10},
 pages = {101--110},
 publisher = {ACM},
 series = {IUI '10},
 title = {SocialSearchBrowser: A Novel Mobile Search and Information Discovery Tool},
 year = {2010}
}


@inproceedings{Smeddinck:2010:QMW:1719970.1720055,
 abstract = {Herein we describe the QuickWoZ system, a Wizard-of-Oz (WoZ) tool that allows for the remote control of the behavior of animated characters in a 3D environment. The complete scene, character, behaviors and sounds can be defined in simple XML documents, which are parsed at runtime, so that setting up an experiment can be done without programming expertise. Quick selection lists and buttons enable the wizard to easily control the agents' behavior and allow for fast reactions to the subjects' input. The system is tailored for experiments with embodied conversational agents (ECAs) featuring multimodal interaction and was designed as a rapid prototyping system for evaluating the impact of an agent's behavior on the user.},
 acmid = {1720055},
 address = {New York, NY, USA},
 author = {Smeddinck, Jan and Wajda, Kamila and Naveed, Adeel and Touma, Leen and Chen, Yuting and Hasan, Muhammad Abu and Latif, Muhammad Waqas and Porzel, Robert},
 booktitle = {Proceedings of the 15th International Conference on Intelligent User Interfaces},
 doi = {10.1145/1719970.1720055},
 isbn = {978-1-60558-515-4},
 keyword = {HCI, conversational agents, embodiment, evaluation},
 link = {http://doi.acm.org/10.1145/1719970.1720055},
 location = {Hong Kong, China},
 numpages = {2},
 pages = {427--428},
 publisher = {ACM},
 series = {IUI '10},
 title = {QuickWoZ: A Multi-purpose Wizard-of-oz Framework for Experiments with Embodied Conversational Agents},
 year = {2010}
}


@inproceedings{Ketabdar:2010:SRC:1719970.1720049,
 abstract = {In this work, we present a method for controlling call alert functionality in mobile phones. It has happened for almost everybody experiencing a situation that call alert functionality is not proper for actual ambient context, leading to missing a phone call or disturbing others by a loud ring. In this work, we use audio and physical movement analysis to distinguish between different situations in which a mobile phone may ring, and adjust the call alert functionality accordingly. Considering the fact that mobile phones are usually carried in a pocket or bag, capturing ambient audio is not usually practically perfect. The novelty in our work is using information about physical movements of user of mobile device in addition to analysis of ambient audio. Analysis of user movements is based on information captured by acceleration sensors integrated in mobile phone. The call alert functionality is then adjusted based on a combination of ambient audio level and physical activities of user.},
 acmid = {1720049},
 address = {New York, NY, USA},
 author = {Ketabdar, Hamed and Y\"{u}ksel, Kamer Ali},
 booktitle = {Proceedings of the 15th International Conference on Intelligent User Interfaces},
 doi = {10.1145/1719970.1720049},
 isbn = {978-1-60558-515-4},
 keyword = {acceleration sensors, ambient audio, ambient context, call alert functionality, physical movements},
 link = {http://doi.acm.org/10.1145/1719970.1720049},
 location = {Hong Kong, China},
 numpages = {2},
 pages = {415--416},
 publisher = {ACM},
 series = {IUI '10},
 title = {Smart Ring: Controlling Call Alert Functionality Based on Audio and Movement Analysis},
 year = {2010}
}


@inproceedings{Freyne:2010:IFP:1719970.1720021,
 abstract = {As the obesity epidemic takes hold across the world many medical professionals are referring users to online systems aimed at educating and persuading users to alter their lifestyle. The challenge for many of these systems is to increase initial adoption and sustain participation for sufficient time to have real impact on the life of its users. In this work we present some preliminary investigation into the design of a recipe recommender, aimed at educating and sustaining user participation, which makes tailored recommendations of healthy recipes. We concentrate on the two initial dimensions of food recommendations: data capture and food-recipe relationships and present a study into the suitability of varying recommender algorithms for the recommendation of recipes.},
 acmid = {1720021},
 address = {New York, NY, USA},
 author = {Freyne, Jill and Berkovsky, Shlomo},
 booktitle = {Proceedings of the 15th International Conference on Intelligent User Interfaces},
 doi = {10.1145/1719970.1720021},
 isbn = {978-1-60558-515-4},
 keyword = {collaborative filtering, food, personalization, recipe, recommender systems},
 link = {http://doi.acm.org/10.1145/1719970.1720021},
 location = {Hong Kong, China},
 numpages = {4},
 pages = {321--324},
 publisher = {ACM},
 series = {IUI '10},
 title = {Intelligent Food Planning: Personalized Recipe Recommendation},
 year = {2010}
}


@inproceedings{Fu:2010:FES:1719970.1719998,
 abstract = {We present an extension of a computational cognitive model of social tagging and exploratory search called the semantic imitation model. The model assumes a probabilistic representation of semantics for both internal and external knowledge, and utilizes social tags as navigational cues during exploratory search. We used the model to generate a measure of information scent that controls exploratory search behavior, and simulated the effects of multiple presentations of navigational cues on both simple information retrieval and exploratory search performance based on a previous model called SNIF-ACT. We found that search performance can be significantly improved by these model-based presentations of navigational cues for both experts and novices. The result suggested that exploratory search performance depends critically on the match between internal knowledge (domain expertise) and external knowledge structures (folksonomies). Results have significant implications on how social information systems should be designed to facilitate knowledge exchange among users with different background knowledge.},
 acmid = {1719998},
 address = {New York, NY, USA},
 author = {Fu, Wai-Tat and Kannampallil, Thomas G. and Kang, Ruogu},
 booktitle = {Proceedings of the 15th International Conference on Intelligent User Interfaces},
 doi = {10.1145/1719970.1719998},
 isbn = {978-1-60558-515-4},
 keyword = {SNIF-ACT, exploratory learning, knowledge exchange, semantic imitation, social tagging},
 link = {http://doi.acm.org/10.1145/1719970.1719998},
 location = {Hong Kong, China},
 numpages = {10},
 pages = {199--208},
 publisher = {ACM},
 series = {IUI '10},
 title = {Facilitating Exploratory Search by Model-based Navigational Cues},
 year = {2010}
}


@inproceedings{Iacobelli:2010:TMM:1719970.1719982,
 abstract = {The Web makes it possible for news readers to learn more about virtually any story that interests them. Media outlets and search engines typically augment their information with links to similar stories. It is up to the user to determine what new information is added by them, if any. In this paper we present Tell Me More, a system that performs this task automatically: given a seed news story, it mines the web for similar stories reported by different sources and selects snippets of text from those stories which offer new information beyond the seed story. New content may be classified as supplying: additional quotes, additional actors, additional figures and additional information depending on the criteria used to select it. In this paper we describe how the system identifies new and informative content with respect to a news story. We also how that providing an explicit categorization of new information is more useful than a binary classification (new/not-new). Lastly, we show encouraging results from a preliminary evaluation of the system that validates our approach and encourages further study.},
 acmid = {1719982},
 address = {New York, NY, USA},
 author = {Iacobelli, Francisco and Birnbaum, Larry and Hammond, Kristian J.},
 booktitle = {Proceedings of the 15th International Conference on Intelligent User Interfaces},
 doi = {10.1145/1719970.1719982},
 isbn = {978-1-60558-515-4},
 keyword = {dimensions of similarity, information retrieval, new information detection},
 link = {http://doi.acm.org/10.1145/1719970.1719982},
 location = {Hong Kong, China},
 numpages = {10},
 pages = {81--90},
 publisher = {ACM},
 series = {IUI '10},
 title = {Tell Me More, Not Just "More of the Same"},
 year = {2010}
}


