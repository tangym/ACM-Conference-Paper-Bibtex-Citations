@proceedings{Kuflik:2014:2559184,
 abstract = {It is our great pleasure to welcome you to the 2014 International Conference on Intelligent User Interfaces (IUI'14). It is the nineteenth IUI conference, continuing its tradition of being the principal international forum for reporting outstanding research at the intersection of Human-Computer Interaction (HCI) and Artificial Intelligence (AI). The work that appears at IUI bridges these two fields and also delves into related fields, such as psychology, cognitive science, computer graphics, the arts, and many others. Members of the IUI community are interested in improving the symbiosis between humans and computers, and in making systems adapt to humans rather then the other way round. The call for papers attracted 191 submissions from Asia, America Europe, Africa, and Australia. The program committee accepted 46 papers, covering a diverse set of topics, reflected in the session titles "From Touch through Air to Brain" "Learning and Skills", "Intelligent Visual Interaction", "Users and Motion", "Leveraging Social Competencies", "Adaptive User Interfaces" and a special session with papers that honor the memory of John Riedl, who left us too early. A great attraction of the conference is provided by the scientific keynotes: Professor Wolfgang Wahlster opens the conference program with a keynote on "Multiadaptive Interfaces to Cyber-Physical Environments", Professor Noam Tractinsky's second day keynote is on "Visual Aesthetics of Interactive Technologies" and the last day keynote, by Professor Mark Billinghurst is on "Using AR to Create Empathic Experiences". In addition we are pleased to offer an invited talk by a relevant industry speaker, Yanki Margalit: "Startup nation and the Makers revolution. Intelligent user interfaces and the future of the Israeli hi-tech". We also have 11 posters and an excellent demonstration program consisting of 27 demos. In addition, the conference provides four very interesting workshops and a student consortium.},
 address = {New York, NY, USA},
 isbn = {978-1-4503-2729-9},
 location = {Haifa, Israel},
 publisher = {ACM},
 title = {IUI Companion '14: Proceedings of the Companion Publication of the 19th International Conference on Intelligent User Interfaces},
 year = {2014}
}


@inproceedings{Knox:2013:LNH:2449396.2449422,
 abstract = {Recent research has demonstrated that human-generated reward signals can be effectively used to train agents to perform a range of reinforcement learning tasks. Such tasks are either episodic - i.e., conducted in unconnected episodes of activity that often end in either goal or failure states - or continuing - i.e., indefinitely ongoing. Another point of difference is whether the learning agent highly discounts the value of future reward - a myopic agent - or conversely values future reward appreciably. In recent work, we found that previous approaches to learning from human reward all used myopic valuation [7]. This study additionally provided evidence for the desirability of myopic valuation in task domains that are both goal-based and episodic. In this paper, we conduct three user studies that examine critical assumptions of our previous research: task episodicity, optimal behavior with respect to a Markov Decision Process, and lack of a failure state in the goal-based task. In the first experiment, we show that converting a simple episodic task to non-episodic (i.e., continuing) task resolves some theoretical issues present in episodic tasks with generally positive reward and - relatedly - enables highly successful learning with non-myopic valuation in multiple user studies. The primary learning algorithm in this paper, which we call "VI-TAMER", is it the first algorithm to successfully learn non-myopically from human-generated reward; we also empirically show that such non-myopic valuation facilitates higher-level understanding of the task. Anticipating the complexity of real-world problems, we perform two subsequent user studies - one with a failure state added - that compare (1) learning when states are updated asynchronously with local bias - i.e., states quickly reachable from the agent's current state are updated more often than other states - to (2) learning with the fully synchronous sweeps across each state in the VI-TAMER algorithm. With these locally biased updates, we find that the general positivity of human reward creates problems even for continuing tasks, revealing a distinct research challenge for future work.},
 acmid = {2449422},
 address = {New York, NY, USA},
 author = {Knox, W. Bradley and Stone, Peter},
 booktitle = {Proceedings of the 2013 International Conference on Intelligent User Interfaces},
 doi = {10.1145/2449396.2449422},
 isbn = {978-1-4503-1965-2},
 keyword = {end-user programming, human teachers, human-agent interaction, interactive machine learning, modeling and prediction of user behavior, reinforcement learning},
 link = {http://doi.acm.org/10.1145/2449396.2449422},
 location = {Santa Monica, California, USA},
 numpages = {12},
 pages = {191--202},
 publisher = {ACM},
 series = {IUI '13},
 title = {Learning Non-myopically from Human-generated Reward},
 year = {2013}
}


@inproceedings{Schneider:2013:RGC:2449396.2449418,
 abstract = {Persuasive technology is now mobile and context-aware. Intelligent analysis of accelerometer signals in smartphones and other specialized devices has recently been used to classify activity (e.g., distinguishing walking from cycling) to encourage physical activity, sustainable transport, and other social goals. Unfortunately, results vary drastically due to differences in methodology and problem domain. The present report begins by structuring a survey of current work within a new framework, which highlights comparable characteristics between studies; this provided a tool by which we and others can understand the current state-of-the art and guide research towards existing gaps. We then present a new user study, positioned in an identified gap, that pushes limits of current success with a challenging problem: the real-time classification of 15 similar and novel gaits suitable for several persuasive application areas, focused on the growing phenomenon of exercise games. We achieve a mean correct classification rate of 78.1% of all 15 gaits with a minimal amount of personalized training of the classifier for each participant when carried in any of 6 different carrying locations (not known a priori). When narrowed to a subset of four gaits and one location that is known, this improves to means of 92.2% with and 87.2% without personalization. Finally, we group our findings into design guidelines and quantify variation in accuracy when an algorithm is trained for a known location and participant.},
 acmid = {2449418},
 address = {New York, NY, USA},
 author = {Schneider, Oliver S. and MacLean, Karon E. and Altun, Kerem and Karuei, Idin and Wu, Michael M.A.},
 booktitle = {Proceedings of the 2013 International Conference on Intelligent User Interfaces},
 doi = {10.1145/2449396.2449418},
 isbn = {978-1-4503-1965-2},
 keyword = {activity detection, exercise games, gait classification, mobile, persuasive computing, survey},
 link = {http://doi.acm.org/10.1145/2449396.2449418},
 location = {Santa Monica, California, USA},
 numpages = {12},
 pages = {161--172},
 publisher = {ACM},
 series = {IUI '13},
 title = {Real-time Gait Classification for Persuasive Smartphone Apps: Structuring the Literature and Pushing the Limits},
 year = {2013}
}


@inproceedings{Boteanu:2013:MDT:2449396.2449409,
 abstract = {CloudPrimer is a tablet-based interactive reading primer that aims to foster early literacy skills and shared parent-child reading through user-targeted discussion topic suggestions. The tablet application records discussions between parents and children as they read a story and leverages this information, in combination with a common sense knowledge base, to develop discussion topic models. The long-term goal of the project is to use such models to provide context-sensitive discussion topic suggestions to parents during the shared reading activity in order to enhance the interactive experience and foster parental engagement in literacy education. In this paper, we present a novel approach for using commonsense reasoning to effectively model topics of discussion in unstructured dialog. We introduce a metric for localizing concepts that the users are interested in at a given moment in the dialog and extract a time sequence of words of interest. We then present algorithms for topic modeling and refinement that leverage semantic knowledge acquired from ConceptNet, a commonsense knowledge base. We evaluate the performance of our algorithms using transcriptions of audio recordings of parent-child pairs interacting with a tablet application, and compare the output of our algorithms to human-generated topics. Our results show that words of interest and discussion topics selected by our algorithm closely match those identified by human readers.},
 acmid = {2449409},
 address = {New York, NY, USA},
 author = {Boteanu, Adrian and Chernova, Sonia},
 booktitle = {Proceedings of the 2013 International Conference on Intelligent User Interfaces},
 doi = {10.1145/2449396.2449409},
 isbn = {978-1-4503-1965-2},
 keyword = {context-aware computing, dialog analysis, user and cognitive models},
 link = {http://doi.acm.org/10.1145/2449396.2449409},
 location = {Santa Monica, California, USA},
 numpages = {10},
 pages = {75--84},
 publisher = {ACM},
 series = {IUI '13},
 title = {Modeling Discussion Topics in Interactions with a Tablet Reading Primer},
 year = {2013}
}


@inproceedings{Wang:2013:ICW:2449396.2449428,
 abstract = {Pupillary response is a popular physiological index of cognitive workload that can be used for design and evaluation of adaptive interface in various areas of human-computer interaction (HCI) research. However, in practice various confounding factors unrelated to workload, including changes of luminance condition and emotional arousal might degrade pupillary response based workload measures such as commonly used mean pupil diameter. This work investigates pupillary response as a cognitive workload measure under the influence of such confounding factors. Video-based eye tracker is used to record pupillary response during arithmetic tasks under luminance and emotional changes. Machine learning based feature selection and classification techniques are proposed to robustly index cognitive workload based on pupillary response even with the influence of noisy factors unrelated to workload.},
 acmid = {2449428},
 address = {New York, NY, USA},
 author = {Wang, Weihong and Li, Zhidong and Wang, Yang and Chen, Fang},
 booktitle = {Proceedings of the 2013 International Conference on Intelligent User Interfaces},
 doi = {10.1145/2449396.2449428},
 isbn = {978-1-4503-1965-2},
 keyword = {cognitive workload, feature extraction, machine learning, pupillary response},
 link = {http://doi.acm.org/10.1145/2449396.2449428},
 location = {Santa Monica, California, USA},
 numpages = {10},
 pages = {247--256},
 publisher = {ACM},
 series = {IUI '13},
 title = {Indexing Cognitive Workload Based on Pupillary Response Under Luminance and Emotional Changes},
 year = {2013}
}


@proceedings{Kim:2013:2449396,
 abstract = {It is our great pleasure to welcome you to the 2013 International Conference on Intelligent User Interfaces (IUI'13). This year marks the eighteenth meeting of this conference, continuing its tradition of being the principal international forum for reporting outstanding research at the intersection of Human-Computer Interaction (HCI) and Artificial Intelligence (AI). The work that appears at IUI bridges these two fields and also delves into related fields, such as psychology, cognitive science, computer graphics, the arts, and many others. Members of the IUI community are interested in improving the symbiosis between humans and computers, increasing the intelligence of both in the process. The call for papers attracted 195 submissions from Asia, Canada, Europe, Africa, and the United States. The program committee accepted 43 papers covering a diverse set of topics, including brain-computer interaction, social media analysis, automated design, and crowdsourcing. The program opens with a keynote by Professor Luis von Ahn on "Duolingo: Learn a Language for Free while Helping to Translate the Web," and closes with a keynote by Professor Monica S. Lam on "How Mobile Disrupts Social As We Know it." We also have an excellent poster and demonstration program consisting of 14 demos and 22 posters selected from a pool of 64 total submissions. In addition, the conference provides two exciting tutorials and four interesting workshops. The tutorials feature an introduction to Human Computation by Edith Law and an introduction to knowledge acquisition from the web and social media by Zornitsa Kozareva. The workshops cover topics ranging from interactive machine learning to IUI for developing worlds. No conference of this size could be organized without the help of a large number of individuals who volunteer an enormous amount of their own time. Their names can be found in the following pages and each and every one of these extraordinary volunteers deserve our thanks. We want to especially recognize all of the members of the organizing committee, who put in countless hours over nearly a year to make the conference happen. If you see one of them in the hotel bar at the conference, please buy them a beverage of their choice. We must also thank our senior program committee for coordinating the review process and all 654 members of the program committee for providing high quality reviews that exceeded even our lofty expectations. Last, but certainly not least, we must thank the authors for providing the content for the program that is the foundation of any successful conference. We look forward to your presentations!},
 address = {New York, NY, USA},
 isbn = {978-1-4503-1965-2},
 location = {Santa Monica, California, USA},
 publisher = {ACM},
 title = {IUI '13: Proceedings of the 2013 International Conference on Intelligent User Interfaces},
 year = {2013}
}


@inproceedings{Glowacka:2013:DES:2449396.2449413,
 abstract = {Techniques for both exploratory and known item search tend to direct only to more specific subtopics or individual documents, as opposed to allowing directing the exploration of the information space. We present an interactive information retrieval system that combines Reinforcement Learning techniques along with a novel user interface design to allow active engagement of users in directing the search. Users can directly manipulate document features (keywords) to indicate their interests and Reinforcement Learning is used to model the user by allowing the system to trade off between exploration and exploitation. This gives users the opportunity to more effectively direct their search nearer, further and following a direction. A task-based user study conducted with 20 participants comparing our system to a traditional query-based baseline indicates that our system significantly improves the effectiveness of information retrieval by providing access to more relevant and novel information without having to spend more time acquiring the information.},
 acmid = {2449413},
 address = {New York, NY, USA},
 author = {Glowacka, Dorota and Ruotsalo, Tuukka and Konuyshkova, Ksenia and Athukorala, kumaripaba and Kaski, Samuel and Jacucci, Giulio},
 booktitle = {Proceedings of the 2013 International Conference on Intelligent User Interfaces},
 doi = {10.1145/2449396.2449413},
 isbn = {978-1-4503-1965-2},
 keyword = {adaptive interfaces, data mining, information filtering, machine learning, recommender systems},
 link = {http://doi.acm.org/10.1145/2449396.2449413},
 location = {Santa Monica, California, USA},
 numpages = {12},
 pages = {117--128},
 publisher = {ACM},
 series = {IUI '13},
 title = {Directing Exploratory Search: Reinforcement Learning from User Interactions with Keywords},
 year = {2013}
}


@inproceedings{Pan:2013:OTT:2449396.2449441,
 abstract = {We are building a topic-based, interactive visual analytic tool that aids users in analyzing large collections of text. To help users quickly discover content evolution and significant content transitions within a topic over time, here we present a novel, constraint-based approach to temporal topic segmentation. Our solution splits a discovered topic into multiple linear, non-overlapping sub-topics along a timeline by satisfying a diverse set of semantic, temporal, and visualization constraints simultaneously. For each derived sub-topic, our solution also automatically selects a set of representative keywords to summarize the main content of the sub-topic. Our extensive evaluation, including a crowd-sourced user study, demonstrates the effectiveness of our method over an existing baseline.},
 acmid = {2449441},
 address = {New York, NY, USA},
 author = {Pan, Shimei and Zhou, Michelle X. and Song, Yangqiu and Qian, Weihong and Wang, Fei and Liu, Shixia},
 booktitle = {Proceedings of the 2013 International Conference on Intelligent User Interfaces},
 doi = {10.1145/2449396.2449441},
 isbn = {978-1-4503-1965-2},
 keyword = {constrained clustering, text visualization, topic-based},
 link = {http://doi.acm.org/10.1145/2449396.2449441},
 location = {Santa Monica, California, USA},
 numpages = {12},
 pages = {339--350},
 publisher = {ACM},
 series = {IUI '13},
 title = {Optimizing Temporal Topic Segmentation for Intelligent Text Visualization},
 year = {2013}
}


@inproceedings{Shi:2013:AMM:2449396.2449407,
 abstract = {Machine translation is increasingly used to support multilingual communication. Because of unavoidable translation errors, multilingual communication cannot accurately transfer information. We propose to shift from the transparent channel metaphor to the human-interpreter (agent) metaphor. Instead of viewing machine translation mediated communication as a transparent channel, the interpreter (agent) encourages the dialog participants to collaborate, as their interactivity will be helpful in reducing the number of translation errors, the noise of the channel. We examine the translation issues raised by multilingual communication, and analyze the impact of interactivity on the elimination of translation errors. We propose an implementation of the agent metaphor, which promotes interactivity between dialog participants and the machine translator. We design the architecture of our agent, analyze the interaction process, describe decision support and autonomous behavior, and provide an example of repair strategy preparation. We conduct an English-Chinese communication task experiment on tangram arrangement. The experiment shows that, compared to the transparent-channel metaphor, our agent metaphor reduced human communication effort by 21.6%.},
 acmid = {2449407},
 address = {New York, NY, USA},
 author = {Shi, Chunqi and Lin, Donghui and Ishida, Toru},
 booktitle = {Proceedings of the 2013 International Conference on Intelligent User Interfaces},
 doi = {10.1145/2449396.2449407},
 isbn = {978-1-4503-1965-2},
 keyword = {agent, interactivity, machine translation mediated communication, repair strategy},
 link = {http://doi.acm.org/10.1145/2449396.2449407},
 location = {Santa Monica, California, USA},
 numpages = {8},
 pages = {67--74},
 publisher = {ACM},
 series = {IUI '13},
 title = {Agent Metaphor for Machine Translation Mediated Communication},
 year = {2013}
}


@inproceedings{Tan:2013:IIU:2449396.2449427,
 abstract = {Intelligent User Interfaces can benefit from having knowledge on the user's emotion. However, current implementations to detect affective states, are often constraining the user's freedom of movement by instrumenting her with sensors. This prevents affective computing from being deployed in naturalistic and ubiquitous computing contexts. In this paper, we present a novel system called mASqUE, which uses a set of association rules to infer someone's affective state from their body postures. This is done without any user instrumentation and using off-the-shelf and non-expensive commodity hardware: a depth camera tracks the body posture of the users and their postures are also used as an indicator of their openness. By combining the posture information with physiological sensors measurements we were able to mine a set of association rules relating postures to affective states. We demonstrate the possibility of inferring affective states from body postures in ubiquitous computing environments and our study also provides insights how this opens up new possibilities for IUI to access the affective states of users from body postures in a nonintrusive way.},
 acmid = {2449427},
 address = {New York, NY, USA},
 author = {Tan, Chiew Seng Sean and Sch\"{o}ning, Johannes and Luyten, Kris and Coninx, Karin},
 booktitle = {Proceedings of the 2013 International Conference on Intelligent User Interfaces},
 doi = {10.1145/2449396.2449427},
 isbn = {978-1-4503-1965-2},
 keyword = {affective computing, emotion recognition, intelligent user interfaces, posture detection, posture tracking, social behavior, ubicomp},
 link = {http://doi.acm.org/10.1145/2449396.2449427},
 location = {Santa Monica, California, USA},
 numpages = {12},
 pages = {235--246},
 publisher = {ACM},
 series = {IUI '13},
 title = {Informing Intelligent User Interfaces by Inferring Affective States from Body Postures in Ubiquitous Computing Environments},
 year = {2013}
}


@inproceedings{Dukes:2013:SSI:2449396.2449447,
 abstract = {One of the most common clinical education methods for teaching patient interaction skills to nursing students is role-playing established scenarios with their classmates. Unfortunately, this is far from simulating real world experiences that they will soon face, and does not provide the immediate, impartial feedback necessary for interviewing skills development. We present a system for Scaffolded Interviews Developed by Nurses In Education (SIDNIE) that supports baccalaureate nursing education by providing multiple guided interview practice sessions with virtual characters. Our scenario depicts a mother who has brought in her five year old child to the clinic. In this paper we describe our system and report on a preliminary usability evaluation conducted with nursing students.},
 acmid = {2449447},
 address = {New York, NY, USA},
 author = {Dukes, Lauren Cairco and Pence, Toni Bloodworth and Hodges, Larry F. and Meehan, Nancy and Johnson, Arlene},
 booktitle = {Proceedings of the 2013 International Conference on Intelligent User Interfaces},
 doi = {10.1145/2449396.2449447},
 isbn = {978-1-4503-1965-2},
 keyword = {interview simulation, pediatric nursing, scaffolded learning, virtual characters, virtual environment, virtual patient},
 link = {http://doi.acm.org/10.1145/2449396.2449447},
 location = {Santa Monica, California, USA},
 numpages = {12},
 pages = {395--406},
 publisher = {ACM},
 series = {IUI '13},
 title = {SIDNIE: Scaffolded Interviews Developed by Nurses in Education},
 year = {2013}
}


@inproceedings{Ahmed:2013:NST:2449396.2449452,
 abstract = {While reading on touch-screens, sighted users can quickly pan through content, skim it, and pick out bits and pieces of information before deciding to read it more carefully. In contrast, blind users have to rely on the screen reader to narrate the content to them. To go through the text quickly, blind users employ gestures that direct the screen reader to skip to the next line or the next paragraph. However, the serial audio interface of the screen reader makes it difficult for blind users to get a sense of what is important before listening to, at least, a part of the content. This makes ad hoc skimming with gestures slow and ineffective. We address this problem in this paper; specifically we propose a non-visual skimming interface that enables blind users to control the amount of content with simple pinch-in and pinch-out gestures. This interface simulates the skimming experience enjoyed by sighted people, and enables blind users to listen to the gist of content, while controlling the speed of information intake. We report on a user study demonstrating that the proposed interface significantly outperforms ad hoc skimming techniques employed by blind users. Our results suggest that the proposed approach holds promise in empowering blind users to access digitized information much faster.},
 acmid = {2449452},
 address = {New York, NY, USA},
 author = {Ahmed, Faisal and Soviak, Andrii and Borodin, Yevgen and Ramakrishnan, I.V.},
 booktitle = {Proceedings of the 2013 International Conference on Intelligent User Interfaces},
 doi = {10.1145/2449396.2449452},
 isbn = {978-1-4503-1965-2},
 keyword = {accessibility, blind, screen reader, skimming, speed reading, summarization, touch interface},
 link = {http://doi.acm.org/10.1145/2449396.2449452},
 location = {Santa Monica, California, USA},
 numpages = {10},
 pages = {435--444},
 publisher = {ACM},
 series = {IUI '13},
 title = {Non-visual Skimming on Touch-screen Devices},
 year = {2013}
}


@proceedings{Kim:2013:2451176,
 abstract = {It is our great pleasure to welcome you to the 2013 International Conference on Intelligent User Interfaces (IUI'13). This year marks the eighteenth meeting of this conference, continuing its tradition of being the principal international forum for reporting outstanding research at the intersection of Human-Computer Interaction (HCI) and Artificial Intelligence (AI). The work that appears at IUI bridges these two fields and also delves into related fields, such as psychology, cognitive science, computer graphics, the arts, and many others. Members of the IUI community are interested in improving the symbiosis between humans and computers, increasing the intelligence of both in the process. The call for papers attracted 195 submissions from Asia, Canada, Europe, Africa, and the United States. The program committee accepted 43 papers covering a diverse set of topics, including brain-computer interaction, social media analysis, automated design, and crowdsourcing. The program opens with a keynote by Professor Luis von Ahn on "Duolingo: Learn a Language for Free while Helping to Translate the Web," and closes with a keynote by Professor Monica S. Lam on "How Mobile Disrupts Social As We Know it." We also have an excellent poster and demonstration program consisting of 14 demos and 22 posters selected from a pool of 64 total submissions. In addition, the conference provides two exciting tutorials and four interesting workshops. The tutorials feature an introduction to Human Computation by Edith Law and an introduction to knowledge acquisition from the web and social media by Zornitsa Kozareva. The workshops cover topics ranging from interactive machine learning to IUI for developing worlds. No conference of this size could be organized without the help of a large number of individuals who volunteer an enormous amount of their own time. Their names can be found in the following pages and each and every one of these extraordinary volunteers deserve our thanks. We want to especially recognize all of the members of the organizing committee, who put in countless hours over nearly a year to make the conference happen. If you see one of them in the hotel bar at the conference, please buy them a beverage of their choice. We must also thank our senior program committee for coordinating the review process and all 654 members of the program committee for providing high quality reviews that exceeded even our lofty expectations. Last, but certainly not least, we must thank the authors for providing the content for the program that is the foundation of any successful conference. We look forward to your presentations!},
 address = {New York, NY, USA},
 isbn = {978-1-4503-1966-9},
 location = {Santa Monica, California, USA},
 publisher = {ACM},
 title = {IUI '13 Companion: Proceedings of the Companion Publication of the 2013 International Conference on Intelligent User Interfaces Companion},
 year = {2013}
}


@inproceedings{Poli:2013:TCB:2449396.2449417,
 abstract = {We explored the possibility of controlling a spacecraft simulator using an analogue Brain-Computer Interface (BCI) for 2-D pointer control. This is a difficult task, for which no previous attempt has been reported in the literature. Our system relies on an active display which produces event-related potentials (ERPs) in the user's brain. These are analysed in real-time to produce control vectors for the user interface. In tests, users of the simulator were told to pass as close as possible to the Sun. Performance was very promising, on average users managing to satisfy the simulation success criterion in 67.5% of the runs. Furthermore, to study the potential of a collaborative approach to spacecraft navigation, we developed BCIs where the system is controlled via the integration of the ERPs of two users. Performance analysis indicates that collaborative BCIs produce trajectories that are statistically significantly superior to those obtained by single users.},
 acmid = {2449417},
 address = {New York, NY, USA},
 author = {Poli, Riccardo and Cinel, Caterina and Matran-Fernandez, Ana and Sepulveda, Francisco and Stoica, Adrian},
 booktitle = {Proceedings of the 2013 International Conference on Intelligent User Interfaces},
 doi = {10.1145/2449396.2449417},
 isbn = {978-1-4503-1965-2},
 keyword = {brain-computer interfaces, cooperative control, pointer control, space applications},
 link = {http://doi.acm.org/10.1145/2449396.2449417},
 location = {Santa Monica, California, USA},
 numpages = {12},
 pages = {149--160},
 publisher = {ACM},
 series = {IUI '13},
 title = {Towards Cooperative Brain-computer Interfaces for Space Navigation},
 year = {2013}
}


@inproceedings{Bohmer:2013:AFU:2449396.2449431,
 abstract = {Mobile phones have evolved from communication to multi-purpose devices that assist people with applications in various contexts and tasks. The size of the mobile ecosystem is steadily growing and new applications become available every day. This increasing number of applications makes it difficult for end-users to find good applications. Recommender systems suggesting mobile applications are being built to help people to find valuable applications. Since the nature of mobile applications differs from classical items to be recommended (e.g. books, movies, other goods), not only can new approaches for recommendation be developed, but also new paradigms for evaluating performance of recommender systems are advisable. During the lifecycle of mobile applications, different events can be observed that provide insights into users' engagement with particular applications. This gives rise to new approaches for evaluation of recommender systems. In this paper, we present AppFunnel: a framework that allows for usage-centric evaluation considering different stages of application engagement. We present a case study and discuss capabilities for evaluating recommender engines by applying metrics to the AppFunnel.},
 acmid = {2449431},
 address = {New York, NY, USA},
 author = {B\"{o}hmer, Matthias and Ganev, Lyubomir and Kr\"{u}ger, Antonio},
 booktitle = {Proceedings of the 2013 International Conference on Intelligent User Interfaces},
 doi = {10.1145/2449396.2449431},
 isbn = {978-1-4503-1965-2},
 keyword = {mobile apps, recommendation, usage-centric evaluation},
 link = {http://doi.acm.org/10.1145/2449396.2449431},
 location = {Santa Monica, California, USA},
 numpages = {10},
 pages = {267--276},
 publisher = {ACM},
 series = {IUI '13},
 title = {AppFunnel: A Framework for Usage-centric Evaluation of Recommender Systems That Suggest Mobile Applications},
 year = {2013}
}


@inproceedings{Orlosky:2013:DTM:2449396.2449443,
 abstract = {Reading text safely and easily while mobile has been an issue with see-through displays for many years. For example, in order to effectively use optical see through Head Mounted Displays (HMDs) or Heads Up Display (HUD) systems in constantly changing dynamic environments, variables like lighting conditions, human or vehicular obstructions in a user's path, and scene variation must be dealt with effectively. This paper introduces a new intelligent text management system that actively manages movement of text in a user's field of view. Research to date lacks a method to migrate user-centric content such as e-mail or text messages throughout a user's environment while mobile. Unlike most current annotation and view management systems, we use camera tracking to find dark, uniform regions along the route on which a user is travelling in real time. We then implement methodology to move text from one viable location to the next to maximize readability. A pilot experiment with 19 participants shows that the text placement of our system is preferred to text in fixed location configurations.},
 acmid = {2449443},
 address = {New York, NY, USA},
 author = {Orlosky, Jason and Kiyokawa, Kiyoshi and Takemura, Haruo},
 booktitle = {Proceedings of the 2013 International Conference on Intelligent User Interfaces},
 doi = {10.1145/2449396.2449443},
 isbn = {978-1-4503-1965-2},
 keyword = {content stabilization, heads up display, scene analysis, text placement, view management, wearable display},
 link = {http://doi.acm.org/10.1145/2449396.2449443},
 location = {Santa Monica, California, USA},
 numpages = {8},
 pages = {363--370},
 publisher = {ACM},
 series = {IUI '13},
 title = {Dynamic Text Management for See-through Wearable and Heads-up Display Systems},
 year = {2013}
}


@inproceedings{Daly:2013:WRW:2449396.2449423,
 abstract = {The advent of real-time traffic streaming offers users the opportunity to visualise current traffic conditions and congestion information. However, real-time information highlighting the underlying reason for tail-backs remains largely unexplored. Broken traffic lights, an accident, a large concert, or road-works reveal important information for citizens and traffic operators alike. Providing such information in real-time requires intelligent mechanisms and user interfaces in order to (i) harness heterogeneous data sources (volume, velocity, variety, veracity) and (ii) make derived knowledge consumable so users can visualize traffic conditions and congestion information making better routing decisions while travelling. This work focuses on surfacing relevant information and explaining the underlying reasons behind traffic conditions. To this end, static data from event providers, planned road works together with dynamically emerging events such as a traffic accidents, localized weather conditions or unplanned obstructions are captured through social media to provide users real-time feedback to highlight the causes of traffic congestion.},
 acmid = {2449423},
 address = {New York, NY, USA},
 author = {Daly, Elizabeth M. and Lecue, Freddy and Bicer, Veli},
 booktitle = {Proceedings of the 2013 International Conference on Intelligent User Interfaces},
 doi = {10.1145/2449396.2449423},
 isbn = {978-1-4503-1965-2},
 keyword = {crowd-sourcing, intelligent user interfaces, semantic reasoning, social media, text mining},
 link = {http://doi.acm.org/10.1145/2449396.2449423},
 location = {Santa Monica, California, USA},
 numpages = {10},
 pages = {203--212},
 publisher = {ACM},
 series = {IUI '13},
 title = {Westland Row Why So Slow?: Fusing Social Media and Linked Data Sources for Understanding Real-time Traffic Conditions},
 year = {2013}
}


@inproceedings{Lam:2013:MDS:2449396.2449437,
 abstract = {Every computer revolution changes our lives dramatically; so will mobile devices. Mobile devices enable billions of people to capture, share, interact, and consume real-time personal media in new and creative ways. In addition, being devices owned by individuals, they can form an autonomous computing fabric that frees us from the domination of existing centralized proprietary social networking services This talk presents a system architecture called Musubi (Mobile, Social, and UBIquitous) that combines a novel and natural mobile social experience with a clean architecture that lets users choose different cloud backup services. In addition, Musubi is an app platform that makes it easy to create privacy-honoring social apps. This can open up new markets for social and collaborative apps in fields like education, health and businesses, where centralized proprietary services are inappropriate. A fully working prototype of Musubi is available on both the Android and iPhone app store.},
 acmid = {2449437},
 address = {New York, NY, USA},
 author = {Lam, Monica S.},
 booktitle = {Proceedings of the 2013 International Conference on Intelligent User Interfaces},
 doi = {10.1145/2449396.2449437},
 isbn = {978-1-4503-1965-2},
 keyword = {mobile computing, social computing, social networks, software architecture},
 link = {http://doi.acm.org/10.1145/2449396.2449437},
 location = {Santa Monica, California, USA},
 numpages = {2},
 pages = {315--316},
 publisher = {ACM},
 series = {IUI '13},
 title = {How Mobile Disrupts Social As We Know It},
 year = {2013}
}


@inproceedings{Bakalov:2013:ACU:2449396.2449405,
 abstract = {Personalization nowadays is a commodity in a broad spectrum of computer systems. Examples range from online shops recommending products identified based on the user's previous purchases to web search engines sorting search hits based on the user browsing history. The aim of such adaptive behavior is to help users to find relevant content easier and faster. However, there are a number of negative aspects of this behavior. Adaptive systems have been criticized for violating the usability principles of direct manipulation systems, namely controllability, predictability, transparency, and unobtrusiveness. In this paper, we propose an approach to controlling adaptive behavior in recommender systems. It allows users to get an overview of personalization effects, view the user profile that is used for personalization, and adjust the profile and personalization effects to their needs and preferences. We present this approach using an example of a personalized portal for biochemical literature, whose users are biochemists, biologists and genomicists. Also, we report on a user study evaluating the impacts of controllable personalization on the usefulness, usability, user satisfaction, transparency, and trustworthiness of personalized systems.},
 acmid = {2449405},
 address = {New York, NY, USA},
 author = {Bakalov, Fedor and Meurs, Marie-Jean and K\"{o}nig-Ries, Birgitta and Sateli, Bahar and Witte, Ren{\'e} and Butler, Greg and Tsang, Adrian},
 booktitle = {Proceedings of the 2013 International Conference on Intelligent User Interfaces},
 doi = {10.1145/2449396.2449405},
 isbn = {978-1-4503-1965-2},
 keyword = {adaptive hypermedia, personalization, usability, user modeling},
 link = {http://doi.acm.org/10.1145/2449396.2449405},
 location = {Santa Monica, California, USA},
 numpages = {8},
 pages = {49--56},
 publisher = {ACM},
 series = {IUI '13},
 title = {An Approach to Controlling User Models and Personalization Effects in Recommender Systems},
 year = {2013}
}


@inproceedings{Steichen:2013:UIV:2449396.2449439,
 abstract = {Information Visualization systems have traditionally followed a one-size-fits-all model, typically ignoring an individual user's needs, abilities and preferences. However, recent research has indicated that visualization performance could be improved by adapting aspects of the visualization to each individual user. To this end, this paper presents research aimed at supporting the design of novel user-adaptive visualization systems. In particular, we discuss results on using information on user eye gaze patterns while interacting with a given visualization to predict the user's visualization tasks, as well as user cognitive abilities including perceptual speed, visual working memory, and verbal working memory. We show that such predictions are significantly better than a baseline classifier even during the early stages of visualization usage. These findings are discussed in view of designing visualization systems that can adapt to each individual user in real-time.},
 acmid = {2449439},
 address = {New York, NY, USA},
 author = {Steichen, Ben and Carenini, Giuseppe and Conati, Cristina},
 booktitle = {Proceedings of the 2013 International Conference on Intelligent User Interfaces},
 doi = {10.1145/2449396.2449439},
 isbn = {978-1-4503-1965-2},
 keyword = {adaptation, adaptive information visualization, eye-tracking, machine learning},
 link = {http://doi.acm.org/10.1145/2449396.2449439},
 location = {Santa Monica, California, USA},
 numpages = {12},
 pages = {317--328},
 publisher = {ACM},
 series = {IUI '13},
 title = {User-adaptive Information Visualization: Using Eye Gaze Data to Infer Visualization Tasks and User Cognitive Abilities},
 year = {2013}
}


@inproceedings{Kaindl:2013:SGR:2449396.2449410,
 abstract = {Creating and optimizing content- and dialogue-based recommendation processes and their GUIs (graphical user interfaces) manually is expensive and slow. Changes in the environment may also be found too late or even be overlooked by humans. We show how to generate such processes and their GUIs semi-automatically by using knowledge derived from unstructured data such as customer feedback on products on the Web. Our approach covers the whole lifecycle from knowledge discovery through text mining techniques to the use of this knowledge for semi-automatic generation of recommendation processes and their user interfaces as well as their comparison in real-world use within the e-commerce domain through A/B-variant tests. These tests indicate that our approach can lead to better results as well as less manual effort.},
 acmid = {2449410},
 address = {New York, NY, USA},
 author = {Kaindl, Hermann and Wach, Elmar P. and Okoli, Ada and Popp, Roman and Hoch, Ralph and Gaulke, Werner and Hussein, Tim},
 booktitle = {Proceedings of the 2013 International Conference on Intelligent User Interfaces},
 doi = {10.1145/2449396.2449410},
 isbn = {978-1-4503-1965-2},
 keyword = {dialogue-based recommenders, e-commerce, model-based ui generation, ontologies, recommender systems},
 link = {http://doi.acm.org/10.1145/2449396.2449410},
 location = {Santa Monica, California, USA},
 numpages = {10},
 pages = {85--94},
 publisher = {ACM},
 series = {IUI '13},
 title = {Semi-automatic Generation of Recommendation Processes and Their GUIs},
 year = {2013}
}


@inproceedings{Fischer:2013:RET:2449396.2449446,
 abstract = {We present a system and study of personalized energy-related recommendation. AgentSwitch utilizes electricity usage data collected from users' households over a period of time to realize a range of smart energy-related recommendations on energy tariffs, load detection and usage shifting. The web service is driven by a third party real-time energy tariff API (uSwitch), an energy data store, a set of algorithms for usage prediction, and appliance-level load disaggregation. We present the system design and user evaluation consisting of interviews and interface walkthroughs. We recruited participants from a previous study during which three months of their household's energy use was recorded to evaluate personalized recommendations in AgentSwitch. Our contributions are a) a systems architecture for personalized energy services; and b) findings from the evaluation that reveal challenges in designing energy-related recommender systems. In response to the challenges we formulate design recommendations to mitigate barriers to switching tariffs, to incentivize load shifting, and to automate energy management.},
 acmid = {2449446},
 address = {New York, NY, USA},
 author = {Fischer, Joel E. and Ramchurn, Sarvapali D. and Osborne, Michael and Parson, Oliver and Huynh, Trung Dong and Alam, Muddasser and Pantidi, Nadia and Moran, Stuart and Bachour, Khaled and Reece, Steve and Costanza, Enrico and Rodden, Tom and Jennings, Nicholas R.},
 booktitle = {Proceedings of the 2013 International Conference on Intelligent User Interfaces},
 doi = {10.1145/2449396.2449446},
 isbn = {978-1-4503-1965-2},
 keyword = {demand response, energy tariffs, load shifting, personalization, recommender systems, smart grid},
 link = {http://doi.acm.org/10.1145/2449396.2449446},
 location = {Santa Monica, California, USA},
 numpages = {12},
 pages = {383--394},
 publisher = {ACM},
 series = {IUI '13},
 title = {Recommending Energy Tariffs and Load Shifting Based on Smart Household Usage Profiling},
 year = {2013}
}


@inproceedings{Jahanian:2013:RSA:2449396.2449411,
 abstract = {In this paper, we present a recommendation system for the automatic design of magazine covers. Our users are non-designer designers: individuals or small and medium businesses who want to design without hiring a professional designer while still wanting to create aesthetically compelling designs. Because a design should have a purpose, we suggest a number of semantic features to the user, e.g., "clean and clear," "dynamic and active," or "formal," to describe the color mood for the purpose of his/her design. Based on these high level features and a number of low level features, such as the complexity of the visual balance in a photo, our system selects the best photos from the user's album for his/her design. Our system then generates several alternative designs that can be rated by the user. Consequently, our system generates future designs based on the user's style. In this fashion, our system personalizes the designs of a user based on his/her preferences.},
 acmid = {2449411},
 address = {New York, NY, USA},
 author = {Jahanian, Ali and Liu, Jerry and Lin, Qian and Tretter, Daniel and O'Brien-Strain, Eamonn and Lee, Seungyon Claire and Lyons, Nic and Allebach, Jan},
 booktitle = {Proceedings of the 2013 International Conference on Intelligent User Interfaces},
 doi = {10.1145/2449396.2449411},
 isbn = {978-1-4503-1965-2},
 keyword = {aesthetics, autonomous, color design, layout, personalization, recommendation, visual balance, visual design principles},
 link = {http://doi.acm.org/10.1145/2449396.2449411},
 location = {Santa Monica, California, USA},
 numpages = {12},
 pages = {95--106},
 publisher = {ACM},
 series = {IUI '13},
 title = {Recommendation System for Automatic Design of Magazine Covers},
 year = {2013}
}


@inproceedings{vandeCamp:2013:GFM:2449396.2449440,
 abstract = {As new input modalities allow interaction not only in front of a single display, but enable interaction in the whole room, application developers face new challenges. They have to handle many new input modalities, each with its own interface and requirements for pre-processing, deal with multiple displays, and applications that are distributed across multiple machines. We present glueTK, a framework that abstracts from the complexities of these input modalities, allows the design of interfaces for a wide range of display sizes, and makes the distribution across multiple machines transparent to the developer as well as the user. With an example application we demonstrate the wide range of input modalities glueTK can support and the functionality this enables. GlueTK moves away from the focus on point and touch like input modalities, enabling the design of applications tailored towards interactive rooms instead of the traditional desktop environment.},
 acmid = {2449440},
 address = {New York, NY, USA},
 author = {van de Camp, Florian and Stiefelhagen, Rainer},
 booktitle = {Proceedings of the 2013 International Conference on Intelligent User Interfaces},
 doi = {10.1145/2449396.2449440},
 isbn = {978-1-4503-1965-2},
 keyword = {application frameworks, multi-modal interfaces},
 link = {http://doi.acm.org/10.1145/2449396.2449440},
 location = {Santa Monica, California, USA},
 numpages = {10},
 pages = {329--338},
 publisher = {ACM},
 series = {IUI '13},
 title = {GlueTK: A Framework for Multi-modal, Multi-display Human-machine-interaction},
 year = {2013}
}


@inproceedings{Moran:2013:TRV:2449396.2449445,
 abstract = {The assumed role of humans as controllers and instructors of machines is changing. As systems become more complex and incomprehensible to humans, it will be increasingly necessary for us to place confidence in intelligent interfaces and follow their instructions and recommendations. This type of relationship becomes particularly intricate when we consider significant numbers of humans and agents working together in collectives. While instruction-based interfaces and agents already exist, our understanding of them within the field of Human-Computer Interaction is still limited. As such, we developed a large-scale pervasive game called 'Cargo', where a semi-autonomous ruled-based agent distributes a number of text-to-speech instructions to multiple teams of players via their mobile phone as an interface. We describe how people received, negotiated and acted upon the instructions in the game both individually and as a team and how players initial plans and expectations shaped their understanding of the instructions.},
 acmid = {2449445},
 address = {New York, NY, USA},
 author = {Moran, Stuart and Pantidi, Nadia and Bachour, Khaled and Fischer, Joel E. and Flintham, Martin and Rodden, Tom and Evans, Simon and Johnson, Simon},
 booktitle = {Proceedings of the 2013 International Conference on Intelligent User Interfaces},
 doi = {10.1145/2449396.2449445},
 isbn = {978-1-4503-1965-2},
 keyword = {human-agent interaction, in situ, instructions, pervasive games},
 link = {http://doi.acm.org/10.1145/2449396.2449445},
 location = {Santa Monica, California, USA},
 numpages = {12},
 pages = {371--382},
 publisher = {ACM},
 series = {IUI '13},
 title = {Team Reactions to Voiced Agent Instructions in a Pervasive Game},
 year = {2013}
}


@inproceedings{vonZezschwitz:2013:MGA:2449396.2449432,
 abstract = {Most of today's smartphones and tablet computers feature touchscreens as the main way of interaction. By using these touchscreens, oily residues of the users' fingers, smudge, remain on the device's display. As this smudge can be used to deduce formerly entered data, authentication tokens are jeopardized. Most notably, grid-based authentication methods, like the Android pattern scheme are prone to such attacks. Based on a thorough development process using low fidelity and high fidelity prototyping, we designed three graphic-based authentication methods in a way to leave smudge traces, which are not easy to interpret. We present one grid-based and two randomized graphical approaches and report on two user studies that we performed to prove the feasibility of these concepts. The authentication schemes were compared to the widely used Android pattern authentication and analyzed in terms of performance, usability and security. The results indicate that our concepts are significantly more secure against smudge attacks while keeping high input speed.},
 acmid = {2449432},
 address = {New York, NY, USA},
 author = {von Zezschwitz, Emanuel and Koslow, Anton and De Luca, Alexander and Hussmann, Heinrich},
 booktitle = {Proceedings of the 2013 International Conference on Intelligent User Interfaces},
 doi = {10.1145/2449396.2449432},
 isbn = {978-1-4503-1965-2},
 keyword = {attacks, authentication, mobile, security, smudge},
 link = {http://doi.acm.org/10.1145/2449396.2449432},
 location = {Santa Monica, California, USA},
 numpages = {10},
 pages = {277--286},
 publisher = {ACM},
 series = {IUI '13},
 title = {Making Graphic-based Authentication Secure Against Smudge Attacks},
 year = {2013}
}


@inproceedings{Zubiaga:2013:CCT:2449396.2449424,
 abstract = {While journalism is evolving toward a rather open-minded participatory paradigm, social media presents overwhelming streams of data that make it difficult to identify the information of a journalist's interest. Given the increasing interest of journalists in broadening and democratizing news by incorporating social media sources, we have developed TweetGathering, a prototype tool that provides curated and contextualized access to news stories on Twitter. This tool was built with the aim of assisting journalists both with gathering and with researching news stories as users comment on them. Five journalism professionals who tested the tool found helpful characteristics that could assist them with gathering additional facts on breaking news, as well as facilitating discovery of potential information sources such as witnesses in the geographical locations of news.},
 acmid = {2449424},
 address = {New York, NY, USA},
 author = {Zubiaga, Arkaitz and Ji, Heng and Knight, Kevin},
 booktitle = {Proceedings of the 2013 International Conference on Intelligent User Interfaces},
 doi = {10.1145/2449396.2449424},
 isbn = {978-1-4503-1965-2},
 keyword = {newsgathering, scoop, social media, trends, user-interface},
 link = {http://doi.acm.org/10.1145/2449396.2449424},
 location = {Santa Monica, California, USA},
 numpages = {12},
 pages = {213--224},
 publisher = {ACM},
 series = {IUI '13},
 title = {Curating and Contextualizing Twitter Stories to Assist with Social Newsgathering},
 year = {2013}
}


@inproceedings{GarciaEsparza:2013:CCT:2449396.2449402,
 abstract = {Real-time information streams such as Twitter have become a common way for users to discover new information. For most users this means curating a set of other users to follow. However, at the moment the following granularity of Twitter is restricted to the level of individual users. Our research has highlighted that many following relationships are motivated by a subset of interests that are shared by the users in question. For example, user A might follow user B because of their technology related tweets, but shares little or no interest in their other tweets. As a result, this all-or-nothing following relationship can quickly overwhelm users' timelines with extraneous information. To improve this situation we propose a user profiling approach based on the topical categorisation of users' posted URLs. These topics can then be used to filter information streams so that they focus on more relevant information from the people they follow, based on their core interests. In particular, we present a system called CatStream that provides for a more fine-grained way to follow users on specific topics and filter our timelines accordingly. We present the results of a live-user study that shows how filtered timelines offer a better way to organise and filter their information streams. Most importantly users are generally satisfied with the categories predicted for their profiles and tweets.},
 acmid = {2449402},
 address = {New York, NY, USA},
 author = {Garcia Esparza, Sandra and O'Mahony, Michael P. and Smyth, Barry},
 booktitle = {Proceedings of the 2013 International Conference on Intelligent User Interfaces},
 doi = {10.1145/2449396.2449402},
 isbn = {978-1-4503-1965-2},
 keyword = {classification, information filtering, real-time web, user profiling},
 link = {http://doi.acm.org/10.1145/2449396.2449402},
 location = {Santa Monica, California, USA},
 numpages = {12},
 pages = {25--36},
 publisher = {ACM},
 series = {IUI '13},
 title = {CatStream: Categorising Tweets for User Profiling and Stream Filtering},
 year = {2013}
}


@inproceedings{Putze:2013:LUA:2449396.2449415,
 abstract = {In expert video analysis, the selection of certain events in a continuous video stream is a frequently occurring operation, e.g., in surveillance applications. Due to the dynamic and rich visual input, the constantly high attention and the required hand-eye coordination for mouse interaction, this is a very demanding and exhausting task. Hence, relevant events might be missed. We propose to use eye tracking and electroencephalography (EEG) as additional input modalities for event selection. From eye tracking, we derive the spatial location of a perceived event and from patterns in the EEG signal we derive its temporal location within the video stream. This reduces the amount of the required active user input in the selection process, and thus has the potential to reduce the user's workload. In this paper, we describe the employed methods for the localization processes and introduce the developed scenario in which we investigate the feasibility of this approach. Finally, we present and discuss results on the accuracy and the speed of the method and investigate how the modalities interact.},
 acmid = {2449415},
 address = {New York, NY, USA},
 author = {Putze, Felix and Hild, Jutta and K\"{a}rgel, Rainer and Herff, Christian and Redmann, Alexander and Beyerer, J\"{u}rgen and Schultz, Tanja},
 booktitle = {Proceedings of the 2013 International Conference on Intelligent User Interfaces},
 doi = {10.1145/2449396.2449415},
 isbn = {978-1-4503-1965-2},
 keyword = {eeg, event detection, expert video analysis, eye tracking},
 link = {http://doi.acm.org/10.1145/2449396.2449415},
 location = {Santa Monica, California, USA},
 numpages = {8},
 pages = {129--136},
 publisher = {ACM},
 series = {IUI '13},
 title = {Locating User Attention Using Eye Tracking and EEG for Spatio-temporal Event Selection},
 year = {2013}
}


@inproceedings{Chen:2013:SSC:2449396.2449433,
 abstract = {People frequently capture photos with their smartphones, and some are starting to capture images of documents. However, the quality of captured document images is often lower than expected, even when an application that performs post-processing to improve the image is used. To improve the quality of captured images before post-processing, we developed the Smart Document Capture (SmartDCap) application that provides real-time feedback to users about the likely quality of a captured image. The quality measures capture the sharpness and framing of a page or regions on a page, such as a set of one or more columns, a part of a column, a figure, or a table. Using our approach, while users adjust the camera position, the application automatically determines when to take a picture of a document to produce a good quality result. We performed a subjective evaluation comparing SmartDCap and the Android Ice Cream Sandwich (ICS) camera application; we also used raters to evaluate the quality of the captured images. Our results indicate that users find SmartDCap to be as easy to use as the standard ICS camera application. Also, images captured using SmartDCap are sharper and better framed on average than images using the ICS camera application.},
 acmid = {2449433},
 address = {New York, NY, USA},
 author = {Chen, Francine and Carter, Scott and Denoue, Laurent and Kumar, Jayant},
 booktitle = {Proceedings of the 2013 International Conference on Intelligent User Interfaces},
 doi = {10.1145/2449396.2449433},
 isbn = {978-1-4503-1965-2},
 keyword = {documents, image analysis, mobile capture, mobile computing},
 link = {http://doi.acm.org/10.1145/2449396.2449433},
 location = {Santa Monica, California, USA},
 numpages = {10},
 pages = {287--296},
 publisher = {ACM},
 series = {IUI '13},
 title = {SmartDCap: Semi-automatic Capture of Higher Quality Document Images from a Smartphone},
 year = {2013}
}


@inproceedings{Knijnenburg:2013:HUI:2449396.2449448,
 abstract = {Personalization relies on personal data about each individual user. Users are quite often reluctant though to disclose information about themselves and to be "tracked" by a system. We investigated whether different types of rationales (justifications) for disclosure that have been suggested in the privacy literature would increase users' willingness to divulge demographic and contextual information about themselves, and would raise their satisfaction with the system. We also looked at the effect of the order of requests, owing to findings from the literature. Our experiment with a mockup of a mobile app recommender shows that there is no single strategy that is optimal for everyone. Heuristics can be defined though that select for each user the most effective justification to raise disclosure or satisfaction, taking the user's gender, disclosure tendency, and the type of solicited personal information into account. We discuss the implications of these findings for research aimed at personalizing privacy strategies to each individual user.},
 acmid = {2449448},
 address = {New York, NY, USA},
 author = {Knijnenburg, Bart P. and Kobsa, Alfred},
 booktitle = {Proceedings of the 2013 International Conference on Intelligent User Interfaces},
 doi = {10.1145/2449396.2449448},
 isbn = {978-1-4503-1965-2},
 keyword = {adaptive interfaces, information disclosure, privacy, recommender systems, satisfaction, trust, user characteristics, user experience},
 link = {http://doi.acm.org/10.1145/2449396.2449448},
 location = {Santa Monica, California, USA},
 numpages = {10},
 pages = {407--416},
 publisher = {ACM},
 series = {IUI '13},
 title = {Helping Users with Information Disclosure Decisions: Potential for Adaptation},
 year = {2013}
}


@inproceedings{vonAhn:2013:DLL:2449396.2449398,
 abstract = {I want to translate the Web into every major language: every webpage, every video, and, yes, even Justin Bieber's tweets. With its content split up into hundreds of languages -- and with over 50% of it in English -- most of the Web is inaccessible to most people in the world. This problem is pressing, now more than ever, with millions of people from China, Russia, Latin America and other quickly developing regions entering the Web. In this talk, I introduce my new project, called Duolingo, which aims at breaking this language barrier, and thus making the Web truly "world wide." We have all seen how systems such as Google Translate are improving every day at translating the gist of things written in other languages. Unfortunately, they are not yet accurate enough for my purpose: Even when what they spit out is intelligible, it's so badly written that I can't read more than a few lines before getting a headache. With Duolingo, our goal is to encourage people, like you and me, to translate the Web into their native languages.},
 acmid = {2449398},
 address = {New York, NY, USA},
 author = {von Ahn, Luis},
 booktitle = {Proceedings of the 2013 International Conference on Intelligent User Interfaces},
 doi = {10.1145/2449396.2449398},
 isbn = {978-1-4503-1965-2},
 keyword = {crowdsourcing, education technology, human computation, natural language processing},
 link = {http://doi.acm.org/10.1145/2449396.2449398},
 location = {Santa Monica, California, USA},
 numpages = {2},
 pages = {1--2},
 publisher = {ACM},
 series = {IUI '13},
 title = {Duolingo: Learn a Language for Free While Helping to Translate the Web},
 year = {2013}
}


@inproceedings{Sucu:2013:HIN:2449396.2449451,
 abstract = {Glare significantly diminishes visual perception, and is a significant cause of traffic accidents. Existing haptic automotive interfaces typically indicate when and in which direction to steer, but they don't convey how much to steer, as a driver typically determines this using visual feedback. We present a novel haptic interface that relies on an intelligent vehicle position system to indicate when, in which direction and how far to steer, as to facilitate steering without any visual feedback. Our interface may improve driving safety when a driver is temporarily blinded, for example, due to glare or fog. Three user studies were performed, the first study tries to understand driving using visual feedback, the second study evaluates two different haptic encoding mechanisms with no visual feedback present, and a third study evaluates the supplemental effect of haptic feedback when used in conjunction with visual feedback. Studies show this interface to allow for blind steering in small curves and that it can improve a driver's lane keeping ability when combined with visual feedback.},
 acmid = {2449451},
 address = {New York, NY, USA},
 author = {Sucu, Burkay and Folmer, Eelke},
 booktitle = {Proceedings of the 2013 International Conference on Intelligent User Interfaces},
 doi = {10.1145/2449396.2449451},
 isbn = {978-1-4503-1965-2},
 keyword = {automotive ui, haptic interfaces, safety, steering},
 link = {http://doi.acm.org/10.1145/2449396.2449451},
 location = {Santa Monica, California, USA},
 numpages = {8},
 pages = {427--434},
 publisher = {ACM},
 series = {IUI '13},
 title = {Haptic Interface for Non-visual Steering},
 year = {2013}
}


@inproceedings{Damaraju:2013:MSA:2449396.2449453,
 abstract = {Research in multi-touch interaction has typically been focused on direct spatial manipulation; techniques have been create to result in the most intuitive mapping between the movement of the hand and the resultant change in the virtual object. However, as we attempt to design for more complex operations, the expectation of spatial manipulation becomes infeasible. We introduce Multi-tap Sliders for operation in what we call abstract parametric spaces that do not have an obvious literal spatial representation, such as exposure, brightness, contrast and saturation for image editing. This new widget design promotes multi-touch interaction for prolonged use in scenarios that require adjustment of multiple parameters as part of an operation. The multi-tap sliders encourage the user to keep her visual focus on the target, instead of the requiring to look back at the interface. Our research emphasizes ergonomics, clear visual design, and fluid transition between the selection of parameters and their subsequent adjustment for a given operation. We demonstrate a new technique for quickly selecting and adjusting multiple numerical parameters. A preliminary user study points out improvements over the traditional sliders.},
 acmid = {2449453},
 address = {New York, NY, USA},
 author = {Damaraju, Sashikanth and Seo, Jinsil Hwaryoung and Hammond, Tracy and Kerne, Andruid},
 booktitle = {Proceedings of the 2013 International Conference on Intelligent User Interfaces},
 doi = {10.1145/2449396.2449453},
 isbn = {978-1-4503-1965-2},
 keyword = {exploratory interfaces, multi-touch, parametric spaces},
 link = {http://doi.acm.org/10.1145/2449396.2449453},
 location = {Santa Monica, California, USA},
 numpages = {8},
 pages = {445--452},
 publisher = {ACM},
 series = {IUI '13},
 title = {Multi-tap Sliders: Advancing Touch Interaction for Parameter Adjustment},
 year = {2013}
}


@inproceedings{Dostal:2013:SGT:2449396.2449416,
 abstract = {This paper explores techniques for visualising display changes in multi-display environments. We present four subtle gaze-dependent techniques for visualising change on unattended displays called FreezeFrame, PixMap, WindowMap and Aura. To enable the techniques to be directly deployed to workstations, we also present a system that automatically identifies the user's eyes using computer vision and a set of web cameras mounted on the displays. An evaluation confirms this system can detect which display the user is attending to with high accuracy. We studied the efficacy of the visualisation techniques in a five-day case study with a working professional. This individual used our system eight hours per day for five consecutive days. The results of the study show that the participant found the system and the techniques useful, subtle, calm and non-intrusive. We conclude by discussing the challenges in evaluating intelligent subtle interaction techniques using traditional experimental paradigms.},
 acmid = {2449416},
 address = {New York, NY, USA},
 author = {Dostal, Jakub and Kristensson, Per Ola and Quigley, Aaron},
 booktitle = {Proceedings of the 2013 International Conference on Intelligent User Interfaces},
 doi = {10.1145/2449396.2449416},
 isbn = {978-1-4503-1965-2},
 keyword = {attentive user interfaces, computer vision, multi-display environments, notifications, subtle interaction},
 link = {http://doi.acm.org/10.1145/2449396.2449416},
 location = {Santa Monica, California, USA},
 numpages = {12},
 pages = {137--148},
 publisher = {ACM},
 series = {IUI '13},
 title = {Subtle Gaze-dependent Techniques for Visualising Display Changes in Multi-display Environments},
 year = {2013}
}


@inproceedings{LavidBenLulu:2013:FCU:2449396.2449434,
 abstract = {In recent years, we have witnessed the incredible popularity and widespread adoption of mobile devices. Millions of Apps are being developed and downloaded by users at an amazing rate. These are multi-feature Apps that address a broad range of needs and functions. Nowadays, every user has dozens of Apps on his mobile device. As time goes on, it becomes more and more difficult simply to find the desired App among those that are installed on the mobile device. In spite of several attempts to address the problem, no good solution for this increasing problem has yet been found. In this paper we suggest the use of unsupervised machine learning for clustering Apps based on their functionality, to allow users to access them easily. The functionality is elicited from their description as retrieved from various App stores and enriched by content from professional blogs. The Apps are clustered and grouped according to their functionality and presented hierarchically to the user in order to facilitate the search on the small screen of the mobile device.},
 acmid = {2449434},
 address = {New York, NY, USA},
 author = {Lavid Ben Lulu, David and Kuflik, Tsvi},
 booktitle = {Proceedings of the 2013 International Conference on Intelligent User Interfaces},
 doi = {10.1145/2449396.2449434},
 isbn = {978-1-4503-1965-2},
 keyword = {clustering, data mining, human-computer interaction, mobile, short and sparse text, smartphone apps, text similarity, visualization},
 link = {http://doi.acm.org/10.1145/2449396.2449434},
 location = {Santa Monica, California, USA},
 numpages = {10},
 pages = {297--306},
 publisher = {ACM},
 series = {IUI '13},
 title = {Functionality-based Clustering Using Short Textual Description: Helping Users to Find Apps Installed on Their Mobile Device},
 year = {2013}
}


@inproceedings{Weltman:2013:MGC:2449396.2449421,
 abstract = {In natural language, there are many gaps between what is stated and what is understood. Speakers and listeners fill in these gaps, presumably from some life experience, but no one knows how to get this experiential data into a computer. As a first step, we have created a methodology and software interface for collecting commonsense data about simple experiences. This work is intended to form the basis of a new resource for natural language processing. We model experience as a sequence of comic frames, annotated with the changing intentional and physical states of the characters and objects. To create an annotated experience, our software interface guides non-experts in identifying facts about experiences that humans normally take for granted. As part of this process, the system asks questions using the Socratic Method to help users notice difficult-to-articulate commonsense data. A test on ten subjects indicates that non-experts are able to produce high quality experiential data.},
 acmid = {2449421},
 address = {New York, NY, USA},
 author = {Weltman, Jerry S. and Iyengar, S. Sitharama and Hegarty, Michael},
 booktitle = {Proceedings of the 2013 International Conference on Intelligent User Interfaces},
 doi = {10.1145/2449396.2449421},
 isbn = {978-1-4503-1965-2},
 keyword = {common sense, knowledge acquisition, large-scale collaboration},
 link = {http://doi.acm.org/10.1145/2449396.2449421},
 location = {Santa Monica, California, USA},
 numpages = {12},
 pages = {179--190},
 publisher = {ACM},
 series = {IUI '13},
 title = {Mind the Gap: Collecting Commonsense Data About Simple Experiences},
 year = {2013}
}


@inproceedings{Huang:2013:LCI:2449396.2449400,
 abstract = {Crowdsourcing and machine learning are both useful techniques for solving difficult problems (e.g., computer vision and natural language processing). In this paper, we propose a novel method that harnesses and combines the strength of these two techniques to better analyze the features and the sentiments toward them in user reviews. To strike a good balance between reducing information overload and providing the original context expressed by review writers, the proposed system (1) allows users to interactively rank the entities based on feature-rating, (2) automatically highlights sentences that are related to relevant features, and (3) utilizes implicit crowdsourcing by encouraging users to provide correct labels of their own reviews to improve the feature-sentiment classifier. The proposed system not only helps users to save time and effort to digest the often massive amount of user reviews, but also provides real-time suggestions on relevant features and ratings as users generate their own reviews. Results from a simulation experiment show that leveraging on the crowd can significantly improve the feature-sentiment analysis of user reviews. Furthermore, results from a user study show that the proposed interface was preferred by more participants than interfaces that use traditional noun-adjective pair summarization, as the current interface allows users to view feature-related information in the original context.},
 acmid = {2449400},
 address = {New York, NY, USA},
 author = {Huang, Shih-Wen and Tu, Pei-Fen and Fu, Wai-Tat and Amanzadeh, Mohammad},
 booktitle = {Proceedings of the 2013 International Conference on Intelligent User Interfaces},
 doi = {10.1145/2449396.2449400},
 isbn = {978-1-4503-1965-2},
 keyword = {crowdsourcing, human computation, interactive machine learning, sentiment analysis, user generated content},
 link = {http://doi.acm.org/10.1145/2449396.2449400},
 location = {Santa Monica, California, USA},
 numpages = {12},
 pages = {3--14},
 publisher = {ACM},
 series = {IUI '13},
 title = {Leveraging the Crowd to Improve Feature-sentiment Analysis of User Reviews},
 year = {2013}
}


@inproceedings{Verbert:2013:VRS:2449396.2449442,
 abstract = {Research on recommender systems has traditionally focused on the development of algorithms to improve accuracy of recommendations. So far, little research has been done to enable user interaction with such systems as a basis to support exploration and control by end users. In this paper, we present our research on the use of information visualization techniques to interact with recommender systems. We investigated how information visualization can improve user understanding of the typically black-box rationale behind recommendations in order to increase their perceived relevance and meaning and to support exploration and user involvement in the recommendation process. Our study has been performed using TalkExplorer, an interactive visualization tool developed for attendees of academic conferences. The results of user studies performed at two conferences allowed us to obtain interesting insights to enhance user interfaces that integrate recommendation technology. More specifically, effectiveness and probability of item selection both increase when users are able to explore and interrelate multiple entities -- i.e. items bookmarked by users, recommendations and tags.},
 acmid = {2449442},
 address = {New York, NY, USA},
 author = {Verbert, Katrien and Parra, Denis and Brusilovsky, Peter and Duval, Erik},
 booktitle = {Proceedings of the 2013 International Conference on Intelligent User Interfaces},
 doi = {10.1145/2449396.2449442},
 isbn = {978-1-4503-1965-2},
 keyword = {information visualization, user interfaces for recommender systems, user studies},
 link = {http://doi.acm.org/10.1145/2449396.2449442},
 location = {Santa Monica, California, USA},
 numpages = {12},
 pages = {351--362},
 publisher = {ACM},
 series = {IUI '13},
 title = {Visualizing Recommendations to Support Exploration, Transparency and Controllability},
 year = {2013}
}


@inproceedings{Chen:2013:ACU:2449396.2449406,
 abstract = {A day in the life of a user can be segmented into a series of tasks: a user begins a task, becomes loaded perceptually and cognitively to some extent by the objects and mental challenge that comprise that task, then at some point switches or is distracted to a new task, and so on. Understanding the contextual task characteristics and user behavior in interaction can benefit the development of intelligent systems to aid user task management. Applications that aid the user in one way or another have proliferated as computing devices become more and more of a constant companion. However, direct and continuous observations of individual tasks in a naturalistic context and subsequent task analysis, for example the diary method, have traditionally been a manual process. We propose a method for automatic task analysis system, which monitors the user's current task and analyzes it in terms of the task transition, and perceptual and cognitive load imposed by the task. An experiment was conducted in which participants were required to work continuously on groups of three sequential tasks of different types. Three classes of eye activity, namely pupillary response, blink and eye movement, were analyzed to detect the task transition and non-transition states, and to estimate three levels of perceptual load and three levels of cognitive load every second to infer task characteristics. This paper reports statistically significant classification accuracies in all cases and demonstrates the feasibility of this approach for task monitoring and analysis.},
 acmid = {2449406},
 address = {New York, NY, USA},
 author = {Chen, Siyuan and Epps, Julien and Chen, Fang},
 booktitle = {Proceedings of the 2013 International Conference on Intelligent User Interfaces},
 doi = {10.1145/2449396.2449406},
 isbn = {978-1-4503-1965-2},
 keyword = {blink, cognitive load, fixation, perceptual load, pupil, saccade, task analysis, task transition, task monitoring},
 link = {http://doi.acm.org/10.1145/2449396.2449406},
 location = {Santa Monica, California, USA},
 numpages = {10},
 pages = {57--66},
 publisher = {ACM},
 series = {IUI '13},
 title = {Automatic and Continuous User Task Analysis via Eye Activity},
 year = {2013}
}


@inproceedings{Chun:2013:RHI:2449396.2449435,
 abstract = {Over the past few years, Augmented Reality has become widely popular in the form of smart phone applications, however most smart phone-based AR applications are limited in user interaction and do not support gesture-based direct manipulation of the augmented scene. In this paper, we introduce a new AR interaction methodology, employing users' hands and fingers to interact with the virtual (and possibly physical) objects that appear on the mobile phone screen. The goal of this project was to support different types of interaction (selection, transformation, and fine-grain control of an input value) while keeping the methodology for hand detection as simple as possible to maintain good performance on smart phones. We evaluated our methods in user studies, collecting task performance data and user impressions about this direct way of interacting with augmented scenes through mobile phones.},
 acmid = {2449435},
 address = {New York, NY, USA},
 author = {Chun, Wendy H. and H\"{o}llerer, Tobias},
 booktitle = {Proceedings of the 2013 International Conference on Intelligent User Interfaces},
 doi = {10.1145/2449396.2449435},
 isbn = {978-1-4503-1965-2},
 keyword = {augmented reality, gesture recognition, hand-based interaction, mobile phone},
 link = {http://doi.acm.org/10.1145/2449396.2449435},
 location = {Santa Monica, California, USA},
 numpages = {8},
 pages = {307--314},
 publisher = {ACM},
 series = {IUI '13},
 title = {Real-time Hand Interaction for Augmented Reality on Mobile Phones},
 year = {2013}
}


@inproceedings{Pfeil:2013:EGM:2449396.2449429,
 abstract = {We present a study exploring upper body 3D spatial interaction metaphors for control and communication with Unmanned Aerial Vehicles (UAV) such as the Parrot AR Drone. We discuss the design and implementation of five interaction techniques using the Microsoft Kinect, based on metaphors inspired by UAVs, to support a variety of flying operations a UAV can perform. Techniques include a first-person interaction metaphor where a user takes a pose like a winged aircraft, a game controller metaphor, where a user's hands mimic the control movements of console joysticks, "proxy" manipulation, where the user imagines manipulating the UAV as if it were in their grasp, and a pointing metaphor in which the user assumes the identity of a monarch and commands the UAV as such. We examine qualitative metrics such as perceived intuition, usability and satisfaction, among others. Our results indicate that novice users appreciate certain 3D spatial techniques over the smartphone application bundled with the AR Drone. We also discuss the trade-offs in the technique design metrics based on results from our study.},
 acmid = {2449429},
 address = {New York, NY, USA},
 author = {Pfeil, Kevin and Koh, Seng Lee and LaViola, Joseph},
 booktitle = {Proceedings of the 2013 International Conference on Intelligent User Interfaces},
 doi = {10.1145/2449396.2449429},
 isbn = {978-1-4503-1965-2},
 keyword = {3d interaction, robots, user studies},
 link = {http://doi.acm.org/10.1145/2449396.2449429},
 location = {Santa Monica, California, USA},
 numpages = {10},
 pages = {257--266},
 publisher = {ACM},
 series = {IUI '13},
 title = {Exploring 3D Gesture Metaphors for Interaction with Unmanned Aerial Vehicles},
 year = {2013}
}


@inproceedings{Bixler:2013:DBE:2449396.2449426,
 abstract = {It is hypothesized that the ability for a system to automatically detect and respond to users' affective states can greatly enhance the human-computer interaction experience. Although there are currently many options for affect detection, keystroke analysis offers several attractive advantages to traditional methods. In this paper, we consider the possibility of automatically discriminating between natural occurrences of boredom, engagement, and neutral by analyzing keystrokes, task appraisals, and stable traits of 44 individuals engaged in a writing task. The analyses explored several different arrangements of the data: using downsampled and/or standardized data; distinguishing between three different affect states or groups of two; and using keystroke/timing features in isolation or coupled with stable traits and/or task appraisals. The results indicated that the use of raw data and the feature set that combined keystroke/timing features with task appraisals and stable traits, yielded accuracies that were 11% to 38% above random guessing and generalized to new individuals. Applications of our affect detector for intelligent interfaces that provide engagement support during writing are discussed.},
 acmid = {2449426},
 address = {New York, NY, USA},
 author = {Bixler, Robert and D'Mello, Sidney},
 booktitle = {Proceedings of the 2013 International Conference on Intelligent User Interfaces},
 doi = {10.1145/2449396.2449426},
 isbn = {978-1-4503-1965-2},
 keyword = {affect detection, boredom, engagement, free text, keystroke dynamics},
 link = {http://doi.acm.org/10.1145/2449396.2449426},
 location = {Santa Monica, California, USA},
 numpages = {10},
 pages = {225--234},
 publisher = {ACM},
 series = {IUI '13},
 title = {Detecting Boredom and Engagement During Writing with Keystroke Analysis, Task Appraisals, and Stable Traits},
 year = {2013}
}


@inproceedings{Mahmud:2013:RTS:2449396.2449403,
 abstract = {We present an intelligent, crowd-powered information collection system that automatically identifies and asks targeted strangers on Twitter for desired information (e.g., current wait time at a nightclub). Our work includes three parts. First, we identify a set of features that characterize one's willingness and readiness to respond based on their exhibited social behavior, including the content of their tweets and social interaction patterns. Second, we use the identified features to build a statistical model that predicts one's likelihood to respond to information solicitations. Third, we develop a recommendation algorithm that selects a set of targeted strangers using the probabilities computed by our statistical model with the goal to maximize the over-all response rate. Our experiments, including several in the real world, demonstrate the effectiveness of our work.},
 acmid = {2449403},
 address = {New York, NY, USA},
 author = {Mahmud, Jalal and Zhou, Michelle X. and Megiddo, Nimrod and Nichols, Jeffrey and Drews, Clemens},
 booktitle = {Proceedings of the 2013 International Conference on Intelligent User Interfaces},
 doi = {10.1145/2449396.2449403},
 isbn = {978-1-4503-1965-2},
 keyword = {personality, response rate, social media, willingness},
 link = {http://doi.acm.org/10.1145/2449396.2449403},
 location = {Santa Monica, California, USA},
 numpages = {12},
 pages = {37--48},
 publisher = {ACM},
 series = {IUI '13},
 title = {Recommending Targeted Strangers from Whom to Solicit Information on Social Media},
 year = {2013}
}


@inproceedings{Bostandjiev:2013:LES:2449396.2449412,
 abstract = {This paper presents LinkedVis, an interactive visual recommender system that combines social and semantic knowledge to produce career recommendations based on the LinkedIn API. A collaborative (social) approach is employed to identify professionals with similar career paths and produce personalized recommendations of both companies and roles. To unify semantically identical but lexically distinct entities and arrive at better user models, we employ lightweight natural language processing and entity resolution using semantic information from a variety of end-points on the web. Elements from the underlying recommendation algorithm are exposed through an interactive interface that allows users to manipulate different aspects of the algorithm and the data it operates on, allowing users to explore a variety of "what-if" scenarios around their current profile. We evaluate LinkedVis through leave-one-out accuracy and diversity experiments on a data corpus collected from 47 users and their LinkedIn connections, as well as through a supervised study of 27 users exploring their own profile and recommendations interactively. Results show that our approach outperforms a benchmark recommendation algorithm without semantic resolution in terms of accuracy and diversity, and that the ability to tweak recommendations interactively by adjusting profile item and social connection weights further improves predictive accuracy. Questionnaires on the user experience with the explanatory and interactive aspects of the application reveal very high user acceptance and satisfaction.},
 acmid = {2449412},
 address = {New York, NY, USA},
 author = {Bostandjiev, Svetlin and O'Donovan, John and H\"{o}llerer, Tobias},
 booktitle = {Proceedings of the 2013 International Conference on Intelligent User Interfaces},
 doi = {10.1145/2449396.2449412},
 isbn = {978-1-4503-1965-2},
 keyword = {data integration, hybrid recommender systems, social web, user interfaces, visual knowledge representation},
 link = {http://doi.acm.org/10.1145/2449396.2449412},
 location = {Santa Monica, California, USA},
 numpages = {10},
 pages = {107--116},
 publisher = {ACM},
 series = {IUI '13},
 title = {LinkedVis: Exploring Social and Semantic Career Recommendations},
 year = {2013}
}


@inproceedings{Kim:2013:TRG:2449396.2449401,
 abstract = {With the rapid popularity of smart devices, users are easily and conveniently accessing rich multimedia content. Consequentially, the increasing need for recommender services, from both individual users and groups of users, has arisen. In this paper, we present a graph-based approach to a recommender system that can make recommendations most notably to groups of users. From rating information, we first model a signed graph that contains both positive and negative links between users and items. On this graph we examine two distinct random walks to separately quantify the degree to which a group of users would like or dislike items. We then employ a differential ranking approach for tailoring recommendations to the group. Our empirical evaluations on the MovieLens dataset demonstrate that the proposed group recommendation method performs better than existing alternatives. We also demonstrate the feasibility of Folkommender for smartphones.},
 acmid = {2449401},
 address = {New York, NY, USA},
 author = {Kim, Heung-Nam and Rawashdeh, Majdi and El Saddik, Abdulmotaleb},
 booktitle = {Proceedings of the 2013 International Conference on Intelligent User Interfaces},
 doi = {10.1145/2449396.2449401},
 isbn = {978-1-4503-1965-2},
 keyword = {group recommendation, random walk with restarts, social recommender system},
 link = {http://doi.acm.org/10.1145/2449396.2449401},
 location = {Santa Monica, California, USA},
 numpages = {10},
 pages = {15--24},
 publisher = {ACM},
 series = {IUI '13},
 title = {Tailoring Recommendations to Groups of Users: A Graph Walk-based Approach},
 year = {2013}
}


@inproceedings{Steltenpohl:2013:VTN:2449396.2449450,
 abstract = {Tactile displays can be used without demanding the attention from the human visual system, which makes them attractive for use in wayfinding contexts, where visual attention should be directed at traffic and other information in the environment. To investigate the potential of tactile navigation for cyclists, we have designed and implemented Vibrobelt. This belt, worn around the waist, gives waypoint, distance and endpoint information using directional tactile cues. We evaluated Vibrobelt by comparing it to a visual navigation application. Twenty participants were asked to cycle two routes, each route with a different application. We measured the spatial knowledge acquisition and analyzed the visual focus of the participants. We found that Vibrobelt was successful at guiding all participants to their destinations over an unfamiliar route. Participants using Vibrobelt showed a lower error rate for recognizing images from the route than users of the visual system. Users of the visual system were generally navigating faster, and were better at recalling the route, showing a higher contextual route understanding. The endpoint distance encoding was not always correctly interpreted. Future research will improve Vibrobelt by making a clearer distinction between waypoint and endpoint information, and will test users in more complex navigational situations.},
 acmid = {2449450},
 address = {New York, NY, USA},
 author = {Steltenpohl, Haska and Bouwer, Anders},
 booktitle = {Proceedings of the 2013 International Conference on Intelligent User Interfaces},
 doi = {10.1145/2449396.2449450},
 isbn = {978-1-4503-1965-2},
 keyword = {bicycle traffic safety, navigation support, tactile display, vibrobelt},
 link = {http://doi.acm.org/10.1145/2449396.2449450},
 location = {Santa Monica, California, USA},
 numpages = {10},
 pages = {417--426},
 publisher = {ACM},
 series = {IUI '13},
 title = {Vibrobelt: Tactile Navigation Support for Cyclists},
 year = {2013}
}


@inproceedings{Kratz:2013:CAG:2449396.2449419,
 abstract = {Motivated by the addition of gyroscopes to a large number of new smart phones, we study the effects of combining accelerometer and gyroscope data on the recognition rate of motion gesture recognizers with dimensionality constraints. Using a large data set of motion gestures we analyze results for the following algorithms: Protractor3D, Dynamic Time Warping (DTW) and Regularized Logistic Regression (LR). We chose to study these algorithms because they are relatively easy to implement, thus well suited for rapid prototyping or early deployment during prototyping stages. For use in our analysis, we contribute a method to extend Protractor3D to work with the 6D data obtained by combining accelerometer and gyroscope data. Our results show that combining accelerometer and gyroscope data is beneficial also for algorithms with dimensionality constraints and improves the gesture recognition rate on our data set by up to 4%.},
 acmid = {2449419},
 address = {New York, NY, USA},
 author = {Kratz, Sven and Rohs, Michael and Essl, Georg},
 booktitle = {Proceedings of the 2013 International Conference on Intelligent User Interfaces},
 doi = {10.1145/2449396.2449419},
 isbn = {978-1-4503-1965-2},
 keyword = {accelerometer, gesture recognition, gyroscope, mobile, motion gestures, sensor fusion},
 link = {http://doi.acm.org/10.1145/2449396.2449419},
 location = {Santa Monica, California, USA},
 numpages = {6},
 pages = {173--178},
 publisher = {ACM},
 series = {IUI '13},
 title = {Combining Acceleration and Gyroscope Data for Motion Gesture Recognition Using Classifiers with Dimensionality Constraints},
 year = {2013}
}


