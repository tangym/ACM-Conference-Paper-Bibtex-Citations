@proceedings{Kuflik:2014:2559184,
 abstract = {It is our great pleasure to welcome you to the 2014 International Conference on Intelligent User Interfaces (IUI'14). It is the nineteenth IUI conference, continuing its tradition of being the principal international forum for reporting outstanding research at the intersection of Human-Computer Interaction (HCI) and Artificial Intelligence (AI). The work that appears at IUI bridges these two fields and also delves into related fields, such as psychology, cognitive science, computer graphics, the arts, and many others. Members of the IUI community are interested in improving the symbiosis between humans and computers, and in making systems adapt to humans rather then the other way round. The call for papers attracted 191 submissions from Asia, America Europe, Africa, and Australia. The program committee accepted 46 papers, covering a diverse set of topics, reflected in the session titles "From Touch through Air to Brain" "Learning and Skills", "Intelligent Visual Interaction", "Users and Motion", "Leveraging Social Competencies", "Adaptive User Interfaces" and a special session with papers that honor the memory of John Riedl, who left us too early. A great attraction of the conference is provided by the scientific keynotes: Professor Wolfgang Wahlster opens the conference program with a keynote on "Multiadaptive Interfaces to Cyber-Physical Environments", Professor Noam Tractinsky's second day keynote is on "Visual Aesthetics of Interactive Technologies" and the last day keynote, by Professor Mark Billinghurst is on "Using AR to Create Empathic Experiences". In addition we are pleased to offer an invited talk by a relevant industry speaker, Yanki Margalit: "Startup nation and the Makers revolution. Intelligent user interfaces and the future of the Israeli hi-tech". We also have 11 posters and an excellent demonstration program consisting of 27 demos. In addition, the conference provides four very interesting workshops and a student consortium.},
 address = {New York, NY, USA},
 isbn = {978-1-4503-2729-9},
 location = {Haifa, Israel},
 publisher = {ACM},
 title = {IUI Companion '14: Proceedings of the Companion Publication of the 19th International Conference on Intelligent User Interfaces},
 year = {2014}
}


@inproceedings{Hofmann:2014:CSI:2557500.2557509,
 abstract = {This paper reports experimental results from a driving simulation study in order to compare different speech-based in-car human-machine interface concepts. The effects of the use of a command-based and a conversational in-car speech dialog system on usability and driver distraction are evaluated. Different graphical user interface concepts have been designed in order to investigate their potential supportive or distracting effects. The results show that only few differences concerning speech dialog quality were found when comparing the speech dialog strategies. The command-based dialog was slightly better accepted than the conversational dialog, which can be attributed to the limited performance of the system's language understanding component. No differences in driver distraction were revealed. Moreover, the study revealed that speech dialog systems without graphical user interface were accepted by participants in the driving environment and that the use of a graphical user interface impaired the driving performance and increased gaze-based distraction. In the driving scenario, the choice of speech dialog strategies does not have a strong influence on usability and no influence on driver distraction. Instead, when designing the graphical user interface of an in-car speech dialog systems, developers should consider reducing the content presented on the display device in order to reduce driver distraction.},
 acmid = {2557509},
 address = {New York, NY, USA},
 author = {Hofmann, Hansj\"{o}rg and Tobisch, Vanessa and Ehrlich, Ute and Berton, Andr{\'e} and Mahr, Angela},
 booktitle = {Proceedings of the 19th International Conference on Intelligent User Interfaces},
 doi = {10.1145/2557500.2557509},
 isbn = {978-1-4503-2184-6},
 keyword = {driver distraction, speech dialog systems, user study},
 link = {http://doi.acm.org/10.1145/2557500.2557509},
 location = {Haifa, Israel},
 numpages = {10},
 pages = {215--224},
 publisher = {ACM},
 series = {IUI '14},
 title = {Comparison of Speech-based In-car HMI Concepts in a Driving Simulation Study},
 year = {2014}
}


@inproceedings{Denaux:2014:ELD:2557500.2557529,
 abstract = {Intercultural competence is an essential 21st Century skill. A key issue for developers of cross-cultural training simulators is the need to provide relevant learning experience adapted to the learnerfis abilities. This paper presents a dialogic approach for a quick assessment of the depth of a learner's current intercultural awareness as part of the EU ImREAL project. To support the dialogue, Linked Data is seen as a rich knowledge base for a diverse range of resources on cultural aspects. This paper investigates how semantic technologies could be used to: (a) extract a pool of concrete culturally-relevant facts from DBpedia that can be linked to various cultural groups and to the learner, (b) model a learner's knowledge on a selected set of cultural themes and (c) provide a novel, adaptive and user-friendly, user modelling dialogue for cultural awareness. The usability and usefulness of the approach is evaluated by CrowdFlower and Expert Inspection.},
 acmid = {2557529},
 address = {New York, NY, USA},
 author = {Denaux, Ronald and Dimitrova, Vania and Lau, Lydia and Brna, Paul and Thakker, Dhaval and Steiner, Christina},
 booktitle = {Proceedings of the 19th International Conference on Intelligent User Interfaces},
 doi = {10.1145/2557500.2557529},
 isbn = {978-1-4503-2184-6},
 keyword = {cultural awareness, dialogue system, learning simulator., linked data, user modelling},
 link = {http://doi.acm.org/10.1145/2557500.2557529},
 location = {Haifa, Israel},
 numpages = {6},
 pages = {241--246},
 publisher = {ACM},
 series = {IUI '14},
 title = {Employing Linked Data and Dialogue for Modelling Cultural Awareness of a User},
 year = {2014}
}


@inproceedings{Cartwright:2014:MRA:2557500.2557530,
 abstract = {A typical audio mixer interface consists of faders and knobs that control the amplitude level as well as processing (e.g. equalization, compression and reverberation) parameters of individual tracks. This interface, while widely used and effective for optimizing a mix, may not be the best interface to facilitate exploration of different mixing options. In this work, we rethink the mixer interface, describing an alternative interface for exploring the space of possible mixes of four audio tracks. In a user study with 24 participants, we compared the effectiveness of this interface to the traditional paradigm for exploring alternative mixes. In the study, users responded that the proposed alternative interface facilitated exploration and that they considered the process of rating mixes to be beneficial.},
 acmid = {2557530},
 address = {New York, NY, USA},
 author = {Cartwright, Mark and Pardo, Bryan and Reiss, Josh},
 booktitle = {Proceedings of the 19th International Conference on Intelligent User Interfaces},
 doi = {10.1145/2557500.2557530},
 isbn = {978-1-4503-2184-6},
 keyword = {audio, exploratory interfaces, mixing, music},
 link = {http://doi.acm.org/10.1145/2557500.2557530},
 location = {Haifa, Israel},
 numpages = {6},
 pages = {365--370},
 publisher = {ACM},
 series = {IUI '14},
 title = {MIXPLORATION: Rethinking the Audio Mixer Interface},
 year = {2014}
}


@inproceedings{Lampe:2014:BIH:2557500.2557533,
 abstract = {We present an Internet-based brain-computer interface (BCI) for controlling an intelligent robotic device with autonomous reinforcement-learning. BCI control was achieved through dry-electrode electroencephalography (EEG) obtained during imaginary movements. Rather than using low-level direct motor control, we employed a high-level control scheme of the robot, acquired via reinforcement learning, to keep the users cognitive load low while allowing control a reaching-grasping task with multiple degrees of freedom. High-level commands were obtained by classification of EEG responses using an artificial neural network approach utilizing time-frequency features and conveyed through an intuitive user interface. The novel ombination of a rapidly operational dry electrode setup, autonomous control and Internet connectivity made it possible to conveniently interface subjects in an EEG laboratory with remote robotic devices in a closed-loop setup with online visual feedback of the robots actions to the subject. The same approach is also suitable to provide home-bound patients with the possibility to control state-of-the-art robotic devices currently confined to a research environment. Thereby, our BCI approach could help severely paralyzed patients by facilitating patient-centered research of new means of communication, mobility and independence.},
 acmid = {2557533},
 address = {New York, NY, USA},
 author = {Lampe, Thomas and Fiederer, Lukas D.J. and Voelker, Martin and Knorr, Alexander and Riedmiller, Martin and Ball, Tonio},
 booktitle = {Proceedings of the 19th International Conference on Intelligent User Interfaces},
 doi = {10.1145/2557500.2557533},
 isbn = {978-1-4503-2184-6},
 keyword = {camera-based uis, machine learning and data mining, robots, semi-autonomous systems},
 link = {http://doi.acm.org/10.1145/2557500.2557533},
 location = {Haifa, Israel},
 numpages = {6},
 pages = {83--88},
 publisher = {ACM},
 series = {IUI '14},
 title = {A Brain-computer Interface for High-level Remote Control of an Autonomous, Reinforcement-learning-based Robotic System for Reaching and Grasping},
 year = {2014}
}


@inproceedings{Wang:2014:HRM:2557500.2557514,
 abstract = {One of the difficulties with standard route maps is accessing to multi-scale routing information. The user needs to display maps in both a large scale to see details and a small scale to see an overview, but this requires tedious interaction such as zooming in and out. We propose to use a hierarchical structure for a route map, called a "Route Tree", to address this problem, and describe an algorithm to automatically construct such a structure. A Route Tree is a hierarchical grouping of all small route segments to allow quick access to meaningful large and small-scale views. We propose two Route Tree applications, "RouteZoom" for interactive map browsing and "TreePrint" for route information printing, to show the applicability and usability of the structure. We conducted a preliminary user study on RouteZoom, and the results showed that RouteZoom significantly lowers the interaction cost for obtaining information from a map compared to a traditional interactive map.},
 acmid = {2557514},
 address = {New York, NY, USA},
 author = {Wang, Fangzhou and Li, Yang and Sakamoto, Daisuke and Igarashi, Takeo},
 booktitle = {Proceedings of the 19th International Conference on Intelligent User Interfaces},
 doi = {10.1145/2557500.2557514},
 isbn = {978-1-4503-2184-6},
 keyword = {multi-scale navigation, route map visualization, view extraction},
 link = {http://doi.acm.org/10.1145/2557500.2557514},
 location = {Haifa, Israel},
 numpages = {10},
 pages = {169--178},
 publisher = {ACM},
 series = {IUI '14},
 title = {Hierarchical Route Maps for Efficient Navigation},
 year = {2014}
}


@inproceedings{Sanchez:2014:AHV:2557500.2557519,
 abstract = {Interactive digital technologies are currently being developed as a novel tool for education and skill development. Audiopolis is an audio and haptic based videogame designed for developing orientation and mobility (O&M) skills in people who are blind. We have evaluated the cognitive impact of videogame play on O&M skills by assessing performance on a series of behavioral tasks carried out in both indoor and outdoor virtual spaces. Our results demonstrate that the use of Audiopolis had a positive impact on the development and use of O&M skills in school-aged learners who are blind. The impact of audio and haptic information on learning is also discussed.},
 acmid = {2557519},
 address = {New York, NY, USA},
 author = {S\'{a}nchez, Jaime and de Borba Campos, Marcia and Espinoza, Mat\'{\i}as and Merabet, Lotfi B.},
 booktitle = {Proceedings of the 19th International Conference on Intelligent User Interfaces},
 doi = {10.1145/2557500.2557519},
 isbn = {978-1-4503-2184-6},
 keyword = {haptic and audio interfaces, mobility, navigation, orientation, people who are blind},
 link = {http://doi.acm.org/10.1145/2557500.2557519},
 location = {Haifa, Israel},
 numpages = {10},
 pages = {199--208},
 publisher = {ACM},
 series = {IUI '14},
 title = {Audio Haptic Videogaming for Developing Wayfinding Skills in Learners Who Are Blind},
 year = {2014}
}


@inproceedings{Lee:2014:RTA:2557500.2557502,
 abstract = {There has been much effort on studying how social media sites, such as Twitter, help propagate information in different situations, including spreading alerts and SOS messages in an emergency. However, existing work has not addressed how to actively identify and engage the right strangers at the right time on social media to help effectively propagate intended information within a desired time frame. To ad-dress this problem, we have developed two models: (i) a feature-based model that leverages peoplesfi exhibited social behavior, including the content of their tweets and social interactions, to characterize their willingness and readiness to propagate information on Twitter via the act of retweeting; and (ii) a wait-time model based on a user's previous retweeting wait times to predict her next retweeting time when asked. Based on these two models, we build a recommender system that predicts the likelihood of a stranger to retweet information when asked, within a specific time window, and recommends the top-N qualified strangers to engage with. Our experiments, including live studies in the real world, demonstrate the effectiveness of our work.},
 acmid = {2557502},
 address = {New York, NY, USA},
 author = {Lee, Kyumin and Mahmud, Jalal and Chen, Jilin and Zhou, Michelle and Nichols, Jeffrey},
 booktitle = {Proceedings of the 19th International Conference on Intelligent User Interfaces},
 doi = {10.1145/2557500.2557502},
 isbn = {978-1-4503-2184-6},
 keyword = {personality, retweet, social media, twitter, willingness},
 link = {http://doi.acm.org/10.1145/2557500.2557502},
 location = {Haifa, Israel},
 numpages = {10},
 pages = {247--256},
 publisher = {ACM},
 series = {IUI '14},
 title = {Who Will Retweet This?: Automatically Identifying and Engaging Strangers on Twitter to Spread Information},
 year = {2014}
}


@inproceedings{Freitas:2014:NLQ:2557500.2557534,
 abstract = {The demand to access large amounts of heterogeneous structured data is emerging as a trend for many users and applications. However, the effort involved in querying heterogeneous and distributed third-party databases can create major barriers for data consumers. At the core of this problem is the semantic gap between the way users express their information needs and the representation of the data. This work aims to provide a natural language interface and an associated semantic index to support an increased level of vocabulary independency for queries over Linked Data/Semantic Web datasets, using a distributional-compositional semantics approach. Distributional semantics focuses on the automatic construction of a semantic model based on the statistical distribution of co-occurring words in large-scale texts. The proposed query model targets the following features: (i) a principled semantic approximation approach with low adaptation effort (independent from manually created resources such as ontologies, thesauri or dictionaries), (ii) comprehensive semantic matching supported by the inclusion of large volumes of distributional (unstructured) commonsense knowledge into the semantic approximation process and (iii) expressive natural language queries. The approach is evaluated using natural language queries on an open domain dataset and achieved avg. recall=0.81, mean avg. precision=0.62 and mean reciprocal rank=0.49.},
 acmid = {2557534},
 address = {New York, NY, USA},
 author = {Freitas, Andre and Curry, Edward},
 booktitle = {Proceedings of the 19th International Conference on Intelligent User Interfaces},
 doi = {10.1145/2557500.2557534},
 isbn = {978-1-4503-2184-6},
 keyword = {databases, distributional semantics, linked data, natural language interface, question answering, semantic interface, semantic search, semantic web},
 link = {http://doi.acm.org/10.1145/2557500.2557534},
 location = {Haifa, Israel},
 numpages = {10},
 pages = {279--288},
 publisher = {ACM},
 series = {IUI '14},
 title = {Natural Language Queries over Heterogeneous Linked Data Graphs: A Distributional-compositional Semantics Approach},
 year = {2014}
}


@inproceedings{Schiavo:2014:OSS:2557500.2557507,
 abstract = {In this paper, we present a system that acts as an automatic facilitator by supporting the flow of communication in a group conversation activity. The system monitors the group members' non-verbal behavior and promotes balanced participation, giving targeted directives to the participants through peripheral displays. We describe an initial study to compare two ways of influencing participantsfi social dynamics: overt directives, explicit recommendations of social actions displayed in the form of text; or subtle directives, where the same recommendations are provided in an implicit manner. Our study indicates that, when the participants understand how the implicit messages work, the subtle facilitation is regarded as more useful than the overt one and it is considered to more positively influence the group behavior.},
 acmid = {2557507},
 address = {New York, NY, USA},
 author = {Schiavo, Gianluca and Cappelletti, Alessandro and Mencarini, Eleonora and Stock, Oliviero and Zancanaro, Massimo},
 booktitle = {Proceedings of the 19th International Conference on Intelligent User Interfaces},
 doi = {10.1145/2557500.2557507},
 isbn = {978-1-4503-2184-6},
 keyword = {conversation support, implicit interaction, persuasive technologies, social dynamics, visual attention},
 link = {http://doi.acm.org/10.1145/2557500.2557507},
 location = {Haifa, Israel},
 numpages = {10},
 pages = {225--234},
 publisher = {ACM},
 series = {IUI '14},
 title = {Overt or Subtle?  Supporting Group Conversations with Automatically Targeted Directives},
 year = {2014}
}


@inproceedings{Martinez-Gomez:2014:RUL:2557500.2557546,
 abstract = {The reading act is an intimate and elusive process that is important to understand. Psycholinguists have long studied the effects of task, personal or document characteristics on reading behavior. An essential factor in the success of those studies lies in the capability of analyzing eye-movements. These studies aim to recognize causal effects on patterns of eye-movements, by contriving variations in task, personal or document characteristics. In this work, we follow the opposite direction. We present a formal framework to recognize reader's level of understanding and language skill given measurements of reading behavior via eye-gaze data. We show significant error reductions to recognize these attributes and provide a detailed study of the most discriminative features.},
 acmid = {2557546},
 address = {New York, NY, USA},
 author = {Mart\'{\i}nez-G\'{o}mez, Pascual and Aizawa, Akiko},
 booktitle = {Proceedings of the 19th International Conference on Intelligent User Interfaces},
 doi = {10.1145/2557500.2557546},
 isbn = {978-1-4503-2184-6},
 keyword = {cognitive modeling, eye-tracking, reading behavior, user profiling},
 link = {http://doi.acm.org/10.1145/2557500.2557546},
 location = {Haifa, Israel},
 numpages = {10},
 pages = {95--104},
 publisher = {ACM},
 series = {IUI '14},
 title = {Recognition of Understanding Level and Language Skill Using Measurements of Reading Behavior},
 year = {2014}
}


@inproceedings{Pittman:2014:EHT:2557500.2557527,
 abstract = {We explore the capabilities of head tracking combined with head mounted displays (HMD) as an input modality for robot navigation. We use a Parrot AR Drone to test five techniques which include metaphors for plane-like banking control, car-like turning control and virtual reality-inspired translation and rotation schemes which we compare with a more traditional game controller interface. We conducted a user study to observe the effectiveness of each of the interfaces we developed in navigating through a number of archways in an indoor course. We examine a number of qualitative and quantitative metrics to determine performance and preference among each metaphor. Our results show an appreciation for head rotation based controls over other head gesture techniques, with the classic controller being preferred overall. We discuss possible shortcomings with head tracked HMDs as a primary input method as well as propose improved metaphors that alleviate some of these drawbacks.},
 acmid = {2557527},
 address = {New York, NY, USA},
 author = {Pittman, Corey and LaViola,Jr., Joseph J.},
 booktitle = {Proceedings of the 19th International Conference on Intelligent User Interfaces},
 doi = {10.1145/2557500.2557527},
 isbn = {978-1-4503-2184-6},
 keyword = {3d interaction, robots, user studies},
 link = {http://doi.acm.org/10.1145/2557500.2557527},
 location = {Haifa, Israel},
 numpages = {6},
 pages = {323--328},
 publisher = {ACM},
 series = {IUI '14},
 title = {Exploring Head Tracked Head Mounted Displays for First Person Robot Teleoperation},
 year = {2014}
}


@inproceedings{Kotoulas:2014:ICI:2557500.2557538,
 abstract = {We present an approach to access and consolidate complex information spanning multiple specialist domains and make it available to non-experts. We are using a combination of business rules and contextual exploration to reduce interface complexity and improve consumability. We present a use case and a prototype on top of a real-world enterprise solution for coordinating Social care and Health care. We evaluate our system through a user study. Our results indicate that our approach reduces the time required to obtain business results compared to a baseline graph exploration approach.},
 acmid = {2557538},
 address = {New York, NY, USA},
 author = {Kotoulas, Spyros and Lopez, Vanessa and Sbodio, Marco Luca and Tommasi, Pierpaolo and Stephenson, Martin and Mac Aonghusa, Pol},
 booktitle = {Proceedings of the 19th International Conference on Intelligent User Interfaces},
 doi = {10.1145/2557500.2557538},
 isbn = {978-1-4503-2184-6},
 keyword = {care coordination, linked data, semantic web},
 link = {http://doi.acm.org/10.1145/2557500.2557538},
 location = {Haifa, Israel},
 numpages = {6},
 pages = {347--352},
 publisher = {ACM},
 series = {IUI '14},
 title = {Improving Cross-domain Information Sharing in Care Coordination Using Semantic Web Technologies},
 year = {2014}
}


@inproceedings{Parra:2014:SYW:2557500.2557542,
 abstract = {Research in recommender systems has traditionally focused on improving the predictive accuracy of recommendations by developing new algorithms or by incorporating new sources of data. However, several studies have shown that accuracy does not always correlate with a better user experience, leading to recent research that puts emphasis on Human-Computer Interaction in order to investigate aspects of the interface and user characteristics that influence the user experience on recommender systems. Following this new research this paper presents SetFusion, a visual user-controllable interface for hybrid recommender system. Our approach enables users to manually fuse and control the importance of recommender strategies and to inspect the fusion results using an interactive Venn diagram visualization. We analyze the results of two field studies in the context of a conference talk recommendation system, performed to investigate the effect of user controllability in a hybrid recommender. Behavioral analysis and subjective evaluation indicate that the proposed controllable interface had a positive effect on the user experience.},
 acmid = {2557542},
 address = {New York, NY, USA},
 author = {Parra, Denis and Brusilovsky, Peter and Trattner, Christoph},
 booktitle = {Proceedings of the 19th International Conference on Intelligent User Interfaces},
 doi = {10.1145/2557500.2557542},
 isbn = {978-1-4503-2184-6},
 keyword = {human factors, recommender systems, setfusion, user interfaces, user studies},
 link = {http://doi.acm.org/10.1145/2557500.2557542},
 location = {Haifa, Israel},
 numpages = {6},
 pages = {235--240},
 publisher = {ACM},
 series = {IUI '14},
 title = {See What You Want to See: Visual User-driven Approach for Hybrid Recommendation},
 year = {2014}
}


@inproceedings{Lai:2014:EES:2557500.2557539,
 abstract = {In this paper, we present Expediting Expertise, a system designed to provide structured support to the otherwise informal process of social learning in the enterprise. It employs a data-driven approach where online content is automatically analyzed and categorized into relevant topics, topic-specific user expertise is calculated by comparing the models of individual users against those of the experts, and personalized recommendation of learning activities is created accordingly to facilitate expertise development. The system's UI is designed to provide users with ongoing feedback of current expertise, progress, and comparison with others. Learning recommendation is visualized with an interactive treemap which presents estimated return on investment and distance to current expertise for each recommended learning activity. Evaluation of the system showed very positive results.},
 acmid = {2557539},
 address = {New York, NY, USA},
 author = {Lai, Jennifer and Lu, Jie and Pan, Shimei and Soroker, Danny and Topkara, Mercan and Weisz, Justin and Boston, Jeff and Crawford, Jason},
 booktitle = {Proceedings of the 19th International Conference on Intelligent User Interfaces},
 doi = {10.1145/2557500.2557539},
 isbn = {978-1-4503-2184-6},
 keyword = {assessment, expertise, informal, learning, personalized, recommendation., social},
 link = {http://doi.acm.org/10.1145/2557500.2557539},
 location = {Haifa, Israel},
 numpages = {10},
 pages = {133--142},
 publisher = {ACM},
 series = {IUI '14},
 title = {Expediting Expertise: Supporting Informal Social Learning in the Enterprise},
 year = {2014}
}


@inproceedings{Keck:2014:ECS:2557500.2557536,
 abstract = {Rapid growth in the number of measures available to describe customer-organization relationships is presenting a serious challenge for Business Intelligence (BI) interface developers as they attempt to provide business users with key customer information without requiring users to painstakingly sift through many interface windows and layers. In this paper we introduce a prototype Intelligent User Interface that we have deployed to partially address this issue. The interface builds on machine learning techniques to construct a ranking model of Key Performance Indicators (KPIs) that are used to select and present the most important customer metrics that can be made available to business users in time critical environments. We provide an overview of the prototype application, the underlying models used for KPI selection, and a comparative evaluation of machine learning and closed form solutions to the ranking and selection problems. Results show that the machine learning based method outperformed the closed form solution with a 66.5% accuracy rate on multi-label attribution in comparison to 54.1% for the closed form solution.},
 acmid = {2557536},
 address = {New York, NY, USA},
 author = {Keck, Ingo R. and Ross, Robert J.},
 booktitle = {Proceedings of the 19th International Conference on Intelligent User Interfaces},
 doi = {10.1145/2557500.2557536},
 isbn = {978-1-4503-2184-6},
 keyword = {data analytics, information filtering, machine learning},
 link = {http://doi.acm.org/10.1145/2557500.2557536},
 location = {Haifa, Israel},
 numpages = {6},
 pages = {341--346},
 publisher = {ACM},
 series = {IUI '14},
 title = {Exploring Customer Specific KPI Selection Strategies for an Adaptive Time Critical User Interface},
 year = {2014}
}


@inproceedings{Billinghurst:2014:UAR:2557500.2568057,
 abstract = {Intelligent user interfaces have traditionally been used to create systems that respond intelligently to user input. However there is a recent trend towards Empathic Interfaces that are designed to go beyond understanding user input and to recognize emotional state and user feelings. In this presentation we explore how Augmented Reality (AR) can be used to convey that emotional state and so allow users to capture and share emotional experiences. In this way AR not only overlays virtual imagery on the real world, but also can create deeper understanding of user's experience at particular locations and points in time. The recent emergence of truly wearable systems, such as Google Glass, provide a platform for Empathic Communication using AR. Examples will be shown from research conducted at the HIT Lab NZ and other research organizations, and key areas for future research described.},
 acmid = {2568057},
 address = {New York, NY, USA},
 author = {Billinghurst, Mark},
 booktitle = {Proceedings of the 19th International Conference on Intelligent User Interfaces},
 doi = {10.1145/2557500.2568057},
 isbn = {978-1-4503-2184-6},
 keyword = {augmented reality, collaboration, empathic computing},
 link = {http://doi.acm.org/10.1145/2557500.2568057},
 location = {Haifa, Israel},
 numpages = {2},
 pages = {5--6},
 publisher = {ACM},
 series = {IUI '14},
 title = {Using Augmented Reality to Create Empathic Experiences},
 year = {2014}
}


@inproceedings{Luo:2014:GAG:2557500.2557531,
 abstract = {On top of an enterprise social platform, we are building a smart social QA system that automatically routes questions to suitable employees who are willing, able, and ready to provide answers. Due to a lack of social QA history (training data) to start with, in this paper, we present an optimization-based approach that recommends both top-matched active (seed) and inactive (prospect) answerers for a given question. Our approach includes three parts. First, it uses a predictive model to find top-ranked seed answerers by their fitness, including their ability and willingness, to answer a question. Second, it uses distance metric learning to discover prospects most similar to the seeds identified in the first step. Third, it uses a constraint-based approach to balance the selection of both seeds and prospects identified in the first two steps. As a result, not only does our solution route questions to top-matched active users, but it also engages inactive users to grow the pool of answerers. Our real-world experiments that routed 114 questions to 684 people identified from 400,000+ employees included 641 prospects (93.7%) and achieved about 70% answering rate with 83% of answers received a lot/full confidence.},
 acmid = {2557531},
 address = {New York, NY, USA},
 author = {Luo, Lin and Wang, Fei and Zhou, Michelle X. and Pan, Yingxin and Chen, Hang},
 booktitle = {Proceedings of the 19th International Conference on Intelligent User Interfaces},
 doi = {10.1145/2557500.2557531},
 isbn = {978-1-4503-2184-6},
 keyword = {answerer recommendation, question routing, social qa},
 link = {http://doi.acm.org/10.1145/2557500.2557531},
 location = {Haifa, Israel},
 numpages = {10},
 pages = {7--16},
 publisher = {ACM},
 series = {IUI '14},
 title = {Who Have Got Answers?: Growing the Pool of Answerers in a Smart Enterprise Social QA System},
 year = {2014}
}


@inproceedings{Wu:2014:MUE:2557500.2557523,
 abstract = {Programming by example enables users to transform data formats without coding. To be practical, the method must synthesize the correct transformation with minimal user input. We present a method that minimizes user effort by color-coding the transformation result and recommending specific records where the user should provide examples. Simulation results and a user study show that our method significantly reduces user effort and increases the success rate for synthesizing correct transformation programs by example.},
 acmid = {2557523},
 address = {New York, NY, USA},
 author = {Wu, Bo and Szekely, Pedro and Knoblock, Craig A.},
 booktitle = {Proceedings of the 19th International Conference on Intelligent User Interfaces},
 doi = {10.1145/2557500.2557523},
 isbn = {978-1-4503-2184-6},
 keyword = {data transformation, programming by example, recommendation},
 link = {http://doi.acm.org/10.1145/2557500.2557523},
 location = {Haifa, Israel},
 numpages = {6},
 pages = {317--322},
 publisher = {ACM},
 series = {IUI '14},
 title = {Minimizing User Effort in Transforming Data by Example},
 year = {2014}
}


@inproceedings{Kim:2014:ALI:2557500.2557505,
 abstract = {Today, people use a computer almost everywhere. At the same time, they still do their work in the old-fashioned way, such as using a pen and paper. A pen is often used in many fields because it is easy to use and familiar. On the other hand, however, it is a quite inconvenient because the information printed on paper is static. If digital features are added to this paper environment, the users can do their work more easily and efficiently. AR (augmented reality) Lamp is a stand-type projector and camera embedded system with the form factor of a desk lamp. Its users can modify the virtually augmented content on top of the paper with seamlessly combined virtual and physical worlds. AR is quite appealing, but it is difficult to popularize due to the lack of interaction. In this paper, the interaction methods that people can use easily and intuitively are focused on. A high-fidelity prototype of the system is presented, and a set of novel interactions is demonstrated. A pilot evaluation of the system is also reported to explore its usage possibility.},
 acmid = {2557505},
 address = {New York, NY, USA},
 author = {Kim, Jeongyun and Seo, Jonghoon and Han, Tack-Don},
 booktitle = {Proceedings of the 19th International Conference on Intelligent User Interfaces},
 doi = {10.1145/2557500.2557505},
 isbn = {978-1-4503-2184-6},
 keyword = {bimanual interaction, finger gesture, pen computing, projection-based augmented reality},
 link = {http://doi.acm.org/10.1145/2557500.2557505},
 location = {Haifa, Israel},
 numpages = {6},
 pages = {353--358},
 publisher = {ACM},
 series = {IUI '14},
 title = {AR Lamp: Interactions on Projection-based Augmented Reality for Interactive Learning},
 year = {2014}
}


@inproceedings{Huang:2014:ALI:2557500.2557544,
 abstract = {Typical synthesizers only provide controls to the low-level parameters of sound-synthesis, such as wave-shapes or filter envelopes. In contrast, composers often want to adjust and express higher-level qualities, such as how "scary" or "steady" sounds are perceived to be. We develop a system which allows users to directly control abstract, high-level qualities of sounds. To do this, our system learns functions that map from synthesizer control settings to perceived levels of high-level qualities. Given these functions, our system can generate high-level knobs that directly adjust sounds to have more or less of those qualities. We model the functions mapping from control-parameters to the degree of each high-level quality using Gaussian processes, a nonparametric Bayesian model. These models can adjust to the complexity of the function being learned, account for nonlinear interaction between control-parameters, and allow us to characterize the uncertainty about the functions being learned. By tracking uncertainty about the functions being learned, we can use active learning to quickly calibrate the tool, by querying the user about the sounds the system expects to most improve its performance. We show through simulations that this model-based active learning approach learns high-level knobs on certain classes of target concepts faster than several baselines, and give examples of the resulting automatically- constructed knobs which adjust levels of non-linear, high- level concepts.},
 acmid = {2557544},
 address = {New York, NY, USA},
 author = {Huang, Cheng-Zhi Anna and Duvenaud, David and Arnold, Kenneth C. and Partridge, Brenton and Oberholtzer, Josiah W. and Gajos, Krzysztof Z.},
 booktitle = {Proceedings of the 19th International Conference on Intelligent User Interfaces},
 doi = {10.1145/2557500.2557544},
 isbn = {978-1-4503-2184-6},
 keyword = {active learning, gaussian processes., intelligent interactive systems, intuitive control knobs, preference learning, sound design, sound synthesis, synthesizers, user interfaces},
 link = {http://doi.acm.org/10.1145/2557500.2557544},
 location = {Haifa, Israel},
 numpages = {10},
 pages = {115--124},
 publisher = {ACM},
 series = {IUI '14},
 title = {Active Learning of Intuitive Control Knobs for Synthesizers Using Gaussian Processes},
 year = {2014}
}


@inproceedings{Lieberman:2014:SML:2557500.2557543,
 abstract = {How can a new user learn an unfamiliar application, especially if it is a high-functionality (hi-fun) application, like Photoshop, Excel, or programming language IDEfi Many applications provide introductory videos, illustrative examples, and documentation on individual operations. Tests show, however, that novice users are likely to ignore the provided help, and try to learn by exploring the application first. In a hi-fun application, though, the user may lack understanding of the basic concepts of an application's operation, even though they were likely explained in the (ignored) documentation. This paper introduces steptorials ("stepper tutorials"), a new interaction strategy for learning hi-fun applications. A steptorial aims to teach the user how to work through a simple, but nontrivial, example of using the application. Steptorials are unique because they allow varying the autonomy of the user at every step. A steptorial has a control structure of a reversible programming language stepper. The user may choose, at any time, to be shown how to do a step, be guided through it, use the application interface without constraint, or to return to a previous step. It reduces the risk in either trying new operations yourself, or conversely, the risk of ceding control to the computer. It introduces a new paradigm of mixed-initiative learning of application interfaces.},
 acmid = {2557543},
 address = {New York, NY, USA},
 author = {Lieberman, Henry and Rosenzweig, Elizabeth and Fry, Christopher},
 booktitle = {Proceedings of the 19th International Conference on Intelligent User Interfaces},
 doi = {10.1145/2557500.2557543},
 isbn = {978-1-4503-2184-6},
 keyword = {documentation, help, program stepper, steptorials, tutorials},
 link = {http://doi.acm.org/10.1145/2557500.2557543},
 location = {Haifa, Israel},
 numpages = {6},
 pages = {359--364},
 publisher = {ACM},
 series = {IUI '14},
 title = {Steptorials: Mixed-initiative Learning of High-functionality Applications},
 year = {2014}
}


@inproceedings{Roemmele:2014:TCD:2557500.2557510,
 abstract = {Humans have a remarkable tendency to anthropomorphize moving objects, ascribing to them intentions and emotions as if they were human. Early social psychology research demonstrated that animated film clips depicting the movements of simple geometric shapes could elicit rich interpretations of intentional behavior from viewers. In attempting to model this reasoning process in software, we first address the problem of automatically recognizing humanlike actions in the trajectories of moving shapes. There are two main difficulties. First, there is no defined vocabulary of actions that are recognizable to people from motion trajectories. Second, in order for an automated system to learn actions from motion trajectories using machine-learning techniques, a vast amount of these action- trajectory pairs is needed as training data. This paper describes an approach to data collection that resolves both of these problems. In a web-based game, called Triangle Charades, players create motion trajectories for actions by animating a triangle to depict those actions. Other players view these animations and guess the action they depict. An action is considered recognizable if players can correctly guess it from animations. To move towards defining a controlled vocabulary and collecting a large dataset, we conducted a pilot study in which 87 users played Triangle Charades. Based on this data, we computed a simple metric for action recognizability. Scores on this metric formed a gradual linear pattern, suggesting there is no clear cutoff for determining if an action is recognizable from motion data. These initial results demonstrate the advantages of using a game to collect data for this action recognition task.},
 acmid = {2557510},
 address = {New York, NY, USA},
 author = {Roemmele, Melissa and Archer-McClellan, Haley and Gordon, Andrew S.},
 booktitle = {Proceedings of the 19th International Conference on Intelligent User Interfaces},
 doi = {10.1145/2557500.2557510},
 isbn = {978-1-4503-2184-6},
 keyword = {animation, crowdsourcing, games and play},
 link = {http://doi.acm.org/10.1145/2557500.2557510},
 location = {Haifa, Israel},
 numpages = {6},
 pages = {209--214},
 publisher = {ACM},
 series = {IUI '14},
 title = {Triangle Charades: A Data-collection Game for Recognizing Actions in Motion Trajectories},
 year = {2014}
}


@inproceedings{Wan:2014:IGS:2557500.2557513,
 abstract = {Social media is an invaluable source of feedback not just about consumer products and services but also about the effectiveness of government services. Our aim is to help analysts identify how government services can be improved based on citizen-contributed feedback found in publicly available social media. We present ongoing research for a social media monitoring interactive prototype with federated search and text analysis functionality. The prototype, developed to fit the workflow of social media monitors in the government sector, collects, analyses, and provides overviews of social media content. It facilitates relevance judgements on specific social media posts to decide whether or not to engage online. Our user log analysis validates the original design requirements and indicates ongoing utility to our federated search approach.},
 acmid = {2557513},
 address = {New York, NY, USA},
 author = {Wan, Stephen and Paris, C{\'e}cile},
 booktitle = {Proceedings of the 19th International Conference on Intelligent User Interfaces},
 doi = {10.1145/2557500.2557513},
 isbn = {978-1-4503-2184-6},
 keyword = {egovernment, federated search, natural language processing, social media monitoring},
 link = {http://doi.acm.org/10.1145/2557500.2557513},
 location = {Haifa, Israel},
 numpages = {10},
 pages = {27--36},
 publisher = {ACM},
 series = {IUI '14},
 title = {Improving Government Services with Social Media Feedback},
 year = {2014}
}


@inproceedings{Toker:2014:TFU:2557500.2557524,
 abstract = {A key challenge for information visualization designers lies in developing systems that best support users in terms of their individual abilities, needs, and preferences. However, most visualizations require users to first gather a certain set of skills before they can efficiently process the displayed information. This paper presents a first step towards designing visualizations that provide personalized support in order to ease the so-called 'learning curve' during a user's skill acquisition phase. We present prediction models, trained on users' gaze data, that can identify if users are still in the skill acquisition phase or if they have gained the necessary abilities. The paper first reveals that users exhibit the learning curve even during the usage of simple information visualizations, and then shows that we can generate reasonably accurate predictions about a user's skill acquisition using solely their eye gaze behavior.},
 acmid = {2557524},
 address = {New York, NY, USA},
 author = {Toker, Dereck and Steichen, Ben and Gingerich, Matthew and Conati, Cristina and Carenini, Giuseppe},
 booktitle = {Proceedings of the 19th International Conference on Intelligent User Interfaces},
 doi = {10.1145/2557500.2557524},
 isbn = {978-1-4503-2184-6},
 keyword = {adaptation, eye-tracking, information visualization, machine learning, skill acquisition},
 link = {http://doi.acm.org/10.1145/2557500.2557524},
 location = {Haifa, Israel},
 numpages = {10},
 pages = {105--114},
 publisher = {ACM},
 series = {IUI '14},
 title = {Towards Facilitating User Skill Acquisition: Identifying Untrained Visualization Users Through Eye Tracking},
 year = {2014}
}


@inproceedings{Xie:2014:LSP:2557500.2557504,
 abstract = {Location-based systems are becoming more popular with the explosive growth in popularity of smart phones. However, the user adoption of these systems is hindered by growing user concerns about privacy. To design better location-based systems that attract more user adoption and protect users from information under/overexposure, it is highly desirable to understand users' location sharing and privacy preferences. This paper makes two main contributions. First, by studying users' location sharing privacy preferences with three groups of people (i.e., Family, Friend and Colleague) in different contexts, including check-in time, companion and emotion, we reveal that location sharing behaviors are highly dynamic, context-aware, audience-aware and personal. In particular, we find that emotion and companion are good contextual predictors of privacy preferences. Moreover, we find that there are strong similarities or correlations among contexts and groups. Our second contribution is to show, in light of the user study, that despite the dynamic and context-dependent nature of location sharing, it is still possible to predict a user's in-situ sharing preference in various contexts. More specifically, we explore whether it is possible to give users a personalized recommendation of the sharing setting they are most likely to prefer, based on context similarity, group correlation and collective check-in preference. PPRec, the proposed recommendation algorithm that incorporates the above three elements, delivers personalized recommendations that could be helpful to reduce both user's burden and privacy risk. It also provides additional insights into the relative usefulness of different personal and contextual factors in predicting users' sharing behavior.},
 acmid = {2557504},
 address = {New York, NY, USA},
 author = {Xie, Jierui and Knijnenburg, Bart Piet and Jin, Hongxia},
 booktitle = {Proceedings of the 19th International Conference on Intelligent User Interfaces},
 doi = {10.1145/2557500.2557504},
 isbn = {978-1-4503-2184-6},
 keyword = {location sharing, privacy, recommendation, user behavior},
 link = {http://doi.acm.org/10.1145/2557500.2557504},
 location = {Haifa, Israel},
 numpages = {10},
 pages = {189--198},
 publisher = {ACM},
 series = {IUI '14},
 title = {Location Sharing Privacy Preference: Analysis and Personalized Recommendation},
 year = {2014}
}


@inproceedings{Warnestal:2014:TUE:2557500.2557506,
 abstract = {We present an initial set of design principles for designing efficient, effective, coherent, and desirable adaptive spoken interaction for traffic information and navigation. The principles are based on a qualitative analysis of driver interactions with an adaptive speech prototype along with driver interviews. The derived set of principles range from high-level fundamental design values, conceptual and behavioral principles, to low-level interface-level principles that can guide the design of adaptive spoken dialogue interaction in the car from a user experience perspective.},
 acmid = {2557506},
 address = {New York, NY, USA},
 author = {W\"{a}rnest{\aa}l, Pontus and Kronlid, Fredrik},
 booktitle = {Proceedings of the 19th International Conference on Intelligent User Interfaces},
 doi = {10.1145/2557500.2557506},
 isbn = {978-1-4503-2184-6},
 keyword = {adaptive interfaces, automotive, interaction design, natural language interaction, navigation, spoken interaction, traffic information, user experience},
 link = {http://doi.acm.org/10.1145/2557500.2557506},
 location = {Haifa, Israel},
 numpages = {6},
 pages = {305--310},
 publisher = {ACM},
 series = {IUI '14},
 title = {Towards a User Experience Design Framework for Adaptive Spoken Dialogue in Automotive Contexts},
 year = {2014}
}


@inproceedings{Kamal:2014:TMG:2557500.2557521,
 abstract = {When using motion gestures, 3D movements of a mobile phone, as an input modality, one significant challenge is how to teach end users the movement parameters necessary to successfully issue a command. Is a simple video or image depicting movement of a smartphone sufficient? Or do we need three-dimensional depictions of movement on external screens to train users? In this paper, we explore mechanisms to teach end users motion gestures, examining two factors. The first factor is how to represent motion gestures: as icons that describe movement, video that depicts movement using the smartphone screen, or a Kinect-based teaching mechanism that captures and depicts the gesture on an external display in three-dimensional space. The second factor we explore is recognizer feedback, i.e. a simple representation of the proximity of a motion gesture to the desired motion gesture based on a distance metric extracted from the recognizer. We show that, by combining video with recognizer feedback, participants master motion gestures equally quickly as end users that learn using a Kinect. These results demonstrate the viability of training end users to perform motion gestures using only the smartphone display.},
 acmid = {2557521},
 address = {New York, NY, USA},
 author = {Kamal, Ankit and Li, Yang and Lank, Edward},
 booktitle = {Proceedings of the 19th International Conference on Intelligent User Interfaces},
 doi = {10.1145/2557500.2557521},
 isbn = {978-1-4503-2184-6},
 keyword = {android, motion gestures, recognizer feedback., sensors, smartphone},
 link = {http://doi.acm.org/10.1145/2557500.2557521},
 location = {Haifa, Israel},
 numpages = {10},
 pages = {73--82},
 publisher = {ACM},
 series = {IUI '14},
 title = {Teaching Motion Gestures via Recognizer Feedback},
 year = {2014}
}


@inproceedings{Wahlster:2014:MIC:2557500.2568055,
 abstract = {Networked cyber-physical systems are the basis for intelli-gent environments in a variety of settings such as smart factories, smart transportation systems, smart shops, and smart buildings. However, one of the remaining grand challenges for the Internet of Things is to transform the way how humans interact with and control such cyber-physical environments (CPE). A major goal of our research is the creation of a multiadaptive dialogue management system that is adaptive in multiple ways: adaptive to various CPE, adaptive to diverse modality combinations, adaptive to a variety of interaction metaphors, and adaptive to diverse task domains and user models.},
 acmid = {2568055},
 address = {New York, NY, USA},
 author = {Wahlster, Wolfgang},
 booktitle = {Proceedings of the 19th International Conference on Intelligent User Interfaces},
 doi = {10.1145/2557500.2568055},
 isbn = {978-1-4503-2184-6},
 keyword = {cyber-physical environment, human-environment communication, multiadaptive dialogue management, multimodal interfaces},
 link = {http://doi.acm.org/10.1145/2557500.2568055},
 location = {Haifa, Israel},
 numpages = {2},
 pages = {1--2},
 publisher = {ACM},
 series = {IUI '14},
 title = {Multiadaptive Interfaces to Cyber-physical Environments},
 year = {2014}
}


@inproceedings{Zhang:2014:LRH:2557500.2557525,
 abstract = {In multi-touch interactive systems, it is of great significance to distinguish which hand of the user is touching the surface in real time. Left-right hand distinction is essential for recognizing the multi-finger gestures and further fully exploring the potential of bimanual interaction. However, left-right hand distinction is beyond the capability of most existing multi-touch systems. In this paper, we present a new method for left and right hand distinction based on the human anatomy, work area, finger orientation and finger position. Considering the ergonomics principles of gesture designing, the body-forearm triangle model was proposed. Furthermore, a heuristic algorithm was introduced to group multi-touch contact points and then made left-right hand distinction. A dataset of 2880 images has been set up to evaluate the proposed left-right hand distinction method. The experimental results demonstrate that our method can guarantee the high recognition accuracy and real time performance in freely bimanual multi-touch interactions.},
 acmid = {2557525},
 address = {New York, NY, USA},
 author = {Zhang, Zhensong and Zhang, Fengjun and Chen, Hui and Liu, Jiasheng and Wang, Hongan and Dai, Guozhong},
 booktitle = {Proceedings of the 19th International Conference on Intelligent User Interfaces},
 doi = {10.1145/2557500.2557525},
 isbn = {978-1-4503-2184-6},
 keyword = {bimanual interaction, left-right hand distinction, multi-touch interaction},
 link = {http://doi.acm.org/10.1145/2557500.2557525},
 location = {Haifa, Israel},
 numpages = {10},
 pages = {47--56},
 publisher = {ACM},
 series = {IUI '14},
 title = {Left and Right Hand Distinction for Multi-touch Tabletop Interactions},
 year = {2014}
}


@proceedings{Brdiczka:2015:2678025,
 abstract = {It is with great pleasure that we welcome you to the 2015 ACM International Conference on Intelligent User Interfaces -- ACM IUI 2015 in Atlanta, Georgia, USA. Starting in 1993, ACM IUI is now in its 20th edition and serves as a premier international forum for reporting outstanding research and development on intelligent user interfaces. ACM IUI is where the Human-Computer Interaction (HCI) community meets the Artificial Intelligence (AI) community. We are also very interested in contributions from related fields, such as psychology, behavioral science, cognitive science, computer graphics, design, the arts, etc. At ACM IUI, we focus on the interaction between machine intelligence and human intelligence. While other conferences focus on one side or the other, we address the complex interaction between the two. We welcome research that explores how to make the interaction between computers and people smarter, which may leverage solutions from data mining, knowledge representation, novel interaction paradigms, and emerging technologies. For the 20th edition of the conference, the call for papers attracted a record number of 205 long and short paper submissions, 41 poster paper submissions, 18 demo paper submissions, and 24 student consortium submissions. The final program of the conference includes 3 keynotes, 47 long and short papers (22.9% acceptance rate), 17 posters, 8 demos, 4 workshops, 3 tutorials, and 12 student consortium papers. The program opens with a keynote by Professor Dan Weld on "Intelligent Control of Crowdsourcing", continues with a keynote by Ed Chi on "Blurring of the Boundary Between Interactive Search and Recommendation" on the second day, and ends with a keynote by Rosalind Picard on "Recognizing Stress, Engagement, and Positive Emotion". Four workshops (Personalized Access to Cultural Heritage, Intelligent Digital Games for Empowerment and Inclusion, Interacting with Smart Objects, and Visual Text Analytics) and three tutorials (Speech-based Interaction, Personalization for Behavior Change, and User Affect and Sentiment Modeling) complement the main conference program.},
 address = {New York, NY, USA},
 isbn = {978-1-4503-3306-1},
 location = {Atlanta, Georgia, USA},
 publisher = {ACM},
 title = {IUI '15: Proceedings of the 20th International Conference on Intelligent User Interfaces},
 year = {2015}
}


@inproceedings{Seri:2014:VES:2557500.2557520,
 abstract = {Exploratory Learning Environments (ELE) are open-ended and flexible software, supporting interaction styles that include exogenous actions and trial-and-error. This paper shows that using AI techniques to visualize worked examples in ELEs improves students' generalization of mathematical concepts across problems, as measured by their performance. Students were exposed to a worked example of a problem solution using an ELE for statistics education. One group in the study was presented with a hierarchical plan of relevant activities that emphasized the sub-goals and the structure relating to the solution. This visualization used an AI algorithm to match a log of activities in the ELEs to ideal solutions. We measured students' performance when using the ELE to solve new problems that required generalization of concepts introduced in the example solution. The results showed that students who were shown the plan visualization significantly outperformed other students who were presented with a step-by-step list of actions in the software used to generate the same solution to the example problem. Analysis of students' explanations of the problem solution shows that the students in the former condition also demonstrated deeper understanding of the solution process. These results demonstrate the benefit to students when using AI technology to visualize worked examples in ELEs and suggests future applications of this approach to actively support students' learning and teachers' understanding of students' activities.},
 acmid = {2557520},
 address = {New York, NY, USA},
 author = {Seri, Or and Gal, Kobi},
 booktitle = {Proceedings of the 19th International Conference on Intelligent User Interfaces},
 doi = {10.1145/2557500.2557520},
 isbn = {978-1-4503-2184-6},
 keyword = {exploratory learning environments, plan recognition, visualizations of students' interactions, worked examples.},
 link = {http://doi.acm.org/10.1145/2557500.2557520},
 location = {Haifa, Israel},
 numpages = {8},
 pages = {125--132},
 publisher = {ACM},
 series = {IUI '14},
 title = {Visualizing Expert Solutions in Exploratory Learning Environments Using Plan Recognition},
 year = {2014}
}


@inproceedings{Li:2014:ACA:2557500.2557511,
 abstract = {Computer users with impaired dexterity often have difficulty accessing small, densely packed user interface elements. Past research in software-based solutions has mainly employed two approaches: modifying the interface and modifying the interaction with the cursor. Each approach, however, has limitations. Modifying the user interface by enlarging interactive elements makes access efficient for simple interfaces but increases the cost of navigation for complex ones by displacing items to screens that require tabs or scrolling to reach. Modifying the interaction with the cursor makes access possible to unmodified interfaces but may perform poorly on densely packed targets or require the user to perform multiple steps. We developed a new approach that combines the strengths of the existing approaches while minimizing their shortcomings, introducing only minimal distortion to the original interface while making access to frequently used parts of the user interface efficient and access to all other parts possible. We instantiated this concept as Adaptive Click-and-Cross, a novel interaction technique. Our user study demonstrates that, for sufficiently complex interfaces, Adaptive Click-and-Cross slightly improves the performance of users with impaired dexterity compared to only modifying the interface or only modifying the cursor.},
 acmid = {2557511},
 address = {New York, NY, USA},
 author = {Li, Louis and Gajos, Krzysztof Z.},
 booktitle = {Proceedings of the 19th International Conference on Intelligent User Interfaces},
 doi = {10.1145/2557500.2557511},
 isbn = {978-1-4503-2184-6},
 keyword = {accessibility, adaptive user interface, area cursors},
 link = {http://doi.acm.org/10.1145/2557500.2557511},
 location = {Haifa, Israel},
 numpages = {6},
 pages = {299--304},
 publisher = {ACM},
 series = {IUI '14},
 title = {Adaptive Click-and-cross: Adapting to Both Abilities and Task Improves Performance of Users with Impaired Dexterity},
 year = {2014}
}


@inproceedings{Rateau:2014:MIS:2557500.2557545,
 abstract = {Pervasive computing is a vision that has been an inspiring long-term target for many years now. Interaction techniques that allow one user to efficiently control many screens, or that allow several users to collaborate on one distant screen, are still hot topics, and are often considered as two different questions. Standard approaches require a strong coupling between the physical location of input device, and users. We propose to consider these two questions through the same basic concept, that uncouples physical location and user input, using a mid-air approach. We present the concept of mimetic interaction spaces (MIS), a dynamic user-definition of an imaginary input space thanks to an iconic gesture, that can be used to define mid-air interaction techniques. We describe a participative design user-study, that shows this technique has interesting acceptability and elicit some definition and deletion gestures. We finally describe a design space for MIS-based interaction, and show how such concept may be used for multi-screen control, as well as screen sharing in pervasive environments.},
 acmid = {2557545},
 address = {New York, NY, USA},
 author = {Rateau, Hanae and Grisoni, Laurent and De Araujo, Bruno},
 booktitle = {Proceedings of the 19th International Conference on Intelligent User Interfaces},
 doi = {10.1145/2557500.2557545},
 isbn = {978-1-4503-2184-6},
 keyword = {contactless interaction, gestural interaction, mid-air gestures},
 link = {http://doi.acm.org/10.1145/2557500.2557545},
 location = {Haifa, Israel},
 numpages = {6},
 pages = {89--94},
 publisher = {ACM},
 series = {IUI '14},
 title = {Mimetic Interaction Spaces: Controlling Distant Displays in Pervasive Environments},
 year = {2014}
}


@inproceedings{Park:2014:TCM:2557500.2557512,
 abstract = {Research that involves human behavior analysis usually requires laborious and costly efforts for obtaining micro-level behavior annotations on a large video corpus. With the emerging paradigm of crowdsourcing however, these efforts can be considerably reduced. We first present OCTAB (Online Crowdsourcing Tool for Annotations of Behaviors), a web-based annotation tool that allows precise and convenient behavior annotations in videos, directly portable to popular crowdsourcing platforms. As part of OCTAB, we introduce a training module with specialized visualizations. The training module's design was inspired by an observational study of local experienced coders, and it enables an iterative procedure for effectively training crowd workers online. Finally, we present an extensive set of experiments that evaluates the feasibility of our crowdsourcing approach for obtaining micro-level behavior annotations in videos, showing the reliability improvement in annotation accuracy when properly training online crowd workers. We also show the generalization of our training approach to a new independent video corpus.},
 acmid = {2557512},
 address = {New York, NY, USA},
 author = {Park, Sunghyun and Shoemark, Philippa and Morency, Louis-Philippe},
 booktitle = {Proceedings of the 19th International Conference on Intelligent User Interfaces},
 doi = {10.1145/2557500.2557512},
 isbn = {978-1-4503-2184-6},
 keyword = {behavior annotations, crowdsourcing, inter-rater reliability, micro-level annotations, training crowd workers},
 link = {http://doi.acm.org/10.1145/2557500.2557512},
 location = {Haifa, Israel},
 numpages = {10},
 pages = {37--46},
 publisher = {ACM},
 series = {IUI '14},
 title = {Toward Crowdsourcing Micro-level Behavior Annotations: The Challenges of Interface, Training, and Generalization},
 year = {2014}
}


@inproceedings{Buschek:2014:IAB:2557500.2557501,
 abstract = {Recent work has shown that a multitouch sensor attached to the back of a handheld device can allow rapid typing engaging all ten fingers. However, high error rates remain a problem, because the user can not see or feel key-targets on the back. We propose a machine learning approach that can significantly improve accuracy. The method considers hand anatomy and movement ranges of fingers. The key insight is a combination of keyboard and hand models in a hierarchical clustering method. This enables dynamic re-estimation of key-locations while typing to account for changes in hand postures and movement ranges of fingers. We also show that accuracy can be further improved with language models. Results from a user study show improvements of over 40% compared to the previously deployed "naive" approach. We examine entropy as a touch precision metric with respect to typing experience. We also find that the QWERTY layout is not ideal. Finally, we conclude with ideas for further improvements.},
 acmid = {2557501},
 address = {New York, NY, USA},
 author = {Buschek, Daniel and Schoenleben, Oliver and Oulasvirta, Antti},
 booktitle = {Proceedings of the 19th International Conference on Intelligent User Interfaces},
 doi = {10.1145/2557500.2557501},
 isbn = {978-1-4503-2184-6},
 keyword = {back-of-device, classification, clustering, machine learning, touch, typing},
 link = {http://doi.acm.org/10.1145/2557500.2557501},
 location = {Haifa, Israel},
 numpages = {10},
 pages = {57--66},
 publisher = {ACM},
 series = {IUI '14},
 title = {Improving Accuracy in Back-of-device Multitouch Typing: A Clustering-based Approach to Keyboard Updating},
 year = {2014}
}


@inproceedings{Mock:2014:UIU:2557500.2557503,
 abstract = {Personalized soft-keyboards which adapt to a user's individual typing behavior can reduce typing errors on interactive displays. In multi-user scenarios a personalized model has to be loaded for each participant. In this paper we describe a user identification technique that is based on raw sensor data from an optical touch screen. For classification of users we use a multi-class support vector machine that is trained with grayscale images from the optical sensor. Our implementation can identify a specific user from a set of 12 users with an average accuracy of 97.51% after one keystroke. It can be used to automatically select individual typing models during free-text entry. The resulting authentication process is completely implicit. We furthermore describe how the approach can be extended to automatic loading of personal information and settings.},
 acmid = {2557503},
 address = {New York, NY, USA},
 author = {Mock, Philipp and Edelmann, J\"{o}rg and Schilling, Andreas and Rosenstiel, Wolfgang},
 booktitle = {Proceedings of the 19th International Conference on Intelligent User Interfaces},
 doi = {10.1145/2557500.2557503},
 isbn = {978-1-4503-2184-6},
 keyword = {interactive displays, machine learning, user identification},
 link = {http://doi.acm.org/10.1145/2557500.2557503},
 location = {Haifa, Israel},
 numpages = {6},
 pages = {67--72},
 publisher = {ACM},
 series = {IUI '14},
 title = {User Identification Using Raw Sensor Data from Typing on Interactive Displays},
 year = {2014}
}


@inproceedings{Artstein:2014:TIH:2557500.2557540,
 abstract = {Time-offset interaction is a new technology that allows for two-way communication with a person who is not available for conversation in real time: a large set of statements are prepared in advance, and users access these statements through natural conversation that mimics face-to-face interaction. Conversational reactions to user questions are retrieved through a statistical classifier, using technology that is similar to previous interactive systems with synthetic characters; however, all of the retrieved utterances are genuine statements by a real person. Recordings of answers, listening and idle behaviors, and blending techniques are used to create a persistent visual image of the person throughout the interaction. A proof-of-concept has been implemented using the likeness of Pinchas Gutter, a Holocaust survivor, enabling short conversations about his family, his religious views, and resistance. This proof-of-concept has been shown to dozens of people, from school children to Holocaust scholars, with many commenting on the impact of the experience and potential for this kind of interface.},
 acmid = {2557540},
 address = {New York, NY, USA},
 author = {Artstein, Ron and Traum, David and Alexander, Oleg and Leuski, Anton and Jones, Andrew and Georgila, Kallirroi and Debevec, Paul and Swartout, William and Maio, Heather and Smith, Stephen},
 booktitle = {Proceedings of the 19th International Conference on Intelligent User Interfaces},
 doi = {10.1145/2557500.2557540},
 isbn = {978-1-4503-2184-6},
 keyword = {agents and intelligent systems, computer-mediated communication, dialogue systems, e-learning and education, holocaust testimony preservation, multi-modal interfaces},
 link = {http://doi.acm.org/10.1145/2557500.2557540},
 location = {Haifa, Israel},
 numpages = {6},
 pages = {163--168},
 publisher = {ACM},
 series = {IUI '14},
 title = {Time-offset Interaction with a Holocaust Survivor},
 year = {2014}
}


@inproceedings{Dostal:2014:SDA:2557500.2557541,
 abstract = {With the proliferation of large multi-faceted datasets, a critical question is how to design collaborative environments, in which this data can be analysed in an efficient and insightful manner. Exploiting people's movements and distance to the data display and to collaborators, proxemic interactions can potentially support such scenarios in a fluid and seamless way, supporting both tightly coupled collaboration as well as parallel explorations. In this paper we introduce the concept of collaborative proxemics: enabling groups of people to collaboratively use attention- and proximity-aware applications. To help designers create such applications we have developed SpiderEyes: a system and toolkit for designing attention- and proximity-aware collaborative interfaces for wall-sized displays. SpiderEyes is based on low-cost technology and allows accurate markerless attention-aware tracking of multiple people interacting in front of a display in real-time. We discuss how this toolkit can be applied to design attention- and proximity-aware collaborative scenarios around large wall-sized displays, and how the information visualisation pipeline can be extended to incorporate proxemic interactions.},
 acmid = {2557541},
 address = {New York, NY, USA},
 author = {Dostal, Jakub and Hinrichs, Uta and Kristensson, Per Ola and Quigley, Aaron},
 booktitle = {Proceedings of the 19th International Conference on Intelligent User Interfaces},
 doi = {10.1145/2557500.2557541},
 isbn = {978-1-4503-2184-6},
 keyword = {attention-aware user interfaces, collaborative proxemics},
 link = {http://doi.acm.org/10.1145/2557500.2557541},
 location = {Haifa, Israel},
 numpages = {10},
 pages = {143--152},
 publisher = {ACM},
 series = {IUI '14},
 title = {SpiderEyes: Designing Attention- and Proximity-aware Collaborative Interfaces for Wall-sized Displays},
 year = {2014}
}


@inproceedings{Rosman:2014:UBA:2557500.2557535,
 abstract = {Different interfaces allow a user to achieve the same end goal through different action sequences, e.g., command lines vs. drop down menus. Interface efficiency can be described in terms of a cost incurred, e.g., time taken, by the user in typical tasks. Realistic users arrive at evaluations of efficiency, hence making choices about which interface to use, over time, based on trial and error experience. Their choices are also determined by prior experience, which determines how much learning time is required. These factors have substantial effect on the adoption of new interfaces. In this paper, we aim at understanding how users adapt under interface change, how much time it takes them to learn to interact optimally with an interface, and how this learning could be expedited through intermediate interfaces. We present results from a series of experiments that make four main points: (a) different interfaces for accomplishing the same task can elicit significant variability in performance, (b) switching interfaces can result in adverse sharp shifts in performance, (c) subject to some variability, there are individual thresholds on tolerance to this kind of performance degradation with an interface, causing users to potentially abandon what may be a pretty good interface, and (d) our main result -- shaping user learning through the presentation of intermediate interfaces can mitigate the adverse shifts in performance while still enabling the eventual improved performance with the complex interface upon the user becoming suitably accustomed. In our experiments, human users use keyboard based interfaces to navigate a simulated ball through a maze. Our results are a first step towards interface adaptation algorithms that architect choice to accommodate personality traits of realistic users.},
 acmid = {2557535},
 address = {New York, NY, USA},
 author = {Rosman, Benjamin and Ramamoorthy, Subramanian and Mahmud, M.M. Hassan and Kohli, Pushmeet},
 booktitle = {Proceedings of the 19th International Conference on Intelligent User Interfaces},
 doi = {10.1145/2557500.2557535},
 isbn = {978-1-4503-2184-6},
 keyword = {input and interaction technologies, usability research, usability testing and evaluation, user interface design},
 link = {http://doi.acm.org/10.1145/2557500.2557535},
 location = {Haifa, Israel},
 numpages = {6},
 pages = {273--278},
 publisher = {ACM},
 series = {IUI '14},
 title = {On User Behaviour Adaptation Under Interface Change},
 year = {2014}
}


@inproceedings{Tiroshi:2014:IBR:2557500.2557526,
 abstract = {Many types of recommender systems rely on a rich ensemble of user, item, and context features when generating recommendations for users. The features can be either manually engineered or automatically extracted from the available data, such that feature engineering becomes an important step in the recommendation process. In this work, we propose to leverage graph based representation of the data in order to generate and automatically populate features. We represent the standard user-item rating matrix and some domain metadata, as graph vertices and edges. Then, we apply a suite of graph theory and network analysis metrics to the graph based data representation, to populate features that augment the original user-item ratings data. The augmented data is fed into a classifier that predicts unknown user ratings, which are used for the generation of recommendations. We evaluate the proposed methodology using the recently released Yelp business ratings dataset. Our results indicate that the automatically populated graph features allow for more accurate and robust predictions, with respect to both the variability and sparsity of ratings.},
 acmid = {2557526},
 address = {New York, NY, USA},
 author = {Tiroshi, Amit and Berkovsky, Shlomo and Kaafar, Mohamed Ali and Vallet, David and Chen, Terence and Kuflik, Tsvi},
 booktitle = {Proceedings of the 19th International Conference on Intelligent User Interfaces},
 doi = {10.1145/2557500.2557526},
 isbn = {978-1-4503-2184-6},
 keyword = {feature extraction, graph-based recommendations, recommender systems},
 link = {http://doi.acm.org/10.1145/2557500.2557526},
 location = {Haifa, Israel},
 numpages = {10},
 pages = {17--26},
 publisher = {ACM},
 series = {IUI '14},
 title = {Improving Business Rating Predictions Using Graph Based Features},
 year = {2014}
}


@inproceedings{Perer:2014:FIM:2557500.2557508,
 abstract = {Extracting insights from temporal event sequences is an important challenge. In particular, mining frequent patterns from event sequences is a desired capability for many domains. However, most techniques for mining frequent patterns are ineffective for real-world data that may be low-resolution, concurrent, or feature many types of events, or the algorithms may produce results too complex to interpret. To address these challenges, we propose Frequence, an intelligent user interface that integrates data mining and visualization in an interactive hierarchical information exploration system for finding frequent patterns from longitudinal event sequences. Frequence features a novel frequent sequence mining algorithm to handle multiple levels-of-detail, temporal context, concurrency, and outcome analysis. Frequence also features a visual interface designed to support insights, and support exploration of patterns of the level-of-detail relevant to users. Frequence's effectiveness is demonstrated with two use cases: medical research mining event sequences from clinical records to understand the progression of a disease, and social network research using frequent sequences from Foursquare to understand the mobility of people in an urban environment.},
 acmid = {2557508},
 address = {New York, NY, USA},
 author = {Perer, Adam and Wang, Fei},
 booktitle = {Proceedings of the 19th International Conference on Intelligent User Interfaces},
 doi = {10.1145/2557500.2557508},
 isbn = {978-1-4503-2184-6},
 keyword = {frequent sequence mining, temporal visualization, visual analytics},
 link = {http://doi.acm.org/10.1145/2557500.2557508},
 location = {Haifa, Israel},
 numpages = {10},
 pages = {153--162},
 publisher = {ACM},
 series = {IUI '14},
 title = {Frequence: Interactive Mining and Visualization of Temporal Frequent Event Sequences},
 year = {2014}
}


@inproceedings{dilorenzo:2014:AVE:2557500.2557532,
 abstract = {The deep penetration of mobile phones offers cities the ability to opportunistically monitor citizensfi mobility and use data-driven insights to better plan and manage services. In this context, transit operators can leverage pervasive mobile sensing to better match observed demand for travel with their service offerings. In this paper we present AllAboard, an intelligent tool that analyses cellphone data to helps city authorities in exploring urban mobility and optimizing public transport. An interactive user interface allows transit operators to explore the travel demand in both space and time, evaluate the quality of service that a transit network provides to the citizens, and test scenarios for transit network improvements. The system has been tested using real telecommunication data for the city of Abidjan, Ivory Coast, and evaluated from a data mining, optimization and user prospective.},
 acmid = {2557532},
 address = {New York, NY, USA},
 author = {di lorenzo, Giusy and Sbodio, Marco Luca and Calabrese, Francesco and Berlingerio, Michele and Nair, Rahul and Pinelli, Fabio},
 booktitle = {Proceedings of the 19th International Conference on Intelligent User Interfaces},
 doi = {10.1145/2557500.2557532},
 isbn = {978-1-4503-2184-6},
 keyword = {cellphone data, mobility, spatio-temporal mining, urban data, visual exploration},
 link = {http://doi.acm.org/10.1145/2557500.2557532},
 location = {Haifa, Israel},
 numpages = {6},
 pages = {335--340},
 publisher = {ACM},
 series = {IUI '14},
 title = {AllAboard: Visual Exploration of Cellphone Mobility Data to Optimise Public Transport},
 year = {2014}
}


@inproceedings{Toyama:2014:MRH:2557500.2557528,
 abstract = {Efficient text recognition has recently been a challenge for augmented reality systems. In this paper, we propose a system with the ability to provide translations to the user in real-time. We use eye gaze for more intuitive and efficient input for ubiquitous text reading and translation in head mounted displays (HMDs). The eyes can be used to indicate regions of interest in text documents and activate optical-character-recognition (OCR) and translation functions. Visual feedback and navigation help in the interaction process, and text snippets with translations from Japanese to English text snippets, are presented in a see-through HMD. We focus on travelers who go to Japan and need to read signs and propose two different gaze gestures for activating the OCR text reading and translation function. We evaluate which type of gesture suits our OCR scenario best. We also show that our gaze-based OCR method on the extracted gaze regions provide faster access times to information than traditional OCR approaches. Other benefits include that visual feedback of the extracted text region can be given in real-time, the Japanese to English translation can be presented in real-time, and the augmentation of the synchronized and calibrated HMD in this mixed reality application are presented at exact locations in the augmented user view to allow for dynamic text translation management in head-up display systems.},
 acmid = {2557528},
 address = {New York, NY, USA},
 author = {Toyama, Takumi and Sonntag, Daniel and Dengel, Andreas and Matsuda, Takahiro and Iwamura, Masakazu and Kise, Koichi},
 booktitle = {Proceedings of the 19th International Conference on Intelligent User Interfaces},
 doi = {10.1145/2557500.2557528},
 isbn = {978-1-4503-2184-6},
 keyword = {augmented reality and projection, mobile and embedded devices, smart environments, ubiquitous computing, visualization},
 link = {http://doi.acm.org/10.1145/2557500.2557528},
 location = {Haifa, Israel},
 numpages = {6},
 pages = {329--334},
 publisher = {ACM},
 series = {IUI '14},
 title = {A Mixed Reality Head-mounted Text Translation System Using Eye Gaze Input},
 year = {2014}
}


@inproceedings{Cheema:2014:PFC:2557500.2557522,
 abstract = {We describe a novel theoretical framework for modeling structured drawings which contain one or more patterns of repetition in their constituent elements. We then present PatternSketch, a sketch-based drawing tool built using our framework to allow quick construction of structured drawings. PatternSketch can recognize and beautify drawings containing line segments, polylines, arcs, and circles. Users can employ a series of gestures to identify repetitive elements and create new elements based on automatically inferred patterns. PatternSketch leverages the programming-by-example (PBE) paradigm, enabling it to infer non-trivial patterns from a few examples. We show that PatternSketch, with its sketch-based user interface and a unique pattern inference algorithm, enables efficient and natural construction of structured drawings.},
 acmid = {2557522},
 address = {New York, NY, USA},
 author = {Cheema, Salman and Buchanan, Sarah and Gulwani, Sumit and LaViola,Jr., Joseph J.},
 booktitle = {Proceedings of the 19th International Conference on Intelligent User Interfaces},
 doi = {10.1145/2557500.2557522},
 isbn = {978-1-4503-2184-6},
 keyword = {pattern inference, programming by example, sketch-based interfaces, structured drawing},
 link = {http://doi.acm.org/10.1145/2557500.2557522},
 location = {Haifa, Israel},
 numpages = {6},
 pages = {311--316},
 publisher = {ACM},
 series = {IUI '14},
 title = {A Practical Framework for Constructing Structured Drawings},
 year = {2014}
}


@inproceedings{Walber:2014:TAI:2557500.2557517,
 abstract = {Labeled image regions provide very valuable information that can be used in different settings such as image search. The manual creation of region labels is a tedious task. Fully automatic approaches lack understanding the image content sufficiently due to the huge variety of depicted objects. Our approach benefits from the expected spread of eye tracking hardware and uses gaze information obtained from users performing image search tasks to automatically label image regions. This allows to exploit the human capabilities regarding the visual perception of image content while performing daily routine tasks. In an experiment with 23 participants, we show that it is possible to assign search terms to photo regions by means of gaze analysis with an average precision of 0.56 and an average F-measure of 0.38 over 361 photos. The participants performed different search tasks while their gaze was recorded. The results of the experiment show that the gaze-based approach performs significantly better than a baseline approach based on saliency maps.},
 acmid = {2557517},
 address = {New York, NY, USA},
 author = {Walber, Tina and Neuhaus, Chantal and Scherp, Ansgar},
 booktitle = {Proceedings of the 19th International Conference on Intelligent User Interfaces},
 doi = {10.1145/2557500.2557517},
 isbn = {978-1-4503-2184-6},
 keyword = {eye tracking, image search, implicit user feedback, region labeling},
 link = {http://doi.acm.org/10.1145/2557500.2557517},
 location = {Haifa, Israel},
 numpages = {10},
 pages = {257--266},
 publisher = {ACM},
 series = {IUI '14},
 title = {Tagging-by-search: Automatic Image Region Labeling Using Gaze Information Obtained from Image Search},
 year = {2014}
}


@inproceedings{Lecue:2014:SST:2557500.2557537,
 abstract = {This paper presents STAR-CITY, a system supporting semantic traffic analytics and reasoning for city. STAR-CITY, which integrates (human and machine-based) sensor data using variety of formats, velocities and volumes, has been designed to provide insight on historical and real-time traffic conditions, all supporting efficient urban planning. Our system demonstrates how the severity of road traffic congestion can be smoothly analyzed, diagnosed, explored and predicted using semantic web technologies. We present how semantic diagnosis and predictive reasoning, both using and interpreting semantics of data to deliver useful, accurate and consistent inferences, have been exploited and adapted systematized in an intelligent user interface. Our prototype of semantics-aware traffic analytics and reasoning, experimented in Dublin City Ireland, works and scales efficiently with historical together with real live and heterogeneous stream data.},
 acmid = {2557537},
 address = {New York, NY, USA},
 author = {L{\'e}cu{\'e}, Freddy and Tallevi-Diotallevi, Simone and Hayes, Jer and Tucker, Robert and Bicer, Veli and Sbodio, Marco Luca and Tommasi, Pierpaolo},
 booktitle = {Proceedings of the 19th International Conference on Intelligent User Interfaces},
 doi = {10.1145/2557500.2557537},
 isbn = {978-1-4503-2184-6},
 keyword = {automated system, intelligent user interfaces, semantic reasoning, semantic web, transportation},
 link = {http://doi.acm.org/10.1145/2557500.2557537},
 location = {Haifa, Israel},
 numpages = {10},
 pages = {179--188},
 publisher = {ACM},
 series = {IUI '14},
 title = {STAR-CITY: Semantic Traffic Analytics and Reasoning for CITY},
 year = {2014}
}


@proceedings{Kuflik:2014:2557500,
 abstract = {It is our great pleasure to welcome you to the 2014 International Conference on Intelligent User Interfaces (IUI'14). It is the nineteenth IUI conference, continuing its tradition of being the principal international forum for reporting outstanding research at the intersection of Human-Computer Interaction (HCI) and Artificial Intelligence (AI). The work that appears at IUI bridges these two fields and also delves into related fields, such as psychology, cognitive science, computer graphics, the arts, and many others. Members of the IUI community are interested in improving the symbiosis between humans and computers, and in making systems adapt to humans rather then the other way round. The call for papers attracted 191 submissions from Asia, America Europe, Africa, and Australia. The program committee accepted 46 papers, covering a diverse set of topics, reflected in the session titles "From Touch through Air to Brain" "Learning and Skills", "Intelligent Visual Interaction", "Users and Motion", "Leveraging Social Competencies", "Adaptive User Interfaces" and a special session with papers that honor the memory of John Riedl, who left us too early. A great attraction of the conference is provided by the scientific keynotes: Professor Wolfgang Wahlster opens the conference program with a keynote on "Multiadaptive Interfaces to Cyber-Physical Environments", Professor Noam Tractinsky's second day keynote is on "Visual Aesthetics of Interactive Technologies" and the last day keynote, by Professor Mark Billinghurst is on "Using AR to Create Empathic Experiences". In addition we are pleased to offer an invited talk by a relevant industry speaker, Yanki Margalit: "Startup nation and the Makers revolution. Intelligent user interfaces and the future of the Israeli hi-tech". We also have 11 posters and an excellent demonstration program consisting of 27 demos. In addition, the conference provides four very interesting workshops and a student consortium.},
 address = {New York, NY, USA},
 isbn = {978-1-4503-2184-6},
 location = {Haifa, Israel},
 note = {608140},
 publisher = {ACM},
 title = {IUI '14: Proceedings of the 19th International Conference on Intelligent User Interfaces},
 year = {2014}
}


@inproceedings{Alt:2014:UES:2557500.2557518,
 abstract = {In this paper, we investigate the concept of gaze-based interaction with 3D user interfaces. We currently see stereo vision displays becoming ubiquitous, particularly as auto-stereoscopy enables the perception of 3D content without the use of glasses. As a result, application areas for 3D beyond entertainment in cinema or at home emerge, including work settings, mobile phones, public displays, and cars. At the same time, eye tracking is hitting the consumer market with low-cost devices. We envision eye trackers in the future to be integrated with consumer devices (laptops, mobile phones, displays), hence allowing the user's gaze to be analyzed and used as input for interactive applications. A particular challenge when applying this concept to 3D displays is that current eye trackers provide the gaze point in 2D only (x and y coordinates). In this paper, we compare the performance of two methods that use the eye's physiology for calculating the gaze point in 3D space, hence enabling gaze-based interaction with stereoscopic content. Furthermore, we provide a comparison of gaze interaction in 2D and 3D with regard to user experience and performance. Our results show that with current technology, eye tracking on stereoscopic displays is possible with similar performance as on standard 2D screens.},
 acmid = {2557518},
 address = {New York, NY, USA},
 author = {Alt, Florian and Schneegass, Stefan and Auda, Jonas and Rzayev, Rufat and Broy, Nora},
 booktitle = {Proceedings of the 19th International Conference on Intelligent User Interfaces},
 doi = {10.1145/2557500.2557518},
 isbn = {978-1-4503-2184-6},
 keyword = {3d, eye tracking, gaze interaction, stereoscopic displays},
 link = {http://doi.acm.org/10.1145/2557500.2557518},
 location = {Haifa, Israel},
 numpages = {6},
 pages = {267--272},
 publisher = {ACM},
 series = {IUI '14},
 title = {Using Eye-tracking to Support Interaction with Layered 3D Interfaces on Stereoscopic Displays},
 year = {2014}
}


@inproceedings{Joyner:2014:MGA:2557500.2557516,
 abstract = {Scientists use both conceptual models and executable simulations to help them make sense of the world. Models and simulations each have unique affordances and limitations, and it is useful to leverage their affordances to mitigate their respective limitations. One way to do this is by generating the simulations based on the conceptual models, preserving the capacity for rapid revision and knowledge sharing allowed by the conceptual models while extending them to provide the repeated testing and feedback of the simulations. In this paper, we present an interactive system called MILAfiS for generating agent-based simulations from conceptual models of ecological systems. Designed with STEM education in mind, this user-centered interface design allows the user to construct a Component-Mechanism-Phenomenon conceptual model of a complex system, and then compile the conceptual model into an executable NetLogo simulation. In this paper, we present the results of a pilot study with this interface with about 50 middle school students in the context of learning about ecosystems.},
 acmid = {2557516},
 address = {New York, NY, USA},
 author = {Joyner, David A. and Goel, Ashok K. and Papin, Nicolas M.},
 booktitle = {Proceedings of the 19th International Conference on Intelligent User Interfaces},
 doi = {10.1145/2557500.2557516},
 isbn = {978-1-4503-2184-6},
 keyword = {agent-based simulations, complex systems, conceptual models, ecological systems, k6-12 education, scientific inquiry, stem education},
 link = {http://doi.acm.org/10.1145/2557500.2557516},
 location = {Haifa, Israel},
 numpages = {10},
 pages = {289--298},
 publisher = {ACM},
 series = {IUI '14},
 title = {MILA--S: Generation of Agent-based Simulations from Conceptual Models of Complex Systems},
 year = {2014}
}


